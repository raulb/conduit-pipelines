 Network infra_default  Creating
 Network infra_default  Created
 Container benchi-kafka  Creating
 Container benchi-postgres  Creating
 Container benchi-kafka  Created
 Container benchi-postgres  Created
Attaching to benchi-kafka, benchi-postgres
benchi-kafka     | ===> User
benchi-kafka     | uid=1000(appuser) gid=1000(appuser) groups=1000(appuser)
benchi-kafka     | ===> Configuring ...
benchi-kafka     | Running in KRaft mode...
benchi-postgres  | The files belonging to this database system will be owned by user "postgres".
benchi-postgres  | This user must also own the server process.
benchi-postgres  | 
benchi-postgres  | The database cluster will be initialized with locale "en_US.utf8".
benchi-postgres  | The default database encoding has accordingly been set to "UTF8".
benchi-postgres  | The default text search configuration will be set to "english".
benchi-postgres  | 
benchi-postgres  | Data page checksums are disabled.
benchi-postgres  | 
benchi-postgres  | fixing permissions on existing directory /var/lib/postgresql/data ... ok
benchi-postgres  | creating subdirectories ... ok
benchi-postgres  | selecting dynamic shared memory implementation ... posix
benchi-postgres  | selecting default max_connections ... 100
benchi-postgres  | selecting default shared_buffers ... 128MB
benchi-postgres  | selecting default time zone ... Etc/UTC
benchi-postgres  | creating configuration files ... ok
benchi-postgres  | running bootstrap script ... ok
benchi-postgres  | performing post-bootstrap initialization ... ok
benchi-postgres  | syncing data to disk ... ok
benchi-postgres  | 
benchi-postgres  | 
benchi-postgres  | Success. You can now start the database server using:
benchi-postgres  | 
benchi-postgres  |     pg_ctl -D /var/lib/postgresql/data -l logfile start
benchi-postgres  | 
benchi-postgres  | initdb: warning: enabling "trust" authentication for local connections
benchi-postgres  | initdb: hint: You can change this by editing pg_hba.conf or using the option -A, or --auth-local and --auth-host, the next time you run initdb.
benchi-postgres  | waiting for server to start....2025-03-18 16:05:43.041 UTC [48] LOG:  starting PostgreSQL 16.4 (Debian 16.4-1.pgdg110+2) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 10.2.1-6) 10.2.1 20210110, 64-bit
benchi-postgres  | 2025-03-18 16:05:43.041 UTC [48] LOG:  listening on Unix socket "/var/run/postgresql/.s.PGSQL.5432"
benchi-postgres  | 2025-03-18 16:05:43.044 UTC [51] LOG:  database system was shut down at 2025-03-18 16:05:42 UTC
benchi-postgres  | 2025-03-18 16:05:43.045 UTC [48] LOG:  database system is ready to accept connections
benchi-postgres  |  done
benchi-postgres  | server started
benchi-postgres  | CREATE DATABASE
benchi-postgres  | 
benchi-postgres  | 
benchi-postgres  | /usr/local/bin/docker-entrypoint.sh: sourcing /docker-entrypoint-initdb.d/init-permissions.sh
benchi-postgres  | 
benchi-postgres  | /usr/local/bin/docker-entrypoint.sh: running /docker-entrypoint-initdb.d/init.sql
benchi-postgres  | DROP TABLE
benchi-postgres  | DROP SEQUENCE
benchi-postgres  | psql:/docker-entrypoint-initdb.d/init.sql:1: NOTICE:  table "employees" does not exist, skipping
benchi-postgres  | psql:/docker-entrypoint-initdb.d/init.sql:2: NOTICE:  sequence "employees_id_seq" does not exist, skipping
benchi-postgres  | CREATE TABLE
benchi-postgres  | CREATE SEQUENCE
benchi-postgres  | ALTER TABLE
benchi-postgres  | ALTER TABLE
benchi-postgres  | 
benchi-postgres  | 
benchi-postgres  | waiting for server to shut down...2025-03-18 16:05:43.193 UTC [48] LOG:  received fast shutdown request
benchi-postgres  | .2025-03-18 16:05:43.194 UTC [48] LOG:  aborting any active transactions
benchi-postgres  | 2025-03-18 16:05:43.194 UTC [48] LOG:  background worker "logical replication launcher" (PID 54) exited with exit code 1
benchi-postgres  | 2025-03-18 16:05:43.195 UTC [49] LOG:  shutting down
benchi-postgres  | 2025-03-18 16:05:43.195 UTC [49] LOG:  checkpoint starting: shutdown immediate
benchi-postgres  | 2025-03-18 16:05:43.221 UTC [49] LOG:  checkpoint complete: wrote 928 buffers (5.7%); 0 WAL file(s) added, 0 removed, 0 recycled; write=0.006 s, sync=0.019 s, total=0.026 s; sync files=304, longest=0.006 s, average=0.001 s; distance=4267 kB, estimate=4267 kB; lsn=0/19D91E8, redo lsn=0/19D91E8
benchi-postgres  | 2025-03-18 16:05:43.222 UTC [48] LOG:  database system is shut down
benchi-postgres  |  done
benchi-postgres  | server stopped
benchi-postgres  | 
benchi-postgres  | PostgreSQL init process complete; ready for start up.
benchi-postgres  | 
benchi-postgres  | 2025-03-18 16:05:43.303 UTC [1] LOG:  starting PostgreSQL 16.4 (Debian 16.4-1.pgdg110+2) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 10.2.1-6) 10.2.1 20210110, 64-bit
benchi-postgres  | 2025-03-18 16:05:43.303 UTC [1] LOG:  listening on IPv4 address "0.0.0.0", port 5432
benchi-postgres  | 2025-03-18 16:05:43.303 UTC [1] LOG:  listening on IPv6 address "::", port 5432
benchi-postgres  | 2025-03-18 16:05:43.304 UTC [1] LOG:  listening on Unix socket "/var/run/postgresql/.s.PGSQL.5432"
benchi-postgres  | 2025-03-18 16:05:43.306 UTC [66] LOG:  database system was shut down at 2025-03-18 16:05:43 UTC
benchi-postgres  | 2025-03-18 16:05:43.307 UTC [1] LOG:  database system is ready to accept connections
benchi-kafka     | ===> Running preflight checks ... 
benchi-kafka     | ===> Check if /var/lib/kafka/data is writable ...
benchi-kafka     | ===> Running in KRaft mode, skipping Zookeeper health check...
benchi-kafka     | ===> Using provided cluster id MkU3OEVBNTcwNTJENDM2Qk ...
benchi-kafka     | ===> Launching ... 
benchi-kafka     | ===> Launching kafka ... 
benchi-kafka     | [2025-03-18 16:05:45,148] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
benchi-kafka     | [2025-03-18 16:05:45,423] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
benchi-kafka     | [2025-03-18 16:05:45,424] INFO RemoteLogManagerConfig values: 
benchi-kafka     | 	log.local.retention.bytes = -2
benchi-kafka     | 	log.local.retention.ms = -2
benchi-kafka     | 	remote.fetch.max.wait.ms = 500
benchi-kafka     | 	remote.log.index.file.cache.total.size.bytes = 1073741824
benchi-kafka     | 	remote.log.manager.copier.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.copy.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.copy.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.expiration.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.fetch.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.fetch.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.task.interval.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.max.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.ms = 500
benchi-kafka     | 	remote.log.manager.task.retry.jitter = 0.2
benchi-kafka     | 	remote.log.manager.thread.pool.size = 10
benchi-kafka     | 	remote.log.metadata.custom.metadata.max.bytes = 128
benchi-kafka     | 	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
benchi-kafka     | 	remote.log.metadata.manager.class.path = null
benchi-kafka     | 	remote.log.metadata.manager.impl.prefix = rlmm.config.
benchi-kafka     | 	remote.log.metadata.manager.listener.name = null
benchi-kafka     | 	remote.log.reader.max.pending.tasks = 100
benchi-kafka     | 	remote.log.reader.threads = 10
benchi-kafka     | 	remote.log.storage.manager.class.name = null
benchi-kafka     | 	remote.log.storage.manager.class.path = null
benchi-kafka     | 	remote.log.storage.manager.impl.prefix = rsm.config.
benchi-kafka     | 	remote.log.storage.system.enable = false
benchi-kafka     |  (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
benchi-kafka     | [2025-03-18 16:05:45,500] INFO RemoteLogManagerConfig values: 
benchi-kafka     | 	log.local.retention.bytes = -2
benchi-kafka     | 	log.local.retention.ms = -2
benchi-kafka     | 	remote.fetch.max.wait.ms = 500
benchi-kafka     | 	remote.log.index.file.cache.total.size.bytes = 1073741824
benchi-kafka     | 	remote.log.manager.copier.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.copy.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.copy.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.expiration.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.fetch.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.fetch.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.task.interval.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.max.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.ms = 500
benchi-kafka     | 	remote.log.manager.task.retry.jitter = 0.2
benchi-kafka     | 	remote.log.manager.thread.pool.size = 10
benchi-kafka     | 	remote.log.metadata.custom.metadata.max.bytes = 128
benchi-kafka     | 	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
benchi-kafka     | 	remote.log.metadata.manager.class.path = null
benchi-kafka     | 	remote.log.metadata.manager.impl.prefix = rlmm.config.
benchi-kafka     | 	remote.log.metadata.manager.listener.name = null
benchi-kafka     | 	remote.log.reader.max.pending.tasks = 100
benchi-kafka     | 	remote.log.reader.threads = 10
benchi-kafka     | 	remote.log.storage.manager.class.name = null
benchi-kafka     | 	remote.log.storage.manager.class.path = null
benchi-kafka     | 	remote.log.storage.manager.impl.prefix = rsm.config.
benchi-kafka     | 	remote.log.storage.system.enable = false
benchi-kafka     |  (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
benchi-kafka     | [2025-03-18 16:05:45,504] INFO RemoteLogManagerConfig values: 
benchi-kafka     | 	log.local.retention.bytes = -2
benchi-kafka     | 	log.local.retention.ms = -2
benchi-kafka     | 	remote.fetch.max.wait.ms = 500
benchi-kafka     | 	remote.log.index.file.cache.total.size.bytes = 1073741824
benchi-kafka     | 	remote.log.manager.copier.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.copy.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.copy.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.expiration.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.fetch.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.fetch.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.task.interval.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.max.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.ms = 500
benchi-kafka     | 	remote.log.manager.task.retry.jitter = 0.2
benchi-kafka     | 	remote.log.manager.thread.pool.size = 10
benchi-kafka     | 	remote.log.metadata.custom.metadata.max.bytes = 128
benchi-kafka     | 	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
benchi-kafka     | 	remote.log.metadata.manager.class.path = null
benchi-kafka     | 	remote.log.metadata.manager.impl.prefix = rlmm.config.
benchi-kafka     | 	remote.log.metadata.manager.listener.name = null
benchi-kafka     | 	remote.log.reader.max.pending.tasks = 100
benchi-kafka     | 	remote.log.reader.threads = 10
benchi-kafka     | 	remote.log.storage.manager.class.name = null
benchi-kafka     | 	remote.log.storage.manager.class.path = null
benchi-kafka     | 	remote.log.storage.manager.impl.prefix = rsm.config.
benchi-kafka     | 	remote.log.storage.system.enable = false
benchi-kafka     |  (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
benchi-kafka     | [2025-03-18 16:05:45,518] INFO RemoteLogManagerConfig values: 
benchi-kafka     | 	log.local.retention.bytes = -2
benchi-kafka     | 	log.local.retention.ms = -2
benchi-kafka     | 	remote.fetch.max.wait.ms = 500
benchi-kafka     | 	remote.log.index.file.cache.total.size.bytes = 1073741824
benchi-kafka     | 	remote.log.manager.copier.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.copy.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.copy.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.expiration.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.fetch.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.fetch.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.task.interval.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.max.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.ms = 500
benchi-kafka     | 	remote.log.manager.task.retry.jitter = 0.2
benchi-kafka     | 	remote.log.manager.thread.pool.size = 10
benchi-kafka     | 	remote.log.metadata.custom.metadata.max.bytes = 128
benchi-kafka     | 	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
benchi-kafka     | 	remote.log.metadata.manager.class.path = null
benchi-kafka     | 	remote.log.metadata.manager.impl.prefix = rlmm.config.
benchi-kafka     | 	remote.log.metadata.manager.listener.name = null
benchi-kafka     | 	remote.log.reader.max.pending.tasks = 100
benchi-kafka     | 	remote.log.reader.threads = 10
benchi-kafka     | 	remote.log.storage.manager.class.name = null
benchi-kafka     | 	remote.log.storage.manager.class.path = null
benchi-kafka     | 	remote.log.storage.manager.impl.prefix = rsm.config.
benchi-kafka     | 	remote.log.storage.system.enable = false
benchi-kafka     |  (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
benchi-kafka     | [2025-03-18 16:05:45,524] INFO KafkaConfig values: 
benchi-kafka     | 	advertised.listeners = PLAINTEXT://broker:29092,PLAINTEXT_HOST://localhost:9092
benchi-kafka     | 	alter.config.policy.class.name = null
benchi-kafka     | 	alter.log.dirs.replication.quota.window.num = 11
benchi-kafka     | 	alter.log.dirs.replication.quota.window.size.seconds = 1
benchi-kafka     | 	authorizer.class.name = 
benchi-kafka     | 	auto.create.topics.enable = true
benchi-kafka     | 	auto.include.jmx.reporter = true
benchi-kafka     | 	auto.leader.rebalance.enable = true
benchi-kafka     | 	background.threads = 10
benchi-kafka     | 	broker.heartbeat.interval.ms = 2000
benchi-kafka     | 	broker.id = 1
benchi-kafka     | 	broker.id.generation.enable = true
benchi-kafka     | 	broker.interceptor.class = class org.apache.kafka.server.interceptor.DefaultBrokerInterceptor
benchi-kafka     | 	broker.rack = null
benchi-kafka     | 	broker.session.timeout.ms = 9000
benchi-kafka     | 	broker.session.uuid = JzUn51KjQwC3OragyPrV9Q
benchi-kafka     | 	client.quota.callback.class = null
benchi-kafka     | 	client.quota.max.throttle.time.ms = 5000
benchi-kafka     | 	compression.gzip.level = -1
benchi-kafka     | 	compression.lz4.level = 9
benchi-kafka     | 	compression.type = producer
benchi-kafka     | 	compression.zstd.level = 3
benchi-kafka     | 	confluent.alter.broker.health.max.demoted.brokers = 2147483647
benchi-kafka     | 	confluent.alter.broker.health.max.demoted.brokers.percentage = 0
benchi-kafka     | 	confluent.ansible.managed = false
benchi-kafka     | 	confluent.api.visibility = DEFAULT
benchi-kafka     | 	confluent.append.record.interceptor.classes = []
benchi-kafka     | 	confluent.apply.create.topic.policy.to.create.partitions = false
benchi-kafka     | 	confluent.authorizer.authority.name = 
benchi-kafka     | 	confluent.automatic.alter.broker.health.retry.backoff.ms = 2000
benchi-kafka     | 	confluent.backpressure.disk.enable = false
benchi-kafka     | 	confluent.backpressure.disk.free.threshold.bytes = 21474836480
benchi-kafka     | 	confluent.backpressure.disk.produce.bytes.per.second = 131072
benchi-kafka     | 	confluent.backpressure.disk.threshold.recovery.factor = 1.5
benchi-kafka     | 	confluent.backpressure.request.min.broker.limit = 200
benchi-kafka     | 	confluent.backpressure.request.queue.size.percentile = p95
benchi-kafka     | 	confluent.backpressure.types = null
benchi-kafka     | 	confluent.balancer.api.state.topic = _confluent_balancer_api_state
benchi-kafka     | 	confluent.balancer.broker.addition.elapsed.time.ms.completion.threshold = 57600000
benchi-kafka     | 	confluent.balancer.broker.addition.mean.cpu.percent.completion.threshold = 0.5
benchi-kafka     | 	confluent.balancer.capacity.threshold.upper.limit = 0.95
benchi-kafka     | 	confluent.balancer.cell.load.upper.bound = 0.7
benchi-kafka     | 	confluent.balancer.cell.overload.detection.interval.ms = 3600000
benchi-kafka     | 	confluent.balancer.cell.overload.duration.ms = 86400000
benchi-kafka     | 	confluent.balancer.class = io.confluent.databalancer.SbcDataBalanceManager
benchi-kafka     | 	confluent.balancer.consumer.out.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.balancer.cpu.balance.threshold = 1.1
benchi-kafka     | 	confluent.balancer.cpu.low.utilization.threshold = 0.2
benchi-kafka     | 	confluent.balancer.cpu.utilization.detector.duration.ms = 600000
benchi-kafka     | 	confluent.balancer.cpu.utilization.detector.enabled = false
benchi-kafka     | 	confluent.balancer.cpu.utilization.detector.overutilization.threshold = 80.0
benchi-kafka     | 	confluent.balancer.cpu.utilization.detector.underutilization.threshold = 50.0
benchi-kafka     | 	confluent.balancer.disk.max.load = 0.85
benchi-kafka     | 	confluent.balancer.disk.min.free.space.gb = 0
benchi-kafka     | 	confluent.balancer.disk.min.free.space.lower.limit.gb = 0
benchi-kafka     | 	confluent.balancer.disk.utilization.detector.duration.ms = 600000
benchi-kafka     | 	confluent.balancer.disk.utilization.detector.enabled = false
benchi-kafka     | 	confluent.balancer.disk.utilization.detector.overutilization.threshold = 80.0
benchi-kafka     | 	confluent.balancer.disk.utilization.detector.reserved.capacity = 150000.0
benchi-kafka     | 	confluent.balancer.disk.utilization.detector.underutilization.threshold = 35.0
benchi-kafka     | 	confluent.balancer.enable = true
benchi-kafka     | 	confluent.balancer.exclude.topic.names = []
benchi-kafka     | 	confluent.balancer.exclude.topic.prefixes = []
benchi-kafka     | 	confluent.balancer.goal.violation.delay.on.new.brokers.ms = 1800000
benchi-kafka     | 	confluent.balancer.goal.violation.distribution.threshold.multiplier = 1.1
benchi-kafka     | 	confluent.balancer.heal.broker.failure.threshold.ms = 3600000
benchi-kafka     | 	confluent.balancer.heal.uneven.load.trigger = EMPTY_BROKER
benchi-kafka     | 	confluent.balancer.incremental.balancing.cpu.top.proposal.tracking.enabled = true
benchi-kafka     | 	confluent.balancer.incremental.balancing.cpu.top.proposal.tracking.num.proposals = 15
benchi-kafka     | 	confluent.balancer.incremental.balancing.enabled = false
benchi-kafka     | 	confluent.balancer.incremental.balancing.goals = []
benchi-kafka     | 	confluent.balancer.incremental.balancing.lower.bound = 0.02
benchi-kafka     | 	confluent.balancer.incremental.balancing.min.valid.windows = 5
benchi-kafka     | 	confluent.balancer.incremental.balancing.step.ratio = 0.2
benchi-kafka     | 	confluent.balancer.inter.cell.balancing.enabled = false
benchi-kafka     | 	confluent.balancer.max.capacity.balancing.delta.percentage = 0.0
benchi-kafka     | 	confluent.balancer.max.replicas = 2147483647
benchi-kafka     | 	confluent.balancer.network.in.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.balancer.network.out.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.balancer.plan.computation.retry.timeout.ms = 3600000
benchi-kafka     | 	confluent.balancer.producer.in.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.balancer.rebalancing.goals = []
benchi-kafka     | 	confluent.balancer.replication.in.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.balancer.resource.utilization.detector.interval.ms = 60000
benchi-kafka     | 	confluent.balancer.sbc.metrics.parser.enabled = false
benchi-kafka     | 	confluent.balancer.self.healing.maximum.rounds = 1
benchi-kafka     | 	confluent.balancer.task.history.retention.days = 30
benchi-kafka     | 	confluent.balancer.tenant.maximum.movements = 0
benchi-kafka     | 	confluent.balancer.tenant.suspension.ms = 86400000
benchi-kafka     | 	confluent.balancer.throttle.bytes.per.second = 10485760
benchi-kafka     | 	confluent.balancer.topic.partition.maximum.movements = 5
benchi-kafka     | 	confluent.balancer.topic.partition.movement.expiration.ms = 10800000
benchi-kafka     | 	confluent.balancer.topic.partition.suspension.ms = 18000000
benchi-kafka     | 	confluent.balancer.topic.replication.factor = 1
benchi-kafka     | 	confluent.balancer.triggering.goals = []
benchi-kafka     | 	confluent.balancer.v2.addition.enabled = false
benchi-kafka     | 	confluent.basic.auth.credentials.source = null
benchi-kafka     | 	confluent.basic.auth.user.info = null
benchi-kafka     | 	confluent.bearer.auth.cache.expiry.buffer.seconds = 300
benchi-kafka     | 	confluent.bearer.auth.client.id = null
benchi-kafka     | 	confluent.bearer.auth.client.secret = null
benchi-kafka     | 	confluent.bearer.auth.credentials.source = null
benchi-kafka     | 	confluent.bearer.auth.identity.pool.id = null
benchi-kafka     | 	confluent.bearer.auth.issuer.endpoint.url = null
benchi-kafka     | 	confluent.bearer.auth.logical.cluster = null
benchi-kafka     | 	confluent.bearer.auth.scope = null
benchi-kafka     | 	confluent.bearer.auth.scope.claim.name = scope
benchi-kafka     | 	confluent.bearer.auth.sub.claim.name = sub
benchi-kafka     | 	confluent.bearer.auth.token = null
benchi-kafka     | 	confluent.broker.health.manager.enabled = true
benchi-kafka     | 	confluent.broker.health.manager.engine.request.handler.threads.stuck.criteria = AllThreadsStuck
benchi-kafka     | 	confluent.broker.health.manager.hard.kill.duration.ms = 60000
benchi-kafka     | 	confluent.broker.health.manager.mitigation.enabled = false
benchi-kafka     | 	confluent.broker.health.manager.num.samples.before.broker.suspect = 30
benchi-kafka     | 	confluent.broker.health.manager.num.samples.before.broker.unhealthy = 180
benchi-kafka     | 	confluent.broker.health.manager.percentage.unhealthy.samples.before.broker.suspect = 90
benchi-kafka     | 	confluent.broker.health.manager.percentage.unhealthy.samples.before.broker.unhealthy = 70
benchi-kafka     | 	confluent.broker.health.manager.sample.duration.ms = 1000
benchi-kafka     | 	confluent.broker.health.manager.storage.background.threads.stuck.criteria = AnyThreadStuck
benchi-kafka     | 	confluent.broker.health.manager.storage.network.threads.stuck.criteria = AnyThreadStuck
benchi-kafka     | 	confluent.broker.health.manager.storage.request.handler.threads.stuck.criteria = AnyThreadStuck
benchi-kafka     | 	confluent.broker.limit.consumer.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.broker.limit.producer.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.broker.load.advertised.limit.load = 0.8
benchi-kafka     | 	confluent.broker.load.average.service.request.time.ms = 0.1
benchi-kafka     | 	confluent.broker.load.delay.metric.start.ms = 180000
benchi-kafka     | 	confluent.broker.load.enabled = false
benchi-kafka     | 	confluent.broker.load.num.samples = 60
benchi-kafka     | 	confluent.broker.load.tenant.metric.enable = false
benchi-kafka     | 	confluent.broker.load.update.metric.tags.interval.ms = 60000
benchi-kafka     | 	confluent.broker.load.window.size.ms = 60000
benchi-kafka     | 	confluent.broker.load.workload.coefficient = 20.0
benchi-kafka     | 	confluent.broker.registration.delay.ms = 0
benchi-kafka     | 	confluent.catalog.collector.destination.topic = telemetry.events.data_catalog_source
benchi-kafka     | 	confluent.catalog.collector.enable = false
benchi-kafka     | 	confluent.catalog.collector.full.configs.enable = false
benchi-kafka     | 	confluent.catalog.collector.max.bytes.per.snapshot = 850000
benchi-kafka     | 	confluent.catalog.collector.max.topics.process = 500
benchi-kafka     | 	confluent.catalog.collector.max.zookeeper.request.per.sec = 100
benchi-kafka     | 	confluent.catalog.collector.snapshot.init.delay.sec = 60
benchi-kafka     | 	confluent.catalog.collector.snapshot.interval.sec = 300
benchi-kafka     | 	confluent.ccloud.host.suffixes = .confluent.cloud,.cpdev.cloud,.confluentgov.com,.confluentgov-internal.com
benchi-kafka     | 	confluent.ccloud.intranet.host.suffixes = .intranet.stag.cpdev.cloud,.intranet.stag.cpdev-untrusted.cloud,.intranet.devel.cpdev.cloud,.intranet.devel.cpdev-untrusted.cloud,.intranet.confluent.cloud,.intranet.confluent-untrusted.cloud
benchi-kafka     | 	confluent.cdc.api.keys.topic = 
benchi-kafka     | 	confluent.cdc.api.keys.topic.load.timeout.ms = 600000
benchi-kafka     | 	confluent.cdc.client.quotas.enable = false
benchi-kafka     | 	confluent.cdc.client.quotas.topic.name = 
benchi-kafka     | 	confluent.cdc.lkc.metadata.topic = 
benchi-kafka     | 	confluent.cdc.user.metadata.enable = false
benchi-kafka     | 	confluent.cdc.user.metadata.topic = _confluent-user_metadata
benchi-kafka     | 	confluent.cell.metrics.refresh.period.ms = 60000
benchi-kafka     | 	confluent.cells.default.size = 15
benchi-kafka     | 	confluent.cells.enable = false
benchi-kafka     | 	confluent.cells.implicit.creation.enable = false
benchi-kafka     | 	confluent.cells.load.refresher.enable = true
benchi-kafka     | 	confluent.cells.max.size = 15
benchi-kafka     | 	confluent.cells.min.size = 6
benchi-kafka     | 	confluent.checksum.enabled.files = [none]
benchi-kafka     | 	confluent.clm.enabled = false
benchi-kafka     | 	confluent.clm.frequency.in.hours = 6
benchi-kafka     | 	confluent.clm.list.object.thread_pool.size = 1
benchi-kafka     | 	confluent.clm.max.backup.days = 3
benchi-kafka     | 	confluent.clm.min.delay.in.minutes = 30
benchi-kafka     | 	confluent.clm.thread.pool.size = 2
benchi-kafka     | 	confluent.clm.topic.retention.days.to.backup.days = 0:0,3:3
benchi-kafka     | 	confluent.close.connections.on.credential.delete = false
benchi-kafka     | 	confluent.cluster.link.admin.max.in.flight.requests = 1000
benchi-kafka     | 	confluent.cluster.link.admin.request.batch.size = 1
benchi-kafka     | 	confluent.cluster.link.allow.config.providers = true
benchi-kafka     | 	confluent.cluster.link.allow.legacy.message.format = false
benchi-kafka     | 	confluent.cluster.link.allow.truncation.below.hwm = true
benchi-kafka     | 	confluent.cluster.link.availability.check.mode = ALL
benchi-kafka     | 	confluent.cluster.link.background.thread.affinity = LINK
benchi-kafka     | 	confluent.cluster.link.clients.max.idle.ms = 3153600000000
benchi-kafka     | 	confluent.cluster.link.enable = true
benchi-kafka     | 	confluent.cluster.link.enable.local.admin = false
benchi-kafka     | 	confluent.cluster.link.enable.metrics.reduction = false
benchi-kafka     | 	confluent.cluster.link.fetch.response.min.bytes = 1
benchi-kafka     | 	confluent.cluster.link.fetch.response.total.bytes = 2147483647
benchi-kafka     | 	confluent.cluster.link.fetcher.auto.tune.enable = false
benchi-kafka     | 	confluent.cluster.link.fetcher.thread.pool.mode = ENDPOINT
benchi-kafka     | 	confluent.cluster.link.insync.fetch.response.min.bytes = 1
benchi-kafka     | 	confluent.cluster.link.insync.fetch.response.total.bytes = 2147483647
benchi-kafka     | 	confluent.cluster.link.intranet.connectivity.enable = false
benchi-kafka     | 	confluent.cluster.link.io.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.cluster.link.local.reverse.connection.listener.map = null
benchi-kafka     | 	confluent.cluster.link.max.client.connections = 2147483647
benchi-kafka     | 	confluent.cluster.link.metadata.topic.create.retry.delay.ms = 1000
benchi-kafka     | 	confluent.cluster.link.metadata.topic.enable = false
benchi-kafka     | 	confluent.cluster.link.metadata.topic.min.isr = 2
benchi-kafka     | 	confluent.cluster.link.metadata.topic.partitions = 50
benchi-kafka     | 	confluent.cluster.link.metadata.topic.replication.factor = 1
benchi-kafka     | 	confluent.cluster.link.mirror.transition.batch.size = 10
benchi-kafka     | 	confluent.cluster.link.num.background.threads = 1
benchi-kafka     | 	confluent.cluster.link.periodic.task.batch.size = 2147483647
benchi-kafka     | 	confluent.cluster.link.periodic.task.min.interval.ms = 1000
benchi-kafka     | 	confluent.cluster.link.replica.fetch.connections.mode = combined
benchi-kafka     | 	confluent.cluster.link.replication.quota.mode = CLUSTER_LINK_ONLY
benchi-kafka     | 	confluent.cluster.link.replication.quota.mode.per.tenant.overrides = 
benchi-kafka     | 	confluent.cluster.link.replication.quota.window.num = 11
benchi-kafka     | 	confluent.cluster.link.replication.quota.window.size.seconds = 2
benchi-kafka     | 	confluent.cluster.link.request.quota.capacity = 400
benchi-kafka     | 	confluent.cluster.link.request.quota.request.percentage.multiplier = 1.0
benchi-kafka     | 	confluent.cluster.link.tenant.replication.quota.enable = false
benchi-kafka     | 	confluent.cluster.link.tenant.request.quota.enable = false
benchi-kafka     | 	confluent.cluster.metadata.snapshot.tier.delete.enable = false
benchi-kafka     | 	confluent.cluster.metadata.snapshot.tier.delete.maintain.min.snapshots = 3
benchi-kafka     | 	confluent.cluster.metadata.snapshot.tier.delete.retention.ms = 604800000
benchi-kafka     | 	confluent.cluster.metadata.snapshot.tier.upload.enable = false
benchi-kafka     | 	confluent.compacted.topic.prefer.tier.fetch.ms = -1
benchi-kafka     | 	confluent.connection.invalid.request.delay.enable = false
benchi-kafka     | 	confluent.connections.idle.expiry.manager.ignore.idleness.requests = []
benchi-kafka     | 	confluent.consumer.fetch.partition.pruning.enable = true
benchi-kafka     | 	confluent.consumer.lag.emitter.enabled = false
benchi-kafka     | 	confluent.consumer.lag.emitter.interval.ms = 60000
benchi-kafka     | 	confluent.dataflow.policy.watch.monitor.ms = 300000
benchi-kafka     | 	confluent.defer.isr.shrink.enable = false
benchi-kafka     | 	confluent.describe.topic.partitions.enabled = false
benchi-kafka     | 	confluent.disk.io.manager.enable = false
benchi-kafka     | 	confluent.disk.throughput.headroom = 10485760
benchi-kafka     | 	confluent.disk.throughput.limit = 10485760000
benchi-kafka     | 	confluent.disk.throughput.quota.tier.archive = 1048576000
benchi-kafka     | 	confluent.disk.throughput.quota.tier.archive.throttled = 104857600
benchi-kafka     | 	confluent.durability.audit.batch.flush.frequency.ms = 900000
benchi-kafka     | 	confluent.durability.audit.checks = PeriodicalAudit,ChecksumAudit
benchi-kafka     | 	confluent.durability.audit.enable = false
benchi-kafka     | 	confluent.durability.audit.idempotent.producer = false
benchi-kafka     | 	confluent.durability.audit.initial.job.delay.ms = 900000
benchi-kafka     | 	confluent.durability.audit.io.bytes.per.sec = 10485760
benchi-kafka     | 	confluent.durability.audit.log.ignored.event.types = 
benchi-kafka     | 	confluent.durability.audit.reporting.batch.ms = 1800000
benchi-kafka     | 	confluent.durability.audit.tier.compaction.audit.duration.ms = 14400000
benchi-kafka     | 	confluent.durability.events.allowed = OffsetChangeType,EpochChangeType,IsrExpandType,DeleteRecordsType,RetentionChangeType,StartOffsetChangeType,DeletePartitionType,HealthCheckType
benchi-kafka     | 	confluent.durability.topic.partition.count = 50
benchi-kafka     | 	confluent.durability.topic.replication.factor = 1
benchi-kafka     | 	confluent.e2e_checksum.protection.enabled = false
benchi-kafka     | 	confluent.e2e_checksum.protection.files = [none]
benchi-kafka     | 	confluent.e2e_checksum.protection.store.entry.ttl.ms = 2592000000
benchi-kafka     | 	confluent.elastic.cku.enabled = false
benchi-kafka     | 	confluent.elastic.cku.scaletozero.enabled = false
benchi-kafka     | 	confluent.eligible.controllers = []
benchi-kafka     | 	confluent.enable.stray.partition.deletion = false
benchi-kafka     | 	confluent.encryption.key.manager.rotation.interval.ms = 31536000000
benchi-kafka     | 	confluent.fetch.from.follower.require.leader.epoch.enable = false
benchi-kafka     | 	confluent.fetch.partition.pruning.enable = true
benchi-kafka     | 	confluent.floor.connection.rate.per.ip = -1.0
benchi-kafka     | 	confluent.floor.connection.rate.per.tenant = -1.0
benchi-kafka     | 	confluent.group.coordinator.offsets.batching.enable = false
benchi-kafka     | 	confluent.group.coordinator.offsets.writer.threads = 2
benchi-kafka     | 	confluent.group.metadata.load.threads = 32
benchi-kafka     | 	confluent.heap.tenured.notify.bytes = 0
benchi-kafka     | 	confluent.heap.tenured.notify.enabled = false
benchi-kafka     | 	confluent.hot.partition.ratio = 0.8
benchi-kafka     | 	confluent.http.server.start.timeout.ms = 60000
benchi-kafka     | 	confluent.http.server.stop.timeout.ms = 30000
benchi-kafka     | 	confluent.internal.metrics.enable = false
benchi-kafka     | 	confluent.internal.rest.server.bind.port = null
benchi-kafka     | 	confluent.internal.tenant.scoped.listener.name = INTERNAL_TENANT_SCOPED
benchi-kafka     | 	confluent.leader.epoch.checkpoint.checksum.enabled = false
benchi-kafka     | 	confluent.log.cleaner.timestamp.validation.enable = true
benchi-kafka     | 	confluent.log.placement.constraints = 
benchi-kafka     | 	confluent.max.connection.creation.rate.per.ip = 1.7976931348623157E308
benchi-kafka     | 	confluent.max.connection.creation.rate.per.tenant = 1.7976931348623157E308
benchi-kafka     | 	confluent.max.connection.rate.per.ip = -1.0
benchi-kafka     | 	confluent.max.connection.rate.per.tenant = -1.0
benchi-kafka     | 	confluent.max.connection.throttle.ms = null
benchi-kafka     | 	confluent.max.segment.ms = 9223372036854775807
benchi-kafka     | 	confluent.metadata.active.encryptor = null
benchi-kafka     | 	confluent.metadata.controlled.shutdown.partition.slice.delay.ms = 100
benchi-kafka     | 	confluent.metadata.encryptor.classes = null
benchi-kafka     | 	confluent.metadata.encryptor.required = false
benchi-kafka     | 	confluent.metadata.encryptor.secret.file = null
benchi-kafka     | 	confluent.metadata.encryptor.secrets = null
benchi-kafka     | 	confluent.metadata.jvm.warmup.ms = 60000
benchi-kafka     | 	confluent.metadata.leader.balance.slice.delay.ms = 100
benchi-kafka     | 	confluent.metadata.max.controlled.shutdown.partition.changes.per.slice = 1000
benchi-kafka     | 	confluent.metadata.max.leader.balance.changes.per.slice = 1000
benchi-kafka     | 	confluent.metadata.server.cluster.registry.clusters = []
benchi-kafka     | 	confluent.metadata.throttle.pre.check.for.non.wildcard.requests.enable = false
benchi-kafka     | 	confluent.metadata.throttle.pre.check.for.wildcard.requests.enable = false
benchi-kafka     | 	confluent.metrics.reporter.bootstrap.servers = localhost:9092
benchi-kafka     | 	confluent.min.acks = 0
benchi-kafka     | 	confluent.min.connection.throttle.ms = 0
benchi-kafka     | 	confluent.min.segment.ms = 1
benchi-kafka     | 	confluent.missing.id.cache.ttl.sec = 60
benchi-kafka     | 	confluent.missing.id.query.range = 20000
benchi-kafka     | 	confluent.missing.schema.cache.ttl.sec = 60
benchi-kafka     | 	confluent.mtls.enable = false
benchi-kafka     | 	confluent.mtls.listener.name = EXTERNAL
benchi-kafka     | 	confluent.mtls.sasl.authenticator.request.max.bytes = 104857600
benchi-kafka     | 	confluent.mtls.truststore.manager.class.name = null
benchi-kafka     | 	confluent.multitenant.authorizer.enable.acl.state = false
benchi-kafka     | 	confluent.multitenant.interceptor.balancer.apis.enabled = false
benchi-kafka     | 	confluent.multitenant.interceptor.collect.client.apiversions.max.per.tenant = 1000
benchi-kafka     | 	confluent.multitenant.interceptor.collect.client.apiversions.metric = false
benchi-kafka     | 	confluent.multitenant.listener.hostname.cluster.prefix.enable = false
benchi-kafka     | 	confluent.multitenant.listener.hostname.subdomain.suffix.enable = false
benchi-kafka     | 	confluent.multitenant.listener.names = null
benchi-kafka     | 	confluent.multitenant.parse.lkc.id.enable = false
benchi-kafka     | 	confluent.multitenant.parse.sni.host.name.enable = false
benchi-kafka     | 	confluent.network.health.manager.enabled = false
benchi-kafka     | 	confluent.network.health.manager.external.listener.name = EXTERNAL
benchi-kafka     | 	confluent.network.health.manager.externalconnectivitystartup.enabled = false
benchi-kafka     | 	confluent.network.health.manager.min.healthy.network.samples = 3
benchi-kafka     | 	confluent.network.health.manager.min.percentage.healthy.network.samples = 3
benchi-kafka     | 	confluent.network.health.manager.mitigation.enabled = false
benchi-kafka     | 	confluent.network.health.manager.network.sample.window.size = 120
benchi-kafka     | 	confluent.network.health.manager.sample.duration.ms = 1000
benchi-kafka     | 	confluent.oauth.flat.networking.verification.enable = false
benchi-kafka     | 	confluent.offsets.log.cleaner.delete.retention.ms = 86400000
benchi-kafka     | 	confluent.offsets.log.cleaner.max.compaction.lag.ms = 9223372036854775807
benchi-kafka     | 	confluent.offsets.log.cleaner.min.cleanable.dirty.ratio = 0.5
benchi-kafka     | 	confluent.offsets.topic.placement.constraints = 
benchi-kafka     | 	confluent.omit.network.processor.metric.tag = false
benchi-kafka     | 	confluent.operator.managed = false
benchi-kafka     | 	confluent.password.encoder.old.secret.ttl.ms = 9223372036854775807
benchi-kafka     | 	confluent.plugins.cluster.link.policy.max.destination.links.per.tenant = 10
benchi-kafka     | 	confluent.plugins.cluster.link.policy.max.source.links.per.tenant = 10
benchi-kafka     | 	confluent.plugins.topic.policy.max.partitions.per.cluster = 2147483647
benchi-kafka     | 	confluent.plugins.topic.policy.max.partitions.per.tenant = 512
benchi-kafka     | 	confluent.plugins.topic.policy.max.replicas.per.broker = 2147483647
benchi-kafka     | 	confluent.plugins.topic.policy.max.topics.per.cluster = 2147483647
benchi-kafka     | 	confluent.prefer.tier.fetch.ms = -1
benchi-kafka     | 	confluent.produce.throttle.pre.check.enable = false
benchi-kafka     | 	confluent.producer.id.cache.broker.hard.limit = -1
benchi-kafka     | 	confluent.producer.id.cache.eviction.minimal.expiration.ms = 900000
benchi-kafka     | 	confluent.producer.id.cache.extra.eviction.percentage = 0
benchi-kafka     | 	confluent.producer.id.cache.limit = 2147483647
benchi-kafka     | 	confluent.producer.id.cache.partition.hard.limit = -1
benchi-kafka     | 	confluent.producer.id.cache.tenant.hard.limit = -1
benchi-kafka     | 	confluent.producer.id.quota.manager.enable = false
benchi-kafka     | 	confluent.producer.id.quota.window.num = 11
benchi-kafka     | 	confluent.producer.id.quota.window.size.seconds = 1
benchi-kafka     | 	confluent.producer.id.throttle.enable = false
benchi-kafka     | 	confluent.producer.id.throttle.enable.threshold.percentage = 100
benchi-kafka     | 	confluent.proxy.mode.local.default = false
benchi-kafka     | 	confluent.proxy.protocol.fallback.enabled = false
benchi-kafka     | 	confluent.proxy.protocol.version = NONE
benchi-kafka     | 	confluent.quota.computing.usage.adjustment = 0.5
benchi-kafka     | 	confluent.quota.dynamic.enable = false
benchi-kafka     | 	confluent.quota.dynamic.publishing.interval.ms = 60000
benchi-kafka     | 	confluent.quota.dynamic.reporting.interval.ms = 30000
benchi-kafka     | 	confluent.quota.dynamic.reporting.min.usage = 102400
benchi-kafka     | 	confluent.quota.tenant.broker.max.consumer.rate = 13107200
benchi-kafka     | 	confluent.quota.tenant.broker.max.producer.rate = 13107200
benchi-kafka     | 	confluent.quota.tenant.default.controller.mutation.rate = 2.147483647E9
benchi-kafka     | 	confluent.quota.tenant.default.producer.id.rate = 2.147483647E9
benchi-kafka     | 	confluent.quota.tenant.fetch.multiplier = 1.0
benchi-kafka     | 	confluent.quota.tenant.follower.broker.min.consumer.rate = 10485760
benchi-kafka     | 	confluent.quota.tenant.follower.broker.min.producer.rate = 10485760
benchi-kafka     | 	confluent.quota.tenant.produce.multiplier = 1.0
benchi-kafka     | 	confluent.quota.tenant.user.quotas.enable = false
benchi-kafka     | 	confluent.regional.metadata.client.class = null
benchi-kafka     | 	confluent.regional.resource.manager.client.scheduler.threads = 2
benchi-kafka     | 	confluent.regional.resource.manager.endpoint = null
benchi-kafka     | 	confluent.regional.resource.manager.watch.endpoint = null
benchi-kafka     | 	confluent.replica.fetch.backoff.max.ms = 1000
benchi-kafka     | 	confluent.replica.fetch.connections.mode = combined
benchi-kafka     | 	confluent.replication.mode = PULL
benchi-kafka     | 	confluent.replication.push.feature.enable = false
benchi-kafka     | 	confluent.reporters.telemetry.auto.enable = true
benchi-kafka     | 	confluent.request.log.filter.class = class org.apache.kafka.common.requests.SamplingRequestLogFilter
benchi-kafka     | 	confluent.request.pipelining.enable = true
benchi-kafka     | 	confluent.request.pipelining.max.in.flight.requests.per.connection = 5
benchi-kafka     | 	confluent.require.calling.resource.identity = false
benchi-kafka     | 	confluent.require.compatible.keystore.updates = true
benchi-kafka     | 	confluent.require.confluent.issuer = false
benchi-kafka     | 	confluent.roll.check.interval.ms = 300000
benchi-kafka     | 	confluent.schema.registry.max.cache.size = 10000
benchi-kafka     | 	confluent.schema.registry.max.retries = 1
benchi-kafka     | 	confluent.schema.registry.retries.wait.ms = 0
benchi-kafka     | 	confluent.schema.registry.url = null
benchi-kafka     | 	confluent.schema.validator.interceptor.class = io.confluent.kafka.schemaregistry.validator.RecordSchemaValidator
benchi-kafka     | 	confluent.schema.validator.multitenant.enable = false
benchi-kafka     | 	confluent.schema.validator.samples.per.min = 0
benchi-kafka     | 	confluent.security.bc.approved.mode.enable = false
benchi-kafka     | 	confluent.security.event.logger.authentication.enable = false
benchi-kafka     | 	confluent.security.event.logger.authentication.event.rate.limit = -1
benchi-kafka     | 	confluent.security.event.logger.authorization.event.rate.limit = -1
benchi-kafka     | 	confluent.security.event.logger.detailed.audit.logs.filter.class = class org.apache.kafka.common.requests.DetailedRequestAuditLogFilter
benchi-kafka     | 	confluent.security.event.logger.enable = true
benchi-kafka     | 	confluent.security.event.logger.kafka.request.rate.limit = -1
benchi-kafka     | 	confluent.security.event.logger.physical.cluster.id = 
benchi-kafka     | 	confluent.security.event.router.config = 
benchi-kafka     | 	confluent.security.revoked.certificate.ids = 
benchi-kafka     | 	confluent.segment.eager.roll.enable = false
benchi-kafka     | 	confluent.segment.speculative.prefetch.enable = false
benchi-kafka     | 	confluent.spiffe.id.principal.extraction.rules = 
benchi-kafka     | 	confluent.ssl.key.password = null
benchi-kafka     | 	confluent.ssl.keystore.location = null
benchi-kafka     | 	confluent.ssl.keystore.password = null
benchi-kafka     | 	confluent.ssl.keystore.type = null
benchi-kafka     | 	confluent.ssl.protocol = null
benchi-kafka     | 	confluent.ssl.truststore.location = null
benchi-kafka     | 	confluent.ssl.truststore.password = null
benchi-kafka     | 	confluent.ssl.truststore.type = null
benchi-kafka     | 	confluent.step.connection.rate.per.ip = -1.0
benchi-kafka     | 	confluent.step.connection.rate.per.tenant = -1.0
benchi-kafka     | 	confluent.storage.probe.period.ms = -1
benchi-kafka     | 	confluent.storage.probe.slow.write.threshold.ms = 5000
benchi-kafka     | 	confluent.stray.log.delete.delay.ms = 604800000
benchi-kafka     | 	confluent.stray.log.max.deletions.per.run = 72
benchi-kafka     | 	confluent.subdomain.prefix = null
benchi-kafka     | 	confluent.subdomain.separator.map = null
benchi-kafka     | 	confluent.subdomain.separator.variable = %sep
benchi-kafka     | 	confluent.system.time.roll.enable = false
benchi-kafka     | 	confluent.telemetry.enabled = false
benchi-kafka     | 	confluent.telemetry.external.client.metrics.push.enabled = false
benchi-kafka     | 	confluent.tenant.latency.metric.enabled = false
benchi-kafka     | 	confluent.tier.archiver.num.threads = 2
benchi-kafka     | 	confluent.tier.azure.block.blob.auto.abort.threshold.bytes = 500000
benchi-kafka     | 	confluent.tier.azure.block.blob.container = null
benchi-kafka     | 	confluent.tier.azure.block.blob.cred.file.path = null
benchi-kafka     | 	confluent.tier.azure.block.blob.endpoint = null
benchi-kafka     | 	confluent.tier.azure.block.blob.prefix = 
benchi-kafka     | 	confluent.tier.backend = 
benchi-kafka     | 	confluent.tier.bucket.probe.period.ms = -1
benchi-kafka     | 	confluent.tier.cleaner.compact.min.efficiency = 0.5
benchi-kafka     | 	confluent.tier.cleaner.compact.segment.min.bytes = 20971520
benchi-kafka     | 	confluent.tier.cleaner.dedupe.buffer.size = 134217728
benchi-kafka     | 	confluent.tier.cleaner.dual.compaction = false
benchi-kafka     | 	confluent.tier.cleaner.dual.compaction.validation.max.bytes = 1073741824
benchi-kafka     | 	confluent.tier.cleaner.dual.compaction.validation.percent = 0
benchi-kafka     | 	confluent.tier.cleaner.enable = false
benchi-kafka     | 	confluent.tier.cleaner.excluded.topics = [^_confluent.*]
benchi-kafka     | 	confluent.tier.cleaner.feature.enable = false
benchi-kafka     | 	confluent.tier.cleaner.io.buffer.load.factor = 0.9
benchi-kafka     | 	confluent.tier.cleaner.io.buffer.size = 10485760
benchi-kafka     | 	confluent.tier.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
benchi-kafka     | 	confluent.tier.cleaner.min.cleanable.ratio = 0.75
benchi-kafka     | 	confluent.tier.cleaner.num.threads = 2
benchi-kafka     | 	confluent.tier.enable = false
benchi-kafka     | 	confluent.tier.feature = false
benchi-kafka     | 	confluent.tier.fenced.segment.delete.delay.ms = 600000
benchi-kafka     | 	confluent.tier.fetcher.async.enable = false
benchi-kafka     | 	confluent.tier.fetcher.async.timestamp.offset.parallelism = 1
benchi-kafka     | 	confluent.tier.fetcher.fetch.based.on.segment_and_metadata_layout.field = false
benchi-kafka     | 	confluent.tier.fetcher.memorypool.bytes = 0
benchi-kafka     | 	confluent.tier.fetcher.num.threads = 4
benchi-kafka     | 	confluent.tier.fetcher.offset.cache.expiration.ms = 1800000
benchi-kafka     | 	confluent.tier.fetcher.offset.cache.period.ms = 60000
benchi-kafka     | 	confluent.tier.fetcher.offset.cache.size = 200000
benchi-kafka     | 	confluent.tier.gcs.bucket = null
benchi-kafka     | 	confluent.tier.gcs.cred.file.path = null
benchi-kafka     | 	confluent.tier.gcs.prefix = 
benchi-kafka     | 	confluent.tier.gcs.region = null
benchi-kafka     | 	confluent.tier.gcs.sse.customer.encryption.key = null
benchi-kafka     | 	confluent.tier.gcs.write.chunk.size = 0
benchi-kafka     | 	confluent.tier.local.hotset.bytes = -1
benchi-kafka     | 	confluent.tier.local.hotset.ms = 86400000
benchi-kafka     | 	confluent.tier.max.partition.fetch.bytes.override = 0
benchi-kafka     | 	confluent.tier.metadata.bootstrap.servers = null
benchi-kafka     | 	confluent.tier.metadata.max.poll.ms = 100
benchi-kafka     | 	confluent.tier.metadata.namespace = null
benchi-kafka     | 	confluent.tier.metadata.num.partitions = 50
benchi-kafka     | 	confluent.tier.metadata.replication.factor = 1
benchi-kafka     | 	confluent.tier.metadata.request.timeout.ms = 30000
benchi-kafka     | 	confluent.tier.metadata.snapshots.enable = false
benchi-kafka     | 	confluent.tier.metadata.snapshots.interval.ms = 86400000
benchi-kafka     | 	confluent.tier.metadata.snapshots.retention.days = 7
benchi-kafka     | 	confluent.tier.metadata.snapshots.threads = 2
benchi-kafka     | 	confluent.tier.object.fetcher.num.threads = 1
benchi-kafka     | 	confluent.tier.partition.state.cleanup.delay.ms = 2592000000
benchi-kafka     | 	confluent.tier.partition.state.cleanup.enable = false
benchi-kafka     | 	confluent.tier.partition.state.cleanup.interval.ms = 86400000
benchi-kafka     | 	confluent.tier.partition.state.commit.interval.ms = 15000
benchi-kafka     | 	confluent.tier.s3.assumerole.arn = null
benchi-kafka     | 	confluent.tier.s3.auto.abort.threshold.bytes = 500000
benchi-kafka     | 	confluent.tier.s3.aws.endpoint.override = null
benchi-kafka     | 	confluent.tier.s3.aws.signer.override = null
benchi-kafka     | 	confluent.tier.s3.bucket = null
benchi-kafka     | 	confluent.tier.s3.cred.file.path = null
benchi-kafka     | 	confluent.tier.s3.force.path.style.access = false
benchi-kafka     | 	confluent.tier.s3.prefix = 
benchi-kafka     | 	confluent.tier.s3.region = null
benchi-kafka     | 	confluent.tier.s3.security.providers = null
benchi-kafka     | 	confluent.tier.s3.sse.algorithm = AES256
benchi-kafka     | 	confluent.tier.s3.sse.customer.encryption.key = null
benchi-kafka     | 	confluent.tier.s3.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	confluent.tier.s3.ssl.key.password = null
benchi-kafka     | 	confluent.tier.s3.ssl.keystore.location = null
benchi-kafka     | 	confluent.tier.s3.ssl.keystore.password = null
benchi-kafka     | 	confluent.tier.s3.ssl.keystore.type = null
benchi-kafka     | 	confluent.tier.s3.ssl.protocol = TLSv1.3
benchi-kafka     | 	confluent.tier.s3.ssl.provider = null
benchi-kafka     | 	confluent.tier.s3.ssl.truststore.location = null
benchi-kafka     | 	confluent.tier.s3.ssl.truststore.password = null
benchi-kafka     | 	confluent.tier.s3.ssl.truststore.type = null
benchi-kafka     | 	confluent.tier.s3.user.agent.prefix = APN/1.0 Confluent/1.0 TieredStorageS3/1.0
benchi-kafka     | 	confluent.tier.segment.hotset.roll.min.bytes = 104857600
benchi-kafka     | 	confluent.tier.segment.metadata.layout.put.mode = LegacyMultiObject
benchi-kafka     | 	confluent.tier.topic.data.loss.validation.fencing.enable = false
benchi-kafka     | 	confluent.tier.topic.delete.backoff.ms = 21600000
benchi-kafka     | 	confluent.tier.topic.delete.check.interval.ms = 300000
benchi-kafka     | 	confluent.tier.topic.delete.max.inprogress.partitions = 100
benchi-kafka     | 	confluent.tier.topic.head.data.loss.validation.enable = true
benchi-kafka     | 	confluent.tier.topic.head.data.loss.validation.max.timeout.ms = 900000
benchi-kafka     | 	confluent.tier.topic.materialization.from.snapshot.enable = false
benchi-kafka     | 	confluent.tier.topic.producer.enable.idempotence = true
benchi-kafka     | 	confluent.tier.topic.snapshots.enable = false
benchi-kafka     | 	confluent.tier.topic.snapshots.interval.ms = 300000
benchi-kafka     | 	confluent.tier.topic.snapshots.max.records = 100000
benchi-kafka     | 	confluent.tier.topic.snapshots.retention.hours = 168
benchi-kafka     | 	confluent.topic.partition.default.placement = 2
benchi-kafka     | 	confluent.topic.policy.use.computed.assignments = false
benchi-kafka     | 	confluent.topic.replica.assignor.builder.class = 
benchi-kafka     | 	confluent.track.api.key.per.ip = false
benchi-kafka     | 	confluent.track.per.ip.max.size = 100000
benchi-kafka     | 	confluent.track.tenant.id.per.ip = false
benchi-kafka     | 	confluent.traffic.cdc.network.id.routes.enable = false
benchi-kafka     | 	confluent.traffic.cdc.network.id.routes.listener.names = EXTERNAL_BACKCHANNEL
benchi-kafka     | 	confluent.traffic.cdc.network.id.routes.periodic.start.task.ms = 300000
benchi-kafka     | 	confluent.traffic.cdc.network.id.routes.topic.name = _confluent-network_id_routes
benchi-kafka     | 	confluent.traffic.network.id = 
benchi-kafka     | 	confluent.transaction.2pc.timeout.ms = -1
benchi-kafka     | 	confluent.transaction.logging.verbosity = 0
benchi-kafka     | 	confluent.transaction.state.log.placement.constraints = 
benchi-kafka     | 	confluent.valid.broker.rack.set = null
benchi-kafka     | 	confluent.verify.group.subscription.prefix = false
benchi-kafka     | 	confluent.virtual.topic.creation.enabled = false
benchi-kafka     | 	confluent.zookeeper.metadata.migration.controller.check.disable = false
benchi-kafka     | 	confluent.zookeeper.metadata.migration.trigger.file.path = null
benchi-kafka     | 	connection.failed.authentication.delay.ms = 100
benchi-kafka     | 	connection.min.expire.interval.ms = 250
benchi-kafka     | 	connections.max.age.ms = 3153600000000
benchi-kafka     | 	connections.max.idle.ms = 600000
benchi-kafka     | 	connections.max.reauth.ms = 0
benchi-kafka     | 	control.plane.listener.name = null
benchi-kafka     | 	controlled.shutdown.enable = true
benchi-kafka     | 	controlled.shutdown.max.retries = 3
benchi-kafka     | 	controlled.shutdown.retry.backoff.ms = 5000
benchi-kafka     | 	controller.listener.names = CONTROLLER
benchi-kafka     | 	controller.quorum.append.linger.ms = 25
benchi-kafka     | 	controller.quorum.bootstrap.servers = []
benchi-kafka     | 	controller.quorum.election.backoff.max.ms = 1000
benchi-kafka     | 	controller.quorum.election.timeout.ms = 1000
benchi-kafka     | 	controller.quorum.fetch.timeout.ms = 2000
benchi-kafka     | 	controller.quorum.request.timeout.ms = 2000
benchi-kafka     | 	controller.quorum.retry.backoff.ms = 20
benchi-kafka     | 	controller.quorum.voters = [1@broker:29093]
benchi-kafka     | 	controller.quota.window.num = 11
benchi-kafka     | 	controller.quota.window.size.seconds = 1
benchi-kafka     | 	controller.socket.timeout.ms = 30000
benchi-kafka     | 	create.cluster.link.policy.class.name = null
benchi-kafka     | 	create.topic.policy.class.name = null
benchi-kafka     | 	default.replication.factor = 1
benchi-kafka     | 	delegation.token.expiry.check.interval.ms = 3600000
benchi-kafka     | 	delegation.token.expiry.time.ms = 86400000
benchi-kafka     | 	delegation.token.master.key = null
benchi-kafka     | 	delegation.token.max.lifetime.ms = 604800000
benchi-kafka     | 	delegation.token.secret.key = null
benchi-kafka     | 	delete.records.purgatory.purge.interval.requests = 1
benchi-kafka     | 	delete.topic.enable = true
benchi-kafka     | 	early.start.listeners = null
benchi-kafka     | 	eligible.leader.replicas.enable = false
benchi-kafka     | 	enable.fips = false
benchi-kafka     | 	fetch.max.bytes = 57671680
benchi-kafka     | 	fetch.purgatory.purge.interval.requests = 1000
benchi-kafka     | 	floor.max.connection.creation.rate = null
benchi-kafka     | 	follower.replication.throttled.rate = 9223372036854775807
benchi-kafka     | 	follower.replication.throttled.replicas = none
benchi-kafka     | 	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
benchi-kafka     | 	group.consumer.heartbeat.interval.ms = 5000
benchi-kafka     | 	group.consumer.max.heartbeat.interval.ms = 15000
benchi-kafka     | 	group.consumer.max.session.timeout.ms = 60000
benchi-kafka     | 	group.consumer.max.size = 2147483647
benchi-kafka     | 	group.consumer.migration.policy = disabled
benchi-kafka     | 	group.consumer.min.heartbeat.interval.ms = 5000
benchi-kafka     | 	group.consumer.min.session.timeout.ms = 45000
benchi-kafka     | 	group.consumer.session.timeout.ms = 45000
benchi-kafka     | 	group.coordinator.append.linger.ms = 10
benchi-kafka     | 	group.coordinator.new.enable = false
benchi-kafka     | 	group.coordinator.rebalance.protocols = [classic]
benchi-kafka     | 	group.coordinator.threads = 1
benchi-kafka     | 	group.initial.rebalance.delay.ms = 0
benchi-kafka     | 	group.max.session.timeout.ms = 1800000
benchi-kafka     | 	group.max.size = 2147483647
benchi-kafka     | 	group.min.session.timeout.ms = 6000
benchi-kafka     | 	initial.broker.registration.timeout.ms = 60000
benchi-kafka     | 	inter.broker.listener.name = PLAINTEXT
benchi-kafka     | 	inter.broker.protocol.version = 3.8-IV0A
benchi-kafka     | 	k2.stack.builder.class.name = null
benchi-kafka     | 	k2.startup.timeout.ms = 60000
benchi-kafka     | 	k2.topic.metadata.max.total.partition.count = 20000
benchi-kafka     | 	k2.topic.metadata.refresh.ms = 10000
benchi-kafka     | 	kafka.metrics.polling.interval.secs = 10
benchi-kafka     | 	kafka.metrics.reporters = []
benchi-kafka     | 	leader.imbalance.check.interval.seconds = 300
benchi-kafka     | 	leader.imbalance.per.broker.percentage = 10
benchi-kafka     | 	leader.replication.throttled.rate = 9223372036854775807
benchi-kafka     | 	leader.replication.throttled.replicas = none
benchi-kafka     | 	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
benchi-kafka     | 	listeners = PLAINTEXT://broker:29092,CONTROLLER://broker:29093,PLAINTEXT_HOST://0.0.0.0:9092
benchi-kafka     | 	log.cleaner.backoff.ms = 15000
benchi-kafka     | 	log.cleaner.dedupe.buffer.size = 134217728
benchi-kafka     | 	log.cleaner.delete.retention.ms = 86400000
benchi-kafka     | 	log.cleaner.enable = true
benchi-kafka     | 	log.cleaner.hash.algorithm = MD5
benchi-kafka     | 	log.cleaner.io.buffer.load.factor = 0.9
benchi-kafka     | 	log.cleaner.io.buffer.size = 524288
benchi-kafka     | 	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
benchi-kafka     | 	log.cleaner.max.compaction.lag.ms = 9223372036854775807
benchi-kafka     | 	log.cleaner.min.cleanable.ratio = 0.5
benchi-kafka     | 	log.cleaner.min.compaction.lag.ms = 0
benchi-kafka     | 	log.cleaner.threads = 1
benchi-kafka     | 	log.cleanup.policy = [delete]
benchi-kafka     | 	log.deletion.max.segments.per.run = 2147483647
benchi-kafka     | 	log.deletion.throttler.disk.free.headroom.bytes = 21474836480
benchi-kafka     | 	log.dir = /tmp/kafka-logs
benchi-kafka     | 	log.dir.failure.timeout.ms = 30000
benchi-kafka     | 	log.dirs = /tmp/kraft-combined-logs
benchi-kafka     | 	log.flush.interval.messages = 9223372036854775807
benchi-kafka     | 	log.flush.interval.ms = null
benchi-kafka     | 	log.flush.offset.checkpoint.interval.ms = 60000
benchi-kafka     | 	log.flush.scheduler.interval.ms = 9223372036854775807
benchi-kafka     | 	log.flush.start.offset.checkpoint.interval.ms = 60000
benchi-kafka     | 	log.index.interval.bytes = 4096
benchi-kafka     | 	log.index.size.max.bytes = 10485760
benchi-kafka     | 	log.initial.task.delay.ms = 30000
benchi-kafka     | 	log.local.retention.bytes = -2
benchi-kafka     | 	log.local.retention.ms = -2
benchi-kafka     | 	log.message.downconversion.enable = true
benchi-kafka     | 	log.message.format.version = 3.0-IV1
benchi-kafka     | 	log.message.timestamp.after.max.ms = 9223372036854775807
benchi-kafka     | 	log.message.timestamp.before.max.ms = 9223372036854775807
benchi-kafka     | 	log.message.timestamp.difference.max.ms = 9223372036854775807
benchi-kafka     | 	log.message.timestamp.type = CreateTime
benchi-kafka     | 	log.preallocate = false
benchi-kafka     | 	log.retention.bytes = -1
benchi-kafka     | 	log.retention.check.interval.ms = 300000
benchi-kafka     | 	log.retention.hours = 168
benchi-kafka     | 	log.retention.minutes = null
benchi-kafka     | 	log.retention.ms = null
benchi-kafka     | 	log.roll.hours = 168
benchi-kafka     | 	log.roll.jitter.hours = 0
benchi-kafka     | 	log.roll.jitter.ms = null
benchi-kafka     | 	log.roll.ms = null
benchi-kafka     | 	log.segment.bytes = 1073741824
benchi-kafka     | 	log.segment.delete.delay.ms = 60000
benchi-kafka     | 	max.connection.creation.rate = 1.7976931348623157E308
benchi-kafka     | 	max.connection.creation.rate.per.ip.enable.threshold = 0.0
benchi-kafka     | 	max.connection.creation.rate.per.tenant.enable.threshold = 0.0
benchi-kafka     | 	max.connections = 2147483647
benchi-kafka     | 	max.connections.per.ip = 2147483647
benchi-kafka     | 	max.connections.per.ip.overrides = 
benchi-kafka     | 	max.connections.per.tenant = 0
benchi-kafka     | 	max.connections.protected.listeners = []
benchi-kafka     | 	max.connections.reap.amount = 0
benchi-kafka     | 	max.incremental.fetch.session.cache.slots = 1000
benchi-kafka     | 	max.request.partition.size.limit = 2000
benchi-kafka     | 	message.max.bytes = 1048588
benchi-kafka     | 	metadata.log.dir = null
benchi-kafka     | 	metadata.log.max.record.bytes.between.snapshots = 20971520
benchi-kafka     | 	metadata.log.max.snapshot.interval.ms = 3600000
benchi-kafka     | 	metadata.log.segment.bytes = 1073741824
benchi-kafka     | 	metadata.log.segment.min.bytes = 8388608
benchi-kafka     | 	metadata.log.segment.ms = 604800000
benchi-kafka     | 	metadata.max.idle.interval.ms = 500
benchi-kafka     | 	metadata.max.retention.bytes = 104857600
benchi-kafka     | 	metadata.max.retention.ms = 604800000
benchi-kafka     | 	metric.reporters = [io.confluent.metrics.reporter.ConfluentMetricsReporter]
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.recording.level = INFO
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	min.insync.replicas = 1
benchi-kafka     | 	multitenant.authorizer.support.resource.ids = false
benchi-kafka     | 	multitenant.metadata.class = null
benchi-kafka     | 	multitenant.metadata.dir = null
benchi-kafka     | 	multitenant.metadata.reload.delay.ms = 120000
benchi-kafka     | 	multitenant.metadata.ssl.certs.path = null
benchi-kafka     | 	multitenant.tenant.delete.batch.size = 10
benchi-kafka     | 	multitenant.tenant.delete.check.ms = 120000
benchi-kafka     | 	multitenant.tenant.delete.delay = 604800000
benchi-kafka     | 	node.id = 1
benchi-kafka     | 	num.io.threads = 8
benchi-kafka     | 	num.network.threads = 3
benchi-kafka     | 	num.partitions = 1
benchi-kafka     | 	num.recovery.threads.per.data.dir = 1
benchi-kafka     | 	num.replica.alter.log.dirs.threads = null
benchi-kafka     | 	num.replica.fetchers = 1
benchi-kafka     | 	offset.metadata.max.bytes = 4096
benchi-kafka     | 	offsets.commit.required.acks = -1
benchi-kafka     | 	offsets.commit.timeout.ms = 5000
benchi-kafka     | 	offsets.load.buffer.size = 5242880
benchi-kafka     | 	offsets.retention.check.interval.ms = 600000
benchi-kafka     | 	offsets.retention.minutes = 10080
benchi-kafka     | 	offsets.topic.compression.codec = 0
benchi-kafka     | 	offsets.topic.num.partitions = 50
benchi-kafka     | 	offsets.topic.replication.factor = 1
benchi-kafka     | 	offsets.topic.segment.bytes = 104857600
benchi-kafka     | 	otel.exporter.otlp.custom.endpoint = default
benchi-kafka     | 	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
benchi-kafka     | 	password.encoder.iterations = 4096
benchi-kafka     | 	password.encoder.key.length = 128
benchi-kafka     | 	password.encoder.keyfactory.algorithm = null
benchi-kafka     | 	password.encoder.old.secret = null
benchi-kafka     | 	password.encoder.secret = null
benchi-kafka     | 	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
benchi-kafka     | 	process.roles = [broker, controller]
benchi-kafka     | 	producer.id.expiration.check.interval.ms = 600000
benchi-kafka     | 	producer.id.expiration.ms = 86400000
benchi-kafka     | 	producer.purgatory.purge.interval.requests = 1000
benchi-kafka     | 	queued.max.request.bytes = -1
benchi-kafka     | 	queued.max.requests = 500
benchi-kafka     | 	quota.window.num = 11
benchi-kafka     | 	quota.window.size.seconds = 1
benchi-kafka     | 	quotas.consumption.expiration.time.ms = 600000
benchi-kafka     | 	quotas.expiration.interval.ms = 3600000
benchi-kafka     | 	quotas.expiration.time.ms = 604800000
benchi-kafka     | 	quotas.lazy.evaluation.threshold = 0.5
benchi-kafka     | 	quotas.topic.append.timeout.ms = 5000
benchi-kafka     | 	quotas.topic.compression.codec = 3
benchi-kafka     | 	quotas.topic.load.buffer.size = 5242880
benchi-kafka     | 	quotas.topic.num.partitions = 50
benchi-kafka     | 	quotas.topic.placement.constraints = 
benchi-kafka     | 	quotas.topic.replication.factor = 3
benchi-kafka     | 	quotas.topic.segment.bytes = 104857600
benchi-kafka     | 	remote.fetch.max.wait.ms = 500
benchi-kafka     | 	remote.log.index.file.cache.total.size.bytes = 1073741824
benchi-kafka     | 	remote.log.manager.copier.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.copy.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.copy.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.expiration.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.fetch.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.fetch.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.task.interval.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.max.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.ms = 500
benchi-kafka     | 	remote.log.manager.task.retry.jitter = 0.2
benchi-kafka     | 	remote.log.manager.thread.pool.size = 10
benchi-kafka     | 	remote.log.metadata.custom.metadata.max.bytes = 128
benchi-kafka     | 	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
benchi-kafka     | 	remote.log.metadata.manager.class.path = null
benchi-kafka     | 	remote.log.metadata.manager.impl.prefix = rlmm.config.
benchi-kafka     | 	remote.log.metadata.manager.listener.name = null
benchi-kafka     | 	remote.log.reader.max.pending.tasks = 100
benchi-kafka     | 	remote.log.reader.threads = 10
benchi-kafka     | 	remote.log.storage.manager.class.name = null
benchi-kafka     | 	remote.log.storage.manager.class.path = null
benchi-kafka     | 	remote.log.storage.manager.impl.prefix = rsm.config.
benchi-kafka     | 	remote.log.storage.system.enable = false
benchi-kafka     | 	replica.fetch.backoff.ms = 1000
benchi-kafka     | 	replica.fetch.max.bytes = 1048576
benchi-kafka     | 	replica.fetch.min.bytes = 1
benchi-kafka     | 	replica.fetch.response.max.bytes = 10485760
benchi-kafka     | 	replica.fetch.wait.max.ms = 500
benchi-kafka     | 	replica.high.watermark.checkpoint.interval.ms = 5000
benchi-kafka     | 	replica.lag.time.max.ms = 30000
benchi-kafka     | 	replica.selector.class = null
benchi-kafka     | 	replica.socket.receive.buffer.bytes = 65536
benchi-kafka     | 	replica.socket.timeout.ms = 30000
benchi-kafka     | 	replication.quota.window.num = 11
benchi-kafka     | 	replication.quota.window.size.seconds = 1
benchi-kafka     | 	request.timeout.ms = 30000
benchi-kafka     | 	reserved.broker.max.id = 1000
benchi-kafka     | 	sasl.client.callback.handler.class = null
benchi-kafka     | 	sasl.enabled.mechanisms = [GSSAPI]
benchi-kafka     | 	sasl.jaas.config = null
benchi-kafka     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
benchi-kafka     | 	sasl.kerberos.min.time.before.relogin = 60000
benchi-kafka     | 	sasl.kerberos.principal.to.local.rules = [DEFAULT]
benchi-kafka     | 	sasl.kerberos.service.name = null
benchi-kafka     | 	sasl.kerberos.ticket.renew.jitter = 0.05
benchi-kafka     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
benchi-kafka     | 	sasl.login.callback.handler.class = null
benchi-kafka     | 	sasl.login.class = null
benchi-kafka     | 	sasl.login.connect.timeout.ms = null
benchi-kafka     | 	sasl.login.read.timeout.ms = null
benchi-kafka     | 	sasl.login.refresh.buffer.seconds = 300
benchi-kafka     | 	sasl.login.refresh.min.period.seconds = 60
benchi-kafka     | 	sasl.login.refresh.window.factor = 0.8
benchi-kafka     | 	sasl.login.refresh.window.jitter = 0.05
benchi-kafka     | 	sasl.login.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.login.retry.backoff.ms = 100
benchi-kafka     | 	sasl.mechanism.controller.protocol = GSSAPI
benchi-kafka     | 	sasl.mechanism.inter.broker.protocol = GSSAPI
benchi-kafka     | 	sasl.oauthbearer.clientassertion.audience = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.expiration = 5
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.jti.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.nbf.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.issuer = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.location = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key.passphrase = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.subject = null
benchi-kafka     | 	sasl.oauthbearer.clock.skew.seconds = 30
benchi-kafka     | 	sasl.oauthbearer.expected.audience = null
benchi-kafka     | 	sasl.oauthbearer.expected.issuer = null
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.url = null
benchi-kafka     | 	sasl.oauthbearer.scope.claim.name = scope
benchi-kafka     | 	sasl.oauthbearer.sub.claim.name = sub
benchi-kafka     | 	sasl.oauthbearer.token.endpoint.url = null
benchi-kafka     | 	sasl.server.authn.async.enable = false
benchi-kafka     | 	sasl.server.authn.async.max.threads = 1
benchi-kafka     | 	sasl.server.authn.async.timeout.ms = 30000
benchi-kafka     | 	sasl.server.callback.handler.class = null
benchi-kafka     | 	sasl.server.max.receive.size = 524288
benchi-kafka     | 	security.inter.broker.protocol = PLAINTEXT
benchi-kafka     | 	security.providers = null
benchi-kafka     | 	server.max.startup.time.ms = 9223372036854775807
benchi-kafka     | 	socket.connection.setup.timeout.max.ms = 30000
benchi-kafka     | 	socket.connection.setup.timeout.ms = 10000
benchi-kafka     | 	socket.listen.backlog.size = 50
benchi-kafka     | 	socket.receive.buffer.bytes = 102400
benchi-kafka     | 	socket.request.max.bytes = 104857600
benchi-kafka     | 	socket.send.buffer.bytes = 102400
benchi-kafka     | 	ssl.allow.dn.changes = false
benchi-kafka     | 	ssl.allow.san.changes = false
benchi-kafka     | 	ssl.cipher.suites = []
benchi-kafka     | 	ssl.client.auth = none
benchi-kafka     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	ssl.engine.factory.class = null
benchi-kafka     | 	ssl.key.password = null
benchi-kafka     | 	ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	ssl.keystore.certificate.chain = null
benchi-kafka     | 	ssl.keystore.key = null
benchi-kafka     | 	ssl.keystore.location = null
benchi-kafka     | 	ssl.keystore.password = null
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.principal.mapping.rules = DEFAULT
benchi-kafka     | 	ssl.protocol = TLSv1.3
benchi-kafka     | 	ssl.provider = null
benchi-kafka     | 	ssl.secure.random.implementation = null
benchi-kafka     | 	ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	ssl.truststore.certificates = null
benchi-kafka     | 	ssl.truststore.location = null
benchi-kafka     | 	ssl.truststore.password = null
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     | 	telemetry.max.bytes = 1048576
benchi-kafka     | 	throughput.quota.window.num = 11
benchi-kafka     | 	token.impersonation.validation = true
benchi-kafka     | 	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
benchi-kafka     | 	transaction.max.timeout.ms = 900000
benchi-kafka     | 	transaction.metadata.load.threads = 32
benchi-kafka     | 	transaction.partition.verification.enable = true
benchi-kafka     | 	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
benchi-kafka     | 	transaction.state.log.load.buffer.size = 5242880
benchi-kafka     | 	transaction.state.log.min.isr = 1
benchi-kafka     | 	transaction.state.log.num.partitions = 50
benchi-kafka     | 	transaction.state.log.replication.factor = 1
benchi-kafka     | 	transaction.state.log.segment.bytes = 104857600
benchi-kafka     | 	transactional.id.expiration.ms = 604800000
benchi-kafka     | 	unclean.leader.election.enable = false
benchi-kafka     | 	unstable.api.versions.enable = false
benchi-kafka     | 	unstable.feature.versions.enable = false
benchi-kafka     | 	zookeeper.acl.change.notification.expiration.ms = 900000
benchi-kafka     | 	zookeeper.clientCnxnSocket = null
benchi-kafka     | 	zookeeper.connect = 
benchi-kafka     | 	zookeeper.connection.timeout.ms = null
benchi-kafka     | 	zookeeper.max.in.flight.requests = 10
benchi-kafka     | 	zookeeper.metadata.migration.enable = false
benchi-kafka     | 	zookeeper.metadata.migration.min.batch.size = 200
benchi-kafka     | 	zookeeper.session.timeout.ms = 18000
benchi-kafka     | 	zookeeper.set.acl = false
benchi-kafka     | 	zookeeper.ssl.cipher.suites = null
benchi-kafka     | 	zookeeper.ssl.client.enable = false
benchi-kafka     | 	zookeeper.ssl.crl.enable = false
benchi-kafka     | 	zookeeper.ssl.enabled.protocols = null
benchi-kafka     | 	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
benchi-kafka     | 	zookeeper.ssl.keystore.location = null
benchi-kafka     | 	zookeeper.ssl.keystore.password = null
benchi-kafka     | 	zookeeper.ssl.keystore.type = null
benchi-kafka     | 	zookeeper.ssl.ocsp.enable = false
benchi-kafka     | 	zookeeper.ssl.protocol = TLSv1.2
benchi-kafka     | 	zookeeper.ssl.truststore.location = null
benchi-kafka     | 	zookeeper.ssl.truststore.password = null
benchi-kafka     | 	zookeeper.ssl.truststore.type = null
benchi-kafka     |  (kafka.server.KafkaConfig)
benchi-kafka     | [2025-03-18 16:05:45,526] INFO RemoteLogManagerConfig values: 
benchi-kafka     | 	log.local.retention.bytes = -2
benchi-kafka     | 	log.local.retention.ms = -2
benchi-kafka     | 	remote.fetch.max.wait.ms = 500
benchi-kafka     | 	remote.log.index.file.cache.total.size.bytes = 1073741824
benchi-kafka     | 	remote.log.manager.copier.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.copy.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.copy.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.expiration.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.fetch.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.fetch.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.task.interval.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.max.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.ms = 500
benchi-kafka     | 	remote.log.manager.task.retry.jitter = 0.2
benchi-kafka     | 	remote.log.manager.thread.pool.size = 10
benchi-kafka     | 	remote.log.metadata.custom.metadata.max.bytes = 128
benchi-kafka     | 	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
benchi-kafka     | 	remote.log.metadata.manager.class.path = null
benchi-kafka     | 	remote.log.metadata.manager.impl.prefix = rlmm.config.
benchi-kafka     | 	remote.log.metadata.manager.listener.name = null
benchi-kafka     | 	remote.log.reader.max.pending.tasks = 100
benchi-kafka     | 	remote.log.reader.threads = 10
benchi-kafka     | 	remote.log.storage.manager.class.name = null
benchi-kafka     | 	remote.log.storage.manager.class.path = null
benchi-kafka     | 	remote.log.storage.manager.impl.prefix = rsm.config.
benchi-kafka     | 	remote.log.storage.system.enable = false
benchi-kafka     |  (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
benchi-kafka     | [2025-03-18 16:05:45,535] INFO RemoteLogManagerConfig values: 
benchi-kafka     | 	log.local.retention.bytes = -2
benchi-kafka     | 	log.local.retention.ms = -2
benchi-kafka     | 	remote.fetch.max.wait.ms = 500
benchi-kafka     | 	remote.log.index.file.cache.total.size.bytes = 1073741824
benchi-kafka     | 	remote.log.manager.copier.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.copy.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.copy.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.expiration.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.fetch.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.fetch.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.task.interval.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.max.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.ms = 500
benchi-kafka     | 	remote.log.manager.task.retry.jitter = 0.2
benchi-kafka     | 	remote.log.manager.thread.pool.size = 10
benchi-kafka     | 	remote.log.metadata.custom.metadata.max.bytes = 128
benchi-kafka     | 	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
benchi-kafka     | 	remote.log.metadata.manager.class.path = null
benchi-kafka     | 	remote.log.metadata.manager.impl.prefix = rlmm.config.
benchi-kafka     | 	remote.log.metadata.manager.listener.name = null
benchi-kafka     | 	remote.log.reader.max.pending.tasks = 100
benchi-kafka     | 	remote.log.reader.threads = 10
benchi-kafka     | 	remote.log.storage.manager.class.name = null
benchi-kafka     | 	remote.log.storage.manager.class.path = null
benchi-kafka     | 	remote.log.storage.manager.impl.prefix = rsm.config.
benchi-kafka     | 	remote.log.storage.system.enable = false
benchi-kafka     |  (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
benchi-kafka     | [2025-03-18 16:05:45,538] INFO KafkaConfig values: 
benchi-kafka     | 	advertised.listeners = PLAINTEXT://broker:29092,PLAINTEXT_HOST://localhost:9092
benchi-kafka     | 	alter.config.policy.class.name = null
benchi-kafka     | 	alter.log.dirs.replication.quota.window.num = 11
benchi-kafka     | 	alter.log.dirs.replication.quota.window.size.seconds = 1
benchi-kafka     | 	authorizer.class.name = 
benchi-kafka     | 	auto.create.topics.enable = true
benchi-kafka     | 	auto.include.jmx.reporter = true
benchi-kafka     | 	auto.leader.rebalance.enable = true
benchi-kafka     | 	background.threads = 10
benchi-kafka     | 	broker.heartbeat.interval.ms = 2000
benchi-kafka     | 	broker.id = 1
benchi-kafka     | 	broker.id.generation.enable = true
benchi-kafka     | 	broker.interceptor.class = class org.apache.kafka.server.interceptor.DefaultBrokerInterceptor
benchi-kafka     | 	broker.rack = null
benchi-kafka     | 	broker.session.timeout.ms = 9000
benchi-kafka     | 	broker.session.uuid = JzUn51KjQwC3OragyPrV9Q
benchi-kafka     | 	client.quota.callback.class = null
benchi-kafka     | 	client.quota.max.throttle.time.ms = 5000
benchi-kafka     | 	compression.gzip.level = -1
benchi-kafka     | 	compression.lz4.level = 9
benchi-kafka     | 	compression.type = producer
benchi-kafka     | 	compression.zstd.level = 3
benchi-kafka     | 	confluent.alter.broker.health.max.demoted.brokers = 2147483647
benchi-kafka     | 	confluent.alter.broker.health.max.demoted.brokers.percentage = 0
benchi-kafka     | 	confluent.ansible.managed = false
benchi-kafka     | 	confluent.api.visibility = DEFAULT
benchi-kafka     | 	confluent.append.record.interceptor.classes = []
benchi-kafka     | 	confluent.apply.create.topic.policy.to.create.partitions = false
benchi-kafka     | 	confluent.authorizer.authority.name = 
benchi-kafka     | 	confluent.automatic.alter.broker.health.retry.backoff.ms = 2000
benchi-kafka     | 	confluent.backpressure.disk.enable = false
benchi-kafka     | 	confluent.backpressure.disk.free.threshold.bytes = 21474836480
benchi-kafka     | 	confluent.backpressure.disk.produce.bytes.per.second = 131072
benchi-kafka     | 	confluent.backpressure.disk.threshold.recovery.factor = 1.5
benchi-kafka     | 	confluent.backpressure.request.min.broker.limit = 200
benchi-kafka     | 	confluent.backpressure.request.queue.size.percentile = p95
benchi-kafka     | 	confluent.backpressure.types = null
benchi-kafka     | 	confluent.balancer.api.state.topic = _confluent_balancer_api_state
benchi-kafka     | 	confluent.balancer.broker.addition.elapsed.time.ms.completion.threshold = 57600000
benchi-kafka     | 	confluent.balancer.broker.addition.mean.cpu.percent.completion.threshold = 0.5
benchi-kafka     | 	confluent.balancer.capacity.threshold.upper.limit = 0.95
benchi-kafka     | 	confluent.balancer.cell.load.upper.bound = 0.7
benchi-kafka     | 	confluent.balancer.cell.overload.detection.interval.ms = 3600000
benchi-kafka     | 	confluent.balancer.cell.overload.duration.ms = 86400000
benchi-kafka     | 	confluent.balancer.class = io.confluent.databalancer.SbcDataBalanceManager
benchi-kafka     | 	confluent.balancer.consumer.out.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.balancer.cpu.balance.threshold = 1.1
benchi-kafka     | 	confluent.balancer.cpu.low.utilization.threshold = 0.2
benchi-kafka     | 	confluent.balancer.cpu.utilization.detector.duration.ms = 600000
benchi-kafka     | 	confluent.balancer.cpu.utilization.detector.enabled = false
benchi-kafka     | 	confluent.balancer.cpu.utilization.detector.overutilization.threshold = 80.0
benchi-kafka     | 	confluent.balancer.cpu.utilization.detector.underutilization.threshold = 50.0
benchi-kafka     | 	confluent.balancer.disk.max.load = 0.85
benchi-kafka     | 	confluent.balancer.disk.min.free.space.gb = 0
benchi-kafka     | 	confluent.balancer.disk.min.free.space.lower.limit.gb = 0
benchi-kafka     | 	confluent.balancer.disk.utilization.detector.duration.ms = 600000
benchi-kafka     | 	confluent.balancer.disk.utilization.detector.enabled = false
benchi-kafka     | 	confluent.balancer.disk.utilization.detector.overutilization.threshold = 80.0
benchi-kafka     | 	confluent.balancer.disk.utilization.detector.reserved.capacity = 150000.0
benchi-kafka     | 	confluent.balancer.disk.utilization.detector.underutilization.threshold = 35.0
benchi-kafka     | 	confluent.balancer.enable = true
benchi-kafka     | 	confluent.balancer.exclude.topic.names = []
benchi-kafka     | 	confluent.balancer.exclude.topic.prefixes = []
benchi-kafka     | 	confluent.balancer.goal.violation.delay.on.new.brokers.ms = 1800000
benchi-kafka     | 	confluent.balancer.goal.violation.distribution.threshold.multiplier = 1.1
benchi-kafka     | 	confluent.balancer.heal.broker.failure.threshold.ms = 3600000
benchi-kafka     | 	confluent.balancer.heal.uneven.load.trigger = EMPTY_BROKER
benchi-kafka     | 	confluent.balancer.incremental.balancing.cpu.top.proposal.tracking.enabled = true
benchi-kafka     | 	confluent.balancer.incremental.balancing.cpu.top.proposal.tracking.num.proposals = 15
benchi-kafka     | 	confluent.balancer.incremental.balancing.enabled = false
benchi-kafka     | 	confluent.balancer.incremental.balancing.goals = []
benchi-kafka     | 	confluent.balancer.incremental.balancing.lower.bound = 0.02
benchi-kafka     | 	confluent.balancer.incremental.balancing.min.valid.windows = 5
benchi-kafka     | 	confluent.balancer.incremental.balancing.step.ratio = 0.2
benchi-kafka     | 	confluent.balancer.inter.cell.balancing.enabled = false
benchi-kafka     | 	confluent.balancer.max.capacity.balancing.delta.percentage = 0.0
benchi-kafka     | 	confluent.balancer.max.replicas = 2147483647
benchi-kafka     | 	confluent.balancer.network.in.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.balancer.network.out.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.balancer.plan.computation.retry.timeout.ms = 3600000
benchi-kafka     | 	confluent.balancer.producer.in.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.balancer.rebalancing.goals = []
benchi-kafka     | 	confluent.balancer.replication.in.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.balancer.resource.utilization.detector.interval.ms = 60000
benchi-kafka     | 	confluent.balancer.sbc.metrics.parser.enabled = false
benchi-kafka     | 	confluent.balancer.self.healing.maximum.rounds = 1
benchi-kafka     | 	confluent.balancer.task.history.retention.days = 30
benchi-kafka     | 	confluent.balancer.tenant.maximum.movements = 0
benchi-kafka     | 	confluent.balancer.tenant.suspension.ms = 86400000
benchi-kafka     | 	confluent.balancer.throttle.bytes.per.second = 10485760
benchi-kafka     | 	confluent.balancer.topic.partition.maximum.movements = 5
benchi-kafka     | 	confluent.balancer.topic.partition.movement.expiration.ms = 10800000
benchi-kafka     | 	confluent.balancer.topic.partition.suspension.ms = 18000000
benchi-kafka     | 	confluent.balancer.topic.replication.factor = 1
benchi-kafka     | 	confluent.balancer.triggering.goals = []
benchi-kafka     | 	confluent.balancer.v2.addition.enabled = false
benchi-kafka     | 	confluent.basic.auth.credentials.source = null
benchi-kafka     | 	confluent.basic.auth.user.info = null
benchi-kafka     | 	confluent.bearer.auth.cache.expiry.buffer.seconds = 300
benchi-kafka     | 	confluent.bearer.auth.client.id = null
benchi-kafka     | 	confluent.bearer.auth.client.secret = null
benchi-kafka     | 	confluent.bearer.auth.credentials.source = null
benchi-kafka     | 	confluent.bearer.auth.identity.pool.id = null
benchi-kafka     | 	confluent.bearer.auth.issuer.endpoint.url = null
benchi-kafka     | 	confluent.bearer.auth.logical.cluster = null
benchi-kafka     | 	confluent.bearer.auth.scope = null
benchi-kafka     | 	confluent.bearer.auth.scope.claim.name = scope
benchi-kafka     | 	confluent.bearer.auth.sub.claim.name = sub
benchi-kafka     | 	confluent.bearer.auth.token = null
benchi-kafka     | 	confluent.broker.health.manager.enabled = true
benchi-kafka     | 	confluent.broker.health.manager.engine.request.handler.threads.stuck.criteria = AllThreadsStuck
benchi-kafka     | 	confluent.broker.health.manager.hard.kill.duration.ms = 60000
benchi-kafka     | 	confluent.broker.health.manager.mitigation.enabled = false
benchi-kafka     | 	confluent.broker.health.manager.num.samples.before.broker.suspect = 30
benchi-kafka     | 	confluent.broker.health.manager.num.samples.before.broker.unhealthy = 180
benchi-kafka     | 	confluent.broker.health.manager.percentage.unhealthy.samples.before.broker.suspect = 90
benchi-kafka     | 	confluent.broker.health.manager.percentage.unhealthy.samples.before.broker.unhealthy = 70
benchi-kafka     | 	confluent.broker.health.manager.sample.duration.ms = 1000
benchi-kafka     | 	confluent.broker.health.manager.storage.background.threads.stuck.criteria = AnyThreadStuck
benchi-kafka     | 	confluent.broker.health.manager.storage.network.threads.stuck.criteria = AnyThreadStuck
benchi-kafka     | 	confluent.broker.health.manager.storage.request.handler.threads.stuck.criteria = AnyThreadStuck
benchi-kafka     | 	confluent.broker.limit.consumer.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.broker.limit.producer.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.broker.load.advertised.limit.load = 0.8
benchi-kafka     | 	confluent.broker.load.average.service.request.time.ms = 0.1
benchi-kafka     | 	confluent.broker.load.delay.metric.start.ms = 180000
benchi-kafka     | 	confluent.broker.load.enabled = false
benchi-kafka     | 	confluent.broker.load.num.samples = 60
benchi-kafka     | 	confluent.broker.load.tenant.metric.enable = false
benchi-kafka     | 	confluent.broker.load.update.metric.tags.interval.ms = 60000
benchi-kafka     | 	confluent.broker.load.window.size.ms = 60000
benchi-kafka     | 	confluent.broker.load.workload.coefficient = 20.0
benchi-kafka     | 	confluent.broker.registration.delay.ms = 0
benchi-kafka     | 	confluent.catalog.collector.destination.topic = telemetry.events.data_catalog_source
benchi-kafka     | 	confluent.catalog.collector.enable = false
benchi-kafka     | 	confluent.catalog.collector.full.configs.enable = false
benchi-kafka     | 	confluent.catalog.collector.max.bytes.per.snapshot = 850000
benchi-kafka     | 	confluent.catalog.collector.max.topics.process = 500
benchi-kafka     | 	confluent.catalog.collector.max.zookeeper.request.per.sec = 100
benchi-kafka     | 	confluent.catalog.collector.snapshot.init.delay.sec = 60
benchi-kafka     | 	confluent.catalog.collector.snapshot.interval.sec = 300
benchi-kafka     | 	confluent.ccloud.host.suffixes = .confluent.cloud,.cpdev.cloud,.confluentgov.com,.confluentgov-internal.com
benchi-kafka     | 	confluent.ccloud.intranet.host.suffixes = .intranet.stag.cpdev.cloud,.intranet.stag.cpdev-untrusted.cloud,.intranet.devel.cpdev.cloud,.intranet.devel.cpdev-untrusted.cloud,.intranet.confluent.cloud,.intranet.confluent-untrusted.cloud
benchi-kafka     | 	confluent.cdc.api.keys.topic = 
benchi-kafka     | 	confluent.cdc.api.keys.topic.load.timeout.ms = 600000
benchi-kafka     | 	confluent.cdc.client.quotas.enable = false
benchi-kafka     | 	confluent.cdc.client.quotas.topic.name = 
benchi-kafka     | 	confluent.cdc.lkc.metadata.topic = 
benchi-kafka     | 	confluent.cdc.user.metadata.enable = false
benchi-kafka     | 	confluent.cdc.user.metadata.topic = _confluent-user_metadata
benchi-kafka     | 	confluent.cell.metrics.refresh.period.ms = 60000
benchi-kafka     | 	confluent.cells.default.size = 15
benchi-kafka     | 	confluent.cells.enable = false
benchi-kafka     | 	confluent.cells.implicit.creation.enable = false
benchi-kafka     | 	confluent.cells.load.refresher.enable = true
benchi-kafka     | 	confluent.cells.max.size = 15
benchi-kafka     | 	confluent.cells.min.size = 6
benchi-kafka     | 	confluent.checksum.enabled.files = [none]
benchi-kafka     | 	confluent.clm.enabled = false
benchi-kafka     | 	confluent.clm.frequency.in.hours = 6
benchi-kafka     | 	confluent.clm.list.object.thread_pool.size = 1
benchi-kafka     | 	confluent.clm.max.backup.days = 3
benchi-kafka     | 	confluent.clm.min.delay.in.minutes = 30
benchi-kafka     | 	confluent.clm.thread.pool.size = 2
benchi-kafka     | 	confluent.clm.topic.retention.days.to.backup.days = 0:0,3:3
benchi-kafka     | 	confluent.close.connections.on.credential.delete = false
benchi-kafka     | 	confluent.cluster.link.admin.max.in.flight.requests = 1000
benchi-kafka     | 	confluent.cluster.link.admin.request.batch.size = 1
benchi-kafka     | 	confluent.cluster.link.allow.config.providers = true
benchi-kafka     | 	confluent.cluster.link.allow.legacy.message.format = false
benchi-kafka     | 	confluent.cluster.link.allow.truncation.below.hwm = true
benchi-kafka     | 	confluent.cluster.link.availability.check.mode = ALL
benchi-kafka     | 	confluent.cluster.link.background.thread.affinity = LINK
benchi-kafka     | 	confluent.cluster.link.clients.max.idle.ms = 3153600000000
benchi-kafka     | 	confluent.cluster.link.enable = true
benchi-kafka     | 	confluent.cluster.link.enable.local.admin = false
benchi-kafka     | 	confluent.cluster.link.enable.metrics.reduction = false
benchi-kafka     | 	confluent.cluster.link.fetch.response.min.bytes = 1
benchi-kafka     | 	confluent.cluster.link.fetch.response.total.bytes = 2147483647
benchi-kafka     | 	confluent.cluster.link.fetcher.auto.tune.enable = false
benchi-kafka     | 	confluent.cluster.link.fetcher.thread.pool.mode = ENDPOINT
benchi-kafka     | 	confluent.cluster.link.insync.fetch.response.min.bytes = 1
benchi-kafka     | 	confluent.cluster.link.insync.fetch.response.total.bytes = 2147483647
benchi-kafka     | 	confluent.cluster.link.intranet.connectivity.enable = false
benchi-kafka     | 	confluent.cluster.link.io.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.cluster.link.local.reverse.connection.listener.map = null
benchi-kafka     | 	confluent.cluster.link.max.client.connections = 2147483647
benchi-kafka     | 	confluent.cluster.link.metadata.topic.create.retry.delay.ms = 1000
benchi-kafka     | 	confluent.cluster.link.metadata.topic.enable = false
benchi-kafka     | 	confluent.cluster.link.metadata.topic.min.isr = 2
benchi-kafka     | 	confluent.cluster.link.metadata.topic.partitions = 50
benchi-kafka     | 	confluent.cluster.link.metadata.topic.replication.factor = 1
benchi-kafka     | 	confluent.cluster.link.mirror.transition.batch.size = 10
benchi-kafka     | 	confluent.cluster.link.num.background.threads = 1
benchi-kafka     | 	confluent.cluster.link.periodic.task.batch.size = 2147483647
benchi-kafka     | 	confluent.cluster.link.periodic.task.min.interval.ms = 1000
benchi-kafka     | 	confluent.cluster.link.replica.fetch.connections.mode = combined
benchi-kafka     | 	confluent.cluster.link.replication.quota.mode = CLUSTER_LINK_ONLY
benchi-kafka     | 	confluent.cluster.link.replication.quota.mode.per.tenant.overrides = 
benchi-kafka     | 	confluent.cluster.link.replication.quota.window.num = 11
benchi-kafka     | 	confluent.cluster.link.replication.quota.window.size.seconds = 2
benchi-kafka     | 	confluent.cluster.link.request.quota.capacity = 400
benchi-kafka     | 	confluent.cluster.link.request.quota.request.percentage.multiplier = 1.0
benchi-kafka     | 	confluent.cluster.link.tenant.replication.quota.enable = false
benchi-kafka     | 	confluent.cluster.link.tenant.request.quota.enable = false
benchi-kafka     | 	confluent.cluster.metadata.snapshot.tier.delete.enable = false
benchi-kafka     | 	confluent.cluster.metadata.snapshot.tier.delete.maintain.min.snapshots = 3
benchi-kafka     | 	confluent.cluster.metadata.snapshot.tier.delete.retention.ms = 604800000
benchi-kafka     | 	confluent.cluster.metadata.snapshot.tier.upload.enable = false
benchi-kafka     | 	confluent.compacted.topic.prefer.tier.fetch.ms = -1
benchi-kafka     | 	confluent.connection.invalid.request.delay.enable = false
benchi-kafka     | 	confluent.connections.idle.expiry.manager.ignore.idleness.requests = []
benchi-kafka     | 	confluent.consumer.fetch.partition.pruning.enable = true
benchi-kafka     | 	confluent.consumer.lag.emitter.enabled = false
benchi-kafka     | 	confluent.consumer.lag.emitter.interval.ms = 60000
benchi-kafka     | 	confluent.dataflow.policy.watch.monitor.ms = 300000
benchi-kafka     | 	confluent.defer.isr.shrink.enable = false
benchi-kafka     | 	confluent.describe.topic.partitions.enabled = false
benchi-kafka     | 	confluent.disk.io.manager.enable = false
benchi-kafka     | 	confluent.disk.throughput.headroom = 10485760
benchi-kafka     | 	confluent.disk.throughput.limit = 10485760000
benchi-kafka     | 	confluent.disk.throughput.quota.tier.archive = 1048576000
benchi-kafka     | 	confluent.disk.throughput.quota.tier.archive.throttled = 104857600
benchi-kafka     | 	confluent.durability.audit.batch.flush.frequency.ms = 900000
benchi-kafka     | 	confluent.durability.audit.checks = PeriodicalAudit,ChecksumAudit
benchi-kafka     | 	confluent.durability.audit.enable = false
benchi-kafka     | 	confluent.durability.audit.idempotent.producer = false
benchi-kafka     | 	confluent.durability.audit.initial.job.delay.ms = 900000
benchi-kafka     | 	confluent.durability.audit.io.bytes.per.sec = 10485760
benchi-kafka     | 	confluent.durability.audit.log.ignored.event.types = 
benchi-kafka     | 	confluent.durability.audit.reporting.batch.ms = 1800000
benchi-kafka     | 	confluent.durability.audit.tier.compaction.audit.duration.ms = 14400000
benchi-kafka     | 	confluent.durability.events.allowed = OffsetChangeType,EpochChangeType,IsrExpandType,DeleteRecordsType,RetentionChangeType,StartOffsetChangeType,DeletePartitionType,HealthCheckType
benchi-kafka     | 	confluent.durability.topic.partition.count = 50
benchi-kafka     | 	confluent.durability.topic.replication.factor = 1
benchi-kafka     | 	confluent.e2e_checksum.protection.enabled = false
benchi-kafka     | 	confluent.e2e_checksum.protection.files = [none]
benchi-kafka     | 	confluent.e2e_checksum.protection.store.entry.ttl.ms = 2592000000
benchi-kafka     | 	confluent.elastic.cku.enabled = false
benchi-kafka     | 	confluent.elastic.cku.scaletozero.enabled = false
benchi-kafka     | 	confluent.eligible.controllers = []
benchi-kafka     | 	confluent.enable.stray.partition.deletion = false
benchi-kafka     | 	confluent.encryption.key.manager.rotation.interval.ms = 31536000000
benchi-kafka     | 	confluent.fetch.from.follower.require.leader.epoch.enable = false
benchi-kafka     | 	confluent.fetch.partition.pruning.enable = true
benchi-kafka     | 	confluent.floor.connection.rate.per.ip = -1.0
benchi-kafka     | 	confluent.floor.connection.rate.per.tenant = -1.0
benchi-kafka     | 	confluent.group.coordinator.offsets.batching.enable = false
benchi-kafka     | 	confluent.group.coordinator.offsets.writer.threads = 2
benchi-kafka     | 	confluent.group.metadata.load.threads = 32
benchi-kafka     | 	confluent.heap.tenured.notify.bytes = 0
benchi-kafka     | 	confluent.heap.tenured.notify.enabled = false
benchi-kafka     | 	confluent.hot.partition.ratio = 0.8
benchi-kafka     | 	confluent.http.server.start.timeout.ms = 60000
benchi-kafka     | 	confluent.http.server.stop.timeout.ms = 30000
benchi-kafka     | 	confluent.internal.metrics.enable = false
benchi-kafka     | 	confluent.internal.rest.server.bind.port = null
benchi-kafka     | 	confluent.internal.tenant.scoped.listener.name = INTERNAL_TENANT_SCOPED
benchi-kafka     | 	confluent.leader.epoch.checkpoint.checksum.enabled = false
benchi-kafka     | 	confluent.log.cleaner.timestamp.validation.enable = true
benchi-kafka     | 	confluent.log.placement.constraints = 
benchi-kafka     | 	confluent.max.connection.creation.rate.per.ip = 1.7976931348623157E308
benchi-kafka     | 	confluent.max.connection.creation.rate.per.tenant = 1.7976931348623157E308
benchi-kafka     | 	confluent.max.connection.rate.per.ip = -1.0
benchi-kafka     | 	confluent.max.connection.rate.per.tenant = -1.0
benchi-kafka     | 	confluent.max.connection.throttle.ms = null
benchi-kafka     | 	confluent.max.segment.ms = 9223372036854775807
benchi-kafka     | 	confluent.metadata.active.encryptor = null
benchi-kafka     | 	confluent.metadata.controlled.shutdown.partition.slice.delay.ms = 100
benchi-kafka     | 	confluent.metadata.encryptor.classes = null
benchi-kafka     | 	confluent.metadata.encryptor.required = false
benchi-kafka     | 	confluent.metadata.encryptor.secret.file = null
benchi-kafka     | 	confluent.metadata.encryptor.secrets = null
benchi-kafka     | 	confluent.metadata.jvm.warmup.ms = 60000
benchi-kafka     | 	confluent.metadata.leader.balance.slice.delay.ms = 100
benchi-kafka     | 	confluent.metadata.max.controlled.shutdown.partition.changes.per.slice = 1000
benchi-kafka     | 	confluent.metadata.max.leader.balance.changes.per.slice = 1000
benchi-kafka     | 	confluent.metadata.server.cluster.registry.clusters = []
benchi-kafka     | 	confluent.metadata.throttle.pre.check.for.non.wildcard.requests.enable = false
benchi-kafka     | 	confluent.metadata.throttle.pre.check.for.wildcard.requests.enable = false
benchi-kafka     | 	confluent.metrics.reporter.bootstrap.servers = localhost:9092
benchi-kafka     | 	confluent.min.acks = 0
benchi-kafka     | 	confluent.min.connection.throttle.ms = 0
benchi-kafka     | 	confluent.min.segment.ms = 1
benchi-kafka     | 	confluent.missing.id.cache.ttl.sec = 60
benchi-kafka     | 	confluent.missing.id.query.range = 20000
benchi-kafka     | 	confluent.missing.schema.cache.ttl.sec = 60
benchi-kafka     | 	confluent.mtls.enable = false
benchi-kafka     | 	confluent.mtls.listener.name = EXTERNAL
benchi-kafka     | 	confluent.mtls.sasl.authenticator.request.max.bytes = 104857600
benchi-kafka     | 	confluent.mtls.truststore.manager.class.name = null
benchi-kafka     | 	confluent.multitenant.authorizer.enable.acl.state = false
benchi-kafka     | 	confluent.multitenant.interceptor.balancer.apis.enabled = false
benchi-kafka     | 	confluent.multitenant.interceptor.collect.client.apiversions.max.per.tenant = 1000
benchi-kafka     | 	confluent.multitenant.interceptor.collect.client.apiversions.metric = false
benchi-kafka     | 	confluent.multitenant.listener.hostname.cluster.prefix.enable = false
benchi-kafka     | 	confluent.multitenant.listener.hostname.subdomain.suffix.enable = false
benchi-kafka     | 	confluent.multitenant.listener.names = null
benchi-kafka     | 	confluent.multitenant.parse.lkc.id.enable = false
benchi-kafka     | 	confluent.multitenant.parse.sni.host.name.enable = false
benchi-kafka     | 	confluent.network.health.manager.enabled = false
benchi-kafka     | 	confluent.network.health.manager.external.listener.name = EXTERNAL
benchi-kafka     | 	confluent.network.health.manager.externalconnectivitystartup.enabled = false
benchi-kafka     | 	confluent.network.health.manager.min.healthy.network.samples = 3
benchi-kafka     | 	confluent.network.health.manager.min.percentage.healthy.network.samples = 3
benchi-kafka     | 	confluent.network.health.manager.mitigation.enabled = false
benchi-kafka     | 	confluent.network.health.manager.network.sample.window.size = 120
benchi-kafka     | 	confluent.network.health.manager.sample.duration.ms = 1000
benchi-kafka     | 	confluent.oauth.flat.networking.verification.enable = false
benchi-kafka     | 	confluent.offsets.log.cleaner.delete.retention.ms = 86400000
benchi-kafka     | 	confluent.offsets.log.cleaner.max.compaction.lag.ms = 9223372036854775807
benchi-kafka     | 	confluent.offsets.log.cleaner.min.cleanable.dirty.ratio = 0.5
benchi-kafka     | 	confluent.offsets.topic.placement.constraints = 
benchi-kafka     | 	confluent.omit.network.processor.metric.tag = false
benchi-kafka     | 	confluent.operator.managed = false
benchi-kafka     | 	confluent.password.encoder.old.secret.ttl.ms = 9223372036854775807
benchi-kafka     | 	confluent.plugins.cluster.link.policy.max.destination.links.per.tenant = 10
benchi-kafka     | 	confluent.plugins.cluster.link.policy.max.source.links.per.tenant = 10
benchi-kafka     | 	confluent.plugins.topic.policy.max.partitions.per.cluster = 2147483647
benchi-kafka     | 	confluent.plugins.topic.policy.max.partitions.per.tenant = 512
benchi-kafka     | 	confluent.plugins.topic.policy.max.replicas.per.broker = 2147483647
benchi-kafka     | 	confluent.plugins.topic.policy.max.topics.per.cluster = 2147483647
benchi-kafka     | 	confluent.prefer.tier.fetch.ms = -1
benchi-kafka     | 	confluent.produce.throttle.pre.check.enable = false
benchi-kafka     | 	confluent.producer.id.cache.broker.hard.limit = -1
benchi-kafka     | 	confluent.producer.id.cache.eviction.minimal.expiration.ms = 900000
benchi-kafka     | 	confluent.producer.id.cache.extra.eviction.percentage = 0
benchi-kafka     | 	confluent.producer.id.cache.limit = 2147483647
benchi-kafka     | 	confluent.producer.id.cache.partition.hard.limit = -1
benchi-kafka     | 	confluent.producer.id.cache.tenant.hard.limit = -1
benchi-kafka     | 	confluent.producer.id.quota.manager.enable = false
benchi-kafka     | 	confluent.producer.id.quota.window.num = 11
benchi-kafka     | 	confluent.producer.id.quota.window.size.seconds = 1
benchi-kafka     | 	confluent.producer.id.throttle.enable = false
benchi-kafka     | 	confluent.producer.id.throttle.enable.threshold.percentage = 100
benchi-kafka     | 	confluent.proxy.mode.local.default = false
benchi-kafka     | 	confluent.proxy.protocol.fallback.enabled = false
benchi-kafka     | 	confluent.proxy.protocol.version = NONE
benchi-kafka     | 	confluent.quota.computing.usage.adjustment = 0.5
benchi-kafka     | 	confluent.quota.dynamic.enable = false
benchi-kafka     | 	confluent.quota.dynamic.publishing.interval.ms = 60000
benchi-kafka     | 	confluent.quota.dynamic.reporting.interval.ms = 30000
benchi-kafka     | 	confluent.quota.dynamic.reporting.min.usage = 102400
benchi-kafka     | 	confluent.quota.tenant.broker.max.consumer.rate = 13107200
benchi-kafka     | 	confluent.quota.tenant.broker.max.producer.rate = 13107200
benchi-kafka     | 	confluent.quota.tenant.default.controller.mutation.rate = 2.147483647E9
benchi-kafka     | 	confluent.quota.tenant.default.producer.id.rate = 2.147483647E9
benchi-kafka     | 	confluent.quota.tenant.fetch.multiplier = 1.0
benchi-kafka     | 	confluent.quota.tenant.follower.broker.min.consumer.rate = 10485760
benchi-kafka     | 	confluent.quota.tenant.follower.broker.min.producer.rate = 10485760
benchi-kafka     | 	confluent.quota.tenant.produce.multiplier = 1.0
benchi-kafka     | 	confluent.quota.tenant.user.quotas.enable = false
benchi-kafka     | 	confluent.regional.metadata.client.class = null
benchi-kafka     | 	confluent.regional.resource.manager.client.scheduler.threads = 2
benchi-kafka     | 	confluent.regional.resource.manager.endpoint = null
benchi-kafka     | 	confluent.regional.resource.manager.watch.endpoint = null
benchi-kafka     | 	confluent.replica.fetch.backoff.max.ms = 1000
benchi-kafka     | 	confluent.replica.fetch.connections.mode = combined
benchi-kafka     | 	confluent.replication.mode = PULL
benchi-kafka     | 	confluent.replication.push.feature.enable = false
benchi-kafka     | 	confluent.reporters.telemetry.auto.enable = true
benchi-kafka     | 	confluent.request.log.filter.class = class org.apache.kafka.common.requests.SamplingRequestLogFilter
benchi-kafka     | 	confluent.request.pipelining.enable = true
benchi-kafka     | 	confluent.request.pipelining.max.in.flight.requests.per.connection = 5
benchi-kafka     | 	confluent.require.calling.resource.identity = false
benchi-kafka     | 	confluent.require.compatible.keystore.updates = true
benchi-kafka     | 	confluent.require.confluent.issuer = false
benchi-kafka     | 	confluent.roll.check.interval.ms = 300000
benchi-kafka     | 	confluent.schema.registry.max.cache.size = 10000
benchi-kafka     | 	confluent.schema.registry.max.retries = 1
benchi-kafka     | 	confluent.schema.registry.retries.wait.ms = 0
benchi-kafka     | 	confluent.schema.registry.url = null
benchi-kafka     | 	confluent.schema.validator.interceptor.class = io.confluent.kafka.schemaregistry.validator.RecordSchemaValidator
benchi-kafka     | 	confluent.schema.validator.multitenant.enable = false
benchi-kafka     | 	confluent.schema.validator.samples.per.min = 0
benchi-kafka     | 	confluent.security.bc.approved.mode.enable = false
benchi-kafka     | 	confluent.security.event.logger.authentication.enable = false
benchi-kafka     | 	confluent.security.event.logger.authentication.event.rate.limit = -1
benchi-kafka     | 	confluent.security.event.logger.authorization.event.rate.limit = -1
benchi-kafka     | 	confluent.security.event.logger.detailed.audit.logs.filter.class = class org.apache.kafka.common.requests.DetailedRequestAuditLogFilter
benchi-kafka     | 	confluent.security.event.logger.enable = true
benchi-kafka     | 	confluent.security.event.logger.kafka.request.rate.limit = -1
benchi-kafka     | 	confluent.security.event.logger.physical.cluster.id = 
benchi-kafka     | 	confluent.security.event.router.config = 
benchi-kafka     | 	confluent.security.revoked.certificate.ids = 
benchi-kafka     | 	confluent.segment.eager.roll.enable = false
benchi-kafka     | 	confluent.segment.speculative.prefetch.enable = false
benchi-kafka     | 	confluent.spiffe.id.principal.extraction.rules = 
benchi-kafka     | 	confluent.ssl.key.password = null
benchi-kafka     | 	confluent.ssl.keystore.location = null
benchi-kafka     | 	confluent.ssl.keystore.password = null
benchi-kafka     | 	confluent.ssl.keystore.type = null
benchi-kafka     | 	confluent.ssl.protocol = null
benchi-kafka     | 	confluent.ssl.truststore.location = null
benchi-kafka     | 	confluent.ssl.truststore.password = null
benchi-kafka     | 	confluent.ssl.truststore.type = null
benchi-kafka     | 	confluent.step.connection.rate.per.ip = -1.0
benchi-kafka     | 	confluent.step.connection.rate.per.tenant = -1.0
benchi-kafka     | 	confluent.storage.probe.period.ms = -1
benchi-kafka     | 	confluent.storage.probe.slow.write.threshold.ms = 5000
benchi-kafka     | 	confluent.stray.log.delete.delay.ms = 604800000
benchi-kafka     | 	confluent.stray.log.max.deletions.per.run = 72
benchi-kafka     | 	confluent.subdomain.prefix = null
benchi-kafka     | 	confluent.subdomain.separator.map = null
benchi-kafka     | 	confluent.subdomain.separator.variable = %sep
benchi-kafka     | 	confluent.system.time.roll.enable = false
benchi-kafka     | 	confluent.telemetry.enabled = false
benchi-kafka     | 	confluent.telemetry.external.client.metrics.push.enabled = false
benchi-kafka     | 	confluent.tenant.latency.metric.enabled = false
benchi-kafka     | 	confluent.tier.archiver.num.threads = 2
benchi-kafka     | 	confluent.tier.azure.block.blob.auto.abort.threshold.bytes = 500000
benchi-kafka     | 	confluent.tier.azure.block.blob.container = null
benchi-kafka     | 	confluent.tier.azure.block.blob.cred.file.path = null
benchi-kafka     | 	confluent.tier.azure.block.blob.endpoint = null
benchi-kafka     | 	confluent.tier.azure.block.blob.prefix = 
benchi-kafka     | 	confluent.tier.backend = 
benchi-kafka     | 	confluent.tier.bucket.probe.period.ms = -1
benchi-kafka     | 	confluent.tier.cleaner.compact.min.efficiency = 0.5
benchi-kafka     | 	confluent.tier.cleaner.compact.segment.min.bytes = 20971520
benchi-kafka     | 	confluent.tier.cleaner.dedupe.buffer.size = 134217728
benchi-kafka     | 	confluent.tier.cleaner.dual.compaction = false
benchi-kafka     | 	confluent.tier.cleaner.dual.compaction.validation.max.bytes = 1073741824
benchi-kafka     | 	confluent.tier.cleaner.dual.compaction.validation.percent = 0
benchi-kafka     | 	confluent.tier.cleaner.enable = false
benchi-kafka     | 	confluent.tier.cleaner.excluded.topics = [^_confluent.*]
benchi-kafka     | 	confluent.tier.cleaner.feature.enable = false
benchi-kafka     | 	confluent.tier.cleaner.io.buffer.load.factor = 0.9
benchi-kafka     | 	confluent.tier.cleaner.io.buffer.size = 10485760
benchi-kafka     | 	confluent.tier.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
benchi-kafka     | 	confluent.tier.cleaner.min.cleanable.ratio = 0.75
benchi-kafka     | 	confluent.tier.cleaner.num.threads = 2
benchi-kafka     | 	confluent.tier.enable = false
benchi-kafka     | 	confluent.tier.feature = false
benchi-kafka     | 	confluent.tier.fenced.segment.delete.delay.ms = 600000
benchi-kafka     | 	confluent.tier.fetcher.async.enable = false
benchi-kafka     | 	confluent.tier.fetcher.async.timestamp.offset.parallelism = 1
benchi-kafka     | 	confluent.tier.fetcher.fetch.based.on.segment_and_metadata_layout.field = false
benchi-kafka     | 	confluent.tier.fetcher.memorypool.bytes = 0
benchi-kafka     | 	confluent.tier.fetcher.num.threads = 4
benchi-kafka     | 	confluent.tier.fetcher.offset.cache.expiration.ms = 1800000
benchi-kafka     | 	confluent.tier.fetcher.offset.cache.period.ms = 60000
benchi-kafka     | 	confluent.tier.fetcher.offset.cache.size = 200000
benchi-kafka     | 	confluent.tier.gcs.bucket = null
benchi-kafka     | 	confluent.tier.gcs.cred.file.path = null
benchi-kafka     | 	confluent.tier.gcs.prefix = 
benchi-kafka     | 	confluent.tier.gcs.region = null
benchi-kafka     | 	confluent.tier.gcs.sse.customer.encryption.key = null
benchi-kafka     | 	confluent.tier.gcs.write.chunk.size = 0
benchi-kafka     | 	confluent.tier.local.hotset.bytes = -1
benchi-kafka     | 	confluent.tier.local.hotset.ms = 86400000
benchi-kafka     | 	confluent.tier.max.partition.fetch.bytes.override = 0
benchi-kafka     | 	confluent.tier.metadata.bootstrap.servers = null
benchi-kafka     | 	confluent.tier.metadata.max.poll.ms = 100
benchi-kafka     | 	confluent.tier.metadata.namespace = null
benchi-kafka     | 	confluent.tier.metadata.num.partitions = 50
benchi-kafka     | 	confluent.tier.metadata.replication.factor = 1
benchi-kafka     | 	confluent.tier.metadata.request.timeout.ms = 30000
benchi-kafka     | 	confluent.tier.metadata.snapshots.enable = false
benchi-kafka     | 	confluent.tier.metadata.snapshots.interval.ms = 86400000
benchi-kafka     | 	confluent.tier.metadata.snapshots.retention.days = 7
benchi-kafka     | 	confluent.tier.metadata.snapshots.threads = 2
benchi-kafka     | 	confluent.tier.object.fetcher.num.threads = 1
benchi-kafka     | 	confluent.tier.partition.state.cleanup.delay.ms = 2592000000
benchi-kafka     | 	confluent.tier.partition.state.cleanup.enable = false
benchi-kafka     | 	confluent.tier.partition.state.cleanup.interval.ms = 86400000
benchi-kafka     | 	confluent.tier.partition.state.commit.interval.ms = 15000
benchi-kafka     | 	confluent.tier.s3.assumerole.arn = null
benchi-kafka     | 	confluent.tier.s3.auto.abort.threshold.bytes = 500000
benchi-kafka     | 	confluent.tier.s3.aws.endpoint.override = null
benchi-kafka     | 	confluent.tier.s3.aws.signer.override = null
benchi-kafka     | 	confluent.tier.s3.bucket = null
benchi-kafka     | 	confluent.tier.s3.cred.file.path = null
benchi-kafka     | 	confluent.tier.s3.force.path.style.access = false
benchi-kafka     | 	confluent.tier.s3.prefix = 
benchi-kafka     | 	confluent.tier.s3.region = null
benchi-kafka     | 	confluent.tier.s3.security.providers = null
benchi-kafka     | 	confluent.tier.s3.sse.algorithm = AES256
benchi-kafka     | 	confluent.tier.s3.sse.customer.encryption.key = null
benchi-kafka     | 	confluent.tier.s3.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	confluent.tier.s3.ssl.key.password = null
benchi-kafka     | 	confluent.tier.s3.ssl.keystore.location = null
benchi-kafka     | 	confluent.tier.s3.ssl.keystore.password = null
benchi-kafka     | 	confluent.tier.s3.ssl.keystore.type = null
benchi-kafka     | 	confluent.tier.s3.ssl.protocol = TLSv1.3
benchi-kafka     | 	confluent.tier.s3.ssl.provider = null
benchi-kafka     | 	confluent.tier.s3.ssl.truststore.location = null
benchi-kafka     | 	confluent.tier.s3.ssl.truststore.password = null
benchi-kafka     | 	confluent.tier.s3.ssl.truststore.type = null
benchi-kafka     | 	confluent.tier.s3.user.agent.prefix = APN/1.0 Confluent/1.0 TieredStorageS3/1.0
benchi-kafka     | 	confluent.tier.segment.hotset.roll.min.bytes = 104857600
benchi-kafka     | 	confluent.tier.segment.metadata.layout.put.mode = LegacyMultiObject
benchi-kafka     | 	confluent.tier.topic.data.loss.validation.fencing.enable = false
benchi-kafka     | 	confluent.tier.topic.delete.backoff.ms = 21600000
benchi-kafka     | 	confluent.tier.topic.delete.check.interval.ms = 300000
benchi-kafka     | 	confluent.tier.topic.delete.max.inprogress.partitions = 100
benchi-kafka     | 	confluent.tier.topic.head.data.loss.validation.enable = true
benchi-kafka     | 	confluent.tier.topic.head.data.loss.validation.max.timeout.ms = 900000
benchi-kafka     | 	confluent.tier.topic.materialization.from.snapshot.enable = false
benchi-kafka     | 	confluent.tier.topic.producer.enable.idempotence = true
benchi-kafka     | 	confluent.tier.topic.snapshots.enable = false
benchi-kafka     | 	confluent.tier.topic.snapshots.interval.ms = 300000
benchi-kafka     | 	confluent.tier.topic.snapshots.max.records = 100000
benchi-kafka     | 	confluent.tier.topic.snapshots.retention.hours = 168
benchi-kafka     | 	confluent.topic.partition.default.placement = 2
benchi-kafka     | 	confluent.topic.policy.use.computed.assignments = false
benchi-kafka     | 	confluent.topic.replica.assignor.builder.class = 
benchi-kafka     | 	confluent.track.api.key.per.ip = false
benchi-kafka     | 	confluent.track.per.ip.max.size = 100000
benchi-kafka     | 	confluent.track.tenant.id.per.ip = false
benchi-kafka     | 	confluent.traffic.cdc.network.id.routes.enable = false
benchi-kafka     | 	confluent.traffic.cdc.network.id.routes.listener.names = EXTERNAL_BACKCHANNEL
benchi-kafka     | 	confluent.traffic.cdc.network.id.routes.periodic.start.task.ms = 300000
benchi-kafka     | 	confluent.traffic.cdc.network.id.routes.topic.name = _confluent-network_id_routes
benchi-kafka     | 	confluent.traffic.network.id = 
benchi-kafka     | 	confluent.transaction.2pc.timeout.ms = -1
benchi-kafka     | 	confluent.transaction.logging.verbosity = 0
benchi-kafka     | 	confluent.transaction.state.log.placement.constraints = 
benchi-kafka     | 	confluent.valid.broker.rack.set = null
benchi-kafka     | 	confluent.verify.group.subscription.prefix = false
benchi-kafka     | 	confluent.virtual.topic.creation.enabled = false
benchi-kafka     | 	confluent.zookeeper.metadata.migration.controller.check.disable = false
benchi-kafka     | 	confluent.zookeeper.metadata.migration.trigger.file.path = null
benchi-kafka     | 	connection.failed.authentication.delay.ms = 100
benchi-kafka     | 	connection.min.expire.interval.ms = 250
benchi-kafka     | 	connections.max.age.ms = 3153600000000
benchi-kafka     | 	connections.max.idle.ms = 600000
benchi-kafka     | 	connections.max.reauth.ms = 0
benchi-kafka     | 	control.plane.listener.name = null
benchi-kafka     | 	controlled.shutdown.enable = true
benchi-kafka     | 	controlled.shutdown.max.retries = 3
benchi-kafka     | 	controlled.shutdown.retry.backoff.ms = 5000
benchi-kafka     | 	controller.listener.names = CONTROLLER
benchi-kafka     | 	controller.quorum.append.linger.ms = 25
benchi-kafka     | 	controller.quorum.bootstrap.servers = []
benchi-kafka     | 	controller.quorum.election.backoff.max.ms = 1000
benchi-kafka     | 	controller.quorum.election.timeout.ms = 1000
benchi-kafka     | 	controller.quorum.fetch.timeout.ms = 2000
benchi-kafka     | 	controller.quorum.request.timeout.ms = 2000
benchi-kafka     | 	controller.quorum.retry.backoff.ms = 20
benchi-kafka     | 	controller.quorum.voters = [1@broker:29093]
benchi-kafka     | 	controller.quota.window.num = 11
benchi-kafka     | 	controller.quota.window.size.seconds = 1
benchi-kafka     | 	controller.socket.timeout.ms = 30000
benchi-kafka     | 	create.cluster.link.policy.class.name = null
benchi-kafka     | 	create.topic.policy.class.name = null
benchi-kafka     | 	default.replication.factor = 1
benchi-kafka     | 	delegation.token.expiry.check.interval.ms = 3600000
benchi-kafka     | 	delegation.token.expiry.time.ms = 86400000
benchi-kafka     | 	delegation.token.master.key = null
benchi-kafka     | 	delegation.token.max.lifetime.ms = 604800000
benchi-kafka     | 	delegation.token.secret.key = null
benchi-kafka     | 	delete.records.purgatory.purge.interval.requests = 1
benchi-kafka     | 	delete.topic.enable = true
benchi-kafka     | 	early.start.listeners = null
benchi-kafka     | 	eligible.leader.replicas.enable = false
benchi-kafka     | 	enable.fips = false
benchi-kafka     | 	fetch.max.bytes = 57671680
benchi-kafka     | 	fetch.purgatory.purge.interval.requests = 1000
benchi-kafka     | 	floor.max.connection.creation.rate = null
benchi-kafka     | 	follower.replication.throttled.rate = 9223372036854775807
benchi-kafka     | 	follower.replication.throttled.replicas = none
benchi-kafka     | 	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
benchi-kafka     | 	group.consumer.heartbeat.interval.ms = 5000
benchi-kafka     | 	group.consumer.max.heartbeat.interval.ms = 15000
benchi-kafka     | 	group.consumer.max.session.timeout.ms = 60000
benchi-kafka     | 	group.consumer.max.size = 2147483647
benchi-kafka     | 	group.consumer.migration.policy = disabled
benchi-kafka     | 	group.consumer.min.heartbeat.interval.ms = 5000
benchi-kafka     | 	group.consumer.min.session.timeout.ms = 45000
benchi-kafka     | 	group.consumer.session.timeout.ms = 45000
benchi-kafka     | 	group.coordinator.append.linger.ms = 10
benchi-kafka     | 	group.coordinator.new.enable = false
benchi-kafka     | 	group.coordinator.rebalance.protocols = [classic]
benchi-kafka     | 	group.coordinator.threads = 1
benchi-kafka     | 	group.initial.rebalance.delay.ms = 0
benchi-kafka     | 	group.max.session.timeout.ms = 1800000
benchi-kafka     | 	group.max.size = 2147483647
benchi-kafka     | 	group.min.session.timeout.ms = 6000
benchi-kafka     | 	initial.broker.registration.timeout.ms = 60000
benchi-kafka     | 	inter.broker.listener.name = PLAINTEXT
benchi-kafka     | 	inter.broker.protocol.version = 3.8-IV0A
benchi-kafka     | 	k2.stack.builder.class.name = null
benchi-kafka     | 	k2.startup.timeout.ms = 60000
benchi-kafka     | 	k2.topic.metadata.max.total.partition.count = 20000
benchi-kafka     | 	k2.topic.metadata.refresh.ms = 10000
benchi-kafka     | 	kafka.metrics.polling.interval.secs = 10
benchi-kafka     | 	kafka.metrics.reporters = []
benchi-kafka     | 	leader.imbalance.check.interval.seconds = 300
benchi-kafka     | 	leader.imbalance.per.broker.percentage = 10
benchi-kafka     | 	leader.replication.throttled.rate = 9223372036854775807
benchi-kafka     | 	leader.replication.throttled.replicas = none
benchi-kafka     | 	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
benchi-kafka     | 	listeners = PLAINTEXT://broker:29092,CONTROLLER://broker:29093,PLAINTEXT_HOST://0.0.0.0:9092
benchi-kafka     | 	log.cleaner.backoff.ms = 15000
benchi-kafka     | 	log.cleaner.dedupe.buffer.size = 134217728
benchi-kafka     | 	log.cleaner.delete.retention.ms = 86400000
benchi-kafka     | 	log.cleaner.enable = true
benchi-kafka     | 	log.cleaner.hash.algorithm = MD5
benchi-kafka     | 	log.cleaner.io.buffer.load.factor = 0.9
benchi-kafka     | 	log.cleaner.io.buffer.size = 524288
benchi-kafka     | 	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
benchi-kafka     | 	log.cleaner.max.compaction.lag.ms = 9223372036854775807
benchi-kafka     | 	log.cleaner.min.cleanable.ratio = 0.5
benchi-kafka     | 	log.cleaner.min.compaction.lag.ms = 0
benchi-kafka     | 	log.cleaner.threads = 1
benchi-kafka     | 	log.cleanup.policy = [delete]
benchi-kafka     | 	log.deletion.max.segments.per.run = 2147483647
benchi-kafka     | 	log.deletion.throttler.disk.free.headroom.bytes = 21474836480
benchi-kafka     | 	log.dir = /tmp/kafka-logs
benchi-kafka     | 	log.dir.failure.timeout.ms = 30000
benchi-kafka     | 	log.dirs = /tmp/kraft-combined-logs
benchi-kafka     | 	log.flush.interval.messages = 9223372036854775807
benchi-kafka     | 	log.flush.interval.ms = null
benchi-kafka     | 	log.flush.offset.checkpoint.interval.ms = 60000
benchi-kafka     | 	log.flush.scheduler.interval.ms = 9223372036854775807
benchi-kafka     | 	log.flush.start.offset.checkpoint.interval.ms = 60000
benchi-kafka     | 	log.index.interval.bytes = 4096
benchi-kafka     | 	log.index.size.max.bytes = 10485760
benchi-kafka     | 	log.initial.task.delay.ms = 30000
benchi-kafka     | 	log.local.retention.bytes = -2
benchi-kafka     | 	log.local.retention.ms = -2
benchi-kafka     | 	log.message.downconversion.enable = true
benchi-kafka     | 	log.message.format.version = 3.0-IV1
benchi-kafka     | 	log.message.timestamp.after.max.ms = 9223372036854775807
benchi-kafka     | 	log.message.timestamp.before.max.ms = 9223372036854775807
benchi-kafka     | 	log.message.timestamp.difference.max.ms = 9223372036854775807
benchi-kafka     | 	log.message.timestamp.type = CreateTime
benchi-kafka     | 	log.preallocate = false
benchi-kafka     | 	log.retention.bytes = -1
benchi-kafka     | 	log.retention.check.interval.ms = 300000
benchi-kafka     | 	log.retention.hours = 168
benchi-kafka     | 	log.retention.minutes = null
benchi-kafka     | 	log.retention.ms = null
benchi-kafka     | 	log.roll.hours = 168
benchi-kafka     | 	log.roll.jitter.hours = 0
benchi-kafka     | 	log.roll.jitter.ms = null
benchi-kafka     | 	log.roll.ms = null
benchi-kafka     | 	log.segment.bytes = 1073741824
benchi-kafka     | 	log.segment.delete.delay.ms = 60000
benchi-kafka     | 	max.connection.creation.rate = 1.7976931348623157E308
benchi-kafka     | 	max.connection.creation.rate.per.ip.enable.threshold = 0.0
benchi-kafka     | 	max.connection.creation.rate.per.tenant.enable.threshold = 0.0
benchi-kafka     | 	max.connections = 2147483647
benchi-kafka     | 	max.connections.per.ip = 2147483647
benchi-kafka     | 	max.connections.per.ip.overrides = 
benchi-kafka     | 	max.connections.per.tenant = 0
benchi-kafka     | 	max.connections.protected.listeners = []
benchi-kafka     | 	max.connections.reap.amount = 0
benchi-kafka     | 	max.incremental.fetch.session.cache.slots = 1000
benchi-kafka     | 	max.request.partition.size.limit = 2000
benchi-kafka     | 	message.max.bytes = 1048588
benchi-kafka     | 	metadata.log.dir = null
benchi-kafka     | 	metadata.log.max.record.bytes.between.snapshots = 20971520
benchi-kafka     | 	metadata.log.max.snapshot.interval.ms = 3600000
benchi-kafka     | 	metadata.log.segment.bytes = 1073741824
benchi-kafka     | 	metadata.log.segment.min.bytes = 8388608
benchi-kafka     | 	metadata.log.segment.ms = 604800000
benchi-kafka     | 	metadata.max.idle.interval.ms = 500
benchi-kafka     | 	metadata.max.retention.bytes = 104857600
benchi-kafka     | 	metadata.max.retention.ms = 604800000
benchi-kafka     | 	metric.reporters = [io.confluent.metrics.reporter.ConfluentMetricsReporter]
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.recording.level = INFO
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	min.insync.replicas = 1
benchi-kafka     | 	multitenant.authorizer.support.resource.ids = false
benchi-kafka     | 	multitenant.metadata.class = null
benchi-kafka     | 	multitenant.metadata.dir = null
benchi-kafka     | 	multitenant.metadata.reload.delay.ms = 120000
benchi-kafka     | 	multitenant.metadata.ssl.certs.path = null
benchi-kafka     | 	multitenant.tenant.delete.batch.size = 10
benchi-kafka     | 	multitenant.tenant.delete.check.ms = 120000
benchi-kafka     | 	multitenant.tenant.delete.delay = 604800000
benchi-kafka     | 	node.id = 1
benchi-kafka     | 	num.io.threads = 8
benchi-kafka     | 	num.network.threads = 3
benchi-kafka     | 	num.partitions = 1
benchi-kafka     | 	num.recovery.threads.per.data.dir = 1
benchi-kafka     | 	num.replica.alter.log.dirs.threads = null
benchi-kafka     | 	num.replica.fetchers = 1
benchi-kafka     | 	offset.metadata.max.bytes = 4096
benchi-kafka     | 	offsets.commit.required.acks = -1
benchi-kafka     | 	offsets.commit.timeout.ms = 5000
benchi-kafka     | 	offsets.load.buffer.size = 5242880
benchi-kafka     | 	offsets.retention.check.interval.ms = 600000
benchi-kafka     | 	offsets.retention.minutes = 10080
benchi-kafka     | 	offsets.topic.compression.codec = 0
benchi-kafka     | 	offsets.topic.num.partitions = 50
benchi-kafka     | 	offsets.topic.replication.factor = 1
benchi-kafka     | 	offsets.topic.segment.bytes = 104857600
benchi-kafka     | 	otel.exporter.otlp.custom.endpoint = default
benchi-kafka     | 	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
benchi-kafka     | 	password.encoder.iterations = 4096
benchi-kafka     | 	password.encoder.key.length = 128
benchi-kafka     | 	password.encoder.keyfactory.algorithm = null
benchi-kafka     | 	password.encoder.old.secret = null
benchi-kafka     | 	password.encoder.secret = null
benchi-kafka     | 	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
benchi-kafka     | 	process.roles = [broker, controller]
benchi-kafka     | 	producer.id.expiration.check.interval.ms = 600000
benchi-kafka     | 	producer.id.expiration.ms = 86400000
benchi-kafka     | 	producer.purgatory.purge.interval.requests = 1000
benchi-kafka     | 	queued.max.request.bytes = -1
benchi-kafka     | 	queued.max.requests = 500
benchi-kafka     | 	quota.window.num = 11
benchi-kafka     | 	quota.window.size.seconds = 1
benchi-kafka     | 	quotas.consumption.expiration.time.ms = 600000
benchi-kafka     | 	quotas.expiration.interval.ms = 3600000
benchi-kafka     | 	quotas.expiration.time.ms = 604800000
benchi-kafka     | 	quotas.lazy.evaluation.threshold = 0.5
benchi-kafka     | 	quotas.topic.append.timeout.ms = 5000
benchi-kafka     | 	quotas.topic.compression.codec = 3
benchi-kafka     | 	quotas.topic.load.buffer.size = 5242880
benchi-kafka     | 	quotas.topic.num.partitions = 50
benchi-kafka     | 	quotas.topic.placement.constraints = 
benchi-kafka     | 	quotas.topic.replication.factor = 3
benchi-kafka     | 	quotas.topic.segment.bytes = 104857600
benchi-kafka     | 	remote.fetch.max.wait.ms = 500
benchi-kafka     | 	remote.log.index.file.cache.total.size.bytes = 1073741824
benchi-kafka     | 	remote.log.manager.copier.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.copy.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.copy.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.expiration.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.fetch.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.fetch.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.task.interval.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.max.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.ms = 500
benchi-kafka     | 	remote.log.manager.task.retry.jitter = 0.2
benchi-kafka     | 	remote.log.manager.thread.pool.size = 10
benchi-kafka     | 	remote.log.metadata.custom.metadata.max.bytes = 128
benchi-kafka     | 	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
benchi-kafka     | 	remote.log.metadata.manager.class.path = null
benchi-kafka     | 	remote.log.metadata.manager.impl.prefix = rlmm.config.
benchi-kafka     | 	remote.log.metadata.manager.listener.name = null
benchi-kafka     | 	remote.log.reader.max.pending.tasks = 100
benchi-kafka     | 	remote.log.reader.threads = 10
benchi-kafka     | 	remote.log.storage.manager.class.name = null
benchi-kafka     | 	remote.log.storage.manager.class.path = null
benchi-kafka     | 	remote.log.storage.manager.impl.prefix = rsm.config.
benchi-kafka     | 	remote.log.storage.system.enable = false
benchi-kafka     | 	replica.fetch.backoff.ms = 1000
benchi-kafka     | 	replica.fetch.max.bytes = 1048576
benchi-kafka     | 	replica.fetch.min.bytes = 1
benchi-kafka     | 	replica.fetch.response.max.bytes = 10485760
benchi-kafka     | 	replica.fetch.wait.max.ms = 500
benchi-kafka     | 	replica.high.watermark.checkpoint.interval.ms = 5000
benchi-kafka     | 	replica.lag.time.max.ms = 30000
benchi-kafka     | 	replica.selector.class = null
benchi-kafka     | 	replica.socket.receive.buffer.bytes = 65536
benchi-kafka     | 	replica.socket.timeout.ms = 30000
benchi-kafka     | 	replication.quota.window.num = 11
benchi-kafka     | 	replication.quota.window.size.seconds = 1
benchi-kafka     | 	request.timeout.ms = 30000
benchi-kafka     | 	reserved.broker.max.id = 1000
benchi-kafka     | 	sasl.client.callback.handler.class = null
benchi-kafka     | 	sasl.enabled.mechanisms = [GSSAPI]
benchi-kafka     | 	sasl.jaas.config = null
benchi-kafka     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
benchi-kafka     | 	sasl.kerberos.min.time.before.relogin = 60000
benchi-kafka     | 	sasl.kerberos.principal.to.local.rules = [DEFAULT]
benchi-kafka     | 	sasl.kerberos.service.name = null
benchi-kafka     | 	sasl.kerberos.ticket.renew.jitter = 0.05
benchi-kafka     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
benchi-kafka     | 	sasl.login.callback.handler.class = null
benchi-kafka     | 	sasl.login.class = null
benchi-kafka     | 	sasl.login.connect.timeout.ms = null
benchi-kafka     | 	sasl.login.read.timeout.ms = null
benchi-kafka     | 	sasl.login.refresh.buffer.seconds = 300
benchi-kafka     | 	sasl.login.refresh.min.period.seconds = 60
benchi-kafka     | 	sasl.login.refresh.window.factor = 0.8
benchi-kafka     | 	sasl.login.refresh.window.jitter = 0.05
benchi-kafka     | 	sasl.login.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.login.retry.backoff.ms = 100
benchi-kafka     | 	sasl.mechanism.controller.protocol = GSSAPI
benchi-kafka     | 	sasl.mechanism.inter.broker.protocol = GSSAPI
benchi-kafka     | 	sasl.oauthbearer.clientassertion.audience = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.expiration = 5
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.jti.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.nbf.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.issuer = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.location = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key.passphrase = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.subject = null
benchi-kafka     | 	sasl.oauthbearer.clock.skew.seconds = 30
benchi-kafka     | 	sasl.oauthbearer.expected.audience = null
benchi-kafka     | 	sasl.oauthbearer.expected.issuer = null
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.url = null
benchi-kafka     | 	sasl.oauthbearer.scope.claim.name = scope
benchi-kafka     | 	sasl.oauthbearer.sub.claim.name = sub
benchi-kafka     | 	sasl.oauthbearer.token.endpoint.url = null
benchi-kafka     | 	sasl.server.authn.async.enable = false
benchi-kafka     | 	sasl.server.authn.async.max.threads = 1
benchi-kafka     | 	sasl.server.authn.async.timeout.ms = 30000
benchi-kafka     | 	sasl.server.callback.handler.class = null
benchi-kafka     | 	sasl.server.max.receive.size = 524288
benchi-kafka     | 	security.inter.broker.protocol = PLAINTEXT
benchi-kafka     | 	security.providers = null
benchi-kafka     | 	server.max.startup.time.ms = 9223372036854775807
benchi-kafka     | 	socket.connection.setup.timeout.max.ms = 30000
benchi-kafka     | 	socket.connection.setup.timeout.ms = 10000
benchi-kafka     | 	socket.listen.backlog.size = 50
benchi-kafka     | 	socket.receive.buffer.bytes = 102400
benchi-kafka     | 	socket.request.max.bytes = 104857600
benchi-kafka     | 	socket.send.buffer.bytes = 102400
benchi-kafka     | 	ssl.allow.dn.changes = false
benchi-kafka     | 	ssl.allow.san.changes = false
benchi-kafka     | 	ssl.cipher.suites = []
benchi-kafka     | 	ssl.client.auth = none
benchi-kafka     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	ssl.engine.factory.class = null
benchi-kafka     | 	ssl.key.password = null
benchi-kafka     | 	ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	ssl.keystore.certificate.chain = null
benchi-kafka     | 	ssl.keystore.key = null
benchi-kafka     | 	ssl.keystore.location = null
benchi-kafka     | 	ssl.keystore.password = null
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.principal.mapping.rules = DEFAULT
benchi-kafka     | 	ssl.protocol = TLSv1.3
benchi-kafka     | 	ssl.provider = null
benchi-kafka     | 	ssl.secure.random.implementation = null
benchi-kafka     | 	ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	ssl.truststore.certificates = null
benchi-kafka     | 	ssl.truststore.location = null
benchi-kafka     | 	ssl.truststore.password = null
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     | 	telemetry.max.bytes = 1048576
benchi-kafka     | 	throughput.quota.window.num = 11
benchi-kafka     | 	token.impersonation.validation = true
benchi-kafka     | 	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
benchi-kafka     | 	transaction.max.timeout.ms = 900000
benchi-kafka     | 	transaction.metadata.load.threads = 32
benchi-kafka     | 	transaction.partition.verification.enable = true
benchi-kafka     | 	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
benchi-kafka     | 	transaction.state.log.load.buffer.size = 5242880
benchi-kafka     | 	transaction.state.log.min.isr = 1
benchi-kafka     | 	transaction.state.log.num.partitions = 50
benchi-kafka     | 	transaction.state.log.replication.factor = 1
benchi-kafka     | 	transaction.state.log.segment.bytes = 104857600
benchi-kafka     | 	transactional.id.expiration.ms = 604800000
benchi-kafka     | 	unclean.leader.election.enable = false
benchi-kafka     | 	unstable.api.versions.enable = false
benchi-kafka     | 	unstable.feature.versions.enable = false
benchi-kafka     | 	zookeeper.acl.change.notification.expiration.ms = 900000
benchi-kafka     | 	zookeeper.clientCnxnSocket = null
benchi-kafka     | 	zookeeper.connect = 
benchi-kafka     | 	zookeeper.connection.timeout.ms = null
benchi-kafka     | 	zookeeper.max.in.flight.requests = 10
benchi-kafka     | 	zookeeper.metadata.migration.enable = false
benchi-kafka     | 	zookeeper.metadata.migration.min.batch.size = 200
benchi-kafka     | 	zookeeper.session.timeout.ms = 18000
benchi-kafka     | 	zookeeper.set.acl = false
benchi-kafka     | 	zookeeper.ssl.cipher.suites = null
benchi-kafka     | 	zookeeper.ssl.client.enable = false
benchi-kafka     | 	zookeeper.ssl.crl.enable = false
benchi-kafka     | 	zookeeper.ssl.enabled.protocols = null
benchi-kafka     | 	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
benchi-kafka     | 	zookeeper.ssl.keystore.location = null
benchi-kafka     | 	zookeeper.ssl.keystore.password = null
benchi-kafka     | 	zookeeper.ssl.keystore.type = null
benchi-kafka     | 	zookeeper.ssl.ocsp.enable = false
benchi-kafka     | 	zookeeper.ssl.protocol = TLSv1.2
benchi-kafka     | 	zookeeper.ssl.truststore.location = null
benchi-kafka     | 	zookeeper.ssl.truststore.password = null
benchi-kafka     | 	zookeeper.ssl.truststore.type = null
benchi-kafka     |  (kafka.server.KafkaConfig)
benchi-kafka     | [2025-03-18 16:05:45,539] INFO RemoteLogManagerConfig values: 
benchi-kafka     | 	log.local.retention.bytes = -2
benchi-kafka     | 	log.local.retention.ms = -2
benchi-kafka     | 	remote.fetch.max.wait.ms = 500
benchi-kafka     | 	remote.log.index.file.cache.total.size.bytes = 1073741824
benchi-kafka     | 	remote.log.manager.copier.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.copy.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.copy.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.expiration.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.fetch.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.fetch.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.task.interval.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.max.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.ms = 500
benchi-kafka     | 	remote.log.manager.task.retry.jitter = 0.2
benchi-kafka     | 	remote.log.manager.thread.pool.size = 10
benchi-kafka     | 	remote.log.metadata.custom.metadata.max.bytes = 128
benchi-kafka     | 	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
benchi-kafka     | 	remote.log.metadata.manager.class.path = null
benchi-kafka     | 	remote.log.metadata.manager.impl.prefix = rlmm.config.
benchi-kafka     | 	remote.log.metadata.manager.listener.name = null
benchi-kafka     | 	remote.log.reader.max.pending.tasks = 100
benchi-kafka     | 	remote.log.reader.threads = 10
benchi-kafka     | 	remote.log.storage.manager.class.name = null
benchi-kafka     | 	remote.log.storage.manager.class.path = null
benchi-kafka     | 	remote.log.storage.manager.impl.prefix = rsm.config.
benchi-kafka     | 	remote.log.storage.system.enable = false
benchi-kafka     |  (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
benchi-kafka     | [2025-03-18 16:05:45,551] INFO Registering metric ActiveBalancerCount (io.confluent.databalancer.KafkaDataBalanceManager)
benchi-kafka     | [2025-03-18 16:05:45,565] INFO Instantiating ClusterBalanceManager with an instance of io.confluent.databalancer.SbcDataBalanceManager (ClusterBalanceManager)
benchi-kafka     | [2025-03-18 16:05:45,566] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)
benchi-kafka     | [2025-03-18 16:05:45,567] INFO [ControllerServer id=1] Starting controller (kafka.server.ControllerServer)
benchi-kafka     | [2025-03-18 16:05:45,568] INFO RemoteLogManagerConfig values: 
benchi-kafka     | 	log.local.retention.bytes = -2
benchi-kafka     | 	log.local.retention.ms = -2
benchi-kafka     | 	remote.fetch.max.wait.ms = 500
benchi-kafka     | 	remote.log.index.file.cache.total.size.bytes = 1073741824
benchi-kafka     | 	remote.log.manager.copier.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.copy.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.copy.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.expiration.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.fetch.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.fetch.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.task.interval.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.max.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.ms = 500
benchi-kafka     | 	remote.log.manager.task.retry.jitter = 0.2
benchi-kafka     | 	remote.log.manager.thread.pool.size = 10
benchi-kafka     | 	remote.log.metadata.custom.metadata.max.bytes = 128
benchi-kafka     | 	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
benchi-kafka     | 	remote.log.metadata.manager.class.path = null
benchi-kafka     | 	remote.log.metadata.manager.impl.prefix = rlmm.config.
benchi-kafka     | 	remote.log.metadata.manager.listener.name = null
benchi-kafka     | 	remote.log.reader.max.pending.tasks = 100
benchi-kafka     | 	remote.log.reader.threads = 10
benchi-kafka     | 	remote.log.storage.manager.class.name = null
benchi-kafka     | 	remote.log.storage.manager.class.path = null
benchi-kafka     | 	remote.log.storage.manager.impl.prefix = rsm.config.
benchi-kafka     | 	remote.log.storage.system.enable = false
benchi-kafka     |  (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
benchi-kafka     | [2025-03-18 16:05:45,570] INFO [ControllerServer id=1] FIPS mode enabled: false (kafka.server.ControllerServer)
benchi-kafka     | [2025-03-18 16:05:45,574] INFO AuditLogConfig values: 
benchi-kafka     | 	confluent.security.event.logger.authentication.enable = false
benchi-kafka     | 	confluent.security.event.logger.authentication.event.rate.limit = -1
benchi-kafka     | 	confluent.security.event.logger.authorization.event.rate.limit = -1
benchi-kafka     | 	confluent.security.event.logger.cloudevent.codec = structured
benchi-kafka     | 	confluent.security.event.logger.enable = true
benchi-kafka     | 	confluent.security.event.logger.exporter.class = class io.confluent.security.audit.telemetry.exporter.NonBlockingKafkaExporter
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.create = true
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.partitions = 12
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.replicas = 3
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.retention.bytes = -1
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.retention.ms = 2592000000
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.roll.ms = 14400000
benchi-kafka     | 	confluent.security.event.logger.kafka.request.rate.limit = -1
benchi-kafka     | 	confluent.security.event.logger.physical.cluster.id = 
benchi-kafka     | 	confluent.security.event.router.cache.entries = 10000
benchi-kafka     | 	confluent.security.event.router.config = 
benchi-kafka     | 	confluent.security.event.router.named.config.enabled = false
benchi-kafka     |  (io.confluent.security.audit.AuditLogConfig)
benchi-kafka     | [2025-03-18 16:05:45,575] INFO CrnAuthorityConfig values: 
benchi-kafka     | 	confluent.authorizer.authority.cache.entries = 10000
benchi-kafka     | 	confluent.authorizer.authority.name = 
benchi-kafka     | 	confluent.metadata.server.api.flavor = CP
benchi-kafka     |  (io.confluent.crn.CrnAuthorityConfig)
benchi-kafka     | [2025-03-18 16:05:45,576] INFO NamedConfigEnabled: false (io.confluent.security.audit.provider.ConfluentAuditLogProvider)
benchi-kafka     | [2025-03-18 16:05:45,576] INFO AuditLogConfig values: 
benchi-kafka     | 	confluent.security.event.logger.authentication.enable = false
benchi-kafka     | 	confluent.security.event.logger.authentication.event.rate.limit = -1
benchi-kafka     | 	confluent.security.event.logger.authorization.event.rate.limit = -1
benchi-kafka     | 	confluent.security.event.logger.cloudevent.codec = structured
benchi-kafka     | 	confluent.security.event.logger.enable = true
benchi-kafka     | 	confluent.security.event.logger.exporter.class = class io.confluent.security.audit.telemetry.exporter.NonBlockingKafkaExporter
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.create = true
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.partitions = 12
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.replicas = 3
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.retention.bytes = -1
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.retention.ms = 2592000000
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.roll.ms = 14400000
benchi-kafka     | 	confluent.security.event.logger.kafka.request.rate.limit = -1
benchi-kafka     | 	confluent.security.event.logger.physical.cluster.id = 
benchi-kafka     | 	confluent.security.event.router.cache.entries = 10000
benchi-kafka     | 	confluent.security.event.router.config = 
benchi-kafka     | 	confluent.security.event.router.named.config.enabled = false
benchi-kafka     |  (io.confluent.security.audit.AuditLogConfig)
benchi-kafka     | [2025-03-18 16:05:45,583] INFO CrnAuthorityConfig values: 
benchi-kafka     | 	confluent.authorizer.authority.cache.entries = 10000
benchi-kafka     | 	confluent.authorizer.authority.name = 
benchi-kafka     | 	confluent.metadata.server.api.flavor = CP
benchi-kafka     |  (io.confluent.crn.CrnAuthorityConfig)
benchi-kafka     | [2025-03-18 16:05:45,584] INFO MultiTenantAuditLogConfig values: 
benchi-kafka     | 	confluent.security.event.logger.client.ip.enable = false
benchi-kafka     | 	confluent.security.event.logger.multitenant.enable = false
benchi-kafka     |  (io.confluent.kafka.multitenant.authorizer.MultiTenantAuditLogConfig)
benchi-kafka     | [2025-03-18 16:05:45,584] INFO AuditLogConfig values: 
benchi-kafka     | 	confluent.security.event.logger.authentication.enable = false
benchi-kafka     | 	confluent.security.event.logger.authentication.event.rate.limit = -1
benchi-kafka     | 	confluent.security.event.logger.authorization.event.rate.limit = -1
benchi-kafka     | 	confluent.security.event.logger.cloudevent.codec = structured
benchi-kafka     | 	confluent.security.event.logger.enable = true
benchi-kafka     | 	confluent.security.event.logger.exporter.class = class io.confluent.security.audit.telemetry.exporter.NonBlockingKafkaExporter
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.create = true
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.partitions = 12
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.replicas = 3
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.retention.bytes = -1
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.retention.ms = 2592000000
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.roll.ms = 14400000
benchi-kafka     | 	confluent.security.event.logger.kafka.request.rate.limit = -1
benchi-kafka     | 	confluent.security.event.logger.physical.cluster.id = 
benchi-kafka     | 	confluent.security.event.router.cache.entries = 10000
benchi-kafka     | 	confluent.security.event.router.config = 
benchi-kafka     | 	confluent.security.event.router.named.config.enabled = false
benchi-kafka     |  (io.confluent.security.audit.AuditLogConfig)
benchi-kafka     | [2025-03-18 16:05:45,584] INFO Audit Log rate limiter reconfigured: Authn: -1, Authz: -1, Kafka request: -1 (io.confluent.security.audit.provider.AuditLogRateLimiter)
benchi-kafka     | [2025-03-18 16:05:45,663] INFO [SocketServer listenerType=CONTROLLER, nodeId=1] Creating data-plane acceptor and processors for endpoint : ListenerName(CONTROLLER) (kafka.network.SocketServer)
benchi-kafka     | [2025-03-18 16:05:45,669] INFO Quota CONTROLLER-per-ip-connection-rate configured - (max: -1.0, floor: -1.0, adjustment: -1.0) (kafka.network.ListenerIpAutoTuningQuota)
benchi-kafka     | [2025-03-18 16:05:45,670] INFO Quota CONTROLLER-per-tenant-connection-rate configured - (max: -1.0, floor: -1.0, adjustment: -1.0) (kafka.network.ListenerTenantAutoTuningQuota)
benchi-kafka     | [2025-03-18 16:05:45,670] INFO Quota CONTROLLER-connection-rate configured - (max: 1.7976931348623157E308, floor: 1.7976931348623157E308, adjustment: 5.0) (kafka.network.ListenerAutoTuningQuota)
benchi-kafka     | [2025-03-18 16:05:45,671] INFO Updated connection-tokens max connection creation rate to 1.7976931348623157E308 (kafka.network.ConnectionQuotas)
benchi-kafka     | [2025-03-18 16:05:45,679] INFO Config values: 
benchi-kafka     | 	confluent.security.event.logger.detailed.audit.logs.disabled.apis = 
benchi-kafka     | 	confluent.security.event.logger.enable.detailed.audit.logs = false
benchi-kafka     | 	confluent.security.event.logger.enable.produce.consume.audit.logs = false
benchi-kafka     |  (org.apache.kafka.common.requests.DetailedRequestAuditLogFilter$Config)
benchi-kafka     | [2025-03-18 16:05:45,708] INFO Config values: 
benchi-kafka     | 	confluent.security.event.logger.detailed.audit.logs.disabled.apis = 
benchi-kafka     | 	confluent.security.event.logger.enable.detailed.audit.logs = false
benchi-kafka     | 	confluent.security.event.logger.enable.produce.consume.audit.logs = false
benchi-kafka     |  (org.apache.kafka.common.requests.DetailedRequestAuditLogFilter$Config)
benchi-kafka     | [2025-03-18 16:05:45,710] INFO Config values: 
benchi-kafka     | 	confluent.security.event.logger.detailed.audit.logs.disabled.apis = 
benchi-kafka     | 	confluent.security.event.logger.enable.detailed.audit.logs = false
benchi-kafka     | 	confluent.security.event.logger.enable.produce.consume.audit.logs = false
benchi-kafka     |  (org.apache.kafka.common.requests.DetailedRequestAuditLogFilter$Config)
benchi-kafka     | [2025-03-18 16:05:45,711] INFO [SocketServer listenerType=CONTROLLER, nodeId=1] Created data-plane acceptor and processors for endpoint : ListenerName(CONTROLLER) (kafka.network.SocketServer)
benchi-kafka     | [2025-03-18 16:05:45,715] INFO [SharedServer id=1] Using broker:29092 as bootstrap.servers for inter broker client config. (kafka.server.SharedServer)
benchi-kafka     | [2025-03-18 16:05:45,720] INFO [SharedServer id=1] Starting SharedServer (kafka.server.SharedServer)
benchi-kafka     | [2025-03-18 16:05:45,721] INFO RemoteLogManagerConfig values: 
benchi-kafka     | 	log.local.retention.bytes = -2
benchi-kafka     | 	log.local.retention.ms = -2
benchi-kafka     | 	remote.fetch.max.wait.ms = 500
benchi-kafka     | 	remote.log.index.file.cache.total.size.bytes = 1073741824
benchi-kafka     | 	remote.log.manager.copier.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.copy.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.copy.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.expiration.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.fetch.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.fetch.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.task.interval.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.max.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.ms = 500
benchi-kafka     | 	remote.log.manager.task.retry.jitter = 0.2
benchi-kafka     | 	remote.log.manager.thread.pool.size = 10
benchi-kafka     | 	remote.log.metadata.custom.metadata.max.bytes = 128
benchi-kafka     | 	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
benchi-kafka     | 	remote.log.metadata.manager.class.path = null
benchi-kafka     | 	remote.log.metadata.manager.impl.prefix = rlmm.config.
benchi-kafka     | 	remote.log.metadata.manager.listener.name = null
benchi-kafka     | 	remote.log.reader.max.pending.tasks = 100
benchi-kafka     | 	remote.log.reader.threads = 10
benchi-kafka     | 	remote.log.storage.manager.class.name = null
benchi-kafka     | 	remote.log.storage.manager.class.path = null
benchi-kafka     | 	remote.log.storage.manager.impl.prefix = rsm.config.
benchi-kafka     | 	remote.log.storage.system.enable = false
benchi-kafka     |  (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
benchi-kafka     | [2025-03-18 16:05:45,726] INFO RemoteLogManagerConfig values: 
benchi-kafka     | 	log.local.retention.bytes = -2
benchi-kafka     | 	log.local.retention.ms = -2
benchi-kafka     | 	remote.fetch.max.wait.ms = 500
benchi-kafka     | 	remote.log.index.file.cache.total.size.bytes = 1073741824
benchi-kafka     | 	remote.log.manager.copier.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.copy.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.copy.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.expiration.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.fetch.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.fetch.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.task.interval.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.max.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.ms = 500
benchi-kafka     | 	remote.log.manager.task.retry.jitter = 0.2
benchi-kafka     | 	remote.log.manager.thread.pool.size = 10
benchi-kafka     | 	remote.log.metadata.custom.metadata.max.bytes = 128
benchi-kafka     | 	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
benchi-kafka     | 	remote.log.metadata.manager.class.path = null
benchi-kafka     | 	remote.log.metadata.manager.impl.prefix = rlmm.config.
benchi-kafka     | 	remote.log.metadata.manager.listener.name = null
benchi-kafka     | 	remote.log.reader.max.pending.tasks = 100
benchi-kafka     | 	remote.log.reader.threads = 10
benchi-kafka     | 	remote.log.storage.manager.class.name = null
benchi-kafka     | 	remote.log.storage.manager.class.path = null
benchi-kafka     | 	remote.log.storage.manager.impl.prefix = rsm.config.
benchi-kafka     | 	remote.log.storage.system.enable = false
benchi-kafka     |  (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
benchi-kafka     | [2025-03-18 16:05:45,772] INFO [MergedLog partition=__cluster_metadata-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:05:45,772] INFO [MergedLog partition=__cluster_metadata-0, dir=/tmp/kraft-combined-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:05:45,772] INFO [MergedLog partition=__cluster_metadata-0, dir=/tmp/kraft-combined-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:05:45,776] INFO Initialized snapshots with IDs SortedSet() from /tmp/kraft-combined-logs/__cluster_metadata-0 (kafka.raft.KafkaMetadataLog$)
benchi-kafka     | [2025-03-18 16:05:45,781] INFO [raft-expiration-reaper]: Starting (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:05:45,785] INFO [RaftManager id=1] Reading KRaft snapshot and log as part of the initialization (org.apache.kafka.raft.KafkaRaftClient)
benchi-kafka     | [2025-03-18 16:05:45,786] INFO [RaftManager id=1] Starting request manager with static voters: [broker:29093 (id: 1 rack: null)] (org.apache.kafka.raft.KafkaRaftClient)
benchi-kafka     | [2025-03-18 16:05:45,798] INFO [RaftManager id=1] Completed transition to Unattached(epoch=0, voters=[1], electionTimeoutMs=1556) from null (org.apache.kafka.raft.QuorumState)
benchi-kafka     | [2025-03-18 16:05:45,802] INFO [RaftManager id=1] Completed transition to CandidateState(localId=1, localDirectoryId=v3Z8JLzy5QaOAeyEyCPGQA,epoch=1, retries=1, voteStates={1=GRANTED}, highWatermark=Optional.empty, electionTimeoutMs=1673) from Unattached(epoch=0, voters=[1], electionTimeoutMs=1556) (org.apache.kafka.raft.QuorumState)
benchi-kafka     | [2025-03-18 16:05:45,805] INFO [RaftManager id=1] Completed transition to Leader(localId=1, epoch=1, epochStartOffset=0, highWatermark=Optional.empty, voterStates={1=ReplicaState(nodeId=1, endOffset=Optional.empty, lastFetchTimestamp=-1, lastCaughtUpTimestamp=-1, hasAcknowledgedLeader=true)}) from CandidateState(localId=1, localDirectoryId=v3Z8JLzy5QaOAeyEyCPGQA,epoch=1, retries=1, voteStates={1=GRANTED}, highWatermark=Optional.empty, electionTimeoutMs=1673) (org.apache.kafka.raft.QuorumState)
benchi-kafka     | [2025-03-18 16:05:45,810] INFO [kafka-1-raft-outbound-request-thread]: Starting (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
benchi-kafka     | [2025-03-18 16:05:45,810] INFO [kafka-1-raft-io-thread]: Starting (org.apache.kafka.raft.KafkaRaftClientDriver)
benchi-kafka     | [2025-03-18 16:05:45,819] INFO [RaftManager id=1] High watermark set to LogOffsetMetadata(offset=1, metadata=Optional[(segmentBaseOffset=0,relativePositionInSegment=91)]) for the first time for epoch 1 based on indexOfHw 0 and voters [ReplicaState(nodeId=1, endOffset=Optional[LogOffsetMetadata(offset=1, metadata=Optional[(segmentBaseOffset=0,relativePositionInSegment=91)])], lastFetchTimestamp=-1, lastCaughtUpTimestamp=-1, hasAcknowledgedLeader=true)] (org.apache.kafka.raft.LeaderState)
benchi-kafka     | [2025-03-18 16:05:45,821] INFO [MetadataLoader id=1] initializeNewPublishers: The loader is still catching up because we have loaded up to offset -1, but the high water mark is 1 (org.apache.kafka.image.loader.MetadataLoader)
benchi-kafka     | [2025-03-18 16:05:45,822] INFO [ControllerServer id=1] Waiting for controller quorum voters future (kafka.server.ControllerServer)
benchi-kafka     | [2025-03-18 16:05:45,822] INFO [ControllerServer id=1] Finished waiting for controller quorum voters future (kafka.server.ControllerServer)
benchi-kafka     | [2025-03-18 16:05:45,826] INFO [RaftManager id=1] Registered the listener org.apache.kafka.image.loader.MetadataLoader@1038090933 (org.apache.kafka.raft.KafkaRaftClient)
benchi-kafka     | [2025-03-18 16:05:45,827] INFO [MetadataLoader id=1] maybePublishMetadata(LOG_DELTA): The loader is still catching up because we have not loaded a controller record as of offset 0 and high water mark is 1 (org.apache.kafka.image.loader.MetadataLoader)
benchi-kafka     | [2025-03-18 16:05:45,844] INFO [ControllerServer id=1] Creating new QuorumController with clusterId MkU3OEVBNTcwNTJENDM2Qk. (org.apache.kafka.controller.QuorumController)
benchi-kafka     | [2025-03-18 16:05:45,845] INFO [RaftManager id=1] Registered the listener org.apache.kafka.controller.QuorumController$QuorumMetaLogListener@2140704519 (org.apache.kafka.raft.KafkaRaftClient)
benchi-kafka     | [2025-03-18 16:05:45,846] INFO [ControllerServer id=1] Becoming the active controller at epoch 1, next write offset 1. (org.apache.kafka.controller.QuorumController)
benchi-kafka     | [2025-03-18 16:05:45,849] WARN [ControllerServer id=1] Performing controller activation. The metadata log appears to be empty. Appending 1 bootstrap record(s) in metadata transaction at metadata.version 3.8-IV0A from bootstrap source 'the binary bootstrap metadata file: /tmp/kraft-combined-logs/bootstrap.checkpoint'. Setting the ZK migration state to NONE since this is a de-novo KRaft cluster. (org.apache.kafka.controller.QuorumController)
benchi-kafka     | [2025-03-18 16:05:45,849] INFO [ControllerServer id=1] Replayed BeginTransactionRecord(name='Bootstrap records') at offset 1. (org.apache.kafka.controller.OffsetControlManager)
benchi-kafka     | [2025-03-18 16:05:45,849] INFO [ControllerServer id=1] Replayed a Confluent FeatureLevelRecord setting metadata version to 3.8-IV0A (org.apache.kafka.controller.FeatureControlManager)
benchi-kafka     | [2025-03-18 16:05:45,850] INFO [ControllerServer id=1] Replayed EndTransactionRecord() at offset 4. (org.apache.kafka.controller.OffsetControlManager)
benchi-kafka     | [2025-03-18 16:05:45,851] INFO Empty logDirs received! Disk based throttling won't be activated! (org.apache.kafka.server.config.DiskUsageBasedThrottlingConfig)
benchi-kafka     | [2025-03-18 16:05:45,851] INFO Empty logDirs received! Disk based throttling won't be activated! (org.apache.kafka.server.config.DiskUsageBasedThrottlingConfig)
benchi-kafka     | [2025-03-18 16:05:45,854] INFO [controller-1-ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:05:45,855] INFO [controller-1-ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:05:45,856] INFO Empty logDirs received! Disk based throttling won't be activated! (org.apache.kafka.server.config.DiskUsageBasedThrottlingConfig)
benchi-kafka     | [2025-03-18 16:05:45,856] INFO [controller-1-ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:05:45,857] INFO Client Quota Max Throttle Time is updated from 5000 to 1000 (kafka.server.ClientRequestQuotaManager)
benchi-kafka     | [2025-03-18 16:05:45,860] INFO Empty logDirs received! Disk based throttling won't be activated! (org.apache.kafka.server.config.DiskUsageBasedThrottlingConfig)
benchi-kafka     | [2025-03-18 16:05:45,861] INFO Client Quota Max Throttle Time is updated from 5000 to 1000 (kafka.server.ClusterLinkRequestQuotaManager)
benchi-kafka     | [2025-03-18 16:05:45,861] INFO [controller-1-ThrottledChannelReaper-ClusterLinkRequest]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:05:45,863] INFO [controller-1-ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:05:45,871] INFO [ExpirationReaper-1-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:05:45,873] INFO Config values: 
benchi-kafka     | 	confluent.request.log.api.samples.per.min = 
benchi-kafka     | 	confluent.request.log.enable.admin.apis = true
benchi-kafka     | 	confluent.request.log.enable.slowlog = false
benchi-kafka     | 	confluent.request.log.samples.per.min = 0
benchi-kafka     | 	confluent.request.slowlog.threshold.override = -1.0
benchi-kafka     | 	confluent.request.slowlog.threshold.p99.min = -1.0
benchi-kafka     |  (org.apache.kafka.common.requests.SamplingRequestLogFilter$Config)
benchi-kafka     | [2025-03-18 16:05:45,873] INFO Config values: 
benchi-kafka     | 	confluent.request.log.api.samples.per.min = 
benchi-kafka     | 	confluent.request.log.enable.admin.apis = true
benchi-kafka     | 	confluent.request.log.enable.slowlog = false
benchi-kafka     | 	confluent.request.log.samples.per.min = 0
benchi-kafka     | 	confluent.request.slowlog.threshold.override = -1.0
benchi-kafka     | 	confluent.request.slowlog.threshold.p99.min = -1.0
benchi-kafka     |  (org.apache.kafka.common.requests.SamplingRequestLogFilter$Config)
benchi-kafka     | [2025-03-18 16:05:45,874] INFO Config values: 
benchi-kafka     | 	confluent.request.log.api.samples.per.min = 
benchi-kafka     | 	confluent.request.log.enable.admin.apis = true
benchi-kafka     | 	confluent.request.log.enable.slowlog = false
benchi-kafka     | 	confluent.request.log.samples.per.min = 0
benchi-kafka     | 	confluent.request.slowlog.threshold.override = -1.0
benchi-kafka     | 	confluent.request.slowlog.threshold.p99.min = -1.0
benchi-kafka     |  (org.apache.kafka.common.requests.SamplingRequestLogFilter$Config)
benchi-kafka     | [2025-03-18 16:05:45,874] INFO Config values: 
benchi-kafka     | 	confluent.request.log.api.samples.per.min = 
benchi-kafka     | 	confluent.request.log.enable.admin.apis = true
benchi-kafka     | 	confluent.request.log.enable.slowlog = false
benchi-kafka     | 	confluent.request.log.samples.per.min = 0
benchi-kafka     | 	confluent.request.slowlog.threshold.override = -1.0
benchi-kafka     | 	confluent.request.slowlog.threshold.p99.min = -1.0
benchi-kafka     |  (org.apache.kafka.common.requests.SamplingRequestLogFilter$Config)
benchi-kafka     | [2025-03-18 16:05:45,874] INFO Config values: 
benchi-kafka     | 	confluent.request.log.api.samples.per.min = 
benchi-kafka     | 	confluent.request.log.enable.admin.apis = true
benchi-kafka     | 	confluent.request.log.enable.slowlog = false
benchi-kafka     | 	confluent.request.log.samples.per.min = 0
benchi-kafka     | 	confluent.request.slowlog.threshold.override = -1.0
benchi-kafka     | 	confluent.request.slowlog.threshold.p99.min = -1.0
benchi-kafka     |  (org.apache.kafka.common.requests.SamplingRequestLogFilter$Config)
benchi-kafka     | [2025-03-18 16:05:45,874] INFO Config values: 
benchi-kafka     | 	confluent.request.log.api.samples.per.min = 
benchi-kafka     | 	confluent.request.log.enable.admin.apis = true
benchi-kafka     | 	confluent.request.log.enable.slowlog = false
benchi-kafka     | 	confluent.request.log.samples.per.min = 0
benchi-kafka     | 	confluent.request.slowlog.threshold.override = -1.0
benchi-kafka     | 	confluent.request.slowlog.threshold.p99.min = -1.0
benchi-kafka     |  (org.apache.kafka.common.requests.SamplingRequestLogFilter$Config)
benchi-kafka     | [2025-03-18 16:05:45,875] INFO Config values: 
benchi-kafka     | 	confluent.request.log.api.samples.per.min = 
benchi-kafka     | 	confluent.request.log.enable.admin.apis = true
benchi-kafka     | 	confluent.request.log.enable.slowlog = false
benchi-kafka     | 	confluent.request.log.samples.per.min = 0
benchi-kafka     | 	confluent.request.slowlog.threshold.override = -1.0
benchi-kafka     | 	confluent.request.slowlog.threshold.p99.min = -1.0
benchi-kafka     |  (org.apache.kafka.common.requests.SamplingRequestLogFilter$Config)
benchi-kafka     | [2025-03-18 16:05:45,875] INFO Config values: 
benchi-kafka     | 	confluent.request.log.api.samples.per.min = 
benchi-kafka     | 	confluent.request.log.enable.admin.apis = true
benchi-kafka     | 	confluent.request.log.enable.slowlog = false
benchi-kafka     | 	confluent.request.log.samples.per.min = 0
benchi-kafka     | 	confluent.request.slowlog.threshold.override = -1.0
benchi-kafka     | 	confluent.request.slowlog.threshold.p99.min = -1.0
benchi-kafka     |  (org.apache.kafka.common.requests.SamplingRequestLogFilter$Config)
benchi-kafka     | [2025-03-18 16:05:45,880] INFO [MetadataLoader id=1] maybePublishMetadata(LOG_DELTA): The loader finished catching up to the current high water mark of 5 (org.apache.kafka.image.loader.MetadataLoader)
benchi-kafka     | [2025-03-18 16:05:45,881] INFO [ControllerServer id=1] Self-Balancing Kafka is enabled and will be installed as a metadata publisher. (kafka.server.ControllerServer)
benchi-kafka     | [2025-03-18 16:05:45,882] INFO [ControllerServer id=1] Waiting for the controller metadata publishers to be installed (kafka.server.ControllerServer)
benchi-kafka     | [2025-03-18 16:05:45,883] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing SnapshotGenerator with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)
benchi-kafka     | [2025-03-18 16:05:45,884] INFO [ControllerServer id=1] Finished waiting for the controller metadata publishers to be installed (kafka.server.ControllerServer)
benchi-kafka     | [2025-03-18 16:05:45,884] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing KRaftMetadataCachePublisher with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)
benchi-kafka     | [2025-03-18 16:05:45,884] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing FeaturesPublisher with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)
benchi-kafka     | [2025-03-18 16:05:45,884] INFO [SocketServer listenerType=CONTROLLER, nodeId=1] Enabling request processing. (kafka.network.SocketServer)
benchi-kafka     | [2025-03-18 16:05:45,884] INFO [ControllerServer id=1] Loaded new metadata Features(metadataVersion=3.8-IV0A, finalizedFeatures={confluent.metadata.version=120}, finalizedFeaturesEpoch=4). (org.apache.kafka.metadata.publisher.FeaturesPublisher)
benchi-kafka     | [2025-03-18 16:05:45,884] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing ControllerRegistrationsPublisher with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)
benchi-kafka     | [2025-03-18 16:05:45,884] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing ControllerRegistrationManager with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)
benchi-kafka     | [2025-03-18 16:05:45,884] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing DynamicConfigPublisher controller id=1 with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)
benchi-kafka     | [2025-03-18 16:05:45,884] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing DynamicClientQuotaPublisher controller id=1 with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)
benchi-kafka     | [2025-03-18 16:05:45,886] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing ScramPublisher controller id=1 with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)
benchi-kafka     | [2025-03-18 16:05:45,886] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing DelegationTokenPublisher controller id=1 with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)
benchi-kafka     | [2025-03-18 16:05:45,886] INFO Awaiting socket connections on broker:29093. (kafka.network.DataPlaneAcceptor)
benchi-kafka     | [2025-03-18 16:05:45,886] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing ControllerMetadataMetricsPublisher with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)
benchi-kafka     | [2025-03-18 16:05:45,886] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing ConfluentControllerMetricsPublisher with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)
benchi-kafka     | [2025-03-18 16:05:45,887] INFO [ConfluentControllerMetricsChanges id=1] Finished reloading all Confluent controller metrics in 0 ms. (org.apache.kafka.controller.metrics.ConfluentControllerMetricsChanges)
benchi-kafka     | [2025-03-18 16:05:45,887] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing CellControllerMetadataMetricsPublisher with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)
benchi-kafka     | [2025-03-18 16:05:45,887] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing SbcDataBalanceManager with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)
benchi-kafka     | [2025-03-18 16:05:45,887] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing AclPublisher controller id=1 with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)
benchi-kafka     | [2025-03-18 16:05:45,889] INFO Balancer received a new Replica Exclusions Image (image: , delta: BrokerReplicaExclusionsDelta{newExclusions=[], removedExclusions=[]}) (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:05:45,889] INFO Handling event SbcAlteredExclusionsEvent-4 (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:05:45,889] INFO SBC Event SbcMetadataUpdateEvent-1 generated 1 more events to enqueue in the following order - [SbcConfigUpdateEvent-3]. Enqueuing... (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:05:45,889] INFO Handling event SbcLeaderUpdateEvent-2 (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:05:45,889] INFO This balancer node is now the metadata quorum leader. Activating kafkadatabalance manager without alive broker snapshot. (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:05:45,889] INFO Handling event SbcConfigUpdateEvent-3 (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:05:45,890] INFO Cluster metadata containing at least one unfenced broker not yet available, SBC config processing delayed. (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:05:45,890] INFO Handling event SbcKraftStartupEvent-5 (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:05:45,890] INFO Cluster metadata containing at least one unfenced broker not yet available, SBC startup delayed. (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:05:45,890] INFO [controller-1-to-controller-registration-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
benchi-kafka     | [2025-03-18 16:05:45,890] INFO [ControllerServer id=1] Waiting for all of the authorizer futures to be completed (kafka.server.ControllerServer)
benchi-kafka     | [2025-03-18 16:05:45,890] INFO [ControllerServer id=1] Finished waiting for all of the authorizer futures to be completed (kafka.server.ControllerServer)
benchi-kafka     | [2025-03-18 16:05:45,890] INFO [ControllerServer id=1] Waiting for all of the SocketServer Acceptors to be started (kafka.server.ControllerServer)
benchi-kafka     | [2025-03-18 16:05:45,890] INFO [ControllerServer id=1] Finished waiting for all of the SocketServer Acceptors to be started (kafka.server.ControllerServer)
benchi-kafka     | [2025-03-18 16:05:45,890] INFO [ControllerRegistrationManager id=1 incarnation=g1LiT4BhQhCyUixJaN6eeg] initialized channel manager. (kafka.server.ControllerRegistrationManager)
benchi-kafka     | [2025-03-18 16:05:45,890] INFO [controller-1-to-controller-registration-channel-manager]: Recorded new KRaft controller, from now on will use node broker:29093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
benchi-kafka     | [2025-03-18 16:05:45,890] INFO [ControllerServer id=1] Waiting for userDeletionHandler futures to be completed (kafka.server.ControllerServer)
benchi-kafka     | [2025-03-18 16:05:45,890] INFO [ControllerServer id=1] Finished waiting for userDeletionHandler futures to be completed (kafka.server.ControllerServer)
benchi-kafka     | [2025-03-18 16:05:45,891] INFO [BrokerServer id=1] Transition from SHUTDOWN to STARTING (kafka.server.BrokerServer)
benchi-kafka     | [2025-03-18 16:05:45,891] INFO [ControllerRegistrationManager id=1 incarnation=g1LiT4BhQhCyUixJaN6eeg] sendControllerRegistration: attempting to send ControllerRegistrationRequestData(controllerId=1, incarnationId=g1LiT4BhQhCyUixJaN6eeg, zkMigrationReady=false, listeners=[Listener(name='CONTROLLER', host='broker', port=29093, securityProtocol=0)], features=[Feature(name='confluent.metadata.version', minSupportedVersion=1, maxSupportedVersion=120), Feature(name='metadata.version', minSupportedVersion=1, maxSupportedVersion=20)], metadataEncryptors=[]) (kafka.server.ControllerRegistrationManager)
benchi-kafka     | [2025-03-18 16:05:45,892] INFO [BrokerServer id=1] Starting broker (kafka.server.BrokerServer)
benchi-kafka     | [2025-03-18 16:05:45,893] INFO RemoteLogManagerConfig values: 
benchi-kafka     | 	log.local.retention.bytes = -2
benchi-kafka     | 	log.local.retention.ms = -2
benchi-kafka     | 	remote.fetch.max.wait.ms = 500
benchi-kafka     | 	remote.log.index.file.cache.total.size.bytes = 1073741824
benchi-kafka     | 	remote.log.manager.copier.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.copy.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.copy.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.expiration.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.fetch.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.fetch.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.task.interval.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.max.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.ms = 500
benchi-kafka     | 	remote.log.manager.task.retry.jitter = 0.2
benchi-kafka     | 	remote.log.manager.thread.pool.size = 10
benchi-kafka     | 	remote.log.metadata.custom.metadata.max.bytes = 128
benchi-kafka     | 	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
benchi-kafka     | 	remote.log.metadata.manager.class.path = null
benchi-kafka     | 	remote.log.metadata.manager.impl.prefix = rlmm.config.
benchi-kafka     | 	remote.log.metadata.manager.listener.name = null
benchi-kafka     | 	remote.log.reader.max.pending.tasks = 100
benchi-kafka     | 	remote.log.reader.threads = 10
benchi-kafka     | 	remote.log.storage.manager.class.name = null
benchi-kafka     | 	remote.log.storage.manager.class.path = null
benchi-kafka     | 	remote.log.storage.manager.impl.prefix = rsm.config.
benchi-kafka     | 	remote.log.storage.system.enable = false
benchi-kafka     |  (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
benchi-kafka     | [2025-03-18 16:05:45,899] INFO [BrokerServer id=1] FIPS mode enabled: false (kafka.server.BrokerServer)
benchi-kafka     | [2025-03-18 16:05:45,901] INFO Empty logDirs received! Disk based throttling won't be activated! (org.apache.kafka.server.config.DiskUsageBasedThrottlingConfig)
benchi-kafka     | [2025-03-18 16:05:45,902] INFO [broker-1-ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:05:45,902] INFO Empty logDirs received! Disk based throttling won't be activated! (org.apache.kafka.server.config.DiskUsageBasedThrottlingConfig)
benchi-kafka     | [2025-03-18 16:05:45,902] INFO [broker-1-ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:05:45,902] INFO [broker-1-ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:05:45,902] INFO Client Quota Max Throttle Time is updated from 5000 to 1000 (kafka.server.ClientRequestQuotaManager)
benchi-kafka     | [2025-03-18 16:05:45,903] INFO Empty logDirs received! Disk based throttling won't be activated! (org.apache.kafka.server.config.DiskUsageBasedThrottlingConfig)
benchi-kafka     | [2025-03-18 16:05:45,903] INFO Client Quota Max Throttle Time is updated from 5000 to 1000 (kafka.server.ClusterLinkRequestQuotaManager)
benchi-kafka     | [2025-03-18 16:05:45,903] INFO [broker-1-ThrottledChannelReaper-ClusterLinkRequest]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:05:45,903] INFO [broker-1-ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:05:45,913] INFO Skip DiskIOManager init: confluent.disk.io.manager.enable = false (kafka.server.resource.DiskIOManager)
benchi-kafka     | [2025-03-18 16:05:45,913] INFO AuditLogConfig values: 
benchi-kafka     | 	confluent.security.event.logger.authentication.enable = false
benchi-kafka     | 	confluent.security.event.logger.authentication.event.rate.limit = -1
benchi-kafka     | 	confluent.security.event.logger.authorization.event.rate.limit = -1
benchi-kafka     | 	confluent.security.event.logger.cloudevent.codec = structured
benchi-kafka     | 	confluent.security.event.logger.enable = true
benchi-kafka     | 	confluent.security.event.logger.exporter.class = class io.confluent.security.audit.telemetry.exporter.NonBlockingKafkaExporter
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.create = true
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.partitions = 12
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.replicas = 3
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.retention.bytes = -1
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.retention.ms = 2592000000
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.roll.ms = 14400000
benchi-kafka     | 	confluent.security.event.logger.kafka.request.rate.limit = -1
benchi-kafka     | 	confluent.security.event.logger.physical.cluster.id = 
benchi-kafka     | 	confluent.security.event.router.cache.entries = 10000
benchi-kafka     | 	confluent.security.event.router.config = 
benchi-kafka     | 	confluent.security.event.router.named.config.enabled = false
benchi-kafka     |  (io.confluent.security.audit.AuditLogConfig)
benchi-kafka     | [2025-03-18 16:05:45,914] INFO CrnAuthorityConfig values: 
benchi-kafka     | 	confluent.authorizer.authority.cache.entries = 10000
benchi-kafka     | 	confluent.authorizer.authority.name = 
benchi-kafka     | 	confluent.metadata.server.api.flavor = CP
benchi-kafka     |  (io.confluent.crn.CrnAuthorityConfig)
benchi-kafka     | [2025-03-18 16:05:45,914] INFO NamedConfigEnabled: false (io.confluent.security.audit.provider.ConfluentAuditLogProvider)
benchi-kafka     | [2025-03-18 16:05:45,914] INFO AuditLogConfig values: 
benchi-kafka     | 	confluent.security.event.logger.authentication.enable = false
benchi-kafka     | 	confluent.security.event.logger.authentication.event.rate.limit = -1
benchi-kafka     | 	confluent.security.event.logger.authorization.event.rate.limit = -1
benchi-kafka     | 	confluent.security.event.logger.cloudevent.codec = structured
benchi-kafka     | 	confluent.security.event.logger.enable = true
benchi-kafka     | 	confluent.security.event.logger.exporter.class = class io.confluent.security.audit.telemetry.exporter.NonBlockingKafkaExporter
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.create = true
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.partitions = 12
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.replicas = 3
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.retention.bytes = -1
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.retention.ms = 2592000000
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.roll.ms = 14400000
benchi-kafka     | 	confluent.security.event.logger.kafka.request.rate.limit = -1
benchi-kafka     | 	confluent.security.event.logger.physical.cluster.id = 
benchi-kafka     | 	confluent.security.event.router.cache.entries = 10000
benchi-kafka     | 	confluent.security.event.router.config = 
benchi-kafka     | 	confluent.security.event.router.named.config.enabled = false
benchi-kafka     |  (io.confluent.security.audit.AuditLogConfig)
benchi-kafka     | [2025-03-18 16:05:45,914] INFO CrnAuthorityConfig values: 
benchi-kafka     | 	confluent.authorizer.authority.cache.entries = 10000
benchi-kafka     | 	confluent.authorizer.authority.name = 
benchi-kafka     | 	confluent.metadata.server.api.flavor = CP
benchi-kafka     |  (io.confluent.crn.CrnAuthorityConfig)
benchi-kafka     | [2025-03-18 16:05:45,914] INFO MultiTenantAuditLogConfig values: 
benchi-kafka     | 	confluent.security.event.logger.client.ip.enable = false
benchi-kafka     | 	confluent.security.event.logger.multitenant.enable = false
benchi-kafka     |  (io.confluent.kafka.multitenant.authorizer.MultiTenantAuditLogConfig)
benchi-kafka     | [2025-03-18 16:05:45,914] INFO AuditLogConfig values: 
benchi-kafka     | 	confluent.security.event.logger.authentication.enable = false
benchi-kafka     | 	confluent.security.event.logger.authentication.event.rate.limit = -1
benchi-kafka     | 	confluent.security.event.logger.authorization.event.rate.limit = -1
benchi-kafka     | 	confluent.security.event.logger.cloudevent.codec = structured
benchi-kafka     | 	confluent.security.event.logger.enable = true
benchi-kafka     | 	confluent.security.event.logger.exporter.class = class io.confluent.security.audit.telemetry.exporter.NonBlockingKafkaExporter
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.create = true
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.partitions = 12
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.replicas = 3
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.retention.bytes = -1
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.retention.ms = 2592000000
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.roll.ms = 14400000
benchi-kafka     | 	confluent.security.event.logger.kafka.request.rate.limit = -1
benchi-kafka     | 	confluent.security.event.logger.physical.cluster.id = 
benchi-kafka     | 	confluent.security.event.router.cache.entries = 10000
benchi-kafka     | 	confluent.security.event.router.config = 
benchi-kafka     | 	confluent.security.event.router.named.config.enabled = false
benchi-kafka     |  (io.confluent.security.audit.AuditLogConfig)
benchi-kafka     | [2025-03-18 16:05:45,914] INFO Audit Log rate limiter reconfigured: Authn: -1, Authz: -1, Kafka request: -1 (io.confluent.security.audit.provider.AuditLogRateLimiter)
benchi-kafka     | [2025-03-18 16:05:45,915] INFO [BrokerServer id=1] Waiting for controller quorum voters future (kafka.server.BrokerServer)
benchi-kafka     | [2025-03-18 16:05:45,915] INFO [BrokerServer id=1] Finished waiting for controller quorum voters future (kafka.server.BrokerServer)
benchi-kafka     | [2025-03-18 16:05:45,916] INFO [broker-1-to-controller-forwarding-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
benchi-kafka     | [2025-03-18 16:05:45,917] INFO [broker-1-to-controller-forwarding-channel-manager]: Recorded new KRaft controller, from now on will use node broker:29093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
benchi-kafka     | [2025-03-18 16:05:45,922] INFO [client-metrics-reaper]: Starting (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
benchi-kafka     | [2025-03-18 16:05:45,927] INFO [ControllerServer id=1] Node 1 identified 0 potential metadata log encryptor rotation candidates: [] (org.apache.kafka.controller.ClusterControlManager)
benchi-kafka     | [2025-03-18 16:05:45,927] INFO [ControllerServer id=1] Potential metadata log encryptor rotation candidates that are existing in all controllers: [] (org.apache.kafka.controller.ClusterControlManager)
benchi-kafka     | [2025-03-18 16:05:45,927] INFO [ControllerServer id=1] Potential metadata log encryptor rotation candidates that are existing in all brokers: [] (org.apache.kafka.controller.ClusterControlManager)
benchi-kafka     | [2025-03-18 16:05:45,928] INFO [ControllerServer id=1] Replayed RegisterControllerRecord contaning ControllerRegistration(id=1, incarnationId=g1LiT4BhQhCyUixJaN6eeg, zkMigrationReady=false, listeners=[Endpoint(listenerName='CONTROLLER', securityProtocol=PLAINTEXT, host='broker', port=29093)], supportedFeatures={confluent.metadata.version: 1-120, metadata.version: 1-20}, metadataEncryptors=[]). (org.apache.kafka.controller.ClusterControlManager)
benchi-kafka     | [2025-03-18 16:05:45,946] INFO [ExpirationReaper-1-ClusterLink]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:05:45,957] INFO [ControllerRegistrationManager id=1 incarnation=g1LiT4BhQhCyUixJaN6eeg] Our registration has been persisted to the metadata log. (kafka.server.ControllerRegistrationManager)
benchi-kafka     | [2025-03-18 16:05:45,958] INFO [ControllerRegistrationManager id=1 incarnation=g1LiT4BhQhCyUixJaN6eeg] RegistrationResponseHandler: controller acknowledged ControllerRegistrationRequest. (kafka.server.ControllerRegistrationManager)
benchi-kafka     | [2025-03-18 16:05:45,958] INFO [SocketServer listenerType=BROKER, nodeId=1] Creating data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)
benchi-kafka     | [2025-03-18 16:05:45,959] INFO Quota PLAINTEXT-per-ip-connection-rate configured - (max: -1.0, floor: -1.0, adjustment: -1.0) (kafka.network.ListenerIpAutoTuningQuota)
benchi-kafka     | [2025-03-18 16:05:45,959] INFO Quota PLAINTEXT-per-tenant-connection-rate configured - (max: -1.0, floor: -1.0, adjustment: -1.0) (kafka.network.ListenerTenantAutoTuningQuota)
benchi-kafka     | [2025-03-18 16:05:45,959] INFO Quota PLAINTEXT-connection-rate configured - (max: 1.7976931348623157E308, floor: 1.7976931348623157E308, adjustment: 5.0) (kafka.network.ListenerAutoTuningQuota)
benchi-kafka     | [2025-03-18 16:05:45,959] INFO Updated connection-tokens max connection creation rate to 1.7976931348623157E308 (kafka.network.ConnectionQuotas)
benchi-kafka     | [2025-03-18 16:05:45,959] INFO Config values: 
benchi-kafka     | 	confluent.security.event.logger.detailed.audit.logs.disabled.apis = 
benchi-kafka     | 	confluent.security.event.logger.enable.detailed.audit.logs = false
benchi-kafka     | 	confluent.security.event.logger.enable.produce.consume.audit.logs = false
benchi-kafka     |  (org.apache.kafka.common.requests.DetailedRequestAuditLogFilter$Config)
benchi-kafka     | [2025-03-18 16:05:45,961] INFO Config values: 
benchi-kafka     | 	confluent.security.event.logger.detailed.audit.logs.disabled.apis = 
benchi-kafka     | 	confluent.security.event.logger.enable.detailed.audit.logs = false
benchi-kafka     | 	confluent.security.event.logger.enable.produce.consume.audit.logs = false
benchi-kafka     |  (org.apache.kafka.common.requests.DetailedRequestAuditLogFilter$Config)
benchi-kafka     | [2025-03-18 16:05:45,961] INFO Config values: 
benchi-kafka     | 	confluent.security.event.logger.detailed.audit.logs.disabled.apis = 
benchi-kafka     | 	confluent.security.event.logger.enable.detailed.audit.logs = false
benchi-kafka     | 	confluent.security.event.logger.enable.produce.consume.audit.logs = false
benchi-kafka     |  (org.apache.kafka.common.requests.DetailedRequestAuditLogFilter$Config)
benchi-kafka     | [2025-03-18 16:05:45,962] INFO [SocketServer listenerType=BROKER, nodeId=1] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)
benchi-kafka     | [2025-03-18 16:05:45,962] INFO [SocketServer listenerType=BROKER, nodeId=1] Creating data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT_HOST) (kafka.network.SocketServer)
benchi-kafka     | [2025-03-18 16:05:45,963] INFO Quota PLAINTEXT_HOST-per-ip-connection-rate configured - (max: -1.0, floor: -1.0, adjustment: -1.0) (kafka.network.ListenerIpAutoTuningQuota)
benchi-kafka     | [2025-03-18 16:05:45,963] INFO Quota PLAINTEXT_HOST-per-tenant-connection-rate configured - (max: -1.0, floor: -1.0, adjustment: -1.0) (kafka.network.ListenerTenantAutoTuningQuota)
benchi-kafka     | [2025-03-18 16:05:45,963] INFO Quota PLAINTEXT_HOST-connection-rate configured - (max: 1.7976931348623157E308, floor: 1.7976931348623157E308, adjustment: 5.0) (kafka.network.ListenerAutoTuningQuota)
benchi-kafka     | [2025-03-18 16:05:45,963] INFO Updated connection-tokens max connection creation rate to 1.7976931348623157E308 (kafka.network.ConnectionQuotas)
benchi-kafka     | [2025-03-18 16:05:45,964] INFO Config values: 
benchi-kafka     | 	confluent.security.event.logger.detailed.audit.logs.disabled.apis = 
benchi-kafka     | 	confluent.security.event.logger.enable.detailed.audit.logs = false
benchi-kafka     | 	confluent.security.event.logger.enable.produce.consume.audit.logs = false
benchi-kafka     |  (org.apache.kafka.common.requests.DetailedRequestAuditLogFilter$Config)
benchi-kafka     | [2025-03-18 16:05:45,964] INFO Config values: 
benchi-kafka     | 	confluent.security.event.logger.detailed.audit.logs.disabled.apis = 
benchi-kafka     | 	confluent.security.event.logger.enable.detailed.audit.logs = false
benchi-kafka     | 	confluent.security.event.logger.enable.produce.consume.audit.logs = false
benchi-kafka     |  (org.apache.kafka.common.requests.DetailedRequestAuditLogFilter$Config)
benchi-kafka     | [2025-03-18 16:05:45,965] INFO Config values: 
benchi-kafka     | 	confluent.security.event.logger.detailed.audit.logs.disabled.apis = 
benchi-kafka     | 	confluent.security.event.logger.enable.detailed.audit.logs = false
benchi-kafka     | 	confluent.security.event.logger.enable.produce.consume.audit.logs = false
benchi-kafka     |  (org.apache.kafka.common.requests.DetailedRequestAuditLogFilter$Config)
benchi-kafka     | [2025-03-18 16:05:45,966] INFO [SocketServer listenerType=BROKER, nodeId=1] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT_HOST) (kafka.network.SocketServer)
benchi-kafka     | [2025-03-18 16:05:45,968] INFO [broker-1-to-controller-alter-partition-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
benchi-kafka     | [2025-03-18 16:05:45,968] INFO [broker-1-to-controller-alter-partition-channel-manager]: Recorded new KRaft controller, from now on will use node broker:29093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
benchi-kafka     | [2025-03-18 16:05:45,972] INFO [broker-1-to-controller-directory-assignments-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
benchi-kafka     | [2025-03-18 16:05:45,972] INFO [broker-1-to-controller-directory-assignments-channel-manager]: Recorded new KRaft controller, from now on will use node broker:29093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
benchi-kafka     | [2025-03-18 16:05:45,980] INFO [ExpirationReaper-1-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:05:45,980] INFO [ExpirationReaper-1-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:05:45,981] INFO [ExpirationReaper-1-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:05:45,981] INFO [ExpirationReaper-1-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:05:45,982] INFO [ExpirationReaper-1-ListOffsets]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:05:45,986] INFO ReplicationConfig values: 
benchi-kafka     | 	confluent.replication.linger.ms = 0
benchi-kafka     | 	confluent.replication.max.in.flight.requests = 1
benchi-kafka     | 	confluent.replication.max.memory.buffer.bytes = 209715200
benchi-kafka     | 	confluent.replication.max.replica.pushers = 4
benchi-kafka     | 	confluent.replication.max.wait.ms = 500
benchi-kafka     | 	confluent.replication.mode = PULL
benchi-kafka     | 	confluent.replication.num.pushers.per.broker = 1
benchi-kafka     | 	confluent.replication.push.internal.topics.enable = false
benchi-kafka     | 	confluent.replication.request.max.bytes = 52428800
benchi-kafka     | 	confluent.replication.request.max.partition.bytes = 52428800
benchi-kafka     | 	confluent.replication.request.timeout.ms = 5000
benchi-kafka     | 	confluent.replication.retry.timeout.ms = 10000
benchi-kafka     | 	confluent.replication.socket.send.buffer.bytes = 1048576
benchi-kafka     |  (io.confluent.kafka.replication.push.ReplicationConfig)
benchi-kafka     | [2025-03-18 16:05:45,991] INFO [BrokerHealthManager]: Starting (kafka.availability.BrokerHealthManager)
benchi-kafka     | [2025-03-18 16:05:45,993] INFO [ExpirationReaper-1-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:05:45,994] INFO [ExpirationReaper-1-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:05:46,009] INFO Unable to read the broker epoch in /tmp/kraft-combined-logs. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:05:46,010] INFO [broker-1-to-controller-heartbeat-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
benchi-kafka     | [2025-03-18 16:05:46,010] INFO [broker-1-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node broker:29093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
benchi-kafka     | [2025-03-18 16:05:46,010] INFO [SharedServer id=1] Using broker:29092 as bootstrap.servers for inter broker client config. (kafka.server.SharedServer)
benchi-kafka     | [2025-03-18 16:05:46,011] INFO [SharedServer id=1] Using broker:29092 as bootstrap.servers for inter broker client config. (kafka.server.SharedServer)
benchi-kafka     | [2025-03-18 16:05:46,011] INFO [BrokerLifecycleManager id=1] Incarnation EBCaSEaNTEyNAV8gpHdu_g of broker 1 in cluster MkU3OEVBNTcwNTJENDM2Qk is now STARTING. (kafka.server.BrokerLifecycleManager)
benchi-kafka     | [2025-03-18 16:05:46,016] INFO [ControllerServer id=1] No previous registration found for broker 1. New incarnation ID is EBCaSEaNTEyNAV8gpHdu_g.  Generated 0 record(s) to clean up previous incarnations. New broker epoch is 6. (org.apache.kafka.controller.ClusterControlManager)
benchi-kafka     | [2025-03-18 16:05:46,016] INFO [ControllerServer id=1] Node 1 identified 0 potential metadata log encryptor rotation candidates: [] (org.apache.kafka.controller.ClusterControlManager)
benchi-kafka     | [2025-03-18 16:05:46,016] INFO [ControllerServer id=1] Potential metadata log encryptor rotation candidates that are existing in all controllers: [] (org.apache.kafka.controller.ClusterControlManager)
benchi-kafka     | [2025-03-18 16:05:46,016] INFO [ControllerServer id=1] Potential metadata log encryptor rotation candidates that are existing in all brokers: [] (org.apache.kafka.controller.ClusterControlManager)
benchi-kafka     | [2025-03-18 16:05:46,017] INFO [ControllerServer id=1] Replayed initial RegisterBrokerRecord for broker 1: RegisterBrokerRecord(brokerId=1, isMigratingZkBroker=false, incarnationId=EBCaSEaNTEyNAV8gpHdu_g, brokerEpoch=6, endPoints=[BrokerEndpoint(name='PLAINTEXT', host='broker', port=29092, securityProtocol=0), BrokerEndpoint(name='PLAINTEXT_HOST', host='localhost', port=9092, securityProtocol=0)], features=[BrokerFeature(name='confluent.metadata.version', minSupportedVersion=1, maxSupportedVersion=120), BrokerFeature(name='metadata.version', minSupportedVersion=1, maxSupportedVersion=20)], rack=null, fenced=true, inControlledShutdown=false, degradedComponents=[], metadataEncryptors=[], logDirs=[v3Z8JLzy5QaOAeyEyCPGQA]) (org.apache.kafka.controller.ClusterControlManager)
benchi-kafka     | [2025-03-18 16:05:46,027] INFO [ExpirationReaper-1-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:05:46,029] INFO Config values: 
benchi-kafka     | 	confluent.request.log.api.samples.per.min = 
benchi-kafka     | 	confluent.request.log.enable.admin.apis = true
benchi-kafka     | 	confluent.request.log.enable.slowlog = false
benchi-kafka     | 	confluent.request.log.samples.per.min = 0
benchi-kafka     | 	confluent.request.slowlog.threshold.override = -1.0
benchi-kafka     | 	confluent.request.slowlog.threshold.p99.min = -1.0
benchi-kafka     |  (org.apache.kafka.common.requests.SamplingRequestLogFilter$Config)
benchi-kafka     | [2025-03-18 16:05:46,029] INFO Config values: 
benchi-kafka     | 	confluent.request.log.api.samples.per.min = 
benchi-kafka     | 	confluent.request.log.enable.admin.apis = true
benchi-kafka     | 	confluent.request.log.enable.slowlog = false
benchi-kafka     | 	confluent.request.log.samples.per.min = 0
benchi-kafka     | 	confluent.request.slowlog.threshold.override = -1.0
benchi-kafka     | 	confluent.request.slowlog.threshold.p99.min = -1.0
benchi-kafka     |  (org.apache.kafka.common.requests.SamplingRequestLogFilter$Config)
benchi-kafka     | [2025-03-18 16:05:46,029] INFO Config values: 
benchi-kafka     | 	confluent.request.log.api.samples.per.min = 
benchi-kafka     | 	confluent.request.log.enable.admin.apis = true
benchi-kafka     | 	confluent.request.log.enable.slowlog = false
benchi-kafka     | 	confluent.request.log.samples.per.min = 0
benchi-kafka     | 	confluent.request.slowlog.threshold.override = -1.0
benchi-kafka     | 	confluent.request.slowlog.threshold.p99.min = -1.0
benchi-kafka     |  (org.apache.kafka.common.requests.SamplingRequestLogFilter$Config)
benchi-kafka     | [2025-03-18 16:05:46,029] INFO Config values: 
benchi-kafka     | 	confluent.request.log.api.samples.per.min = 
benchi-kafka     | 	confluent.request.log.enable.admin.apis = true
benchi-kafka     | 	confluent.request.log.enable.slowlog = false
benchi-kafka     | 	confluent.request.log.samples.per.min = 0
benchi-kafka     | 	confluent.request.slowlog.threshold.override = -1.0
benchi-kafka     | 	confluent.request.slowlog.threshold.p99.min = -1.0
benchi-kafka     |  (org.apache.kafka.common.requests.SamplingRequestLogFilter$Config)
benchi-kafka     | [2025-03-18 16:05:46,030] INFO Config values: 
benchi-kafka     | 	confluent.request.log.api.samples.per.min = 
benchi-kafka     | 	confluent.request.log.enable.admin.apis = true
benchi-kafka     | 	confluent.request.log.enable.slowlog = false
benchi-kafka     | 	confluent.request.log.samples.per.min = 0
benchi-kafka     | 	confluent.request.slowlog.threshold.override = -1.0
benchi-kafka     | 	confluent.request.slowlog.threshold.p99.min = -1.0
benchi-kafka     |  (org.apache.kafka.common.requests.SamplingRequestLogFilter$Config)
benchi-kafka     | [2025-03-18 16:05:46,030] INFO Config values: 
benchi-kafka     | 	confluent.request.log.api.samples.per.min = 
benchi-kafka     | 	confluent.request.log.enable.admin.apis = true
benchi-kafka     | 	confluent.request.log.enable.slowlog = false
benchi-kafka     | 	confluent.request.log.samples.per.min = 0
benchi-kafka     | 	confluent.request.slowlog.threshold.override = -1.0
benchi-kafka     | 	confluent.request.slowlog.threshold.p99.min = -1.0
benchi-kafka     |  (org.apache.kafka.common.requests.SamplingRequestLogFilter$Config)
benchi-kafka     | [2025-03-18 16:05:46,030] INFO Config values: 
benchi-kafka     | 	confluent.request.log.api.samples.per.min = 
benchi-kafka     | 	confluent.request.log.enable.admin.apis = true
benchi-kafka     | 	confluent.request.log.enable.slowlog = false
benchi-kafka     | 	confluent.request.log.samples.per.min = 0
benchi-kafka     | 	confluent.request.slowlog.threshold.override = -1.0
benchi-kafka     | 	confluent.request.slowlog.threshold.p99.min = -1.0
benchi-kafka     |  (org.apache.kafka.common.requests.SamplingRequestLogFilter$Config)
benchi-kafka     | [2025-03-18 16:05:46,031] INFO Config values: 
benchi-kafka     | 	confluent.request.log.api.samples.per.min = 
benchi-kafka     | 	confluent.request.log.enable.admin.apis = true
benchi-kafka     | 	confluent.request.log.enable.slowlog = false
benchi-kafka     | 	confluent.request.log.samples.per.min = 0
benchi-kafka     | 	confluent.request.slowlog.threshold.override = -1.0
benchi-kafka     | 	confluent.request.slowlog.threshold.p99.min = -1.0
benchi-kafka     |  (org.apache.kafka.common.requests.SamplingRequestLogFilter$Config)
benchi-kafka     | [2025-03-18 16:05:46,031] INFO [BrokerServer id=1] Waiting for broker metadata to catch up (kafka.server.BrokerServer)
benchi-kafka     | [2025-03-18 16:05:46,045] INFO Handling event SbcConfigUpdateEvent-3 (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:05:46,045] INFO Cluster metadata containing at least one unfenced broker not yet available, SBC config processing delayed. (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:05:46,045] INFO Handling event SbcKraftStartupEvent-5 (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:05:46,045] INFO Cluster metadata containing at least one unfenced broker not yet available, SBC startup delayed. (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:05:46,046] INFO [BrokerLifecycleManager id=1] Successfully registered broker 1 with broker epoch 6 (kafka.server.BrokerLifecycleManager)
benchi-kafka     | [2025-03-18 16:05:46,048] INFO [BrokerLifecycleManager id=1] The broker has caught up. Transitioning from STARTING to RECOVERY. (kafka.server.BrokerLifecycleManager)
benchi-kafka     | [2025-03-18 16:05:46,048] INFO [BrokerServer id=1] Finished waiting for broker metadata to catch up (kafka.server.BrokerServer)
benchi-kafka     | [2025-03-18 16:05:46,062] INFO [BrokerLifecycleManager id=1] The broker is in RECOVERY. (kafka.server.BrokerLifecycleManager)
benchi-kafka     | [2025-03-18 16:05:46,072] INFO ConfluentMetricsReporterConfig values: 
benchi-kafka     | 	confluent.metrics.reporter.bootstrap.servers = localhost:9092
benchi-kafka     | 	confluent.metrics.reporter.include = .*MaxLag.*|kafka.log:type=Log,name=Size.*|.*name=(ActiveControllerCount|BytesInPerSec|BytesOutPerSec|CaughtUpReplicasCount|FailedFetchRequestsPerSec|FailedProduceRequestsPerSec|InSyncReplicasCount|LeaderCount|LeaderElectionRateAndTimeMs|LocalTimeMs|LogEndOffset|LogStartOffset|NetworkProcessorAvgIdlePercent|NumLogSegments|NumPartitionsInError|OfflinePartitionsCount|ObserverReplicasCount|PartitionCount|RemoteTimeMs|ReplicasCount|RequestHandlerAvgIdlePercent|RequestQueueSize|RequestQueueTimeMs|RequestsPerSec|ResponseQueueSize|ResponseQueueTimeMs|ResponseSendTimeMs|Size|TierSize|TotalFetchRequestsPerSec|TotalLag|TotalProduceRequestsPerSec|TotalSize|TotalTimeMs|UncleanLeaderElectionsPerSec|UnderReplicated|UnderReplicatedPartitions|UnderMinIsrPartitionCount|ZooKeeperDisconnectsPerSec|ZooKeeperExpiresPerSec|TieredPartitionsUndergoingUncleanLeaderRecoveryCount|NonTieredPartitionsUndergoingUncleanLeaderRecoveryCount|TierTopicPartitionsUndergoingUncleanLeaderRecoveryCount|ContiguousUnhealthySamples|ContiguousSamplesEngineThreadGroupsStuck|ContiguousSamplesStorageThreadGroupsStuck|ContiguousSamplesNoStorageThreadMakingProgress).*|.*(BytesFetchedRate).*
benchi-kafka     | 	confluent.metrics.reporter.publish.ms = 15000
benchi-kafka     | 	confluent.metrics.reporter.topic = _confluent-metrics
benchi-kafka     | 	confluent.metrics.reporter.topic.create = true
benchi-kafka     | 	confluent.metrics.reporter.topic.max.message.bytes = 10485760
benchi-kafka     | 	confluent.metrics.reporter.topic.partitions = 12
benchi-kafka     | 	confluent.metrics.reporter.topic.replicas = 3
benchi-kafka     | 	confluent.metrics.reporter.topic.retention.bytes = -1
benchi-kafka     | 	confluent.metrics.reporter.topic.retention.ms = 259200000
benchi-kafka     | 	confluent.metrics.reporter.topic.roll.ms = 14400000
benchi-kafka     | 	confluent.metrics.reporter.volume.metrics.refresh.ms = 15000
benchi-kafka     | 	confluent.metrics.reporter.whitelist = null
benchi-kafka     |  (io.confluent.metrics.reporter.ConfluentMetricsReporterConfig)
benchi-kafka     | [2025-03-18 16:05:46,085] INFO ProducerConfig values: 
benchi-kafka     | 	acks = -1
benchi-kafka     | 	auto.include.jmx.reporter = true
benchi-kafka     | 	batch.size = 16384
benchi-kafka     | 	bootstrap.servers = [localhost:9092]
benchi-kafka     | 	buffer.memory = 33554432
benchi-kafka     | 	client.dns.lookup = use_all_dns_ips
benchi-kafka     | 	client.id = confluent-metrics-reporter
benchi-kafka     | 	compression.gzip.level = -1
benchi-kafka     | 	compression.lz4.level = 9
benchi-kafka     | 	compression.type = zstd
benchi-kafka     | 	compression.zstd.level = 3
benchi-kafka     | 	confluent.lkc.id = null
benchi-kafka     | 	confluent.proxy.protocol.client.address = null
benchi-kafka     | 	confluent.proxy.protocol.client.mode = PROXY
benchi-kafka     | 	confluent.proxy.protocol.client.port = null
benchi-kafka     | 	confluent.proxy.protocol.client.version = NONE
benchi-kafka     | 	connections.max.idle.ms = 540000
benchi-kafka     | 	delivery.timeout.ms = 120000
benchi-kafka     | 	enable.idempotence = false
benchi-kafka     | 	enable.metrics.push = true
benchi-kafka     | 	interceptor.classes = []
benchi-kafka     | 	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
benchi-kafka     | 	linger.ms = 500
benchi-kafka     | 	max.block.ms = 60000
benchi-kafka     | 	max.in.flight.requests.per.connection = 1
benchi-kafka     | 	max.request.size = 10485760
benchi-kafka     | 	metadata.max.age.ms = 300000
benchi-kafka     | 	metadata.max.idle.ms = 300000
benchi-kafka     | 	metadata.recovery.strategy = none
benchi-kafka     | 	metric.reporters = []
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.recording.level = INFO
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	partitioner.adaptive.partitioning.enable = true
benchi-kafka     | 	partitioner.availability.timeout.ms = 0
benchi-kafka     | 	partitioner.class = null
benchi-kafka     | 	partitioner.ignore.keys = false
benchi-kafka     | 	receive.buffer.bytes = 32768
benchi-kafka     | 	reconnect.backoff.max.ms = 1000
benchi-kafka     | 	reconnect.backoff.ms = 50
benchi-kafka     | 	request.timeout.ms = 30000
benchi-kafka     | 	retries = 2147483647
benchi-kafka     | 	retry.backoff.max.ms = 1000
benchi-kafka     | 	retry.backoff.ms = 500
benchi-kafka     | 	sasl.client.callback.handler.class = null
benchi-kafka     | 	sasl.jaas.config = null
benchi-kafka     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
benchi-kafka     | 	sasl.kerberos.min.time.before.relogin = 60000
benchi-kafka     | 	sasl.kerberos.service.name = null
benchi-kafka     | 	sasl.kerberos.ticket.renew.jitter = 0.05
benchi-kafka     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
benchi-kafka     | 	sasl.login.callback.handler.class = null
benchi-kafka     | 	sasl.login.class = null
benchi-kafka     | 	sasl.login.connect.timeout.ms = null
benchi-kafka     | 	sasl.login.read.timeout.ms = null
benchi-kafka     | 	sasl.login.refresh.buffer.seconds = 300
benchi-kafka     | 	sasl.login.refresh.min.period.seconds = 60
benchi-kafka     | 	sasl.login.refresh.window.factor = 0.8
benchi-kafka     | 	sasl.login.refresh.window.jitter = 0.05
benchi-kafka     | 	sasl.login.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.login.retry.backoff.ms = 100
benchi-kafka     | 	sasl.mechanism = GSSAPI
benchi-kafka     | 	sasl.oauthbearer.clientassertion.audience = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.expiration = 5
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.jti.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.nbf.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.issuer = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.location = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key.passphrase = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.subject = null
benchi-kafka     | 	sasl.oauthbearer.clock.skew.seconds = 30
benchi-kafka     | 	sasl.oauthbearer.expected.audience = null
benchi-kafka     | 	sasl.oauthbearer.expected.issuer = null
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.url = null
benchi-kafka     | 	sasl.oauthbearer.scope.claim.name = scope
benchi-kafka     | 	sasl.oauthbearer.sub.claim.name = sub
benchi-kafka     | 	sasl.oauthbearer.token.endpoint.url = null
benchi-kafka     | 	security.protocol = PLAINTEXT
benchi-kafka     | 	security.providers = null
benchi-kafka     | 	send.buffer.bytes = 131072
benchi-kafka     | 	socket.connection.setup.timeout.max.ms = 30000
benchi-kafka     | 	socket.connection.setup.timeout.ms = 10000
benchi-kafka     | 	ssl.cipher.suites = null
benchi-kafka     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	ssl.engine.factory.class = null
benchi-kafka     | 	ssl.key.password = null
benchi-kafka     | 	ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	ssl.keystore.certificate.chain = null
benchi-kafka     | 	ssl.keystore.key = null
benchi-kafka     | 	ssl.keystore.location = null
benchi-kafka     | 	ssl.keystore.password = null
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.protocol = TLSv1.3
benchi-kafka     | 	ssl.provider = null
benchi-kafka     | 	ssl.secure.random.implementation = null
benchi-kafka     | 	ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	ssl.truststore.certificates = null
benchi-kafka     | 	ssl.truststore.location = null
benchi-kafka     | 	ssl.truststore.password = null
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     | 	transaction.timeout.ms = 60000
benchi-kafka     | 	transactional.id = null
benchi-kafka     | 	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
benchi-kafka     |  (org.apache.kafka.clients.producer.ProducerConfig)
benchi-kafka     | [2025-03-18 16:05:46,094] INFO initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector)
benchi-kafka     | [2025-03-18 16:05:46,104] INFO Kafka version: 7.8.1-ce (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:05:46,104] INFO Kafka commitId: eefb51c6f2f1b888b8b03296353338afbac8e213 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:05:46,104] INFO Kafka startTimeMs: 1742313946104 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:05:46,105] INFO [Producer clientId=confluent-metrics-reporter] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:05:46,106] WARN [Producer clientId=confluent-metrics-reporter] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:05:46,106] WARN [Producer clientId=confluent-metrics-reporter] Bootstrap broker localhost:9092 (id: -1 rack: null) disconnected (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:05:46,107] INFO EventEmitterConfig values: 
benchi-kafka     |  (io.confluent.shaded.io.confluent.telemetry.events.EventEmitterConfig)
benchi-kafka     | [2025-03-18 16:05:46,114] INFO Linux CPU collector enabled: true (io.confluent.telemetry.ConfluentTelemetryConfig)
benchi-kafka     | [2025-03-18 16:05:46,114] INFO Using cpu metric: io\.confluent\.kafka\.server/server/linux_system_cpu_utilization_1m (io.confluent.telemetry.ConfluentTelemetryConfig)
benchi-kafka     | [2025-03-18 16:05:46,119] INFO Configuring named client _confluentClient for exporter _confluent (io.confluent.telemetry.exporter.http.HttpExporterConfig)
benchi-kafka     | [2025-03-18 16:05:46,120] WARN no telemetry exporters are enabled (io.confluent.telemetry.ConfluentTelemetryConfig)
benchi-kafka     | [2025-03-18 16:05:46,125] WARN Ignoring redefinition of existing telemetry label kafka.version (io.confluent.shaded.io.confluent.telemetry.ResourceBuilderFacade)
benchi-kafka     | [2025-03-18 16:05:46,130] INFO ConfluentTelemetryConfig values: 
benchi-kafka     | 	confluent.telemetry.api.key = null
benchi-kafka     | 	confluent.telemetry.api.secret = null
benchi-kafka     | 	confluent.telemetry.cluster.id = null
benchi-kafka     | 	confluent.telemetry.debug.enabled = false
benchi-kafka     | 	confluent.telemetry.enabled = false
benchi-kafka     | 	confluent.telemetry.events.collector.include = .*transaction.remove.expired.transaction.cleanup.interval.ms.*|.*transaction.state.log.load.buffer.size.*|.*transaction.state.log.min.isr.*|.*transaction.state.log.num.partitions.*|.*transaction.state.log.replication.factor.*|.*transaction.state.log.segment.bytes.*|.*transactional.id.expiration.ms.*|.*controlled.shutdown.enable.*|.*controlled.shutdown.max.retries.*|.*controlled.shutdown.retry.backoff.ms.*|.*fetch.max.bytes.*|.*fetch.purgatory.purge.interval.requests.*|.*group.initial.rebalance.delay.ms.*|.*group.max.session.timeout.ms.*|.*group.max.size.*|.*group.min.session.timeout.ms.*|.*log.cleaner.backoff.ms.*|.*log.cleaner.dedupe.buffer.size.*|.*log.cleaner.delete.retention.ms.*|.*log.cleaner.enable.*|.*log.cleaner.io.buffer.load.factor.*|.*log.cleaner.io.buffer.size.*|.*log.cleaner.io.max.bytes.per.second.*|.*log.cleaner.max.compaction.lag.ms.*|.*log.cleaner.min.cleanable.ratio.*|.*log.cleaner.min.compaction.lag.ms.*|.*log.cleaner.threads.*|.*log.cleanup.policy.*|.*log.index.interval.bytes.*|.*log.index.size.max.bytes.*|.*log.message.downconversion.enable.*|.*log.message.format.version.*|.*log.message.timestamp.difference.max.ms.*|.*log.message.timestamp.type.*|.*max.connection.creation.rate.*|.*max.connections.*|.*max.connections.per.ip.*|.*max.incremental.fetch.session.cache.slots.*|.*replica.fetch.backoff.ms.*|.*replica.fetch.max.bytes.*|.*replica.fetch.min.bytes.*|.*replica.fetch.response.max.bytes.*|.*replica.fetch.wait.max.ms.*|.*replica.high.watermark.checkpoint.interval.ms.*|.*replica.lag.time.max.ms.*|.*replica.selector.class.*|.*replica.socket.receive.buffer.bytes.*|.*replica.socket.timeout.ms.*|.*alter.config.policy.class.name.*|.*alter.log.dirs.replication.quota.window.num.*|.*alter.log.dirs.replication.quota.window.size.seconds.*|.*metrics.num.samples.*|.*metrics.recording.level.*|.*metrics.sample.window.ms.*|.*quota.window.num.*|.*quota.window.size.seconds.*|.*replication.quota.window.num.*|.*replication.quota.window.size.seconds.*|.*confluent.balancer.enable.*|.*confluent.balancer.throttle.bytes.per.second.*|.*confluent.balancer.heal.uneven.load.trigger.*|.*confluent.balancer.heal.broker.failure.threshold.ms.*|.*confluent.balancer.disk.max.load.*|.*confluent.balancer.exclude.topic.prefixes.*|.*confluent.balancer.exclude.topic.names.*|.*confluent.tier.local.hotset.bytes.*|.*confluent.tier.local.hotset.ms.*|.*confluent.tier.archiver.num.threads.*|.*confluent.tier.backend.*|.*confluent.tier.enable.*|.*confluent.tier.feature.*|.*confluent.tier.fetcher.num.threads.*|.*confluent.tier.max.partition.fetch.bytes.override.*|.*confluent.tier.metadata.replication.factor.*|.*confluent.operator.managed.*|.*confluent.ansible.managed.*|.*confluent.license.*
benchi-kafka     | 	confluent.telemetry.events.enable = true
benchi-kafka     | 	confluent.telemetry.metrics.collector.include = .*io.confluent.telemetry/.*.*|.*io.confluent.system/.*(process_cpu_load|max_file_descriptor_count|open_file_descriptor_count|system_cpu_load|system_load_average|free_physical_memory_size|total_physical_memory_size|disk_total_bytes|disk_usable_bytes|jvm/mem|jvm/gc).*|.*io.confluent.kafka.server/.*(confluent_audit/audit_log_fallback_rate_per_minute|confluent_audit/audit_log_rate_per_minute|confluent_authorizer/authorization_request_rate_per_minute|confluent_authorizer/authorization_allowed_rate_per_minute|confluent_authorizer/authorization_denied_rate_per_minute|confluent_auth_store/rbac_role_bindings_count|confluent_auth_store/rbac_access_rules_count|confluent_auth_store/acl_access_rules_count).*|.*io.confluent.kafka.server/.*(acl_authorizer/zookeeper_disconnects/total/delta|acl_authorizer/zookeeper_expires/total/delta|broker_failure/zookeeper_disconnects/total/delta|broker_failure/zookeeper_expires/total/delta|broker_topic/bytes_in/total/delta|broker_topic/bytes_out/total/delta|broker_topic/failed_produce_requests/total/delta|broker_topic/failed_fetch_requests/total/delta|broker_topic/produce_message_conversions/total/delta|broker_topic/fetch_message_conversions/total/delta|cluster_link/active_link_count|cluster_link/consumer_offset_committed_rate|cluster_link/consumer_offset_committed_total|cluster_link/fetch_throttle_time_avg|cluster_link/fetch_throttle_time_max|cluster_link/link_count|cluster_link/linked_leader_epoch_change_rate|cluster_link/linked_leader_epoch_change_total|cluster_link/linked_topic_partition_addition_rate|cluster_link/linked_topic_partition_addition_total|cluster_link/mirror_partition_count|cluster_link/mirror_topic_byte_total|cluster_link/mirror_topic_count|cluster_link/mirror_topic_lag|cluster_link/topic_config_update_rate|cluster_link/topic_config_update_total|cluster_link_fetcher/connection_count|cluster_link_fetcher/failed_reauthentication_rate|cluster_link_fetcher/failed_reauthentication_total|cluster_link_fetcher/incoming_byte_rate|cluster_link_fetcher/incoming_byte_total|cluster_link_fetcher/outgoing_byte_rate|cluster_link_fetcher/outgoing_byte_total|cluster_link_fetcher/reauthentication_latency_avg|cluster_link_fetcher_manager/max_lag|controller/active_controller_count|controller/leader_election_rate_and_time_ms|controller/offline_partitions_count|controller/partition_availability|controller/preferred_replica_imbalance_count|controller/tenant_partition_availability|controller/global_under_min_isr_partition_count|controller/unclean_leader_elections/total|controller_channel/connection_close_rate|controller_channel/connection_close_total|controller_channel/connection_count|controller_channel/connection_creation_rate|controller_channel/connection_creation_total|controller_channel/request_size_avg|controller_channel/request_size_max|controller_channel_manager/queue_size|controller_channel_manager/total_queue_size|controller_event_manager/event_queue_size|delayed_operation_purgatory/purgatory_size|executor/zookeeper_disconnects/total/delta|executor/zookeeper_expires/total/delta|fetch/queue_size|fetcher/bytes_per_sec|fetcher_lag/consumer_lag|group_coordinator/partition_load_time_max|log_cleaner_manager/achieved_cleaning_ratio/time/delta|log_cleaner_manager/achieved_cleaning_ratio/total/delta|log_cleaner_manager/compacted_partition_bytes|log_cleaner_manager/max_dirty_percent|log_cleaner_manager/time_since_last_run_ms|log_cleaner_manager/uncleanable_bytes|log_cleaner_manager/uncleanable_partitions_count|replica_alter_log_dirs_manager/max_lag|replica_fetcher/request_size_avg|replica_fetcher/request_size_max|replica_fetcher_manager/max_lag|replica_manager/blocked_on_mirror_source_partition_count|replica_manager/isr_shrinks|replica_manager/leader_count|replica_manager/partition_count|replica_manager/under_min_isr_mirror_partition_count|replica_manager/under_min_isr_partition_count|replica_manager/under_replicated_mirror_partitions|replica_manager/under_replicated_partitions|request/errors/total/delta|request/local_time_ms/time/delta|request/local_time_ms/total/delta|request/queue_size|request/remote_time_ms/time/delta|request/remote_time_ms/total/delta|request/request_queue_time_ms/time/delta|request/request_queue_time_ms/total/delta|request/requests|request/response_queue_time_ms/time/delta|request/response_queue_time_ms/total/delta|request/response_send_time_ms/time/delta|request/response_send_time_ms/total/delta|request/total_time_ms/time/delta|request/total_time_ms/total/delta|request_channel/request_queue_size|request_channel/response_queue_size|request_handler_pool/request_handler_avg_idle_percent|session_expire_listener/zookeeper_disconnects/total/delta|session_expire_listener/zookeeper_expires/total/delta|socket_server/connections|socket_server/successful_authentication_total/delta|socket_server/failed_authentication_total/delta|socket_server/network_processor_avg_idle_percent|socket_server/request_size_avg|socket_server/request_size_max|tenant/consumer_lag_offsets).*|.*org\.apache\.kafka\.(producer\.connection\.creation\.rate|producer\.node\.request\.latency\.avg|producer\.node\.request\.latency\.max|producer\.produce\.throttle\.time\.avg|producer\.produce\.throttle\.time\.max|producer\.record\.queue\.time\.avg|producer\.record\.queue\.time\.max|producer\.connection\.creation\.total|consumer\.connection\.creation\.rate|consumer\.connection\.creation\.total|consumer\.node\.request\.latency\.avg|consumer\.node\.request\.latency\.max|consumer\.poll\.idle\.ratio\.avg|consumer\.coordinator\.commit\.latency\.avg|consumer\.coordinator\.commit\.latency\.max|consumer\.coordinator\.assigned\.partitions|consumer\.coordinator\.rebalance\.latency\.avg|consumer\.coordinator\.rebalance\.latency\.max|consumer\.coordinator\.rebalance\.latency\.total|consumer\.fetch\.manager\.fetch\.latency\.avg|consumer\.fetch\.manager\.fetch\.latency\.max).*
benchi-kafka     | 	confluent.telemetry.metrics.collector.interval.ms = 60000
benchi-kafka     | 	confluent.telemetry.metrics.collector.slo.enabled = false
benchi-kafka     | 	confluent.telemetry.proxy.password = null
benchi-kafka     | 	confluent.telemetry.proxy.url = null
benchi-kafka     | 	confluent.telemetry.proxy.username = null
benchi-kafka     |  (io.confluent.telemetry.ConfluentTelemetryConfig)
benchi-kafka     | [2025-03-18 16:05:46,131] INFO VolumeMetricsCollectorConfig values: 
benchi-kafka     | 	confluent.telemetry.metrics.collector.volume.update.ms = 15000
benchi-kafka     |  (io.confluent.telemetry.collector.VolumeMetricsCollector$VolumeMetricsCollectorConfig)
benchi-kafka     | [2025-03-18 16:05:46,131] INFO HttpClientConfig values: 
benchi-kafka     | 	api.key = null
benchi-kafka     | 	api.secret = null
benchi-kafka     | 	buffer.batch.duration.max.ms = null
benchi-kafka     | 	buffer.batch.items.max = null
benchi-kafka     | 	buffer.inflight.submissions.max = null
benchi-kafka     | 	buffer.pending.batches.max = null
benchi-kafka     | 	client.attempts.max = null
benchi-kafka     | 	client.base.url = https://collector.telemetry.confluent.cloud
benchi-kafka     | 	client.compression = null
benchi-kafka     | 	client.connect.timeout.ms = null
benchi-kafka     | 	client.metrics.path.override = /v1/metrics
benchi-kafka     | 	client.request.timeout.ms = null
benchi-kafka     | 	client.retry.delay.seconds = null
benchi-kafka     | 	proxy.password = null
benchi-kafka     | 	proxy.url = null
benchi-kafka     | 	proxy.username = null
benchi-kafka     | 	type = httpTelemetryClient
benchi-kafka     |  (io.confluent.telemetry.exporter.http.HttpClientConfig)
benchi-kafka     | [2025-03-18 16:05:46,131] INFO HttpExporterConfig values: 
benchi-kafka     | 	api.key = null
benchi-kafka     | 	api.secret = null
benchi-kafka     | 	buffer.batch.duration.max.ms = null
benchi-kafka     | 	buffer.batch.items.max = null
benchi-kafka     | 	buffer.inflight.submissions.max = null
benchi-kafka     | 	buffer.pending.batches.max = null
benchi-kafka     | 	client = _confluentClient
benchi-kafka     | 	client.attempts.max = null
benchi-kafka     | 	client.base.url = null
benchi-kafka     | 	client.compression = null
benchi-kafka     | 	client.connect.timeout.ms = null
benchi-kafka     | 	client.metrics.path.override = /v1/metrics
benchi-kafka     | 	client.request.timeout.ms = null
benchi-kafka     | 	client.retry.delay.seconds = null
benchi-kafka     | 	enabled = false
benchi-kafka     | 	events.enabled = true
benchi-kafka     | 	metrics.enabled = true
benchi-kafka     | 	metrics.include = null
benchi-kafka     | 	proxy.password = null
benchi-kafka     | 	proxy.url = null
benchi-kafka     | 	proxy.username = null
benchi-kafka     | 	remote.configurable = false
benchi-kafka     | 	type = http
benchi-kafka     |  (io.confluent.telemetry.exporter.http.HttpExporterConfig)
benchi-kafka     | [2025-03-18 16:05:46,131] INFO Configuring named client _confluentClient for exporter _confluent (io.confluent.telemetry.exporter.http.HttpExporterConfig)
benchi-kafka     | [2025-03-18 16:05:46,132] INFO KafkaExporterConfig values: 
benchi-kafka     | 	client = 
benchi-kafka     | 	enabled = true
benchi-kafka     | 	events.enabled = true
benchi-kafka     | 	metrics.enabled = true
benchi-kafka     | 	metrics.include = (io\.confluent\.kafka\.server/broker_load/broker_load_percent|io\.confluent\.kafka\.server/broker_topic/bytes_in/rate/1_min|io\.confluent\.kafka\.server/broker_topic/bytes_out/rate/1_min|io\.confluent\.kafka\.server/broker_topic/fetch_from_follower_bytes_out/rate/1_min|io\.confluent\.kafka\.server/broker_topic/messages_in/rate/1_min|io\.confluent\.kafka\.server/broker_topic/replication_bytes_in/rate/1_min|io\.confluent\.kafka\.server/broker_topic/replication_bytes_out/rate/1_min|io\.confluent\.kafka\.server/broker_topic/total_fetch_requests/rate/1_min|io\.confluent\.kafka\.server/broker_topic/total_follower_fetch_requests/rate/1_min|io\.confluent\.kafka\.server/broker_topic/mirror_bytes_in/rate/1_min|io\.confluent\.kafka\.server/broker_topic/total_fetch_from_follower_requests/rate/1_min|io\.confluent\.kafka\.server/broker_topic/total_produce_requests/rate/1_min|io\.confluent\.kafka\.server/log/size|io\.confluent\.kafka\.server/request/requests/rate/1_min|io\.confluent\.kafka\.server/request_handler_pool/request_handler_avg_idle_percent/rate/1_min|io\.confluent\.kafka\.server/server/linux_system_cpu_utilization_1m|io\.confluent\.system/volume/disk_total_bytes)
benchi-kafka     | 	producer.bootstrap.servers = broker:29092
benchi-kafka     | 	remote.configurable = false
benchi-kafka     | 	topic.create = true
benchi-kafka     | 	topic.max.message.bytes = 10485760
benchi-kafka     | 	topic.name = _confluent-telemetry-metrics
benchi-kafka     | 	topic.partitions = 12
benchi-kafka     | 	topic.replicas = 1
benchi-kafka     | 	topic.retention.bytes = -1
benchi-kafka     | 	topic.retention.ms = 259200000
benchi-kafka     | 	topic.roll.ms = 14400000
benchi-kafka     | 	type = kafka
benchi-kafka     |  (io.confluent.telemetry.exporter.kafka.KafkaExporterConfig)
benchi-kafka     | [2025-03-18 16:05:46,132] INFO PollingRemoteConfigurationConfig values: 
benchi-kafka     | 	enabled = true
benchi-kafka     | 	refresh.interval.ms = 60000
benchi-kafka     |  (io.confluent.shaded.io.confluent.telemetry.config.remote.polling.PollingRemoteConfigurationConfig)
benchi-kafka     | [2025-03-18 16:05:46,132] INFO Initializing the event logger (io.confluent.telemetry.reporter.TelemetryReporter)
benchi-kafka     | [2025-03-18 16:05:46,134] INFO EventLoggerConfig values: 
benchi-kafka     | 	event.logger.cloudevent.codec = structured
benchi-kafka     | 	event.logger.exporter.class = class io.confluent.shaded.io.confluent.telemetry.events.exporter.http.EventHttpExporter
benchi-kafka     |  (io.confluent.shaded.io.confluent.telemetry.events.EventLoggerConfig)
benchi-kafka     | [2025-03-18 16:05:46,136] INFO HttpExporterConfig values: 
benchi-kafka     | 	api.key = null
benchi-kafka     | 	api.secret = null
benchi-kafka     | 	buffer.batch.duration.max.ms = null
benchi-kafka     | 	buffer.batch.items.max = null
benchi-kafka     | 	buffer.inflight.submissions.max = null
benchi-kafka     | 	buffer.pending.batches.max = null
benchi-kafka     | 	client.attempts.max = null
benchi-kafka     | 	client.base.url = https://collector.telemetry.confluent.cloud
benchi-kafka     | 	client.compression = null
benchi-kafka     | 	client.connect.timeout.ms = null
benchi-kafka     | 	client.request.timeout.ms = null
benchi-kafka     | 	client.retry.delay.seconds = null
benchi-kafka     | 	enabled = true
benchi-kafka     | 	events.enabled = true
benchi-kafka     | 	metrics.enabled = true
benchi-kafka     | 	proxy.password = null
benchi-kafka     | 	proxy.url = null
benchi-kafka     | 	proxy.username = null
benchi-kafka     | 	type = http
benchi-kafka     |  (io.confluent.shaded.io.confluent.telemetry.events.exporter.http.HttpExporterConfig)
benchi-kafka     | [2025-03-18 16:05:46,161] INFO [Producer clientId=confluent-metrics-reporter] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:05:46,162] WARN [Producer clientId=confluent-metrics-reporter] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:05:46,162] WARN [Producer clientId=confluent-metrics-reporter] Bootstrap broker localhost:9092 (id: -1 rack: null) disconnected (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:05:46,265] INFO [Producer clientId=confluent-metrics-reporter] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:05:46,265] WARN [Producer clientId=confluent-metrics-reporter] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:05:46,266] WARN [Producer clientId=confluent-metrics-reporter] Bootstrap broker localhost:9092 (id: -1 rack: null) disconnected (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:05:46,340] INFO Creating kafka exporter named '_local' (io.confluent.telemetry.reporter.TelemetryReporter)
benchi-kafka     | [2025-03-18 16:05:46,343] INFO Kafka Exporter _local getting producer client  (io.confluent.telemetry.exporter.kafka.KafkaExporter)
benchi-kafka     | [2025-03-18 16:05:46,343] INFO Creating new non-static producer client (io.confluent.telemetry.exporter.kafka.KafkaClientFactory)
benchi-kafka     | [2025-03-18 16:05:46,344] INFO ProducerConfig values: 
benchi-kafka     | 	acks = -1
benchi-kafka     | 	auto.include.jmx.reporter = true
benchi-kafka     | 	batch.size = 16384
benchi-kafka     | 	bootstrap.servers = [broker:29092]
benchi-kafka     | 	buffer.memory = 33554432
benchi-kafka     | 	client.dns.lookup = use_all_dns_ips
benchi-kafka     | 	client.id = confluent-telemetry-reporter-local-producer
benchi-kafka     | 	compression.gzip.level = -1
benchi-kafka     | 	compression.lz4.level = 9
benchi-kafka     | 	compression.type = zstd
benchi-kafka     | 	compression.zstd.level = 3
benchi-kafka     | 	confluent.lkc.id = null
benchi-kafka     | 	confluent.proxy.protocol.client.address = null
benchi-kafka     | 	confluent.proxy.protocol.client.mode = PROXY
benchi-kafka     | 	confluent.proxy.protocol.client.port = null
benchi-kafka     | 	confluent.proxy.protocol.client.version = NONE
benchi-kafka     | 	connections.max.idle.ms = 540000
benchi-kafka     | 	delivery.timeout.ms = 120000
benchi-kafka     | 	enable.idempotence = false
benchi-kafka     | 	enable.metrics.push = true
benchi-kafka     | 	interceptor.classes = []
benchi-kafka     | 	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
benchi-kafka     | 	linger.ms = 500
benchi-kafka     | 	max.block.ms = 60000
benchi-kafka     | 	max.in.flight.requests.per.connection = 1
benchi-kafka     | 	max.request.size = 10485760
benchi-kafka     | 	metadata.max.age.ms = 300000
benchi-kafka     | 	metadata.max.idle.ms = 300000
benchi-kafka     | 	metadata.recovery.strategy = none
benchi-kafka     | 	metric.reporters = []
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.recording.level = INFO
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	partitioner.adaptive.partitioning.enable = true
benchi-kafka     | 	partitioner.availability.timeout.ms = 0
benchi-kafka     | 	partitioner.class = class io.confluent.shaded.io.confluent.telemetry.events.exporter.kafka.RandomBrokerPartitionSubsetPartitioner
benchi-kafka     | 	partitioner.ignore.keys = false
benchi-kafka     | 	receive.buffer.bytes = 32768
benchi-kafka     | 	reconnect.backoff.max.ms = 1000
benchi-kafka     | 	reconnect.backoff.ms = 50
benchi-kafka     | 	request.timeout.ms = 30000
benchi-kafka     | 	retries = 2147483647
benchi-kafka     | 	retry.backoff.max.ms = 1000
benchi-kafka     | 	retry.backoff.ms = 500
benchi-kafka     | 	sasl.client.callback.handler.class = null
benchi-kafka     | 	sasl.jaas.config = null
benchi-kafka     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
benchi-kafka     | 	sasl.kerberos.min.time.before.relogin = 60000
benchi-kafka     | 	sasl.kerberos.service.name = null
benchi-kafka     | 	sasl.kerberos.ticket.renew.jitter = 0.05
benchi-kafka     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
benchi-kafka     | 	sasl.login.callback.handler.class = null
benchi-kafka     | 	sasl.login.class = null
benchi-kafka     | 	sasl.login.connect.timeout.ms = null
benchi-kafka     | 	sasl.login.read.timeout.ms = null
benchi-kafka     | 	sasl.login.refresh.buffer.seconds = 300
benchi-kafka     | 	sasl.login.refresh.min.period.seconds = 60
benchi-kafka     | 	sasl.login.refresh.window.factor = 0.8
benchi-kafka     | 	sasl.login.refresh.window.jitter = 0.05
benchi-kafka     | 	sasl.login.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.login.retry.backoff.ms = 100
benchi-kafka     | 	sasl.mechanism = GSSAPI
benchi-kafka     | 	sasl.oauthbearer.clientassertion.audience = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.expiration = 5
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.jti.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.nbf.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.issuer = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.location = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key.passphrase = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.subject = null
benchi-kafka     | 	sasl.oauthbearer.clock.skew.seconds = 30
benchi-kafka     | 	sasl.oauthbearer.expected.audience = null
benchi-kafka     | 	sasl.oauthbearer.expected.issuer = null
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.url = null
benchi-kafka     | 	sasl.oauthbearer.scope.claim.name = scope
benchi-kafka     | 	sasl.oauthbearer.sub.claim.name = sub
benchi-kafka     | 	sasl.oauthbearer.token.endpoint.url = null
benchi-kafka     | 	security.protocol = PLAINTEXT
benchi-kafka     | 	security.providers = null
benchi-kafka     | 	send.buffer.bytes = 131072
benchi-kafka     | 	socket.connection.setup.timeout.max.ms = 30000
benchi-kafka     | 	socket.connection.setup.timeout.ms = 10000
benchi-kafka     | 	ssl.cipher.suites = null
benchi-kafka     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	ssl.engine.factory.class = null
benchi-kafka     | 	ssl.key.password = null
benchi-kafka     | 	ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	ssl.keystore.certificate.chain = null
benchi-kafka     | 	ssl.keystore.key = null
benchi-kafka     | 	ssl.keystore.location = null
benchi-kafka     | 	ssl.keystore.password = null
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.protocol = TLSv1.3
benchi-kafka     | 	ssl.provider = null
benchi-kafka     | 	ssl.secure.random.implementation = null
benchi-kafka     | 	ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	ssl.truststore.certificates = null
benchi-kafka     | 	ssl.truststore.location = null
benchi-kafka     | 	ssl.truststore.password = null
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     | 	transaction.timeout.ms = 60000
benchi-kafka     | 	transactional.id = null
benchi-kafka     | 	value.serializer = class io.confluent.telemetry.serde.OpenTelemetryMetricsSerde
benchi-kafka     |  (org.apache.kafka.clients.producer.ProducerConfig)
benchi-kafka     | [2025-03-18 16:05:46,344] INFO initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector)
benchi-kafka     | [2025-03-18 16:05:46,346] INFO These configurations '[confluent.metrics.reporter.bootstrap.servers]' were supplied but are not used yet. (org.apache.kafka.clients.producer.ProducerConfig)
benchi-kafka     | [2025-03-18 16:05:46,347] INFO Kafka version: 7.8.1-ce (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:05:46,347] INFO Kafka commitId: eefb51c6f2f1b888b8b03296353338afbac8e213 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:05:46,347] INFO Kafka startTimeMs: 1742313946347 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:05:46,347] INFO [Producer clientId=confluent-telemetry-reporter-local-producer] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:05:46,347] WARN [Producer clientId=confluent-telemetry-reporter-local-producer] Connection to node -1 (broker/172.20.0.2:29092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:05:46,347] WARN [Producer clientId=confluent-telemetry-reporter-local-producer] Bootstrap broker broker:29092 (id: -1 rack: null) disconnected (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:05:46,380] INFO Handling event SbcConfigUpdateEvent-3 (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:05:46,380] INFO Cluster metadata containing at least one unfenced broker not yet available, SBC config processing delayed. (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:05:46,380] INFO Handling event SbcKraftStartupEvent-5 (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:05:46,380] INFO Cluster metadata containing at least one unfenced broker not yet available, SBC startup delayed. (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:05:46,399] INFO [Producer clientId=confluent-telemetry-reporter-local-producer] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:05:46,399] WARN [Producer clientId=confluent-telemetry-reporter-local-producer] Connection to node -1 (broker/172.20.0.2:29092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:05:46,399] WARN [Producer clientId=confluent-telemetry-reporter-local-producer] Bootstrap broker broker:29092 (id: -1 rack: null) disconnected (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:05:46,477] INFO [Producer clientId=confluent-metrics-reporter] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:05:46,477] WARN [Producer clientId=confluent-metrics-reporter] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:05:46,477] WARN [Producer clientId=confluent-metrics-reporter] Bootstrap broker localhost:9092 (id: -1 rack: null) disconnected (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:05:46,488] INFO Starting Confluent telemetry reporter with an interval of 60000 ms) (io.confluent.telemetry.reporter.TelemetryReporter)
benchi-kafka     | [2025-03-18 16:05:46,507] INFO [Producer clientId=confluent-telemetry-reporter-local-producer] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:05:46,507] WARN [Producer clientId=confluent-telemetry-reporter-local-producer] Connection to node -1 (broker/172.20.0.2:29092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:05:46,507] WARN [Producer clientId=confluent-telemetry-reporter-local-producer] Bootstrap broker broker:29092 (id: -1 rack: null) disconnected (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:05:46,517] INFO Starting Confluent metrics reporter for cluster id MkU3OEVBNTcwNTJENDM2Qk with an interval of 15000 ms (io.confluent.metrics.reporter.ConfluentMetricsReporter)
benchi-kafka     | [2025-03-18 16:05:46,524] INFO [BrokerServer id=1] Waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
benchi-kafka     | [2025-03-18 16:05:46,524] INFO [BrokerServer id=1] Finished waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
benchi-kafka     | [2025-03-18 16:05:46,524] INFO [BrokerServer id=1] Waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
benchi-kafka     | [2025-03-18 16:05:46,524] INFO [BrokerServer id=1] Finished waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
benchi-kafka     | [2025-03-18 16:05:46,524] INFO [BrokerServer id=1] Waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
benchi-kafka     | [2025-03-18 16:05:46,525] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing MetadataVersionPublisher(id=1) with a snapshot at offset 7 (org.apache.kafka.image.loader.MetadataLoader)
benchi-kafka     | [2025-03-18 16:05:46,525] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing BrokerMetadataPublisher with a snapshot at offset 7 (org.apache.kafka.image.loader.MetadataLoader)
benchi-kafka     | [2025-03-18 16:05:46,525] INFO [BrokerMetadataPublisher id=1] Publishing initial metadata at offset OffsetAndEpoch(offset=7, epoch=1) with metadata.version 3.8-IV0A. (kafka.server.metadata.BrokerMetadataPublisher)
benchi-kafka     | [2025-03-18 16:05:46,525] INFO Loading logs from log dirs ArrayBuffer(/tmp/kraft-combined-logs) (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:05:46,527] INFO No logs found to be loaded in /tmp/kraft-combined-logs (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:05:46,528] INFO Loaded 0 logs in 2ms (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:05:46,528] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:05:46,529] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:05:46,529] INFO Starting log roller with a period of 300000 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:05:46,535] INFO Starting the log cleaner (kafka.log.LogCleaner)
benchi-kafka     | [2025-03-18 16:05:46,545] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner$CleanerThread)
benchi-kafka     | [2025-03-18 16:05:46,548] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
benchi-kafka     | [2025-03-18 16:05:46,548] INFO [AddPartitionsToTxnSenderThread-1]: Starting (kafka.server.AddPartitionsToTxnManager)
benchi-kafka     | [2025-03-18 16:05:46,554] INFO AdminClientConfig values: 
benchi-kafka     | 	auto.include.jmx.reporter = true
benchi-kafka     | 	bootstrap.controllers = []
benchi-kafka     | 	bootstrap.servers = [broker:29092]
benchi-kafka     | 	client.dns.lookup = use_all_dns_ips
benchi-kafka     | 	client.id = cluster-link--local-admin-1
benchi-kafka     | 	confluent.lkc.id = null
benchi-kafka     | 	confluent.metrics.reporter.bootstrap.servers = localhost:9092
benchi-kafka     | 	confluent.proxy.protocol.client.address = null
benchi-kafka     | 	confluent.proxy.protocol.client.mode = PROXY
benchi-kafka     | 	confluent.proxy.protocol.client.port = null
benchi-kafka     | 	confluent.proxy.protocol.client.version = NONE
benchi-kafka     | 	connections.max.idle.ms = 300000
benchi-kafka     | 	default.api.timeout.ms = 60000
benchi-kafka     | 	enable.metrics.push = true
benchi-kafka     | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
benchi-kafka     | 	metadata.max.age.ms = 300000
benchi-kafka     | 	metadata.recovery.strategy = none
benchi-kafka     | 	metric.reporters = []
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.recording.level = INFO
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	receive.buffer.bytes = 65536
benchi-kafka     | 	reconnect.backoff.max.ms = 1000
benchi-kafka     | 	reconnect.backoff.ms = 50
benchi-kafka     | 	request.timeout.ms = 30000
benchi-kafka     | 	retries = 2147483647
benchi-kafka     | 	retry.backoff.max.ms = 1000
benchi-kafka     | 	retry.backoff.ms = 100
benchi-kafka     | 	sasl.client.callback.handler.class = null
benchi-kafka     | 	sasl.jaas.config = null
benchi-kafka     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
benchi-kafka     | 	sasl.kerberos.min.time.before.relogin = 60000
benchi-kafka     | 	sasl.kerberos.service.name = null
benchi-kafka     | 	sasl.kerberos.ticket.renew.jitter = 0.05
benchi-kafka     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
benchi-kafka     | 	sasl.login.callback.handler.class = null
benchi-kafka     | 	sasl.login.class = null
benchi-kafka     | 	sasl.login.connect.timeout.ms = null
benchi-kafka     | 	sasl.login.read.timeout.ms = null
benchi-kafka     | 	sasl.login.refresh.buffer.seconds = 300
benchi-kafka     | 	sasl.login.refresh.min.period.seconds = 60
benchi-kafka     | 	sasl.login.refresh.window.factor = 0.8
benchi-kafka     | 	sasl.login.refresh.window.jitter = 0.05
benchi-kafka     | 	sasl.login.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.login.retry.backoff.ms = 100
benchi-kafka     | 	sasl.mechanism = GSSAPI
benchi-kafka     | 	sasl.oauthbearer.clientassertion.audience = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.expiration = 5
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.jti.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.nbf.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.issuer = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.location = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key.passphrase = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.subject = null
benchi-kafka     | 	sasl.oauthbearer.clock.skew.seconds = 30
benchi-kafka     | 	sasl.oauthbearer.expected.audience = null
benchi-kafka     | 	sasl.oauthbearer.expected.issuer = null
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.url = null
benchi-kafka     | 	sasl.oauthbearer.scope.claim.name = scope
benchi-kafka     | 	sasl.oauthbearer.sub.claim.name = sub
benchi-kafka     | 	sasl.oauthbearer.token.endpoint.url = null
benchi-kafka     | 	security.protocol = PLAINTEXT
benchi-kafka     | 	security.providers = null
benchi-kafka     | 	send.buffer.bytes = 131072
benchi-kafka     | 	socket.connection.setup.timeout.max.ms = 30000
benchi-kafka     | 	socket.connection.setup.timeout.ms = 10000
benchi-kafka     | 	ssl.cipher.suites = null
benchi-kafka     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	ssl.engine.factory.class = null
benchi-kafka     | 	ssl.key.password = null
benchi-kafka     | 	ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	ssl.keystore.certificate.chain = null
benchi-kafka     | 	ssl.keystore.key = null
benchi-kafka     | 	ssl.keystore.location = null
benchi-kafka     | 	ssl.keystore.password = null
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.protocol = TLSv1.3
benchi-kafka     | 	ssl.provider = null
benchi-kafka     | 	ssl.secure.random.implementation = null
benchi-kafka     | 	ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	ssl.truststore.certificates = null
benchi-kafka     | 	ssl.truststore.location = null
benchi-kafka     | 	ssl.truststore.password = null
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     |  (org.apache.kafka.clients.admin.AdminClientConfig)
benchi-kafka     | [2025-03-18 16:05:46,566] INFO These configurations '[confluent.metrics.topic.replicas, confluent.metrics.reporter.bootstrap.servers, jmx.hostname, cluster.link.metadata.topic.replication.factor, jmx.port, confluent.license.topic.replication.factor]' were supplied but are not used yet. (org.apache.kafka.clients.admin.AdminClientConfig)
benchi-kafka     | [2025-03-18 16:05:46,567] INFO Kafka version: 7.8.1-ce (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:05:46,567] INFO Kafka commitId: eefb51c6f2f1b888b8b03296353338afbac8e213 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:05:46,567] INFO Kafka startTimeMs: 1742313946567 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:05:46,568] INFO [ClusterLinkManager-broker-1] ClusterLinkManager has started up. (kafka.server.link.ClusterLinkManager)
benchi-kafka     | [2025-03-18 16:05:46,569] INFO [GroupCoordinator 1]: Starting up. (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:05:46,569] INFO [AdminClient clientId=cluster-link--local-admin-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:05:46,569] WARN [AdminClient clientId=cluster-link--local-admin-1] Connection to node -1 (broker/172.20.0.2:29092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:05:46,569] INFO [GroupCoordinator 1]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:05:46,569] INFO [TransactionCoordinator id=1] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
benchi-kafka     | [2025-03-18 16:05:46,570] INFO [TxnMarkerSenderThread-1]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
benchi-kafka     | [2025-03-18 16:05:46,570] INFO [TransactionCoordinator id=1] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
benchi-kafka     | [2025-03-18 16:05:46,574] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing BrokerRegistrationTracker(id=1) with a snapshot at offset 7 (org.apache.kafka.image.loader.MetadataLoader)
benchi-kafka     | [2025-03-18 16:05:46,574] INFO [BrokerServer id=1] Finished waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
benchi-kafka     | [2025-03-18 16:05:46,574] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing ClusterLinkCoordinatorListener with a snapshot at offset 7 (org.apache.kafka.image.loader.MetadataLoader)
benchi-kafka     | [2025-03-18 16:05:46,575] INFO KafkaConfig values: 
benchi-kafka     | 	advertised.listeners = PLAINTEXT://broker:29092,PLAINTEXT_HOST://localhost:9092
benchi-kafka     | 	alter.config.policy.class.name = null
benchi-kafka     | 	alter.log.dirs.replication.quota.window.num = 11
benchi-kafka     | 	alter.log.dirs.replication.quota.window.size.seconds = 1
benchi-kafka     | 	authorizer.class.name = 
benchi-kafka     | 	auto.create.topics.enable = true
benchi-kafka     | 	auto.include.jmx.reporter = true
benchi-kafka     | 	auto.leader.rebalance.enable = true
benchi-kafka     | 	background.threads = 10
benchi-kafka     | 	broker.heartbeat.interval.ms = 2000
benchi-kafka     | 	broker.id = 1
benchi-kafka     | 	broker.id.generation.enable = true
benchi-kafka     | 	broker.interceptor.class = class org.apache.kafka.server.interceptor.DefaultBrokerInterceptor
benchi-kafka     | 	broker.rack = null
benchi-kafka     | 	broker.session.timeout.ms = 9000
benchi-kafka     | 	broker.session.uuid = JzUn51KjQwC3OragyPrV9Q
benchi-kafka     | 	client.quota.callback.class = null
benchi-kafka     | 	client.quota.max.throttle.time.ms = 5000
benchi-kafka     | 	compression.gzip.level = -1
benchi-kafka     | 	compression.lz4.level = 9
benchi-kafka     | 	compression.type = producer
benchi-kafka     | 	compression.zstd.level = 3
benchi-kafka     | 	confluent.alter.broker.health.max.demoted.brokers = 2147483647
benchi-kafka     | 	confluent.alter.broker.health.max.demoted.brokers.percentage = 0
benchi-kafka     | 	confluent.ansible.managed = false
benchi-kafka     | 	confluent.api.visibility = DEFAULT
benchi-kafka     | 	confluent.append.record.interceptor.classes = []
benchi-kafka     | 	confluent.apply.create.topic.policy.to.create.partitions = false
benchi-kafka     | 	confluent.authorizer.authority.name = 
benchi-kafka     | 	confluent.automatic.alter.broker.health.retry.backoff.ms = 2000
benchi-kafka     | 	confluent.backpressure.disk.enable = false
benchi-kafka     | 	confluent.backpressure.disk.free.threshold.bytes = 21474836480
benchi-kafka     | 	confluent.backpressure.disk.produce.bytes.per.second = 131072
benchi-kafka     | 	confluent.backpressure.disk.threshold.recovery.factor = 1.5
benchi-kafka     | 	confluent.backpressure.request.min.broker.limit = 200
benchi-kafka     | 	confluent.backpressure.request.queue.size.percentile = p95
benchi-kafka     | 	confluent.backpressure.types = null
benchi-kafka     | 	confluent.balancer.api.state.topic = _confluent_balancer_api_state
benchi-kafka     | 	confluent.balancer.broker.addition.elapsed.time.ms.completion.threshold = 57600000
benchi-kafka     | 	confluent.balancer.broker.addition.mean.cpu.percent.completion.threshold = 0.5
benchi-kafka     | 	confluent.balancer.capacity.threshold.upper.limit = 0.95
benchi-kafka     | 	confluent.balancer.cell.load.upper.bound = 0.7
benchi-kafka     | 	confluent.balancer.cell.overload.detection.interval.ms = 3600000
benchi-kafka     | 	confluent.balancer.cell.overload.duration.ms = 86400000
benchi-kafka     | 	confluent.balancer.class = io.confluent.databalancer.SbcDataBalanceManager
benchi-kafka     | 	confluent.balancer.consumer.out.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.balancer.cpu.balance.threshold = 1.1
benchi-kafka     | 	confluent.balancer.cpu.low.utilization.threshold = 0.2
benchi-kafka     | 	confluent.balancer.cpu.utilization.detector.duration.ms = 600000
benchi-kafka     | 	confluent.balancer.cpu.utilization.detector.enabled = false
benchi-kafka     | 	confluent.balancer.cpu.utilization.detector.overutilization.threshold = 80.0
benchi-kafka     | 	confluent.balancer.cpu.utilization.detector.underutilization.threshold = 50.0
benchi-kafka     | 	confluent.balancer.disk.max.load = 0.85
benchi-kafka     | 	confluent.balancer.disk.min.free.space.gb = 0
benchi-kafka     | 	confluent.balancer.disk.min.free.space.lower.limit.gb = 0
benchi-kafka     | 	confluent.balancer.disk.utilization.detector.duration.ms = 600000
benchi-kafka     | 	confluent.balancer.disk.utilization.detector.enabled = false
benchi-kafka     | 	confluent.balancer.disk.utilization.detector.overutilization.threshold = 80.0
benchi-kafka     | 	confluent.balancer.disk.utilization.detector.reserved.capacity = 150000.0
benchi-kafka     | 	confluent.balancer.disk.utilization.detector.underutilization.threshold = 35.0
benchi-kafka     | 	confluent.balancer.enable = true
benchi-kafka     | 	confluent.balancer.exclude.topic.names = []
benchi-kafka     | 	confluent.balancer.exclude.topic.prefixes = []
benchi-kafka     | 	confluent.balancer.goal.violation.delay.on.new.brokers.ms = 1800000
benchi-kafka     | 	confluent.balancer.goal.violation.distribution.threshold.multiplier = 1.1
benchi-kafka     | 	confluent.balancer.heal.broker.failure.threshold.ms = 3600000
benchi-kafka     | 	confluent.balancer.heal.uneven.load.trigger = EMPTY_BROKER
benchi-kafka     | 	confluent.balancer.incremental.balancing.cpu.top.proposal.tracking.enabled = true
benchi-kafka     | 	confluent.balancer.incremental.balancing.cpu.top.proposal.tracking.num.proposals = 15
benchi-kafka     | 	confluent.balancer.incremental.balancing.enabled = false
benchi-kafka     | 	confluent.balancer.incremental.balancing.goals = []
benchi-kafka     | 	confluent.balancer.incremental.balancing.lower.bound = 0.02
benchi-kafka     | 	confluent.balancer.incremental.balancing.min.valid.windows = 5
benchi-kafka     | 	confluent.balancer.incremental.balancing.step.ratio = 0.2
benchi-kafka     | 	confluent.balancer.inter.cell.balancing.enabled = false
benchi-kafka     | 	confluent.balancer.max.capacity.balancing.delta.percentage = 0.0
benchi-kafka     | 	confluent.balancer.max.replicas = 2147483647
benchi-kafka     | 	confluent.balancer.network.in.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.balancer.network.out.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.balancer.plan.computation.retry.timeout.ms = 3600000
benchi-kafka     | 	confluent.balancer.producer.in.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.balancer.rebalancing.goals = []
benchi-kafka     | 	confluent.balancer.replication.in.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.balancer.resource.utilization.detector.interval.ms = 60000
benchi-kafka     | 	confluent.balancer.sbc.metrics.parser.enabled = false
benchi-kafka     | 	confluent.balancer.self.healing.maximum.rounds = 1
benchi-kafka     | 	confluent.balancer.task.history.retention.days = 30
benchi-kafka     | 	confluent.balancer.tenant.maximum.movements = 0
benchi-kafka     | 	confluent.balancer.tenant.suspension.ms = 86400000
benchi-kafka     | 	confluent.balancer.throttle.bytes.per.second = 10485760
benchi-kafka     | 	confluent.balancer.topic.partition.maximum.movements = 5
benchi-kafka     | 	confluent.balancer.topic.partition.movement.expiration.ms = 10800000
benchi-kafka     | 	confluent.balancer.topic.partition.suspension.ms = 18000000
benchi-kafka     | 	confluent.balancer.topic.replication.factor = 1
benchi-kafka     | 	confluent.balancer.triggering.goals = []
benchi-kafka     | 	confluent.balancer.v2.addition.enabled = false
benchi-kafka     | 	confluent.basic.auth.credentials.source = null
benchi-kafka     | 	confluent.basic.auth.user.info = null
benchi-kafka     | 	confluent.bearer.auth.cache.expiry.buffer.seconds = 300
benchi-kafka     | 	confluent.bearer.auth.client.id = null
benchi-kafka     | 	confluent.bearer.auth.client.secret = null
benchi-kafka     | 	confluent.bearer.auth.credentials.source = null
benchi-kafka     | 	confluent.bearer.auth.identity.pool.id = null
benchi-kafka     | 	confluent.bearer.auth.issuer.endpoint.url = null
benchi-kafka     | 	confluent.bearer.auth.logical.cluster = null
benchi-kafka     | 	confluent.bearer.auth.scope = null
benchi-kafka     | 	confluent.bearer.auth.scope.claim.name = scope
benchi-kafka     | 	confluent.bearer.auth.sub.claim.name = sub
benchi-kafka     | 	confluent.bearer.auth.token = null
benchi-kafka     | 	confluent.broker.health.manager.enabled = true
benchi-kafka     | 	confluent.broker.health.manager.engine.request.handler.threads.stuck.criteria = AllThreadsStuck
benchi-kafka     | 	confluent.broker.health.manager.hard.kill.duration.ms = 60000
benchi-kafka     | 	confluent.broker.health.manager.mitigation.enabled = false
benchi-kafka     | 	confluent.broker.health.manager.num.samples.before.broker.suspect = 30
benchi-kafka     | 	confluent.broker.health.manager.num.samples.before.broker.unhealthy = 180
benchi-kafka     | 	confluent.broker.health.manager.percentage.unhealthy.samples.before.broker.suspect = 90
benchi-kafka     | 	confluent.broker.health.manager.percentage.unhealthy.samples.before.broker.unhealthy = 70
benchi-kafka     | 	confluent.broker.health.manager.sample.duration.ms = 1000
benchi-kafka     | 	confluent.broker.health.manager.storage.background.threads.stuck.criteria = AnyThreadStuck
benchi-kafka     | 	confluent.broker.health.manager.storage.network.threads.stuck.criteria = AnyThreadStuck
benchi-kafka     | 	confluent.broker.health.manager.storage.request.handler.threads.stuck.criteria = AnyThreadStuck
benchi-kafka     | 	confluent.broker.limit.consumer.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.broker.limit.producer.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.broker.load.advertised.limit.load = 0.8
benchi-kafka     | 	confluent.broker.load.average.service.request.time.ms = 0.1
benchi-kafka     | 	confluent.broker.load.delay.metric.start.ms = 180000
benchi-kafka     | 	confluent.broker.load.enabled = false
benchi-kafka     | 	confluent.broker.load.num.samples = 60
benchi-kafka     | 	confluent.broker.load.tenant.metric.enable = false
benchi-kafka     | 	confluent.broker.load.update.metric.tags.interval.ms = 60000
benchi-kafka     | 	confluent.broker.load.window.size.ms = 60000
benchi-kafka     | 	confluent.broker.load.workload.coefficient = 20.0
benchi-kafka     | 	confluent.broker.registration.delay.ms = 0
benchi-kafka     | 	confluent.catalog.collector.destination.topic = telemetry.events.data_catalog_source
benchi-kafka     | 	confluent.catalog.collector.enable = false
benchi-kafka     | 	confluent.catalog.collector.full.configs.enable = false
benchi-kafka     | 	confluent.catalog.collector.max.bytes.per.snapshot = 850000
benchi-kafka     | 	confluent.catalog.collector.max.topics.process = 500
benchi-kafka     | 	confluent.catalog.collector.max.zookeeper.request.per.sec = 100
benchi-kafka     | 	confluent.catalog.collector.snapshot.init.delay.sec = 60
benchi-kafka     | 	confluent.catalog.collector.snapshot.interval.sec = 300
benchi-kafka     | 	confluent.ccloud.host.suffixes = .confluent.cloud,.cpdev.cloud,.confluentgov.com,.confluentgov-internal.com
benchi-kafka     | 	confluent.ccloud.intranet.host.suffixes = .intranet.stag.cpdev.cloud,.intranet.stag.cpdev-untrusted.cloud,.intranet.devel.cpdev.cloud,.intranet.devel.cpdev-untrusted.cloud,.intranet.confluent.cloud,.intranet.confluent-untrusted.cloud
benchi-kafka     | 	confluent.cdc.api.keys.topic = 
benchi-kafka     | 	confluent.cdc.api.keys.topic.load.timeout.ms = 600000
benchi-kafka     | 	confluent.cdc.client.quotas.enable = false
benchi-kafka     | 	confluent.cdc.client.quotas.topic.name = 
benchi-kafka     | 	confluent.cdc.lkc.metadata.topic = 
benchi-kafka     | 	confluent.cdc.user.metadata.enable = false
benchi-kafka     | 	confluent.cdc.user.metadata.topic = _confluent-user_metadata
benchi-kafka     | 	confluent.cell.metrics.refresh.period.ms = 60000
benchi-kafka     | 	confluent.cells.default.size = 15
benchi-kafka     | 	confluent.cells.enable = false
benchi-kafka     | 	confluent.cells.implicit.creation.enable = false
benchi-kafka     | 	confluent.cells.load.refresher.enable = true
benchi-kafka     | 	confluent.cells.max.size = 15
benchi-kafka     | 	confluent.cells.min.size = 6
benchi-kafka     | 	confluent.checksum.enabled.files = [none]
benchi-kafka     | 	confluent.clm.enabled = false
benchi-kafka     | 	confluent.clm.frequency.in.hours = 6
benchi-kafka     | 	confluent.clm.list.object.thread_pool.size = 1
benchi-kafka     | 	confluent.clm.max.backup.days = 3
benchi-kafka     | 	confluent.clm.min.delay.in.minutes = 30
benchi-kafka     | 	confluent.clm.thread.pool.size = 2
benchi-kafka     | 	confluent.clm.topic.retention.days.to.backup.days = 0:0,3:3
benchi-kafka     | 	confluent.close.connections.on.credential.delete = false
benchi-kafka     | 	confluent.cluster.link.admin.max.in.flight.requests = 1000
benchi-kafka     | 	confluent.cluster.link.admin.request.batch.size = 1
benchi-kafka     | 	confluent.cluster.link.allow.config.providers = true
benchi-kafka     | 	confluent.cluster.link.allow.legacy.message.format = false
benchi-kafka     | 	confluent.cluster.link.allow.truncation.below.hwm = true
benchi-kafka     | 	confluent.cluster.link.availability.check.mode = ALL
benchi-kafka     | 	confluent.cluster.link.background.thread.affinity = LINK
benchi-kafka     | 	confluent.cluster.link.clients.max.idle.ms = 3153600000000
benchi-kafka     | 	confluent.cluster.link.enable = true
benchi-kafka     | 	confluent.cluster.link.enable.local.admin = false
benchi-kafka     | 	confluent.cluster.link.enable.metrics.reduction = false
benchi-kafka     | 	confluent.cluster.link.fetch.response.min.bytes = 1
benchi-kafka     | 	confluent.cluster.link.fetch.response.total.bytes = 2147483647
benchi-kafka     | 	confluent.cluster.link.fetcher.auto.tune.enable = false
benchi-kafka     | 	confluent.cluster.link.fetcher.thread.pool.mode = ENDPOINT
benchi-kafka     | 	confluent.cluster.link.insync.fetch.response.min.bytes = 1
benchi-kafka     | 	confluent.cluster.link.insync.fetch.response.total.bytes = 2147483647
benchi-kafka     | 	confluent.cluster.link.intranet.connectivity.enable = false
benchi-kafka     | 	confluent.cluster.link.io.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.cluster.link.local.reverse.connection.listener.map = null
benchi-kafka     | 	confluent.cluster.link.max.client.connections = 2147483647
benchi-kafka     | 	confluent.cluster.link.metadata.topic.create.retry.delay.ms = 1000
benchi-kafka     | 	confluent.cluster.link.metadata.topic.enable = false
benchi-kafka     | 	confluent.cluster.link.metadata.topic.min.isr = 2
benchi-kafka     | 	confluent.cluster.link.metadata.topic.partitions = 50
benchi-kafka     | 	confluent.cluster.link.metadata.topic.replication.factor = 1
benchi-kafka     | 	confluent.cluster.link.mirror.transition.batch.size = 10
benchi-kafka     | 	confluent.cluster.link.num.background.threads = 1
benchi-kafka     | 	confluent.cluster.link.periodic.task.batch.size = 2147483647
benchi-kafka     | 	confluent.cluster.link.periodic.task.min.interval.ms = 1000
benchi-kafka     | 	confluent.cluster.link.replica.fetch.connections.mode = combined
benchi-kafka     | 	confluent.cluster.link.replication.quota.mode = CLUSTER_LINK_ONLY
benchi-kafka     | 	confluent.cluster.link.replication.quota.mode.per.tenant.overrides = 
benchi-kafka     | 	confluent.cluster.link.replication.quota.window.num = 11
benchi-kafka     | 	confluent.cluster.link.replication.quota.window.size.seconds = 2
benchi-kafka     | 	confluent.cluster.link.request.quota.capacity = 400
benchi-kafka     | 	confluent.cluster.link.request.quota.request.percentage.multiplier = 1.0
benchi-kafka     | 	confluent.cluster.link.tenant.replication.quota.enable = false
benchi-kafka     | 	confluent.cluster.link.tenant.request.quota.enable = false
benchi-kafka     | 	confluent.cluster.metadata.snapshot.tier.delete.enable = false
benchi-kafka     | 	confluent.cluster.metadata.snapshot.tier.delete.maintain.min.snapshots = 3
benchi-kafka     | 	confluent.cluster.metadata.snapshot.tier.delete.retention.ms = 604800000
benchi-kafka     | 	confluent.cluster.metadata.snapshot.tier.upload.enable = false
benchi-kafka     | 	confluent.compacted.topic.prefer.tier.fetch.ms = -1
benchi-kafka     | 	confluent.connection.invalid.request.delay.enable = false
benchi-kafka     | 	confluent.connections.idle.expiry.manager.ignore.idleness.requests = []
benchi-kafka     | 	confluent.consumer.fetch.partition.pruning.enable = true
benchi-kafka     | 	confluent.consumer.lag.emitter.enabled = false
benchi-kafka     | 	confluent.consumer.lag.emitter.interval.ms = 60000
benchi-kafka     | 	confluent.dataflow.policy.watch.monitor.ms = 300000
benchi-kafka     | 	confluent.defer.isr.shrink.enable = false
benchi-kafka     | 	confluent.describe.topic.partitions.enabled = false
benchi-kafka     | 	confluent.disk.io.manager.enable = false
benchi-kafka     | 	confluent.disk.throughput.headroom = 10485760
benchi-kafka     | 	confluent.disk.throughput.limit = 10485760000
benchi-kafka     | 	confluent.disk.throughput.quota.tier.archive = 1048576000
benchi-kafka     | 	confluent.disk.throughput.quota.tier.archive.throttled = 104857600
benchi-kafka     | 	confluent.durability.audit.batch.flush.frequency.ms = 900000
benchi-kafka     | 	confluent.durability.audit.checks = PeriodicalAudit,ChecksumAudit
benchi-kafka     | 	confluent.durability.audit.enable = false
benchi-kafka     | 	confluent.durability.audit.idempotent.producer = false
benchi-kafka     | 	confluent.durability.audit.initial.job.delay.ms = 900000
benchi-kafka     | 	confluent.durability.audit.io.bytes.per.sec = 10485760
benchi-kafka     | 	confluent.durability.audit.log.ignored.event.types = 
benchi-kafka     | 	confluent.durability.audit.reporting.batch.ms = 1800000
benchi-kafka     | 	confluent.durability.audit.tier.compaction.audit.duration.ms = 14400000
benchi-kafka     | 	confluent.durability.events.allowed = OffsetChangeType,EpochChangeType,IsrExpandType,DeleteRecordsType,RetentionChangeType,StartOffsetChangeType,DeletePartitionType,HealthCheckType
benchi-kafka     | 	confluent.durability.topic.partition.count = 50
benchi-kafka     | 	confluent.durability.topic.replication.factor = 1
benchi-kafka     | 	confluent.e2e_checksum.protection.enabled = false
benchi-kafka     | 	confluent.e2e_checksum.protection.files = [none]
benchi-kafka     | 	confluent.e2e_checksum.protection.store.entry.ttl.ms = 2592000000
benchi-kafka     | 	confluent.elastic.cku.enabled = false
benchi-kafka     | 	confluent.elastic.cku.scaletozero.enabled = false
benchi-kafka     | 	confluent.eligible.controllers = []
benchi-kafka     | 	confluent.enable.stray.partition.deletion = false
benchi-kafka     | 	confluent.encryption.key.manager.rotation.interval.ms = 31536000000
benchi-kafka     | 	confluent.fetch.from.follower.require.leader.epoch.enable = false
benchi-kafka     | 	confluent.fetch.partition.pruning.enable = true
benchi-kafka     | 	confluent.floor.connection.rate.per.ip = -1.0
benchi-kafka     | 	confluent.floor.connection.rate.per.tenant = -1.0
benchi-kafka     | 	confluent.group.coordinator.offsets.batching.enable = false
benchi-kafka     | 	confluent.group.coordinator.offsets.writer.threads = 2
benchi-kafka     | 	confluent.group.metadata.load.threads = 32
benchi-kafka     | 	confluent.heap.tenured.notify.bytes = 0
benchi-kafka     | 	confluent.heap.tenured.notify.enabled = false
benchi-kafka     | 	confluent.hot.partition.ratio = 0.8
benchi-kafka     | 	confluent.http.server.start.timeout.ms = 60000
benchi-kafka     | 	confluent.http.server.stop.timeout.ms = 30000
benchi-kafka     | 	confluent.internal.metrics.enable = false
benchi-kafka     | 	confluent.internal.rest.server.bind.port = null
benchi-kafka     | 	confluent.internal.tenant.scoped.listener.name = INTERNAL_TENANT_SCOPED
benchi-kafka     | 	confluent.leader.epoch.checkpoint.checksum.enabled = false
benchi-kafka     | 	confluent.log.cleaner.timestamp.validation.enable = true
benchi-kafka     | 	confluent.log.placement.constraints = 
benchi-kafka     | 	confluent.max.connection.creation.rate.per.ip = 1.7976931348623157E308
benchi-kafka     | 	confluent.max.connection.creation.rate.per.tenant = 1.7976931348623157E308
benchi-kafka     | 	confluent.max.connection.rate.per.ip = -1.0
benchi-kafka     | 	confluent.max.connection.rate.per.tenant = -1.0
benchi-kafka     | 	confluent.max.connection.throttle.ms = null
benchi-kafka     | 	confluent.max.segment.ms = 9223372036854775807
benchi-kafka     | 	confluent.metadata.active.encryptor = null
benchi-kafka     | 	confluent.metadata.controlled.shutdown.partition.slice.delay.ms = 100
benchi-kafka     | 	confluent.metadata.encryptor.classes = null
benchi-kafka     | 	confluent.metadata.encryptor.required = false
benchi-kafka     | 	confluent.metadata.encryptor.secret.file = null
benchi-kafka     | 	confluent.metadata.encryptor.secrets = null
benchi-kafka     | 	confluent.metadata.jvm.warmup.ms = 60000
benchi-kafka     | 	confluent.metadata.leader.balance.slice.delay.ms = 100
benchi-kafka     | 	confluent.metadata.max.controlled.shutdown.partition.changes.per.slice = 1000
benchi-kafka     | 	confluent.metadata.max.leader.balance.changes.per.slice = 1000
benchi-kafka     | 	confluent.metadata.server.cluster.registry.clusters = []
benchi-kafka     | 	confluent.metadata.throttle.pre.check.for.non.wildcard.requests.enable = false
benchi-kafka     | 	confluent.metadata.throttle.pre.check.for.wildcard.requests.enable = false
benchi-kafka     | 	confluent.metrics.reporter.bootstrap.servers = localhost:9092
benchi-kafka     | 	confluent.min.acks = 0
benchi-kafka     | 	confluent.min.connection.throttle.ms = 0
benchi-kafka     | 	confluent.min.segment.ms = 1
benchi-kafka     | 	confluent.missing.id.cache.ttl.sec = 60
benchi-kafka     | 	confluent.missing.id.query.range = 20000
benchi-kafka     | 	confluent.missing.schema.cache.ttl.sec = 60
benchi-kafka     | 	confluent.mtls.enable = false
benchi-kafka     | 	confluent.mtls.listener.name = EXTERNAL
benchi-kafka     | 	confluent.mtls.sasl.authenticator.request.max.bytes = 104857600
benchi-kafka     | 	confluent.mtls.truststore.manager.class.name = null
benchi-kafka     | 	confluent.multitenant.authorizer.enable.acl.state = false
benchi-kafka     | 	confluent.multitenant.interceptor.balancer.apis.enabled = false
benchi-kafka     | 	confluent.multitenant.interceptor.collect.client.apiversions.max.per.tenant = 1000
benchi-kafka     | 	confluent.multitenant.interceptor.collect.client.apiversions.metric = false
benchi-kafka     | 	confluent.multitenant.listener.hostname.cluster.prefix.enable = false
benchi-kafka     | 	confluent.multitenant.listener.hostname.subdomain.suffix.enable = false
benchi-kafka     | 	confluent.multitenant.listener.names = null
benchi-kafka     | 	confluent.multitenant.parse.lkc.id.enable = false
benchi-kafka     | 	confluent.multitenant.parse.sni.host.name.enable = false
benchi-kafka     | 	confluent.network.health.manager.enabled = false
benchi-kafka     | 	confluent.network.health.manager.external.listener.name = EXTERNAL
benchi-kafka     | 	confluent.network.health.manager.externalconnectivitystartup.enabled = false
benchi-kafka     | 	confluent.network.health.manager.min.healthy.network.samples = 3
benchi-kafka     | 	confluent.network.health.manager.min.percentage.healthy.network.samples = 3
benchi-kafka     | 	confluent.network.health.manager.mitigation.enabled = false
benchi-kafka     | 	confluent.network.health.manager.network.sample.window.size = 120
benchi-kafka     | 	confluent.network.health.manager.sample.duration.ms = 1000
benchi-kafka     | 	confluent.oauth.flat.networking.verification.enable = false
benchi-kafka     | 	confluent.offsets.log.cleaner.delete.retention.ms = 86400000
benchi-kafka     | 	confluent.offsets.log.cleaner.max.compaction.lag.ms = 9223372036854775807
benchi-kafka     | 	confluent.offsets.log.cleaner.min.cleanable.dirty.ratio = 0.5
benchi-kafka     | 	confluent.offsets.topic.placement.constraints = 
benchi-kafka     | 	confluent.omit.network.processor.metric.tag = false
benchi-kafka     | 	confluent.operator.managed = false
benchi-kafka     | 	confluent.password.encoder.old.secret.ttl.ms = 9223372036854775807
benchi-kafka     | 	confluent.plugins.cluster.link.policy.max.destination.links.per.tenant = 10
benchi-kafka     | 	confluent.plugins.cluster.link.policy.max.source.links.per.tenant = 10
benchi-kafka     | 	confluent.plugins.topic.policy.max.partitions.per.cluster = 2147483647
benchi-kafka     | 	confluent.plugins.topic.policy.max.partitions.per.tenant = 512
benchi-kafka     | 	confluent.plugins.topic.policy.max.replicas.per.broker = 2147483647
benchi-kafka     | 	confluent.plugins.topic.policy.max.topics.per.cluster = 2147483647
benchi-kafka     | 	confluent.prefer.tier.fetch.ms = -1
benchi-kafka     | 	confluent.produce.throttle.pre.check.enable = false
benchi-kafka     | 	confluent.producer.id.cache.broker.hard.limit = -1
benchi-kafka     | 	confluent.producer.id.cache.eviction.minimal.expiration.ms = 900000
benchi-kafka     | 	confluent.producer.id.cache.extra.eviction.percentage = 0
benchi-kafka     | 	confluent.producer.id.cache.limit = 2147483647
benchi-kafka     | 	confluent.producer.id.cache.partition.hard.limit = -1
benchi-kafka     | 	confluent.producer.id.cache.tenant.hard.limit = -1
benchi-kafka     | 	confluent.producer.id.quota.manager.enable = false
benchi-kafka     | 	confluent.producer.id.quota.window.num = 11
benchi-kafka     | 	confluent.producer.id.quota.window.size.seconds = 1
benchi-kafka     | 	confluent.producer.id.throttle.enable = false
benchi-kafka     | 	confluent.producer.id.throttle.enable.threshold.percentage = 100
benchi-kafka     | 	confluent.proxy.mode.local.default = false
benchi-kafka     | 	confluent.proxy.protocol.fallback.enabled = false
benchi-kafka     | 	confluent.proxy.protocol.version = NONE
benchi-kafka     | 	confluent.quota.computing.usage.adjustment = 0.5
benchi-kafka     | 	confluent.quota.dynamic.enable = false
benchi-kafka     | 	confluent.quota.dynamic.publishing.interval.ms = 60000
benchi-kafka     | 	confluent.quota.dynamic.reporting.interval.ms = 30000
benchi-kafka     | 	confluent.quota.dynamic.reporting.min.usage = 102400
benchi-kafka     | 	confluent.quota.tenant.broker.max.consumer.rate = 13107200
benchi-kafka     | 	confluent.quota.tenant.broker.max.producer.rate = 13107200
benchi-kafka     | 	confluent.quota.tenant.default.controller.mutation.rate = 2.147483647E9
benchi-kafka     | 	confluent.quota.tenant.default.producer.id.rate = 2.147483647E9
benchi-kafka     | 	confluent.quota.tenant.fetch.multiplier = 1.0
benchi-kafka     | 	confluent.quota.tenant.follower.broker.min.consumer.rate = 10485760
benchi-kafka     | 	confluent.quota.tenant.follower.broker.min.producer.rate = 10485760
benchi-kafka     | 	confluent.quota.tenant.produce.multiplier = 1.0
benchi-kafka     | 	confluent.quota.tenant.user.quotas.enable = false
benchi-kafka     | 	confluent.regional.metadata.client.class = null
benchi-kafka     | 	confluent.regional.resource.manager.client.scheduler.threads = 2
benchi-kafka     | 	confluent.regional.resource.manager.endpoint = null
benchi-kafka     | 	confluent.regional.resource.manager.watch.endpoint = null
benchi-kafka     | 	confluent.replica.fetch.backoff.max.ms = 1000
benchi-kafka     | 	confluent.replica.fetch.connections.mode = combined
benchi-kafka     | 	confluent.replication.mode = PULL
benchi-kafka     | 	confluent.replication.push.feature.enable = false
benchi-kafka     | 	confluent.reporters.telemetry.auto.enable = true
benchi-kafka     | 	confluent.request.log.filter.class = class org.apache.kafka.common.requests.SamplingRequestLogFilter
benchi-kafka     | 	confluent.request.pipelining.enable = true
benchi-kafka     | 	confluent.request.pipelining.max.in.flight.requests.per.connection = 5
benchi-kafka     | 	confluent.require.calling.resource.identity = false
benchi-kafka     | 	confluent.require.compatible.keystore.updates = true
benchi-kafka     | 	confluent.require.confluent.issuer = false
benchi-kafka     | 	confluent.roll.check.interval.ms = 300000
benchi-kafka     | 	confluent.schema.registry.max.cache.size = 10000
benchi-kafka     | 	confluent.schema.registry.max.retries = 1
benchi-kafka     | 	confluent.schema.registry.retries.wait.ms = 0
benchi-kafka     | 	confluent.schema.registry.url = null
benchi-kafka     | 	confluent.schema.validator.interceptor.class = io.confluent.kafka.schemaregistry.validator.RecordSchemaValidator
benchi-kafka     | 	confluent.schema.validator.multitenant.enable = false
benchi-kafka     | 	confluent.schema.validator.samples.per.min = 0
benchi-kafka     | 	confluent.security.bc.approved.mode.enable = false
benchi-kafka     | 	confluent.security.event.logger.authentication.enable = false
benchi-kafka     | 	confluent.security.event.logger.authentication.event.rate.limit = -1
benchi-kafka     | 	confluent.security.event.logger.authorization.event.rate.limit = -1
benchi-kafka     | 	confluent.security.event.logger.detailed.audit.logs.filter.class = class org.apache.kafka.common.requests.DetailedRequestAuditLogFilter
benchi-kafka     | 	confluent.security.event.logger.enable = true
benchi-kafka     | 	confluent.security.event.logger.kafka.request.rate.limit = -1
benchi-kafka     | 	confluent.security.event.logger.physical.cluster.id = 
benchi-kafka     | 	confluent.security.event.router.config = 
benchi-kafka     | 	confluent.security.revoked.certificate.ids = 
benchi-kafka     | 	confluent.segment.eager.roll.enable = false
benchi-kafka     | 	confluent.segment.speculative.prefetch.enable = false
benchi-kafka     | 	confluent.spiffe.id.principal.extraction.rules = 
benchi-kafka     | 	confluent.ssl.key.password = null
benchi-kafka     | 	confluent.ssl.keystore.location = null
benchi-kafka     | 	confluent.ssl.keystore.password = null
benchi-kafka     | 	confluent.ssl.keystore.type = null
benchi-kafka     | 	confluent.ssl.protocol = null
benchi-kafka     | 	confluent.ssl.truststore.location = null
benchi-kafka     | 	confluent.ssl.truststore.password = null
benchi-kafka     | 	confluent.ssl.truststore.type = null
benchi-kafka     | 	confluent.step.connection.rate.per.ip = -1.0
benchi-kafka     | 	confluent.step.connection.rate.per.tenant = -1.0
benchi-kafka     | 	confluent.storage.probe.period.ms = -1
benchi-kafka     | 	confluent.storage.probe.slow.write.threshold.ms = 5000
benchi-kafka     | 	confluent.stray.log.delete.delay.ms = 604800000
benchi-kafka     | 	confluent.stray.log.max.deletions.per.run = 72
benchi-kafka     | 	confluent.subdomain.prefix = null
benchi-kafka     | 	confluent.subdomain.separator.map = null
benchi-kafka     | 	confluent.subdomain.separator.variable = %sep
benchi-kafka     | 	confluent.system.time.roll.enable = false
benchi-kafka     | 	confluent.telemetry.enabled = false
benchi-kafka     | 	confluent.telemetry.external.client.metrics.push.enabled = false
benchi-kafka     | 	confluent.tenant.latency.metric.enabled = false
benchi-kafka     | 	confluent.tier.archiver.num.threads = 2
benchi-kafka     | 	confluent.tier.azure.block.blob.auto.abort.threshold.bytes = 500000
benchi-kafka     | 	confluent.tier.azure.block.blob.container = null
benchi-kafka     | 	confluent.tier.azure.block.blob.cred.file.path = null
benchi-kafka     | 	confluent.tier.azure.block.blob.endpoint = null
benchi-kafka     | 	confluent.tier.azure.block.blob.prefix = 
benchi-kafka     | 	confluent.tier.backend = 
benchi-kafka     | 	confluent.tier.bucket.probe.period.ms = -1
benchi-kafka     | 	confluent.tier.cleaner.compact.min.efficiency = 0.5
benchi-kafka     | 	confluent.tier.cleaner.compact.segment.min.bytes = 20971520
benchi-kafka     | 	confluent.tier.cleaner.dedupe.buffer.size = 134217728
benchi-kafka     | 	confluent.tier.cleaner.dual.compaction = false
benchi-kafka     | 	confluent.tier.cleaner.dual.compaction.validation.max.bytes = 1073741824
benchi-kafka     | 	confluent.tier.cleaner.dual.compaction.validation.percent = 0
benchi-kafka     | 	confluent.tier.cleaner.enable = false
benchi-kafka     | 	confluent.tier.cleaner.excluded.topics = [^_confluent.*]
benchi-kafka     | 	confluent.tier.cleaner.feature.enable = false
benchi-kafka     | 	confluent.tier.cleaner.io.buffer.load.factor = 0.9
benchi-kafka     | 	confluent.tier.cleaner.io.buffer.size = 10485760
benchi-kafka     | 	confluent.tier.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
benchi-kafka     | 	confluent.tier.cleaner.min.cleanable.ratio = 0.75
benchi-kafka     | 	confluent.tier.cleaner.num.threads = 2
benchi-kafka     | 	confluent.tier.enable = false
benchi-kafka     | 	confluent.tier.feature = false
benchi-kafka     | 	confluent.tier.fenced.segment.delete.delay.ms = 600000
benchi-kafka     | 	confluent.tier.fetcher.async.enable = false
benchi-kafka     | 	confluent.tier.fetcher.async.timestamp.offset.parallelism = 1
benchi-kafka     | 	confluent.tier.fetcher.fetch.based.on.segment_and_metadata_layout.field = false
benchi-kafka     | 	confluent.tier.fetcher.memorypool.bytes = 0
benchi-kafka     | 	confluent.tier.fetcher.num.threads = 4
benchi-kafka     | 	confluent.tier.fetcher.offset.cache.expiration.ms = 1800000
benchi-kafka     | 	confluent.tier.fetcher.offset.cache.period.ms = 60000
benchi-kafka     | 	confluent.tier.fetcher.offset.cache.size = 200000
benchi-kafka     | 	confluent.tier.gcs.bucket = null
benchi-kafka     | 	confluent.tier.gcs.cred.file.path = null
benchi-kafka     | 	confluent.tier.gcs.prefix = 
benchi-kafka     | 	confluent.tier.gcs.region = null
benchi-kafka     | 	confluent.tier.gcs.sse.customer.encryption.key = null
benchi-kafka     | 	confluent.tier.gcs.write.chunk.size = 0
benchi-kafka     | 	confluent.tier.local.hotset.bytes = -1
benchi-kafka     | 	confluent.tier.local.hotset.ms = 86400000
benchi-kafka     | 	confluent.tier.max.partition.fetch.bytes.override = 0
benchi-kafka     | 	confluent.tier.metadata.bootstrap.servers = null
benchi-kafka     | 	confluent.tier.metadata.max.poll.ms = 100
benchi-kafka     | 	confluent.tier.metadata.namespace = null
benchi-kafka     | 	confluent.tier.metadata.num.partitions = 50
benchi-kafka     | 	confluent.tier.metadata.replication.factor = 1
benchi-kafka     | 	confluent.tier.metadata.request.timeout.ms = 30000
benchi-kafka     | 	confluent.tier.metadata.snapshots.enable = false
benchi-kafka     | 	confluent.tier.metadata.snapshots.interval.ms = 86400000
benchi-kafka     | 	confluent.tier.metadata.snapshots.retention.days = 7
benchi-kafka     | 	confluent.tier.metadata.snapshots.threads = 2
benchi-kafka     | 	confluent.tier.object.fetcher.num.threads = 1
benchi-kafka     | 	confluent.tier.partition.state.cleanup.delay.ms = 2592000000
benchi-kafka     | 	confluent.tier.partition.state.cleanup.enable = false
benchi-kafka     | 	confluent.tier.partition.state.cleanup.interval.ms = 86400000
benchi-kafka     | 	confluent.tier.partition.state.commit.interval.ms = 15000
benchi-kafka     | 	confluent.tier.s3.assumerole.arn = null
benchi-kafka     | 	confluent.tier.s3.auto.abort.threshold.bytes = 500000
benchi-kafka     | 	confluent.tier.s3.aws.endpoint.override = null
benchi-kafka     | 	confluent.tier.s3.aws.signer.override = null
benchi-kafka     | 	confluent.tier.s3.bucket = null
benchi-kafka     | 	confluent.tier.s3.cred.file.path = null
benchi-kafka     | 	confluent.tier.s3.force.path.style.access = false
benchi-kafka     | 	confluent.tier.s3.prefix = 
benchi-kafka     | 	confluent.tier.s3.region = null
benchi-kafka     | 	confluent.tier.s3.security.providers = null
benchi-kafka     | 	confluent.tier.s3.sse.algorithm = AES256
benchi-kafka     | 	confluent.tier.s3.sse.customer.encryption.key = null
benchi-kafka     | 	confluent.tier.s3.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	confluent.tier.s3.ssl.key.password = null
benchi-kafka     | 	confluent.tier.s3.ssl.keystore.location = null
benchi-kafka     | 	confluent.tier.s3.ssl.keystore.password = null
benchi-kafka     | 	confluent.tier.s3.ssl.keystore.type = null
benchi-kafka     | 	confluent.tier.s3.ssl.protocol = TLSv1.3
benchi-kafka     | 	confluent.tier.s3.ssl.provider = null
benchi-kafka     | 	confluent.tier.s3.ssl.truststore.location = null
benchi-kafka     | 	confluent.tier.s3.ssl.truststore.password = null
benchi-kafka     | 	confluent.tier.s3.ssl.truststore.type = null
benchi-kafka     | 	confluent.tier.s3.user.agent.prefix = APN/1.0 Confluent/1.0 TieredStorageS3/1.0
benchi-kafka     | 	confluent.tier.segment.hotset.roll.min.bytes = 104857600
benchi-kafka     | 	confluent.tier.segment.metadata.layout.put.mode = LegacyMultiObject
benchi-kafka     | 	confluent.tier.topic.data.loss.validation.fencing.enable = false
benchi-kafka     | 	confluent.tier.topic.delete.backoff.ms = 21600000
benchi-kafka     | 	confluent.tier.topic.delete.check.interval.ms = 300000
benchi-kafka     | 	confluent.tier.topic.delete.max.inprogress.partitions = 100
benchi-kafka     | 	confluent.tier.topic.head.data.loss.validation.enable = true
benchi-kafka     | 	confluent.tier.topic.head.data.loss.validation.max.timeout.ms = 900000
benchi-kafka     | 	confluent.tier.topic.materialization.from.snapshot.enable = false
benchi-kafka     | 	confluent.tier.topic.producer.enable.idempotence = true
benchi-kafka     | 	confluent.tier.topic.snapshots.enable = false
benchi-kafka     | 	confluent.tier.topic.snapshots.interval.ms = 300000
benchi-kafka     | 	confluent.tier.topic.snapshots.max.records = 100000
benchi-kafka     | 	confluent.tier.topic.snapshots.retention.hours = 168
benchi-kafka     | 	confluent.topic.partition.default.placement = 2
benchi-kafka     | 	confluent.topic.policy.use.computed.assignments = false
benchi-kafka     | 	confluent.topic.replica.assignor.builder.class = 
benchi-kafka     | 	confluent.track.api.key.per.ip = false
benchi-kafka     | 	confluent.track.per.ip.max.size = 100000
benchi-kafka     | 	confluent.track.tenant.id.per.ip = false
benchi-kafka     | 	confluent.traffic.cdc.network.id.routes.enable = false
benchi-kafka     | 	confluent.traffic.cdc.network.id.routes.listener.names = EXTERNAL_BACKCHANNEL
benchi-kafka     | 	confluent.traffic.cdc.network.id.routes.periodic.start.task.ms = 300000
benchi-kafka     | 	confluent.traffic.cdc.network.id.routes.topic.name = _confluent-network_id_routes
benchi-kafka     | 	confluent.traffic.network.id = 
benchi-kafka     | 	confluent.transaction.2pc.timeout.ms = -1
benchi-kafka     | 	confluent.transaction.logging.verbosity = 0
benchi-kafka     | 	confluent.transaction.state.log.placement.constraints = 
benchi-kafka     | 	confluent.valid.broker.rack.set = null
benchi-kafka     | 	confluent.verify.group.subscription.prefix = false
benchi-kafka     | 	confluent.virtual.topic.creation.enabled = false
benchi-kafka     | 	confluent.zookeeper.metadata.migration.controller.check.disable = false
benchi-kafka     | 	confluent.zookeeper.metadata.migration.trigger.file.path = null
benchi-kafka     | 	connection.failed.authentication.delay.ms = 100
benchi-kafka     | 	connection.min.expire.interval.ms = 250
benchi-kafka     | 	connections.max.age.ms = 3153600000000
benchi-kafka     | 	connections.max.idle.ms = 600000
benchi-kafka     | 	connections.max.reauth.ms = 0
benchi-kafka     | 	control.plane.listener.name = null
benchi-kafka     | 	controlled.shutdown.enable = true
benchi-kafka     | 	controlled.shutdown.max.retries = 3
benchi-kafka     | 	controlled.shutdown.retry.backoff.ms = 5000
benchi-kafka     | 	controller.listener.names = CONTROLLER
benchi-kafka     | 	controller.quorum.append.linger.ms = 25
benchi-kafka     | 	controller.quorum.bootstrap.servers = []
benchi-kafka     | 	controller.quorum.election.backoff.max.ms = 1000
benchi-kafka     | 	controller.quorum.election.timeout.ms = 1000
benchi-kafka     | 	controller.quorum.fetch.timeout.ms = 2000
benchi-kafka     | 	controller.quorum.request.timeout.ms = 2000
benchi-kafka     | 	controller.quorum.retry.backoff.ms = 20
benchi-kafka     | 	controller.quorum.voters = [1@broker:29093]
benchi-kafka     | 	controller.quota.window.num = 11
benchi-kafka     | 	controller.quota.window.size.seconds = 1
benchi-kafka     | 	controller.socket.timeout.ms = 30000
benchi-kafka     | 	create.cluster.link.policy.class.name = null
benchi-kafka     | 	create.topic.policy.class.name = null
benchi-kafka     | 	default.replication.factor = 1
benchi-kafka     | 	delegation.token.expiry.check.interval.ms = 3600000
benchi-kafka     | 	delegation.token.expiry.time.ms = 86400000
benchi-kafka     | 	delegation.token.master.key = null
benchi-kafka     | 	delegation.token.max.lifetime.ms = 604800000
benchi-kafka     | 	delegation.token.secret.key = null
benchi-kafka     | 	delete.records.purgatory.purge.interval.requests = 1
benchi-kafka     | 	delete.topic.enable = true
benchi-kafka     | 	early.start.listeners = null
benchi-kafka     | 	eligible.leader.replicas.enable = false
benchi-kafka     | 	enable.fips = false
benchi-kafka     | 	fetch.max.bytes = 57671680
benchi-kafka     | 	fetch.purgatory.purge.interval.requests = 1000
benchi-kafka     | 	floor.max.connection.creation.rate = null
benchi-kafka     | 	follower.replication.throttled.rate = 9223372036854775807
benchi-kafka     | 	follower.replication.throttled.replicas = none
benchi-kafka     | 	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
benchi-kafka     | 	group.consumer.heartbeat.interval.ms = 5000
benchi-kafka     | 	group.consumer.max.heartbeat.interval.ms = 15000
benchi-kafka     | 	group.consumer.max.session.timeout.ms = 60000
benchi-kafka     | 	group.consumer.max.size = 2147483647
benchi-kafka     | 	group.consumer.migration.policy = disabled
benchi-kafka     | 	group.consumer.min.heartbeat.interval.ms = 5000
benchi-kafka     | 	group.consumer.min.session.timeout.ms = 45000
benchi-kafka     | 	group.consumer.session.timeout.ms = 45000
benchi-kafka     | 	group.coordinator.append.linger.ms = 10
benchi-kafka     | 	group.coordinator.new.enable = false
benchi-kafka     | 	group.coordinator.rebalance.protocols = [classic]
benchi-kafka     | 	group.coordinator.threads = 1
benchi-kafka     | 	group.initial.rebalance.delay.ms = 0
benchi-kafka     | 	group.max.session.timeout.ms = 1800000
benchi-kafka     | 	group.max.size = 2147483647
benchi-kafka     | 	group.min.session.timeout.ms = 6000
benchi-kafka     | 	initial.broker.registration.timeout.ms = 60000
benchi-kafka     | 	inter.broker.listener.name = PLAINTEXT
benchi-kafka     | 	inter.broker.protocol.version = 3.8-IV0A
benchi-kafka     | 	k2.stack.builder.class.name = null
benchi-kafka     | 	k2.startup.timeout.ms = 60000
benchi-kafka     | 	k2.topic.metadata.max.total.partition.count = 20000
benchi-kafka     | 	k2.topic.metadata.refresh.ms = 10000
benchi-kafka     | 	kafka.metrics.polling.interval.secs = 10
benchi-kafka     | 	kafka.metrics.reporters = []
benchi-kafka     | 	leader.imbalance.check.interval.seconds = 300
benchi-kafka     | 	leader.imbalance.per.broker.percentage = 10
benchi-kafka     | 	leader.replication.throttled.rate = 9223372036854775807
benchi-kafka     | 	leader.replication.throttled.replicas = none
benchi-kafka     | 	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
benchi-kafka     | 	listeners = PLAINTEXT://broker:29092,CONTROLLER://broker:29093,PLAINTEXT_HOST://0.0.0.0:9092
benchi-kafka     | 	log.cleaner.backoff.ms = 15000
benchi-kafka     | 	log.cleaner.dedupe.buffer.size = 134217728
benchi-kafka     | 	log.cleaner.delete.retention.ms = 86400000
benchi-kafka     | 	log.cleaner.enable = true
benchi-kafka     | 	log.cleaner.hash.algorithm = MD5
benchi-kafka     | 	log.cleaner.io.buffer.load.factor = 0.9
benchi-kafka     | 	log.cleaner.io.buffer.size = 524288
benchi-kafka     | 	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
benchi-kafka     | 	log.cleaner.max.compaction.lag.ms = 9223372036854775807
benchi-kafka     | 	log.cleaner.min.cleanable.ratio = 0.5
benchi-kafka     | 	log.cleaner.min.compaction.lag.ms = 0
benchi-kafka     | 	log.cleaner.threads = 1
benchi-kafka     | 	log.cleanup.policy = [delete]
benchi-kafka     | 	log.deletion.max.segments.per.run = 2147483647
benchi-kafka     | 	log.deletion.throttler.disk.free.headroom.bytes = 21474836480
benchi-kafka     | 	log.dir = /tmp/kafka-logs
benchi-kafka     | 	log.dir.failure.timeout.ms = 30000
benchi-kafka     | 	log.dirs = /tmp/kraft-combined-logs
benchi-kafka     | 	log.flush.interval.messages = 9223372036854775807
benchi-kafka     | 	log.flush.interval.ms = null
benchi-kafka     | 	log.flush.offset.checkpoint.interval.ms = 60000
benchi-kafka     | 	log.flush.scheduler.interval.ms = 9223372036854775807
benchi-kafka     | 	log.flush.start.offset.checkpoint.interval.ms = 60000
benchi-kafka     | 	log.index.interval.bytes = 4096
benchi-kafka     | 	log.index.size.max.bytes = 10485760
benchi-kafka     | 	log.initial.task.delay.ms = 30000
benchi-kafka     | 	log.local.retention.bytes = -2
benchi-kafka     | 	log.local.retention.ms = -2
benchi-kafka     | 	log.message.downconversion.enable = true
benchi-kafka     | 	log.message.format.version = 3.0-IV1
benchi-kafka     | 	log.message.timestamp.after.max.ms = 9223372036854775807
benchi-kafka     | 	log.message.timestamp.before.max.ms = 9223372036854775807
benchi-kafka     | 	log.message.timestamp.difference.max.ms = 9223372036854775807
benchi-kafka     | 	log.message.timestamp.type = CreateTime
benchi-kafka     | 	log.preallocate = false
benchi-kafka     | 	log.retention.bytes = -1
benchi-kafka     | 	log.retention.check.interval.ms = 300000
benchi-kafka     | 	log.retention.hours = 168
benchi-kafka     | 	log.retention.minutes = null
benchi-kafka     | 	log.retention.ms = null
benchi-kafka     | 	log.roll.hours = 168
benchi-kafka     | 	log.roll.jitter.hours = 0
benchi-kafka     | 	log.roll.jitter.ms = null
benchi-kafka     | 	log.roll.ms = null
benchi-kafka     | 	log.segment.bytes = 1073741824
benchi-kafka     | 	log.segment.delete.delay.ms = 60000
benchi-kafka     | 	max.connection.creation.rate = 1.7976931348623157E308
benchi-kafka     | 	max.connection.creation.rate.per.ip.enable.threshold = 0.0
benchi-kafka     | 	max.connection.creation.rate.per.tenant.enable.threshold = 0.0
benchi-kafka     | 	max.connections = 2147483647
benchi-kafka     | 	max.connections.per.ip = 2147483647
benchi-kafka     | 	max.connections.per.ip.overrides = 
benchi-kafka     | 	max.connections.per.tenant = 0
benchi-kafka     | 	max.connections.protected.listeners = []
benchi-kafka     | 	max.connections.reap.amount = 0
benchi-kafka     | 	max.incremental.fetch.session.cache.slots = 1000
benchi-kafka     | 	max.request.partition.size.limit = 2000
benchi-kafka     | 	message.max.bytes = 1048588
benchi-kafka     | 	metadata.log.dir = null
benchi-kafka     | 	metadata.log.max.record.bytes.between.snapshots = 20971520
benchi-kafka     | 	metadata.log.max.snapshot.interval.ms = 3600000
benchi-kafka     | 	metadata.log.segment.bytes = 1073741824
benchi-kafka     | 	metadata.log.segment.min.bytes = 8388608
benchi-kafka     | 	metadata.log.segment.ms = 604800000
benchi-kafka     | 	metadata.max.idle.interval.ms = 500
benchi-kafka     | 	metadata.max.retention.bytes = 104857600
benchi-kafka     | 	metadata.max.retention.ms = 604800000
benchi-kafka     | 	metric.reporters = [io.confluent.metrics.reporter.ConfluentMetricsReporter]
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.recording.level = INFO
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	min.insync.replicas = 1
benchi-kafka     | 	multitenant.authorizer.support.resource.ids = false
benchi-kafka     | 	multitenant.metadata.class = null
benchi-kafka     | 	multitenant.metadata.dir = null
benchi-kafka     | 	multitenant.metadata.reload.delay.ms = 120000
benchi-kafka     | 	multitenant.metadata.ssl.certs.path = null
benchi-kafka     | 	multitenant.tenant.delete.batch.size = 10
benchi-kafka     | 	multitenant.tenant.delete.check.ms = 120000
benchi-kafka     | 	multitenant.tenant.delete.delay = 604800000
benchi-kafka     | 	node.id = 1
benchi-kafka     | 	num.io.threads = 8
benchi-kafka     | 	num.network.threads = 3
benchi-kafka     | 	num.partitions = 1
benchi-kafka     | 	num.recovery.threads.per.data.dir = 1
benchi-kafka     | 	num.replica.alter.log.dirs.threads = null
benchi-kafka     | 	num.replica.fetchers = 1
benchi-kafka     | 	offset.metadata.max.bytes = 4096
benchi-kafka     | 	offsets.commit.required.acks = -1
benchi-kafka     | 	offsets.commit.timeout.ms = 5000
benchi-kafka     | 	offsets.load.buffer.size = 5242880
benchi-kafka     | 	offsets.retention.check.interval.ms = 600000
benchi-kafka     | 	offsets.retention.minutes = 10080
benchi-kafka     | 	offsets.topic.compression.codec = 0
benchi-kafka     | 	offsets.topic.num.partitions = 50
benchi-kafka     | 	offsets.topic.replication.factor = 1
benchi-kafka     | 	offsets.topic.segment.bytes = 104857600
benchi-kafka     | 	otel.exporter.otlp.custom.endpoint = default
benchi-kafka     | 	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
benchi-kafka     | 	password.encoder.iterations = 4096
benchi-kafka     | 	password.encoder.key.length = 128
benchi-kafka     | 	password.encoder.keyfactory.algorithm = null
benchi-kafka     | 	password.encoder.old.secret = null
benchi-kafka     | 	password.encoder.secret = null
benchi-kafka     | 	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
benchi-kafka     | 	process.roles = [broker, controller]
benchi-kafka     | 	producer.id.expiration.check.interval.ms = 600000
benchi-kafka     | 	producer.id.expiration.ms = 86400000
benchi-kafka     | 	producer.purgatory.purge.interval.requests = 1000
benchi-kafka     | 	queued.max.request.bytes = -1
benchi-kafka     | 	queued.max.requests = 500
benchi-kafka     | 	quota.window.num = 11
benchi-kafka     | 	quota.window.size.seconds = 1
benchi-kafka     | 	quotas.consumption.expiration.time.ms = 600000
benchi-kafka     | 	quotas.expiration.interval.ms = 3600000
benchi-kafka     | 	quotas.expiration.time.ms = 604800000
benchi-kafka     | 	quotas.lazy.evaluation.threshold = 0.5
benchi-kafka     | 	quotas.topic.append.timeout.ms = 5000
benchi-kafka     | 	quotas.topic.compression.codec = 3
benchi-kafka     | 	quotas.topic.load.buffer.size = 5242880
benchi-kafka     | 	quotas.topic.num.partitions = 50
benchi-kafka     | 	quotas.topic.placement.constraints = 
benchi-kafka     | 	quotas.topic.replication.factor = 3
benchi-kafka     | 	quotas.topic.segment.bytes = 104857600
benchi-kafka     | 	remote.fetch.max.wait.ms = 500
benchi-kafka     | 	remote.log.index.file.cache.total.size.bytes = 1073741824
benchi-kafka     | 	remote.log.manager.copier.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.copy.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.copy.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.expiration.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.fetch.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.fetch.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.task.interval.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.max.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.ms = 500
benchi-kafka     | 	remote.log.manager.task.retry.jitter = 0.2
benchi-kafka     | 	remote.log.manager.thread.pool.size = 10
benchi-kafka     | 	remote.log.metadata.custom.metadata.max.bytes = 128
benchi-kafka     | 	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
benchi-kafka     | 	remote.log.metadata.manager.class.path = null
benchi-kafka     | 	remote.log.metadata.manager.impl.prefix = rlmm.config.
benchi-kafka     | 	remote.log.metadata.manager.listener.name = null
benchi-kafka     | 	remote.log.reader.max.pending.tasks = 100
benchi-kafka     | 	remote.log.reader.threads = 10
benchi-kafka     | 	remote.log.storage.manager.class.name = null
benchi-kafka     | 	remote.log.storage.manager.class.path = null
benchi-kafka     | 	remote.log.storage.manager.impl.prefix = rsm.config.
benchi-kafka     | 	remote.log.storage.system.enable = false
benchi-kafka     | 	replica.fetch.backoff.ms = 1000
benchi-kafka     | 	replica.fetch.max.bytes = 1048576
benchi-kafka     | 	replica.fetch.min.bytes = 1
benchi-kafka     | 	replica.fetch.response.max.bytes = 10485760
benchi-kafka     | 	replica.fetch.wait.max.ms = 500
benchi-kafka     | 	replica.high.watermark.checkpoint.interval.ms = 5000
benchi-kafka     | 	replica.lag.time.max.ms = 30000
benchi-kafka     | 	replica.selector.class = null
benchi-kafka     | 	replica.socket.receive.buffer.bytes = 65536
benchi-kafka     | 	replica.socket.timeout.ms = 30000
benchi-kafka     | 	replication.quota.window.num = 11
benchi-kafka     | 	replication.quota.window.size.seconds = 1
benchi-kafka     | 	request.timeout.ms = 30000
benchi-kafka     | 	reserved.broker.max.id = 1000
benchi-kafka     | 	sasl.client.callback.handler.class = null
benchi-kafka     | 	sasl.enabled.mechanisms = [GSSAPI]
benchi-kafka     | 	sasl.jaas.config = null
benchi-kafka     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
benchi-kafka     | 	sasl.kerberos.min.time.before.relogin = 60000
benchi-kafka     | 	sasl.kerberos.principal.to.local.rules = [DEFAULT]
benchi-kafka     | 	sasl.kerberos.service.name = null
benchi-kafka     | 	sasl.kerberos.ticket.renew.jitter = 0.05
benchi-kafka     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
benchi-kafka     | 	sasl.login.callback.handler.class = null
benchi-kafka     | 	sasl.login.class = null
benchi-kafka     | 	sasl.login.connect.timeout.ms = null
benchi-kafka     | 	sasl.login.read.timeout.ms = null
benchi-kafka     | 	sasl.login.refresh.buffer.seconds = 300
benchi-kafka     | 	sasl.login.refresh.min.period.seconds = 60
benchi-kafka     | 	sasl.login.refresh.window.factor = 0.8
benchi-kafka     | 	sasl.login.refresh.window.jitter = 0.05
benchi-kafka     | 	sasl.login.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.login.retry.backoff.ms = 100
benchi-kafka     | 	sasl.mechanism.controller.protocol = GSSAPI
benchi-kafka     | 	sasl.mechanism.inter.broker.protocol = GSSAPI
benchi-kafka     | 	sasl.oauthbearer.clientassertion.audience = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.expiration = 5
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.jti.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.nbf.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.issuer = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.location = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key.passphrase = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.subject = null
benchi-kafka     | 	sasl.oauthbearer.clock.skew.seconds = 30
benchi-kafka     | 	sasl.oauthbearer.expected.audience = null
benchi-kafka     | 	sasl.oauthbearer.expected.issuer = null
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.url = null
benchi-kafka     | 	sasl.oauthbearer.scope.claim.name = scope
benchi-kafka     | 	sasl.oauthbearer.sub.claim.name = sub
benchi-kafka     | 	sasl.oauthbearer.token.endpoint.url = null
benchi-kafka     | 	sasl.server.authn.async.enable = false
benchi-kafka     | 	sasl.server.authn.async.max.threads = 1
benchi-kafka     | 	sasl.server.authn.async.timeout.ms = 30000
benchi-kafka     | 	sasl.server.callback.handler.class = null
benchi-kafka     | 	sasl.server.max.receive.size = 524288
benchi-kafka     | 	security.inter.broker.protocol = PLAINTEXT
benchi-kafka     | 	security.providers = null
benchi-kafka     | 	server.max.startup.time.ms = 9223372036854775807
benchi-kafka     | 	socket.connection.setup.timeout.max.ms = 30000
benchi-kafka     | 	socket.connection.setup.timeout.ms = 10000
benchi-kafka     | 	socket.listen.backlog.size = 50
benchi-kafka     | 	socket.receive.buffer.bytes = 102400
benchi-kafka     | 	socket.request.max.bytes = 104857600
benchi-kafka     | 	socket.send.buffer.bytes = 102400
benchi-kafka     | 	ssl.allow.dn.changes = false
benchi-kafka     | 	ssl.allow.san.changes = false
benchi-kafka     | 	ssl.cipher.suites = []
benchi-kafka     | 	ssl.client.auth = none
benchi-kafka     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	ssl.engine.factory.class = null
benchi-kafka     | 	ssl.key.password = null
benchi-kafka     | 	ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	ssl.keystore.certificate.chain = null
benchi-kafka     | 	ssl.keystore.key = null
benchi-kafka     | 	ssl.keystore.location = null
benchi-kafka     | 	ssl.keystore.password = null
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.principal.mapping.rules = DEFAULT
benchi-kafka     | 	ssl.protocol = TLSv1.3
benchi-kafka     | 	ssl.provider = null
benchi-kafka     | 	ssl.secure.random.implementation = null
benchi-kafka     | 	ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	ssl.truststore.certificates = null
benchi-kafka     | 	ssl.truststore.location = null
benchi-kafka     | 	ssl.truststore.password = null
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     | 	telemetry.max.bytes = 1048576
benchi-kafka     | 	throughput.quota.window.num = 11
benchi-kafka     | 	token.impersonation.validation = true
benchi-kafka     | 	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
benchi-kafka     | 	transaction.max.timeout.ms = 900000
benchi-kafka     | 	transaction.metadata.load.threads = 32
benchi-kafka     | 	transaction.partition.verification.enable = true
benchi-kafka     | 	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
benchi-kafka     | 	transaction.state.log.load.buffer.size = 5242880
benchi-kafka     | 	transaction.state.log.min.isr = 1
benchi-kafka     | 	transaction.state.log.num.partitions = 50
benchi-kafka     | 	transaction.state.log.replication.factor = 1
benchi-kafka     | 	transaction.state.log.segment.bytes = 104857600
benchi-kafka     | 	transactional.id.expiration.ms = 604800000
benchi-kafka     | 	unclean.leader.election.enable = false
benchi-kafka     | 	unstable.api.versions.enable = false
benchi-kafka     | 	unstable.feature.versions.enable = false
benchi-kafka     | 	zookeeper.acl.change.notification.expiration.ms = 900000
benchi-kafka     | 	zookeeper.clientCnxnSocket = null
benchi-kafka     | 	zookeeper.connect = 
benchi-kafka     | 	zookeeper.connection.timeout.ms = null
benchi-kafka     | 	zookeeper.max.in.flight.requests = 10
benchi-kafka     | 	zookeeper.metadata.migration.enable = false
benchi-kafka     | 	zookeeper.metadata.migration.min.batch.size = 200
benchi-kafka     | 	zookeeper.session.timeout.ms = 18000
benchi-kafka     | 	zookeeper.set.acl = false
benchi-kafka     | 	zookeeper.ssl.cipher.suites = null
benchi-kafka     | 	zookeeper.ssl.client.enable = false
benchi-kafka     | 	zookeeper.ssl.crl.enable = false
benchi-kafka     | 	zookeeper.ssl.enabled.protocols = null
benchi-kafka     | 	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
benchi-kafka     | 	zookeeper.ssl.keystore.location = null
benchi-kafka     | 	zookeeper.ssl.keystore.password = null
benchi-kafka     | 	zookeeper.ssl.keystore.type = null
benchi-kafka     | 	zookeeper.ssl.ocsp.enable = false
benchi-kafka     | 	zookeeper.ssl.protocol = TLSv1.2
benchi-kafka     | 	zookeeper.ssl.truststore.location = null
benchi-kafka     | 	zookeeper.ssl.truststore.password = null
benchi-kafka     | 	zookeeper.ssl.truststore.type = null
benchi-kafka     |  (kafka.server.KafkaConfig)
benchi-kafka     | [2025-03-18 16:05:46,576] INFO RemoteLogManagerConfig values: 
benchi-kafka     | 	log.local.retention.bytes = -2
benchi-kafka     | 	log.local.retention.ms = -2
benchi-kafka     | 	remote.fetch.max.wait.ms = 500
benchi-kafka     | 	remote.log.index.file.cache.total.size.bytes = 1073741824
benchi-kafka     | 	remote.log.manager.copier.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.copy.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.copy.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.expiration.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.fetch.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.fetch.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.task.interval.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.max.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.ms = 500
benchi-kafka     | 	remote.log.manager.task.retry.jitter = 0.2
benchi-kafka     | 	remote.log.manager.thread.pool.size = 10
benchi-kafka     | 	remote.log.metadata.custom.metadata.max.bytes = 128
benchi-kafka     | 	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
benchi-kafka     | 	remote.log.metadata.manager.class.path = null
benchi-kafka     | 	remote.log.metadata.manager.impl.prefix = rlmm.config.
benchi-kafka     | 	remote.log.metadata.manager.listener.name = null
benchi-kafka     | 	remote.log.reader.max.pending.tasks = 100
benchi-kafka     | 	remote.log.reader.threads = 10
benchi-kafka     | 	remote.log.storage.manager.class.name = null
benchi-kafka     | 	remote.log.storage.manager.class.path = null
benchi-kafka     | 	remote.log.storage.manager.impl.prefix = rsm.config.
benchi-kafka     | 	remote.log.storage.system.enable = false
benchi-kafka     |  (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
benchi-kafka     | [2025-03-18 16:05:46,579] INFO [BrokerServer id=1] Waiting for the broker to be unfenced (kafka.server.BrokerServer)
benchi-kafka     | [2025-03-18 16:05:46,580] INFO [ControllerServer id=1] The request from broker 1 to unfence has been granted because it has caught up with the offset of its register broker record 6. (org.apache.kafka.controller.BrokerHeartbeatManager)
benchi-kafka     | [2025-03-18 16:05:46,582] INFO [ControllerServer id=1] Replayed BrokerRegistrationChangeRecord modifying the registration for broker 1: BrokerRegistrationChangeRecord(brokerId=1, brokerEpoch=6, fenced=-1, inControlledShutdown=0, degradedComponents=null, logDirs=[]) (org.apache.kafka.controller.ClusterControlManager)
benchi-kafka     | [2025-03-18 16:05:46,611] INFO [BrokerLifecycleManager id=1] The broker has been unfenced. Transitioning from RECOVERY to RUNNING. (kafka.server.BrokerLifecycleManager)
benchi-kafka     | [2025-03-18 16:05:46,611] INFO [BrokerServer id=1] Finished waiting for the broker to be unfenced (kafka.server.BrokerServer)
benchi-kafka     | [2025-03-18 16:05:46,611] INFO [SocketServer listenerType=BROKER, nodeId=1] Enabling request processing. (kafka.network.SocketServer)
benchi-kafka     | [2025-03-18 16:05:46,611] INFO SBC Event SbcMetadataUpdateEvent-9 generated 1 more events to enqueue in the following order - [SbcKraftBrokerAdditionEvent-10]. Enqueuing... (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:05:46,611] INFO Handling event SbcConfigUpdateEvent-3 (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:05:46,611] INFO Awaiting socket connections on broker:29092. (kafka.network.DataPlaneAcceptor)
benchi-kafka     | [2025-03-18 16:05:46,612] INFO Balancer notified of a config change: ConfigurationsDelta(changes={}) (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:05:46,612] INFO There were 0 change(s) and 0 deletion(s) to balancer configs. Changed Configs: {}, Deleted Configs: [] (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:05:46,612] INFO Handling event SbcKraftStartupEvent-5 (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:05:46,612] INFO Configs metadata not yet available, SBC startup delayed. (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:05:46,612] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.DataPlaneAcceptor)
benchi-kafka     | [2025-03-18 16:05:46,612] INFO Handling event SbcKraftBrokerAdditionEvent-10 (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:05:46,612] INFO Topics Image not present, pausing broker addition event of brokers (new brokers: [1]) until it is received. (io.confluent.databalancer.event.SbcKraftBrokerAdditionEvent)
benchi-kafka     | [2025-03-18 16:05:46,613] INFO [BrokerServer id=1] Waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
benchi-kafka     | [2025-03-18 16:05:46,613] INFO [BrokerServer id=1] Finished waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
benchi-kafka     | [2025-03-18 16:05:46,613] INFO [BrokerServer id=1] Waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
benchi-kafka     | [2025-03-18 16:05:46,613] INFO [BrokerServer id=1] Finished waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
benchi-kafka     | [2025-03-18 16:05:46,625] INFO RestConfig values: 
benchi-kafka     | 	access.control.allow.headers = 
benchi-kafka     | 	access.control.allow.methods = 
benchi-kafka     | 	access.control.allow.origin = 
benchi-kafka     | 	access.control.skip.options = true
benchi-kafka     | 	authentication.method = NONE
benchi-kafka     | 	authentication.realm = 
benchi-kafka     | 	authentication.roles = [*]
benchi-kafka     | 	authentication.skip.paths = []
benchi-kafka     | 	compression.enable = true
benchi-kafka     | 	connector.connection.limit = 0
benchi-kafka     | 	csrf.prevention.enable = false
benchi-kafka     | 	csrf.prevention.token.endpoint = /csrf
benchi-kafka     | 	csrf.prevention.token.expiration.minutes = 30
benchi-kafka     | 	csrf.prevention.token.max.entries = 10000
benchi-kafka     | 	debug = false
benchi-kafka     | 	dos.filter.delay.ms = 100
benchi-kafka     | 	dos.filter.enabled = false
benchi-kafka     | 	dos.filter.insert.headers = true
benchi-kafka     | 	dos.filter.ip.whitelist = []
benchi-kafka     | 	dos.filter.managed.attr = false
benchi-kafka     | 	dos.filter.max.idle.tracker.ms = 30000
benchi-kafka     | 	dos.filter.max.requests.ms = 30000
benchi-kafka     | 	dos.filter.max.requests.per.connection.per.sec = 25
benchi-kafka     | 	dos.filter.max.requests.per.sec = 25
benchi-kafka     | 	dos.filter.max.wait.ms = 50
benchi-kafka     | 	dos.filter.throttle.ms = 30000
benchi-kafka     | 	dos.filter.throttled.requests = 5
benchi-kafka     | 	http2.enabled = true
benchi-kafka     | 	idle.timeout.ms = 30000
benchi-kafka     | 	listener.protocol.map = []
benchi-kafka     | 	listeners = []
benchi-kafka     | 	max.request.header.size = 8192
benchi-kafka     | 	max.response.header.size = 8192
benchi-kafka     | 	metric.reporters = []
benchi-kafka     | 	metrics.global.stats.request.tags.enable = false
benchi-kafka     | 	metrics.jmx.prefix = rest-utils
benchi-kafka     | 	metrics.latency.sla.ms = 50
benchi-kafka     | 	metrics.latency.slo.ms = 5
benchi-kafka     | 	metrics.latency.slo.sla.enable = false
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	metrics.tag.map = []
benchi-kafka     | 	network.traffic.rate.limit.backend = guava
benchi-kafka     | 	network.traffic.rate.limit.bytes.per.sec = 20971520
benchi-kafka     | 	network.traffic.rate.limit.enable = false
benchi-kafka     | 	nosniff.prevention.enable = false
benchi-kafka     | 	port = 8080
benchi-kafka     | 	proxy.protocol.enabled = false
benchi-kafka     | 	reject.options.request = false
benchi-kafka     | 	request.logger.name = io.confluent.rest-utils.requests
benchi-kafka     | 	request.queue.capacity = 2147483647
benchi-kafka     | 	request.queue.capacity.growby = 64
benchi-kafka     | 	request.queue.capacity.init = 128
benchi-kafka     | 	resource.extension.classes = []
benchi-kafka     | 	response.http.headers.config = 
benchi-kafka     | 	response.mediatype.default = application/json
benchi-kafka     | 	response.mediatype.preferred = [application/json]
benchi-kafka     | 	rest.servlet.initializor.classes = []
benchi-kafka     | 	server.connection.limit = 0
benchi-kafka     | 	shutdown.graceful.ms = 1000
benchi-kafka     | 	sni.check.enabled = false
benchi-kafka     | 	ssl.cipher.suites = []
benchi-kafka     | 	ssl.client.auth = false
benchi-kafka     | 	ssl.client.authentication = NONE
benchi-kafka     | 	ssl.enabled.protocols = []
benchi-kafka     | 	ssl.endpoint.identification.algorithm = null
benchi-kafka     | 	ssl.key.password = [hidden]
benchi-kafka     | 	ssl.keymanager.algorithm = 
benchi-kafka     | 	ssl.keystore.location = 
benchi-kafka     | 	ssl.keystore.password = [hidden]
benchi-kafka     | 	ssl.keystore.reload = false
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.keystore.watch.location = 
benchi-kafka     | 	ssl.protocol = TLS
benchi-kafka     | 	ssl.provider = 
benchi-kafka     | 	ssl.trustmanager.algorithm = 
benchi-kafka     | 	ssl.truststore.location = 
benchi-kafka     | 	ssl.truststore.password = [hidden]
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     | 	suppress.stack.trace.response = true
benchi-kafka     | 	thread.pool.max = 200
benchi-kafka     | 	thread.pool.min = 8
benchi-kafka     | 	websocket.path.prefix = /ws
benchi-kafka     | 	websocket.servlet.initializor.classes = []
benchi-kafka     |  (io.confluent.rest.RestConfig)
benchi-kafka     | [2025-03-18 16:05:46,628] INFO Logging initialized @1742ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log)
benchi-kafka     | [2025-03-18 16:05:46,639] INFO Application provider 'MetadataApiApplicationProvider' provided 1 instance(s). (io.confluent.http.server.KafkaHttpApplicationLoader)
benchi-kafka     | [2025-03-18 16:05:46,640] INFO MetadataServerConfig values: 
benchi-kafka     | 	confluent.http.server.listeners = [http://0.0.0.0:8090]
benchi-kafka     | 	confluent.metadata.server.advertised.listeners = null
benchi-kafka     | 	confluent.metadata.server.enable = false
benchi-kafka     | 	confluent.metadata.server.kraft.controller.enabled = false
benchi-kafka     | 	confluent.metadata.server.listeners = null
benchi-kafka     |  (org.apache.kafka.server.http.MetadataServerConfig)
benchi-kafka     | [2025-03-18 16:05:46,640] INFO Application provider 'RbacApplicationProvider' did not provide any instances. (io.confluent.http.server.KafkaHttpApplicationLoader)
benchi-kafka     | [2025-03-18 16:05:46,641] INFO KafkaConfig values: 
benchi-kafka     | 	advertised.listeners = PLAINTEXT://broker:29092,PLAINTEXT_HOST://localhost:9092
benchi-kafka     | 	alter.config.policy.class.name = null
benchi-kafka     | 	alter.log.dirs.replication.quota.window.num = 11
benchi-kafka     | 	alter.log.dirs.replication.quota.window.size.seconds = 1
benchi-kafka     | 	authorizer.class.name = 
benchi-kafka     | 	auto.create.topics.enable = true
benchi-kafka     | 	auto.include.jmx.reporter = true
benchi-kafka     | 	auto.leader.rebalance.enable = true
benchi-kafka     | 	background.threads = 10
benchi-kafka     | 	broker.heartbeat.interval.ms = 2000
benchi-kafka     | 	broker.id = 1
benchi-kafka     | 	broker.id.generation.enable = true
benchi-kafka     | 	broker.interceptor.class = class org.apache.kafka.server.interceptor.DefaultBrokerInterceptor
benchi-kafka     | 	broker.rack = null
benchi-kafka     | 	broker.session.timeout.ms = 9000
benchi-kafka     | 	broker.session.uuid = JzUn51KjQwC3OragyPrV9Q
benchi-kafka     | 	client.quota.callback.class = null
benchi-kafka     | 	client.quota.max.throttle.time.ms = 5000
benchi-kafka     | 	compression.gzip.level = -1
benchi-kafka     | 	compression.lz4.level = 9
benchi-kafka     | 	compression.type = producer
benchi-kafka     | 	compression.zstd.level = 3
benchi-kafka     | 	confluent.alter.broker.health.max.demoted.brokers = 2147483647
benchi-kafka     | 	confluent.alter.broker.health.max.demoted.brokers.percentage = 0
benchi-kafka     | 	confluent.ansible.managed = false
benchi-kafka     | 	confluent.api.visibility = DEFAULT
benchi-kafka     | 	confluent.append.record.interceptor.classes = []
benchi-kafka     | 	confluent.apply.create.topic.policy.to.create.partitions = false
benchi-kafka     | 	confluent.authorizer.authority.name = 
benchi-kafka     | 	confluent.automatic.alter.broker.health.retry.backoff.ms = 2000
benchi-kafka     | 	confluent.backpressure.disk.enable = false
benchi-kafka     | 	confluent.backpressure.disk.free.threshold.bytes = 21474836480
benchi-kafka     | 	confluent.backpressure.disk.produce.bytes.per.second = 131072
benchi-kafka     | 	confluent.backpressure.disk.threshold.recovery.factor = 1.5
benchi-kafka     | 	confluent.backpressure.request.min.broker.limit = 200
benchi-kafka     | 	confluent.backpressure.request.queue.size.percentile = p95
benchi-kafka     | 	confluent.backpressure.types = null
benchi-kafka     | 	confluent.balancer.api.state.topic = _confluent_balancer_api_state
benchi-kafka     | 	confluent.balancer.broker.addition.elapsed.time.ms.completion.threshold = 57600000
benchi-kafka     | 	confluent.balancer.broker.addition.mean.cpu.percent.completion.threshold = 0.5
benchi-kafka     | 	confluent.balancer.capacity.threshold.upper.limit = 0.95
benchi-kafka     | 	confluent.balancer.cell.load.upper.bound = 0.7
benchi-kafka     | 	confluent.balancer.cell.overload.detection.interval.ms = 3600000
benchi-kafka     | 	confluent.balancer.cell.overload.duration.ms = 86400000
benchi-kafka     | 	confluent.balancer.class = io.confluent.databalancer.SbcDataBalanceManager
benchi-kafka     | 	confluent.balancer.consumer.out.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.balancer.cpu.balance.threshold = 1.1
benchi-kafka     | 	confluent.balancer.cpu.low.utilization.threshold = 0.2
benchi-kafka     | 	confluent.balancer.cpu.utilization.detector.duration.ms = 600000
benchi-kafka     | 	confluent.balancer.cpu.utilization.detector.enabled = false
benchi-kafka     | 	confluent.balancer.cpu.utilization.detector.overutilization.threshold = 80.0
benchi-kafka     | 	confluent.balancer.cpu.utilization.detector.underutilization.threshold = 50.0
benchi-kafka     | 	confluent.balancer.disk.max.load = 0.85
benchi-kafka     | 	confluent.balancer.disk.min.free.space.gb = 0
benchi-kafka     | 	confluent.balancer.disk.min.free.space.lower.limit.gb = 0
benchi-kafka     | 	confluent.balancer.disk.utilization.detector.duration.ms = 600000
benchi-kafka     | 	confluent.balancer.disk.utilization.detector.enabled = false
benchi-kafka     | 	confluent.balancer.disk.utilization.detector.overutilization.threshold = 80.0
benchi-kafka     | 	confluent.balancer.disk.utilization.detector.reserved.capacity = 150000.0
benchi-kafka     | 	confluent.balancer.disk.utilization.detector.underutilization.threshold = 35.0
benchi-kafka     | 	confluent.balancer.enable = true
benchi-kafka     | 	confluent.balancer.exclude.topic.names = []
benchi-kafka     | 	confluent.balancer.exclude.topic.prefixes = []
benchi-kafka     | 	confluent.balancer.goal.violation.delay.on.new.brokers.ms = 1800000
benchi-kafka     | 	confluent.balancer.goal.violation.distribution.threshold.multiplier = 1.1
benchi-kafka     | 	confluent.balancer.heal.broker.failure.threshold.ms = 3600000
benchi-kafka     | 	confluent.balancer.heal.uneven.load.trigger = EMPTY_BROKER
benchi-kafka     | 	confluent.balancer.incremental.balancing.cpu.top.proposal.tracking.enabled = true
benchi-kafka     | 	confluent.balancer.incremental.balancing.cpu.top.proposal.tracking.num.proposals = 15
benchi-kafka     | 	confluent.balancer.incremental.balancing.enabled = false
benchi-kafka     | 	confluent.balancer.incremental.balancing.goals = []
benchi-kafka     | 	confluent.balancer.incremental.balancing.lower.bound = 0.02
benchi-kafka     | 	confluent.balancer.incremental.balancing.min.valid.windows = 5
benchi-kafka     | 	confluent.balancer.incremental.balancing.step.ratio = 0.2
benchi-kafka     | 	confluent.balancer.inter.cell.balancing.enabled = false
benchi-kafka     | 	confluent.balancer.max.capacity.balancing.delta.percentage = 0.0
benchi-kafka     | 	confluent.balancer.max.replicas = 2147483647
benchi-kafka     | 	confluent.balancer.network.in.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.balancer.network.out.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.balancer.plan.computation.retry.timeout.ms = 3600000
benchi-kafka     | 	confluent.balancer.producer.in.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.balancer.rebalancing.goals = []
benchi-kafka     | 	confluent.balancer.replication.in.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.balancer.resource.utilization.detector.interval.ms = 60000
benchi-kafka     | 	confluent.balancer.sbc.metrics.parser.enabled = false
benchi-kafka     | 	confluent.balancer.self.healing.maximum.rounds = 1
benchi-kafka     | 	confluent.balancer.task.history.retention.days = 30
benchi-kafka     | 	confluent.balancer.tenant.maximum.movements = 0
benchi-kafka     | 	confluent.balancer.tenant.suspension.ms = 86400000
benchi-kafka     | 	confluent.balancer.throttle.bytes.per.second = 10485760
benchi-kafka     | 	confluent.balancer.topic.partition.maximum.movements = 5
benchi-kafka     | 	confluent.balancer.topic.partition.movement.expiration.ms = 10800000
benchi-kafka     | 	confluent.balancer.topic.partition.suspension.ms = 18000000
benchi-kafka     | 	confluent.balancer.topic.replication.factor = 1
benchi-kafka     | 	confluent.balancer.triggering.goals = []
benchi-kafka     | 	confluent.balancer.v2.addition.enabled = false
benchi-kafka     | 	confluent.basic.auth.credentials.source = null
benchi-kafka     | 	confluent.basic.auth.user.info = null
benchi-kafka     | 	confluent.bearer.auth.cache.expiry.buffer.seconds = 300
benchi-kafka     | 	confluent.bearer.auth.client.id = null
benchi-kafka     | 	confluent.bearer.auth.client.secret = null
benchi-kafka     | 	confluent.bearer.auth.credentials.source = null
benchi-kafka     | 	confluent.bearer.auth.identity.pool.id = null
benchi-kafka     | 	confluent.bearer.auth.issuer.endpoint.url = null
benchi-kafka     | 	confluent.bearer.auth.logical.cluster = null
benchi-kafka     | 	confluent.bearer.auth.scope = null
benchi-kafka     | 	confluent.bearer.auth.scope.claim.name = scope
benchi-kafka     | 	confluent.bearer.auth.sub.claim.name = sub
benchi-kafka     | 	confluent.bearer.auth.token = null
benchi-kafka     | 	confluent.broker.health.manager.enabled = true
benchi-kafka     | 	confluent.broker.health.manager.engine.request.handler.threads.stuck.criteria = AllThreadsStuck
benchi-kafka     | 	confluent.broker.health.manager.hard.kill.duration.ms = 60000
benchi-kafka     | 	confluent.broker.health.manager.mitigation.enabled = false
benchi-kafka     | 	confluent.broker.health.manager.num.samples.before.broker.suspect = 30
benchi-kafka     | 	confluent.broker.health.manager.num.samples.before.broker.unhealthy = 180
benchi-kafka     | 	confluent.broker.health.manager.percentage.unhealthy.samples.before.broker.suspect = 90
benchi-kafka     | 	confluent.broker.health.manager.percentage.unhealthy.samples.before.broker.unhealthy = 70
benchi-kafka     | 	confluent.broker.health.manager.sample.duration.ms = 1000
benchi-kafka     | 	confluent.broker.health.manager.storage.background.threads.stuck.criteria = AnyThreadStuck
benchi-kafka     | 	confluent.broker.health.manager.storage.network.threads.stuck.criteria = AnyThreadStuck
benchi-kafka     | 	confluent.broker.health.manager.storage.request.handler.threads.stuck.criteria = AnyThreadStuck
benchi-kafka     | 	confluent.broker.limit.consumer.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.broker.limit.producer.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.broker.load.advertised.limit.load = 0.8
benchi-kafka     | 	confluent.broker.load.average.service.request.time.ms = 0.1
benchi-kafka     | 	confluent.broker.load.delay.metric.start.ms = 180000
benchi-kafka     | 	confluent.broker.load.enabled = false
benchi-kafka     | 	confluent.broker.load.num.samples = 60
benchi-kafka     | 	confluent.broker.load.tenant.metric.enable = false
benchi-kafka     | 	confluent.broker.load.update.metric.tags.interval.ms = 60000
benchi-kafka     | 	confluent.broker.load.window.size.ms = 60000
benchi-kafka     | 	confluent.broker.load.workload.coefficient = 20.0
benchi-kafka     | 	confluent.broker.registration.delay.ms = 0
benchi-kafka     | 	confluent.catalog.collector.destination.topic = telemetry.events.data_catalog_source
benchi-kafka     | 	confluent.catalog.collector.enable = false
benchi-kafka     | 	confluent.catalog.collector.full.configs.enable = false
benchi-kafka     | 	confluent.catalog.collector.max.bytes.per.snapshot = 850000
benchi-kafka     | 	confluent.catalog.collector.max.topics.process = 500
benchi-kafka     | 	confluent.catalog.collector.max.zookeeper.request.per.sec = 100
benchi-kafka     | 	confluent.catalog.collector.snapshot.init.delay.sec = 60
benchi-kafka     | 	confluent.catalog.collector.snapshot.interval.sec = 300
benchi-kafka     | 	confluent.ccloud.host.suffixes = .confluent.cloud,.cpdev.cloud,.confluentgov.com,.confluentgov-internal.com
benchi-kafka     | 	confluent.ccloud.intranet.host.suffixes = .intranet.stag.cpdev.cloud,.intranet.stag.cpdev-untrusted.cloud,.intranet.devel.cpdev.cloud,.intranet.devel.cpdev-untrusted.cloud,.intranet.confluent.cloud,.intranet.confluent-untrusted.cloud
benchi-kafka     | 	confluent.cdc.api.keys.topic = 
benchi-kafka     | 	confluent.cdc.api.keys.topic.load.timeout.ms = 600000
benchi-kafka     | 	confluent.cdc.client.quotas.enable = false
benchi-kafka     | 	confluent.cdc.client.quotas.topic.name = 
benchi-kafka     | 	confluent.cdc.lkc.metadata.topic = 
benchi-kafka     | 	confluent.cdc.user.metadata.enable = false
benchi-kafka     | 	confluent.cdc.user.metadata.topic = _confluent-user_metadata
benchi-kafka     | 	confluent.cell.metrics.refresh.period.ms = 60000
benchi-kafka     | 	confluent.cells.default.size = 15
benchi-kafka     | 	confluent.cells.enable = false
benchi-kafka     | 	confluent.cells.implicit.creation.enable = false
benchi-kafka     | 	confluent.cells.load.refresher.enable = true
benchi-kafka     | 	confluent.cells.max.size = 15
benchi-kafka     | 	confluent.cells.min.size = 6
benchi-kafka     | 	confluent.checksum.enabled.files = [none]
benchi-kafka     | 	confluent.clm.enabled = false
benchi-kafka     | 	confluent.clm.frequency.in.hours = 6
benchi-kafka     | 	confluent.clm.list.object.thread_pool.size = 1
benchi-kafka     | 	confluent.clm.max.backup.days = 3
benchi-kafka     | 	confluent.clm.min.delay.in.minutes = 30
benchi-kafka     | 	confluent.clm.thread.pool.size = 2
benchi-kafka     | 	confluent.clm.topic.retention.days.to.backup.days = 0:0,3:3
benchi-kafka     | 	confluent.close.connections.on.credential.delete = false
benchi-kafka     | 	confluent.cluster.link.admin.max.in.flight.requests = 1000
benchi-kafka     | 	confluent.cluster.link.admin.request.batch.size = 1
benchi-kafka     | 	confluent.cluster.link.allow.config.providers = true
benchi-kafka     | 	confluent.cluster.link.allow.legacy.message.format = false
benchi-kafka     | 	confluent.cluster.link.allow.truncation.below.hwm = true
benchi-kafka     | 	confluent.cluster.link.availability.check.mode = ALL
benchi-kafka     | 	confluent.cluster.link.background.thread.affinity = LINK
benchi-kafka     | 	confluent.cluster.link.clients.max.idle.ms = 3153600000000
benchi-kafka     | 	confluent.cluster.link.enable = true
benchi-kafka     | 	confluent.cluster.link.enable.local.admin = false
benchi-kafka     | 	confluent.cluster.link.enable.metrics.reduction = false
benchi-kafka     | 	confluent.cluster.link.fetch.response.min.bytes = 1
benchi-kafka     | 	confluent.cluster.link.fetch.response.total.bytes = 2147483647
benchi-kafka     | 	confluent.cluster.link.fetcher.auto.tune.enable = false
benchi-kafka     | 	confluent.cluster.link.fetcher.thread.pool.mode = ENDPOINT
benchi-kafka     | 	confluent.cluster.link.insync.fetch.response.min.bytes = 1
benchi-kafka     | 	confluent.cluster.link.insync.fetch.response.total.bytes = 2147483647
benchi-kafka     | 	confluent.cluster.link.intranet.connectivity.enable = false
benchi-kafka     | 	confluent.cluster.link.io.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.cluster.link.local.reverse.connection.listener.map = null
benchi-kafka     | 	confluent.cluster.link.max.client.connections = 2147483647
benchi-kafka     | 	confluent.cluster.link.metadata.topic.create.retry.delay.ms = 1000
benchi-kafka     | 	confluent.cluster.link.metadata.topic.enable = false
benchi-kafka     | 	confluent.cluster.link.metadata.topic.min.isr = 2
benchi-kafka     | 	confluent.cluster.link.metadata.topic.partitions = 50
benchi-kafka     | 	confluent.cluster.link.metadata.topic.replication.factor = 1
benchi-kafka     | 	confluent.cluster.link.mirror.transition.batch.size = 10
benchi-kafka     | 	confluent.cluster.link.num.background.threads = 1
benchi-kafka     | 	confluent.cluster.link.periodic.task.batch.size = 2147483647
benchi-kafka     | 	confluent.cluster.link.periodic.task.min.interval.ms = 1000
benchi-kafka     | 	confluent.cluster.link.replica.fetch.connections.mode = combined
benchi-kafka     | 	confluent.cluster.link.replication.quota.mode = CLUSTER_LINK_ONLY
benchi-kafka     | 	confluent.cluster.link.replication.quota.mode.per.tenant.overrides = 
benchi-kafka     | 	confluent.cluster.link.replication.quota.window.num = 11
benchi-kafka     | 	confluent.cluster.link.replication.quota.window.size.seconds = 2
benchi-kafka     | 	confluent.cluster.link.request.quota.capacity = 400
benchi-kafka     | 	confluent.cluster.link.request.quota.request.percentage.multiplier = 1.0
benchi-kafka     | 	confluent.cluster.link.tenant.replication.quota.enable = false
benchi-kafka     | 	confluent.cluster.link.tenant.request.quota.enable = false
benchi-kafka     | 	confluent.cluster.metadata.snapshot.tier.delete.enable = false
benchi-kafka     | 	confluent.cluster.metadata.snapshot.tier.delete.maintain.min.snapshots = 3
benchi-kafka     | 	confluent.cluster.metadata.snapshot.tier.delete.retention.ms = 604800000
benchi-kafka     | 	confluent.cluster.metadata.snapshot.tier.upload.enable = false
benchi-kafka     | 	confluent.compacted.topic.prefer.tier.fetch.ms = -1
benchi-kafka     | 	confluent.connection.invalid.request.delay.enable = false
benchi-kafka     | 	confluent.connections.idle.expiry.manager.ignore.idleness.requests = []
benchi-kafka     | 	confluent.consumer.fetch.partition.pruning.enable = true
benchi-kafka     | 	confluent.consumer.lag.emitter.enabled = false
benchi-kafka     | 	confluent.consumer.lag.emitter.interval.ms = 60000
benchi-kafka     | 	confluent.dataflow.policy.watch.monitor.ms = 300000
benchi-kafka     | 	confluent.defer.isr.shrink.enable = false
benchi-kafka     | 	confluent.describe.topic.partitions.enabled = false
benchi-kafka     | 	confluent.disk.io.manager.enable = false
benchi-kafka     | 	confluent.disk.throughput.headroom = 10485760
benchi-kafka     | 	confluent.disk.throughput.limit = 10485760000
benchi-kafka     | 	confluent.disk.throughput.quota.tier.archive = 1048576000
benchi-kafka     | 	confluent.disk.throughput.quota.tier.archive.throttled = 104857600
benchi-kafka     | 	confluent.durability.audit.batch.flush.frequency.ms = 900000
benchi-kafka     | 	confluent.durability.audit.checks = PeriodicalAudit,ChecksumAudit
benchi-kafka     | 	confluent.durability.audit.enable = false
benchi-kafka     | 	confluent.durability.audit.idempotent.producer = false
benchi-kafka     | 	confluent.durability.audit.initial.job.delay.ms = 900000
benchi-kafka     | 	confluent.durability.audit.io.bytes.per.sec = 10485760
benchi-kafka     | 	confluent.durability.audit.log.ignored.event.types = 
benchi-kafka     | 	confluent.durability.audit.reporting.batch.ms = 1800000
benchi-kafka     | 	confluent.durability.audit.tier.compaction.audit.duration.ms = 14400000
benchi-kafka     | 	confluent.durability.events.allowed = OffsetChangeType,EpochChangeType,IsrExpandType,DeleteRecordsType,RetentionChangeType,StartOffsetChangeType,DeletePartitionType,HealthCheckType
benchi-kafka     | 	confluent.durability.topic.partition.count = 50
benchi-kafka     | 	confluent.durability.topic.replication.factor = 1
benchi-kafka     | 	confluent.e2e_checksum.protection.enabled = false
benchi-kafka     | 	confluent.e2e_checksum.protection.files = [none]
benchi-kafka     | 	confluent.e2e_checksum.protection.store.entry.ttl.ms = 2592000000
benchi-kafka     | 	confluent.elastic.cku.enabled = false
benchi-kafka     | 	confluent.elastic.cku.scaletozero.enabled = false
benchi-kafka     | 	confluent.eligible.controllers = []
benchi-kafka     | 	confluent.enable.stray.partition.deletion = false
benchi-kafka     | 	confluent.encryption.key.manager.rotation.interval.ms = 31536000000
benchi-kafka     | 	confluent.fetch.from.follower.require.leader.epoch.enable = false
benchi-kafka     | 	confluent.fetch.partition.pruning.enable = true
benchi-kafka     | 	confluent.floor.connection.rate.per.ip = -1.0
benchi-kafka     | 	confluent.floor.connection.rate.per.tenant = -1.0
benchi-kafka     | 	confluent.group.coordinator.offsets.batching.enable = false
benchi-kafka     | 	confluent.group.coordinator.offsets.writer.threads = 2
benchi-kafka     | 	confluent.group.metadata.load.threads = 32
benchi-kafka     | 	confluent.heap.tenured.notify.bytes = 0
benchi-kafka     | 	confluent.heap.tenured.notify.enabled = false
benchi-kafka     | 	confluent.hot.partition.ratio = 0.8
benchi-kafka     | 	confluent.http.server.start.timeout.ms = 60000
benchi-kafka     | 	confluent.http.server.stop.timeout.ms = 30000
benchi-kafka     | 	confluent.internal.metrics.enable = false
benchi-kafka     | 	confluent.internal.rest.server.bind.port = null
benchi-kafka     | 	confluent.internal.tenant.scoped.listener.name = INTERNAL_TENANT_SCOPED
benchi-kafka     | 	confluent.leader.epoch.checkpoint.checksum.enabled = false
benchi-kafka     | 	confluent.log.cleaner.timestamp.validation.enable = true
benchi-kafka     | 	confluent.log.placement.constraints = 
benchi-kafka     | 	confluent.max.connection.creation.rate.per.ip = 1.7976931348623157E308
benchi-kafka     | 	confluent.max.connection.creation.rate.per.tenant = 1.7976931348623157E308
benchi-kafka     | 	confluent.max.connection.rate.per.ip = -1.0
benchi-kafka     | 	confluent.max.connection.rate.per.tenant = -1.0
benchi-kafka     | 	confluent.max.connection.throttle.ms = null
benchi-kafka     | 	confluent.max.segment.ms = 9223372036854775807
benchi-kafka     | 	confluent.metadata.active.encryptor = null
benchi-kafka     | 	confluent.metadata.controlled.shutdown.partition.slice.delay.ms = 100
benchi-kafka     | 	confluent.metadata.encryptor.classes = null
benchi-kafka     | 	confluent.metadata.encryptor.required = false
benchi-kafka     | 	confluent.metadata.encryptor.secret.file = null
benchi-kafka     | 	confluent.metadata.encryptor.secrets = null
benchi-kafka     | 	confluent.metadata.jvm.warmup.ms = 60000
benchi-kafka     | 	confluent.metadata.leader.balance.slice.delay.ms = 100
benchi-kafka     | 	confluent.metadata.max.controlled.shutdown.partition.changes.per.slice = 1000
benchi-kafka     | 	confluent.metadata.max.leader.balance.changes.per.slice = 1000
benchi-kafka     | 	confluent.metadata.server.cluster.registry.clusters = []
benchi-kafka     | 	confluent.metadata.throttle.pre.check.for.non.wildcard.requests.enable = false
benchi-kafka     | 	confluent.metadata.throttle.pre.check.for.wildcard.requests.enable = false
benchi-kafka     | 	confluent.metrics.reporter.bootstrap.servers = localhost:9092
benchi-kafka     | 	confluent.min.acks = 0
benchi-kafka     | 	confluent.min.connection.throttle.ms = 0
benchi-kafka     | 	confluent.min.segment.ms = 1
benchi-kafka     | 	confluent.missing.id.cache.ttl.sec = 60
benchi-kafka     | 	confluent.missing.id.query.range = 20000
benchi-kafka     | 	confluent.missing.schema.cache.ttl.sec = 60
benchi-kafka     | 	confluent.mtls.enable = false
benchi-kafka     | 	confluent.mtls.listener.name = EXTERNAL
benchi-kafka     | 	confluent.mtls.sasl.authenticator.request.max.bytes = 104857600
benchi-kafka     | 	confluent.mtls.truststore.manager.class.name = null
benchi-kafka     | 	confluent.multitenant.authorizer.enable.acl.state = false
benchi-kafka     | 	confluent.multitenant.interceptor.balancer.apis.enabled = false
benchi-kafka     | 	confluent.multitenant.interceptor.collect.client.apiversions.max.per.tenant = 1000
benchi-kafka     | 	confluent.multitenant.interceptor.collect.client.apiversions.metric = false
benchi-kafka     | 	confluent.multitenant.listener.hostname.cluster.prefix.enable = false
benchi-kafka     | 	confluent.multitenant.listener.hostname.subdomain.suffix.enable = false
benchi-kafka     | 	confluent.multitenant.listener.names = null
benchi-kafka     | 	confluent.multitenant.parse.lkc.id.enable = false
benchi-kafka     | 	confluent.multitenant.parse.sni.host.name.enable = false
benchi-kafka     | 	confluent.network.health.manager.enabled = false
benchi-kafka     | 	confluent.network.health.manager.external.listener.name = EXTERNAL
benchi-kafka     | 	confluent.network.health.manager.externalconnectivitystartup.enabled = false
benchi-kafka     | 	confluent.network.health.manager.min.healthy.network.samples = 3
benchi-kafka     | 	confluent.network.health.manager.min.percentage.healthy.network.samples = 3
benchi-kafka     | 	confluent.network.health.manager.mitigation.enabled = false
benchi-kafka     | 	confluent.network.health.manager.network.sample.window.size = 120
benchi-kafka     | 	confluent.network.health.manager.sample.duration.ms = 1000
benchi-kafka     | 	confluent.oauth.flat.networking.verification.enable = false
benchi-kafka     | 	confluent.offsets.log.cleaner.delete.retention.ms = 86400000
benchi-kafka     | 	confluent.offsets.log.cleaner.max.compaction.lag.ms = 9223372036854775807
benchi-kafka     | 	confluent.offsets.log.cleaner.min.cleanable.dirty.ratio = 0.5
benchi-kafka     | 	confluent.offsets.topic.placement.constraints = 
benchi-kafka     | 	confluent.omit.network.processor.metric.tag = false
benchi-kafka     | 	confluent.operator.managed = false
benchi-kafka     | 	confluent.password.encoder.old.secret.ttl.ms = 9223372036854775807
benchi-kafka     | 	confluent.plugins.cluster.link.policy.max.destination.links.per.tenant = 10
benchi-kafka     | 	confluent.plugins.cluster.link.policy.max.source.links.per.tenant = 10
benchi-kafka     | 	confluent.plugins.topic.policy.max.partitions.per.cluster = 2147483647
benchi-kafka     | 	confluent.plugins.topic.policy.max.partitions.per.tenant = 512
benchi-kafka     | 	confluent.plugins.topic.policy.max.replicas.per.broker = 2147483647
benchi-kafka     | 	confluent.plugins.topic.policy.max.topics.per.cluster = 2147483647
benchi-kafka     | 	confluent.prefer.tier.fetch.ms = -1
benchi-kafka     | 	confluent.produce.throttle.pre.check.enable = false
benchi-kafka     | 	confluent.producer.id.cache.broker.hard.limit = -1
benchi-kafka     | 	confluent.producer.id.cache.eviction.minimal.expiration.ms = 900000
benchi-kafka     | 	confluent.producer.id.cache.extra.eviction.percentage = 0
benchi-kafka     | 	confluent.producer.id.cache.limit = 2147483647
benchi-kafka     | 	confluent.producer.id.cache.partition.hard.limit = -1
benchi-kafka     | 	confluent.producer.id.cache.tenant.hard.limit = -1
benchi-kafka     | 	confluent.producer.id.quota.manager.enable = false
benchi-kafka     | 	confluent.producer.id.quota.window.num = 11
benchi-kafka     | 	confluent.producer.id.quota.window.size.seconds = 1
benchi-kafka     | 	confluent.producer.id.throttle.enable = false
benchi-kafka     | 	confluent.producer.id.throttle.enable.threshold.percentage = 100
benchi-kafka     | 	confluent.proxy.mode.local.default = false
benchi-kafka     | 	confluent.proxy.protocol.fallback.enabled = false
benchi-kafka     | 	confluent.proxy.protocol.version = NONE
benchi-kafka     | 	confluent.quota.computing.usage.adjustment = 0.5
benchi-kafka     | 	confluent.quota.dynamic.enable = false
benchi-kafka     | 	confluent.quota.dynamic.publishing.interval.ms = 60000
benchi-kafka     | 	confluent.quota.dynamic.reporting.interval.ms = 30000
benchi-kafka     | 	confluent.quota.dynamic.reporting.min.usage = 102400
benchi-kafka     | 	confluent.quota.tenant.broker.max.consumer.rate = 13107200
benchi-kafka     | 	confluent.quota.tenant.broker.max.producer.rate = 13107200
benchi-kafka     | 	confluent.quota.tenant.default.controller.mutation.rate = 2.147483647E9
benchi-kafka     | 	confluent.quota.tenant.default.producer.id.rate = 2.147483647E9
benchi-kafka     | 	confluent.quota.tenant.fetch.multiplier = 1.0
benchi-kafka     | 	confluent.quota.tenant.follower.broker.min.consumer.rate = 10485760
benchi-kafka     | 	confluent.quota.tenant.follower.broker.min.producer.rate = 10485760
benchi-kafka     | 	confluent.quota.tenant.produce.multiplier = 1.0
benchi-kafka     | 	confluent.quota.tenant.user.quotas.enable = false
benchi-kafka     | 	confluent.regional.metadata.client.class = null
benchi-kafka     | 	confluent.regional.resource.manager.client.scheduler.threads = 2
benchi-kafka     | 	confluent.regional.resource.manager.endpoint = null
benchi-kafka     | 	confluent.regional.resource.manager.watch.endpoint = null
benchi-kafka     | 	confluent.replica.fetch.backoff.max.ms = 1000
benchi-kafka     | 	confluent.replica.fetch.connections.mode = combined
benchi-kafka     | 	confluent.replication.mode = PULL
benchi-kafka     | 	confluent.replication.push.feature.enable = false
benchi-kafka     | 	confluent.reporters.telemetry.auto.enable = true
benchi-kafka     | 	confluent.request.log.filter.class = class org.apache.kafka.common.requests.SamplingRequestLogFilter
benchi-kafka     | 	confluent.request.pipelining.enable = true
benchi-kafka     | 	confluent.request.pipelining.max.in.flight.requests.per.connection = 5
benchi-kafka     | 	confluent.require.calling.resource.identity = false
benchi-kafka     | 	confluent.require.compatible.keystore.updates = true
benchi-kafka     | 	confluent.require.confluent.issuer = false
benchi-kafka     | 	confluent.roll.check.interval.ms = 300000
benchi-kafka     | 	confluent.schema.registry.max.cache.size = 10000
benchi-kafka     | 	confluent.schema.registry.max.retries = 1
benchi-kafka     | 	confluent.schema.registry.retries.wait.ms = 0
benchi-kafka     | 	confluent.schema.registry.url = null
benchi-kafka     | 	confluent.schema.validator.interceptor.class = io.confluent.kafka.schemaregistry.validator.RecordSchemaValidator
benchi-kafka     | 	confluent.schema.validator.multitenant.enable = false
benchi-kafka     | 	confluent.schema.validator.samples.per.min = 0
benchi-kafka     | 	confluent.security.bc.approved.mode.enable = false
benchi-kafka     | 	confluent.security.event.logger.authentication.enable = false
benchi-kafka     | 	confluent.security.event.logger.authentication.event.rate.limit = -1
benchi-kafka     | 	confluent.security.event.logger.authorization.event.rate.limit = -1
benchi-kafka     | 	confluent.security.event.logger.detailed.audit.logs.filter.class = class org.apache.kafka.common.requests.DetailedRequestAuditLogFilter
benchi-kafka     | 	confluent.security.event.logger.enable = true
benchi-kafka     | 	confluent.security.event.logger.kafka.request.rate.limit = -1
benchi-kafka     | 	confluent.security.event.logger.physical.cluster.id = 
benchi-kafka     | 	confluent.security.event.router.config = 
benchi-kafka     | 	confluent.security.revoked.certificate.ids = 
benchi-kafka     | 	confluent.segment.eager.roll.enable = false
benchi-kafka     | 	confluent.segment.speculative.prefetch.enable = false
benchi-kafka     | 	confluent.spiffe.id.principal.extraction.rules = 
benchi-kafka     | 	confluent.ssl.key.password = null
benchi-kafka     | 	confluent.ssl.keystore.location = null
benchi-kafka     | 	confluent.ssl.keystore.password = null
benchi-kafka     | 	confluent.ssl.keystore.type = null
benchi-kafka     | 	confluent.ssl.protocol = null
benchi-kafka     | 	confluent.ssl.truststore.location = null
benchi-kafka     | 	confluent.ssl.truststore.password = null
benchi-kafka     | 	confluent.ssl.truststore.type = null
benchi-kafka     | 	confluent.step.connection.rate.per.ip = -1.0
benchi-kafka     | 	confluent.step.connection.rate.per.tenant = -1.0
benchi-kafka     | 	confluent.storage.probe.period.ms = -1
benchi-kafka     | 	confluent.storage.probe.slow.write.threshold.ms = 5000
benchi-kafka     | 	confluent.stray.log.delete.delay.ms = 604800000
benchi-kafka     | 	confluent.stray.log.max.deletions.per.run = 72
benchi-kafka     | 	confluent.subdomain.prefix = null
benchi-kafka     | 	confluent.subdomain.separator.map = null
benchi-kafka     | 	confluent.subdomain.separator.variable = %sep
benchi-kafka     | 	confluent.system.time.roll.enable = false
benchi-kafka     | 	confluent.telemetry.enabled = false
benchi-kafka     | 	confluent.telemetry.external.client.metrics.push.enabled = false
benchi-kafka     | 	confluent.tenant.latency.metric.enabled = false
benchi-kafka     | 	confluent.tier.archiver.num.threads = 2
benchi-kafka     | 	confluent.tier.azure.block.blob.auto.abort.threshold.bytes = 500000
benchi-kafka     | 	confluent.tier.azure.block.blob.container = null
benchi-kafka     | 	confluent.tier.azure.block.blob.cred.file.path = null
benchi-kafka     | 	confluent.tier.azure.block.blob.endpoint = null
benchi-kafka     | 	confluent.tier.azure.block.blob.prefix = 
benchi-kafka     | 	confluent.tier.backend = 
benchi-kafka     | 	confluent.tier.bucket.probe.period.ms = -1
benchi-kafka     | 	confluent.tier.cleaner.compact.min.efficiency = 0.5
benchi-kafka     | 	confluent.tier.cleaner.compact.segment.min.bytes = 20971520
benchi-kafka     | 	confluent.tier.cleaner.dedupe.buffer.size = 134217728
benchi-kafka     | 	confluent.tier.cleaner.dual.compaction = false
benchi-kafka     | 	confluent.tier.cleaner.dual.compaction.validation.max.bytes = 1073741824
benchi-kafka     | 	confluent.tier.cleaner.dual.compaction.validation.percent = 0
benchi-kafka     | 	confluent.tier.cleaner.enable = false
benchi-kafka     | 	confluent.tier.cleaner.excluded.topics = [^_confluent.*]
benchi-kafka     | 	confluent.tier.cleaner.feature.enable = false
benchi-kafka     | 	confluent.tier.cleaner.io.buffer.load.factor = 0.9
benchi-kafka     | 	confluent.tier.cleaner.io.buffer.size = 10485760
benchi-kafka     | 	confluent.tier.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
benchi-kafka     | 	confluent.tier.cleaner.min.cleanable.ratio = 0.75
benchi-kafka     | 	confluent.tier.cleaner.num.threads = 2
benchi-kafka     | 	confluent.tier.enable = false
benchi-kafka     | 	confluent.tier.feature = false
benchi-kafka     | 	confluent.tier.fenced.segment.delete.delay.ms = 600000
benchi-kafka     | 	confluent.tier.fetcher.async.enable = false
benchi-kafka     | 	confluent.tier.fetcher.async.timestamp.offset.parallelism = 1
benchi-kafka     | 	confluent.tier.fetcher.fetch.based.on.segment_and_metadata_layout.field = false
benchi-kafka     | 	confluent.tier.fetcher.memorypool.bytes = 0
benchi-kafka     | 	confluent.tier.fetcher.num.threads = 4
benchi-kafka     | 	confluent.tier.fetcher.offset.cache.expiration.ms = 1800000
benchi-kafka     | 	confluent.tier.fetcher.offset.cache.period.ms = 60000
benchi-kafka     | 	confluent.tier.fetcher.offset.cache.size = 200000
benchi-kafka     | 	confluent.tier.gcs.bucket = null
benchi-kafka     | 	confluent.tier.gcs.cred.file.path = null
benchi-kafka     | 	confluent.tier.gcs.prefix = 
benchi-kafka     | 	confluent.tier.gcs.region = null
benchi-kafka     | 	confluent.tier.gcs.sse.customer.encryption.key = null
benchi-kafka     | 	confluent.tier.gcs.write.chunk.size = 0
benchi-kafka     | 	confluent.tier.local.hotset.bytes = -1
benchi-kafka     | 	confluent.tier.local.hotset.ms = 86400000
benchi-kafka     | 	confluent.tier.max.partition.fetch.bytes.override = 0
benchi-kafka     | 	confluent.tier.metadata.bootstrap.servers = null
benchi-kafka     | 	confluent.tier.metadata.max.poll.ms = 100
benchi-kafka     | 	confluent.tier.metadata.namespace = null
benchi-kafka     | 	confluent.tier.metadata.num.partitions = 50
benchi-kafka     | 	confluent.tier.metadata.replication.factor = 1
benchi-kafka     | 	confluent.tier.metadata.request.timeout.ms = 30000
benchi-kafka     | 	confluent.tier.metadata.snapshots.enable = false
benchi-kafka     | 	confluent.tier.metadata.snapshots.interval.ms = 86400000
benchi-kafka     | 	confluent.tier.metadata.snapshots.retention.days = 7
benchi-kafka     | 	confluent.tier.metadata.snapshots.threads = 2
benchi-kafka     | 	confluent.tier.object.fetcher.num.threads = 1
benchi-kafka     | 	confluent.tier.partition.state.cleanup.delay.ms = 2592000000
benchi-kafka     | 	confluent.tier.partition.state.cleanup.enable = false
benchi-kafka     | 	confluent.tier.partition.state.cleanup.interval.ms = 86400000
benchi-kafka     | 	confluent.tier.partition.state.commit.interval.ms = 15000
benchi-kafka     | 	confluent.tier.s3.assumerole.arn = null
benchi-kafka     | 	confluent.tier.s3.auto.abort.threshold.bytes = 500000
benchi-kafka     | 	confluent.tier.s3.aws.endpoint.override = null
benchi-kafka     | 	confluent.tier.s3.aws.signer.override = null
benchi-kafka     | 	confluent.tier.s3.bucket = null
benchi-kafka     | 	confluent.tier.s3.cred.file.path = null
benchi-kafka     | 	confluent.tier.s3.force.path.style.access = false
benchi-kafka     | 	confluent.tier.s3.prefix = 
benchi-kafka     | 	confluent.tier.s3.region = null
benchi-kafka     | 	confluent.tier.s3.security.providers = null
benchi-kafka     | 	confluent.tier.s3.sse.algorithm = AES256
benchi-kafka     | 	confluent.tier.s3.sse.customer.encryption.key = null
benchi-kafka     | 	confluent.tier.s3.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	confluent.tier.s3.ssl.key.password = null
benchi-kafka     | 	confluent.tier.s3.ssl.keystore.location = null
benchi-kafka     | 	confluent.tier.s3.ssl.keystore.password = null
benchi-kafka     | 	confluent.tier.s3.ssl.keystore.type = null
benchi-kafka     | 	confluent.tier.s3.ssl.protocol = TLSv1.3
benchi-kafka     | 	confluent.tier.s3.ssl.provider = null
benchi-kafka     | 	confluent.tier.s3.ssl.truststore.location = null
benchi-kafka     | 	confluent.tier.s3.ssl.truststore.password = null
benchi-kafka     | 	confluent.tier.s3.ssl.truststore.type = null
benchi-kafka     | 	confluent.tier.s3.user.agent.prefix = APN/1.0 Confluent/1.0 TieredStorageS3/1.0
benchi-kafka     | 	confluent.tier.segment.hotset.roll.min.bytes = 104857600
benchi-kafka     | 	confluent.tier.segment.metadata.layout.put.mode = LegacyMultiObject
benchi-kafka     | 	confluent.tier.topic.data.loss.validation.fencing.enable = false
benchi-kafka     | 	confluent.tier.topic.delete.backoff.ms = 21600000
benchi-kafka     | 	confluent.tier.topic.delete.check.interval.ms = 300000
benchi-kafka     | 	confluent.tier.topic.delete.max.inprogress.partitions = 100
benchi-kafka     | 	confluent.tier.topic.head.data.loss.validation.enable = true
benchi-kafka     | 	confluent.tier.topic.head.data.loss.validation.max.timeout.ms = 900000
benchi-kafka     | 	confluent.tier.topic.materialization.from.snapshot.enable = false
benchi-kafka     | 	confluent.tier.topic.producer.enable.idempotence = true
benchi-kafka     | 	confluent.tier.topic.snapshots.enable = false
benchi-kafka     | 	confluent.tier.topic.snapshots.interval.ms = 300000
benchi-kafka     | 	confluent.tier.topic.snapshots.max.records = 100000
benchi-kafka     | 	confluent.tier.topic.snapshots.retention.hours = 168
benchi-kafka     | 	confluent.topic.partition.default.placement = 2
benchi-kafka     | 	confluent.topic.policy.use.computed.assignments = false
benchi-kafka     | 	confluent.topic.replica.assignor.builder.class = 
benchi-kafka     | 	confluent.track.api.key.per.ip = false
benchi-kafka     | 	confluent.track.per.ip.max.size = 100000
benchi-kafka     | 	confluent.track.tenant.id.per.ip = false
benchi-kafka     | 	confluent.traffic.cdc.network.id.routes.enable = false
benchi-kafka     | 	confluent.traffic.cdc.network.id.routes.listener.names = EXTERNAL_BACKCHANNEL
benchi-kafka     | 	confluent.traffic.cdc.network.id.routes.periodic.start.task.ms = 300000
benchi-kafka     | 	confluent.traffic.cdc.network.id.routes.topic.name = _confluent-network_id_routes
benchi-kafka     | 	confluent.traffic.network.id = 
benchi-kafka     | 	confluent.transaction.2pc.timeout.ms = -1
benchi-kafka     | 	confluent.transaction.logging.verbosity = 0
benchi-kafka     | 	confluent.transaction.state.log.placement.constraints = 
benchi-kafka     | 	confluent.valid.broker.rack.set = null
benchi-kafka     | 	confluent.verify.group.subscription.prefix = false
benchi-kafka     | 	confluent.virtual.topic.creation.enabled = false
benchi-kafka     | 	confluent.zookeeper.metadata.migration.controller.check.disable = false
benchi-kafka     | 	confluent.zookeeper.metadata.migration.trigger.file.path = null
benchi-kafka     | 	connection.failed.authentication.delay.ms = 100
benchi-kafka     | 	connection.min.expire.interval.ms = 250
benchi-kafka     | 	connections.max.age.ms = 3153600000000
benchi-kafka     | 	connections.max.idle.ms = 600000
benchi-kafka     | 	connections.max.reauth.ms = 0
benchi-kafka     | 	control.plane.listener.name = null
benchi-kafka     | 	controlled.shutdown.enable = true
benchi-kafka     | 	controlled.shutdown.max.retries = 3
benchi-kafka     | 	controlled.shutdown.retry.backoff.ms = 5000
benchi-kafka     | 	controller.listener.names = CONTROLLER
benchi-kafka     | 	controller.quorum.append.linger.ms = 25
benchi-kafka     | 	controller.quorum.bootstrap.servers = []
benchi-kafka     | 	controller.quorum.election.backoff.max.ms = 1000
benchi-kafka     | 	controller.quorum.election.timeout.ms = 1000
benchi-kafka     | 	controller.quorum.fetch.timeout.ms = 2000
benchi-kafka     | 	controller.quorum.request.timeout.ms = 2000
benchi-kafka     | 	controller.quorum.retry.backoff.ms = 20
benchi-kafka     | 	controller.quorum.voters = [1@broker:29093]
benchi-kafka     | 	controller.quota.window.num = 11
benchi-kafka     | 	controller.quota.window.size.seconds = 1
benchi-kafka     | 	controller.socket.timeout.ms = 30000
benchi-kafka     | 	create.cluster.link.policy.class.name = null
benchi-kafka     | 	create.topic.policy.class.name = null
benchi-kafka     | 	default.replication.factor = 1
benchi-kafka     | 	delegation.token.expiry.check.interval.ms = 3600000
benchi-kafka     | 	delegation.token.expiry.time.ms = 86400000
benchi-kafka     | 	delegation.token.master.key = null
benchi-kafka     | 	delegation.token.max.lifetime.ms = 604800000
benchi-kafka     | 	delegation.token.secret.key = null
benchi-kafka     | 	delete.records.purgatory.purge.interval.requests = 1
benchi-kafka     | 	delete.topic.enable = true
benchi-kafka     | 	early.start.listeners = null
benchi-kafka     | 	eligible.leader.replicas.enable = false
benchi-kafka     | 	enable.fips = false
benchi-kafka     | 	fetch.max.bytes = 57671680
benchi-kafka     | 	fetch.purgatory.purge.interval.requests = 1000
benchi-kafka     | 	floor.max.connection.creation.rate = null
benchi-kafka     | 	follower.replication.throttled.rate = 9223372036854775807
benchi-kafka     | 	follower.replication.throttled.replicas = none
benchi-kafka     | 	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
benchi-kafka     | 	group.consumer.heartbeat.interval.ms = 5000
benchi-kafka     | 	group.consumer.max.heartbeat.interval.ms = 15000
benchi-kafka     | 	group.consumer.max.session.timeout.ms = 60000
benchi-kafka     | 	group.consumer.max.size = 2147483647
benchi-kafka     | 	group.consumer.migration.policy = disabled
benchi-kafka     | 	group.consumer.min.heartbeat.interval.ms = 5000
benchi-kafka     | 	group.consumer.min.session.timeout.ms = 45000
benchi-kafka     | 	group.consumer.session.timeout.ms = 45000
benchi-kafka     | 	group.coordinator.append.linger.ms = 10
benchi-kafka     | 	group.coordinator.new.enable = false
benchi-kafka     | 	group.coordinator.rebalance.protocols = [classic]
benchi-kafka     | 	group.coordinator.threads = 1
benchi-kafka     | 	group.initial.rebalance.delay.ms = 0
benchi-kafka     | 	group.max.session.timeout.ms = 1800000
benchi-kafka     | 	group.max.size = 2147483647
benchi-kafka     | 	group.min.session.timeout.ms = 6000
benchi-kafka     | 	initial.broker.registration.timeout.ms = 60000
benchi-kafka     | 	inter.broker.listener.name = PLAINTEXT
benchi-kafka     | 	inter.broker.protocol.version = 3.8-IV0A
benchi-kafka     | 	k2.stack.builder.class.name = null
benchi-kafka     | 	k2.startup.timeout.ms = 60000
benchi-kafka     | 	k2.topic.metadata.max.total.partition.count = 20000
benchi-kafka     | 	k2.topic.metadata.refresh.ms = 10000
benchi-kafka     | 	kafka.metrics.polling.interval.secs = 10
benchi-kafka     | 	kafka.metrics.reporters = []
benchi-kafka     | 	leader.imbalance.check.interval.seconds = 300
benchi-kafka     | 	leader.imbalance.per.broker.percentage = 10
benchi-kafka     | 	leader.replication.throttled.rate = 9223372036854775807
benchi-kafka     | 	leader.replication.throttled.replicas = none
benchi-kafka     | 	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
benchi-kafka     | 	listeners = PLAINTEXT://broker:29092,CONTROLLER://broker:29093,PLAINTEXT_HOST://0.0.0.0:9092
benchi-kafka     | 	log.cleaner.backoff.ms = 15000
benchi-kafka     | 	log.cleaner.dedupe.buffer.size = 134217728
benchi-kafka     | 	log.cleaner.delete.retention.ms = 86400000
benchi-kafka     | 	log.cleaner.enable = true
benchi-kafka     | 	log.cleaner.hash.algorithm = MD5
benchi-kafka     | 	log.cleaner.io.buffer.load.factor = 0.9
benchi-kafka     | 	log.cleaner.io.buffer.size = 524288
benchi-kafka     | 	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
benchi-kafka     | 	log.cleaner.max.compaction.lag.ms = 9223372036854775807
benchi-kafka     | 	log.cleaner.min.cleanable.ratio = 0.5
benchi-kafka     | 	log.cleaner.min.compaction.lag.ms = 0
benchi-kafka     | 	log.cleaner.threads = 1
benchi-kafka     | 	log.cleanup.policy = [delete]
benchi-kafka     | 	log.deletion.max.segments.per.run = 2147483647
benchi-kafka     | 	log.deletion.throttler.disk.free.headroom.bytes = 21474836480
benchi-kafka     | 	log.dir = /tmp/kafka-logs
benchi-kafka     | 	log.dir.failure.timeout.ms = 30000
benchi-kafka     | 	log.dirs = /tmp/kraft-combined-logs
benchi-kafka     | 	log.flush.interval.messages = 9223372036854775807
benchi-kafka     | 	log.flush.interval.ms = null
benchi-kafka     | 	log.flush.offset.checkpoint.interval.ms = 60000
benchi-kafka     | 	log.flush.scheduler.interval.ms = 9223372036854775807
benchi-kafka     | 	log.flush.start.offset.checkpoint.interval.ms = 60000
benchi-kafka     | 	log.index.interval.bytes = 4096
benchi-kafka     | 	log.index.size.max.bytes = 10485760
benchi-kafka     | 	log.initial.task.delay.ms = 30000
benchi-kafka     | 	log.local.retention.bytes = -2
benchi-kafka     | 	log.local.retention.ms = -2
benchi-kafka     | 	log.message.downconversion.enable = true
benchi-kafka     | 	log.message.format.version = 3.0-IV1
benchi-kafka     | 	log.message.timestamp.after.max.ms = 9223372036854775807
benchi-kafka     | 	log.message.timestamp.before.max.ms = 9223372036854775807
benchi-kafka     | 	log.message.timestamp.difference.max.ms = 9223372036854775807
benchi-kafka     | 	log.message.timestamp.type = CreateTime
benchi-kafka     | 	log.preallocate = false
benchi-kafka     | 	log.retention.bytes = -1
benchi-kafka     | 	log.retention.check.interval.ms = 300000
benchi-kafka     | 	log.retention.hours = 168
benchi-kafka     | 	log.retention.minutes = null
benchi-kafka     | 	log.retention.ms = null
benchi-kafka     | 	log.roll.hours = 168
benchi-kafka     | 	log.roll.jitter.hours = 0
benchi-kafka     | 	log.roll.jitter.ms = null
benchi-kafka     | 	log.roll.ms = null
benchi-kafka     | 	log.segment.bytes = 1073741824
benchi-kafka     | 	log.segment.delete.delay.ms = 60000
benchi-kafka     | 	max.connection.creation.rate = 1.7976931348623157E308
benchi-kafka     | 	max.connection.creation.rate.per.ip.enable.threshold = 0.0
benchi-kafka     | 	max.connection.creation.rate.per.tenant.enable.threshold = 0.0
benchi-kafka     | 	max.connections = 2147483647
benchi-kafka     | 	max.connections.per.ip = 2147483647
benchi-kafka     | 	max.connections.per.ip.overrides = 
benchi-kafka     | 	max.connections.per.tenant = 0
benchi-kafka     | 	max.connections.protected.listeners = []
benchi-kafka     | 	max.connections.reap.amount = 0
benchi-kafka     | 	max.incremental.fetch.session.cache.slots = 1000
benchi-kafka     | 	max.request.partition.size.limit = 2000
benchi-kafka     | 	message.max.bytes = 1048588
benchi-kafka     | 	metadata.log.dir = null
benchi-kafka     | 	metadata.log.max.record.bytes.between.snapshots = 20971520
benchi-kafka     | 	metadata.log.max.snapshot.interval.ms = 3600000
benchi-kafka     | 	metadata.log.segment.bytes = 1073741824
benchi-kafka     | 	metadata.log.segment.min.bytes = 8388608
benchi-kafka     | 	metadata.log.segment.ms = 604800000
benchi-kafka     | 	metadata.max.idle.interval.ms = 500
benchi-kafka     | 	metadata.max.retention.bytes = 104857600
benchi-kafka     | 	metadata.max.retention.ms = 604800000
benchi-kafka     | 	metric.reporters = [io.confluent.metrics.reporter.ConfluentMetricsReporter]
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.recording.level = INFO
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	min.insync.replicas = 1
benchi-kafka     | 	multitenant.authorizer.support.resource.ids = false
benchi-kafka     | 	multitenant.metadata.class = null
benchi-kafka     | 	multitenant.metadata.dir = null
benchi-kafka     | 	multitenant.metadata.reload.delay.ms = 120000
benchi-kafka     | 	multitenant.metadata.ssl.certs.path = null
benchi-kafka     | 	multitenant.tenant.delete.batch.size = 10
benchi-kafka     | 	multitenant.tenant.delete.check.ms = 120000
benchi-kafka     | 	multitenant.tenant.delete.delay = 604800000
benchi-kafka     | 	node.id = 1
benchi-kafka     | 	num.io.threads = 8
benchi-kafka     | 	num.network.threads = 3
benchi-kafka     | 	num.partitions = 1
benchi-kafka     | 	num.recovery.threads.per.data.dir = 1
benchi-kafka     | 	num.replica.alter.log.dirs.threads = null
benchi-kafka     | 	num.replica.fetchers = 1
benchi-kafka     | 	offset.metadata.max.bytes = 4096
benchi-kafka     | 	offsets.commit.required.acks = -1
benchi-kafka     | 	offsets.commit.timeout.ms = 5000
benchi-kafka     | 	offsets.load.buffer.size = 5242880
benchi-kafka     | 	offsets.retention.check.interval.ms = 600000
benchi-kafka     | 	offsets.retention.minutes = 10080
benchi-kafka     | 	offsets.topic.compression.codec = 0
benchi-kafka     | 	offsets.topic.num.partitions = 50
benchi-kafka     | 	offsets.topic.replication.factor = 1
benchi-kafka     | 	offsets.topic.segment.bytes = 104857600
benchi-kafka     | 	otel.exporter.otlp.custom.endpoint = default
benchi-kafka     | 	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
benchi-kafka     | 	password.encoder.iterations = 4096
benchi-kafka     | 	password.encoder.key.length = 128
benchi-kafka     | 	password.encoder.keyfactory.algorithm = null
benchi-kafka     | 	password.encoder.old.secret = null
benchi-kafka     | 	password.encoder.secret = null
benchi-kafka     | 	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
benchi-kafka     | 	process.roles = [broker, controller]
benchi-kafka     | 	producer.id.expiration.check.interval.ms = 600000
benchi-kafka     | 	producer.id.expiration.ms = 86400000
benchi-kafka     | 	producer.purgatory.purge.interval.requests = 1000
benchi-kafka     | 	queued.max.request.bytes = -1
benchi-kafka     | 	queued.max.requests = 500
benchi-kafka     | 	quota.window.num = 11
benchi-kafka     | 	quota.window.size.seconds = 1
benchi-kafka     | 	quotas.consumption.expiration.time.ms = 600000
benchi-kafka     | 	quotas.expiration.interval.ms = 3600000
benchi-kafka     | 	quotas.expiration.time.ms = 604800000
benchi-kafka     | 	quotas.lazy.evaluation.threshold = 0.5
benchi-kafka     | 	quotas.topic.append.timeout.ms = 5000
benchi-kafka     | 	quotas.topic.compression.codec = 3
benchi-kafka     | 	quotas.topic.load.buffer.size = 5242880
benchi-kafka     | 	quotas.topic.num.partitions = 50
benchi-kafka     | 	quotas.topic.placement.constraints = 
benchi-kafka     | 	quotas.topic.replication.factor = 3
benchi-kafka     | 	quotas.topic.segment.bytes = 104857600
benchi-kafka     | 	remote.fetch.max.wait.ms = 500
benchi-kafka     | 	remote.log.index.file.cache.total.size.bytes = 1073741824
benchi-kafka     | 	remote.log.manager.copier.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.copy.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.copy.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.expiration.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.fetch.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.fetch.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.task.interval.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.max.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.ms = 500
benchi-kafka     | 	remote.log.manager.task.retry.jitter = 0.2
benchi-kafka     | 	remote.log.manager.thread.pool.size = 10
benchi-kafka     | 	remote.log.metadata.custom.metadata.max.bytes = 128
benchi-kafka     | 	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
benchi-kafka     | 	remote.log.metadata.manager.class.path = null
benchi-kafka     | 	remote.log.metadata.manager.impl.prefix = rlmm.config.
benchi-kafka     | 	remote.log.metadata.manager.listener.name = null
benchi-kafka     | 	remote.log.reader.max.pending.tasks = 100
benchi-kafka     | 	remote.log.reader.threads = 10
benchi-kafka     | 	remote.log.storage.manager.class.name = null
benchi-kafka     | 	remote.log.storage.manager.class.path = null
benchi-kafka     | 	remote.log.storage.manager.impl.prefix = rsm.config.
benchi-kafka     | 	remote.log.storage.system.enable = false
benchi-kafka     | 	replica.fetch.backoff.ms = 1000
benchi-kafka     | 	replica.fetch.max.bytes = 1048576
benchi-kafka     | 	replica.fetch.min.bytes = 1
benchi-kafka     | 	replica.fetch.response.max.bytes = 10485760
benchi-kafka     | 	replica.fetch.wait.max.ms = 500
benchi-kafka     | 	replica.high.watermark.checkpoint.interval.ms = 5000
benchi-kafka     | 	replica.lag.time.max.ms = 30000
benchi-kafka     | 	replica.selector.class = null
benchi-kafka     | 	replica.socket.receive.buffer.bytes = 65536
benchi-kafka     | 	replica.socket.timeout.ms = 30000
benchi-kafka     | 	replication.quota.window.num = 11
benchi-kafka     | 	replication.quota.window.size.seconds = 1
benchi-kafka     | 	request.timeout.ms = 30000
benchi-kafka     | 	reserved.broker.max.id = 1000
benchi-kafka     | 	sasl.client.callback.handler.class = null
benchi-kafka     | 	sasl.enabled.mechanisms = [GSSAPI]
benchi-kafka     | 	sasl.jaas.config = null
benchi-kafka     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
benchi-kafka     | 	sasl.kerberos.min.time.before.relogin = 60000
benchi-kafka     | 	sasl.kerberos.principal.to.local.rules = [DEFAULT]
benchi-kafka     | 	sasl.kerberos.service.name = null
benchi-kafka     | 	sasl.kerberos.ticket.renew.jitter = 0.05
benchi-kafka     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
benchi-kafka     | 	sasl.login.callback.handler.class = null
benchi-kafka     | 	sasl.login.class = null
benchi-kafka     | 	sasl.login.connect.timeout.ms = null
benchi-kafka     | 	sasl.login.read.timeout.ms = null
benchi-kafka     | 	sasl.login.refresh.buffer.seconds = 300
benchi-kafka     | 	sasl.login.refresh.min.period.seconds = 60
benchi-kafka     | 	sasl.login.refresh.window.factor = 0.8
benchi-kafka     | 	sasl.login.refresh.window.jitter = 0.05
benchi-kafka     | 	sasl.login.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.login.retry.backoff.ms = 100
benchi-kafka     | 	sasl.mechanism.controller.protocol = GSSAPI
benchi-kafka     | 	sasl.mechanism.inter.broker.protocol = GSSAPI
benchi-kafka     | 	sasl.oauthbearer.clientassertion.audience = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.expiration = 5
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.jti.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.nbf.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.issuer = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.location = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key.passphrase = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.subject = null
benchi-kafka     | 	sasl.oauthbearer.clock.skew.seconds = 30
benchi-kafka     | 	sasl.oauthbearer.expected.audience = null
benchi-kafka     | 	sasl.oauthbearer.expected.issuer = null
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.url = null
benchi-kafka     | 	sasl.oauthbearer.scope.claim.name = scope
benchi-kafka     | 	sasl.oauthbearer.sub.claim.name = sub
benchi-kafka     | 	sasl.oauthbearer.token.endpoint.url = null
benchi-kafka     | 	sasl.server.authn.async.enable = false
benchi-kafka     | 	sasl.server.authn.async.max.threads = 1
benchi-kafka     | 	sasl.server.authn.async.timeout.ms = 30000
benchi-kafka     | 	sasl.server.callback.handler.class = null
benchi-kafka     | 	sasl.server.max.receive.size = 524288
benchi-kafka     | 	security.inter.broker.protocol = PLAINTEXT
benchi-kafka     | 	security.providers = null
benchi-kafka     | 	server.max.startup.time.ms = 9223372036854775807
benchi-kafka     | 	socket.connection.setup.timeout.max.ms = 30000
benchi-kafka     | 	socket.connection.setup.timeout.ms = 10000
benchi-kafka     | 	socket.listen.backlog.size = 50
benchi-kafka     | 	socket.receive.buffer.bytes = 102400
benchi-kafka     | 	socket.request.max.bytes = 104857600
benchi-kafka     | 	socket.send.buffer.bytes = 102400
benchi-kafka     | 	ssl.allow.dn.changes = false
benchi-kafka     | 	ssl.allow.san.changes = false
benchi-kafka     | 	ssl.cipher.suites = []
benchi-kafka     | 	ssl.client.auth = none
benchi-kafka     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	ssl.engine.factory.class = null
benchi-kafka     | 	ssl.key.password = null
benchi-kafka     | 	ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	ssl.keystore.certificate.chain = null
benchi-kafka     | 	ssl.keystore.key = null
benchi-kafka     | 	ssl.keystore.location = null
benchi-kafka     | 	ssl.keystore.password = null
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.principal.mapping.rules = DEFAULT
benchi-kafka     | 	ssl.protocol = TLSv1.3
benchi-kafka     | 	ssl.provider = null
benchi-kafka     | 	ssl.secure.random.implementation = null
benchi-kafka     | 	ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	ssl.truststore.certificates = null
benchi-kafka     | 	ssl.truststore.location = null
benchi-kafka     | 	ssl.truststore.password = null
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     | 	telemetry.max.bytes = 1048576
benchi-kafka     | 	throughput.quota.window.num = 11
benchi-kafka     | 	token.impersonation.validation = true
benchi-kafka     | 	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
benchi-kafka     | 	transaction.max.timeout.ms = 900000
benchi-kafka     | 	transaction.metadata.load.threads = 32
benchi-kafka     | 	transaction.partition.verification.enable = true
benchi-kafka     | 	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
benchi-kafka     | 	transaction.state.log.load.buffer.size = 5242880
benchi-kafka     | 	transaction.state.log.min.isr = 1
benchi-kafka     | 	transaction.state.log.num.partitions = 50
benchi-kafka     | 	transaction.state.log.replication.factor = 1
benchi-kafka     | 	transaction.state.log.segment.bytes = 104857600
benchi-kafka     | 	transactional.id.expiration.ms = 604800000
benchi-kafka     | 	unclean.leader.election.enable = false
benchi-kafka     | 	unstable.api.versions.enable = false
benchi-kafka     | 	unstable.feature.versions.enable = false
benchi-kafka     | 	zookeeper.acl.change.notification.expiration.ms = 900000
benchi-kafka     | 	zookeeper.clientCnxnSocket = null
benchi-kafka     | 	zookeeper.connect = 
benchi-kafka     | 	zookeeper.connection.timeout.ms = null
benchi-kafka     | 	zookeeper.max.in.flight.requests = 10
benchi-kafka     | 	zookeeper.metadata.migration.enable = false
benchi-kafka     | 	zookeeper.metadata.migration.min.batch.size = 200
benchi-kafka     | 	zookeeper.session.timeout.ms = 18000
benchi-kafka     | 	zookeeper.set.acl = false
benchi-kafka     | 	zookeeper.ssl.cipher.suites = null
benchi-kafka     | 	zookeeper.ssl.client.enable = false
benchi-kafka     | 	zookeeper.ssl.crl.enable = false
benchi-kafka     | 	zookeeper.ssl.enabled.protocols = null
benchi-kafka     | 	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
benchi-kafka     | 	zookeeper.ssl.keystore.location = null
benchi-kafka     | 	zookeeper.ssl.keystore.password = null
benchi-kafka     | 	zookeeper.ssl.keystore.type = null
benchi-kafka     | 	zookeeper.ssl.ocsp.enable = false
benchi-kafka     | 	zookeeper.ssl.protocol = TLSv1.2
benchi-kafka     | 	zookeeper.ssl.truststore.location = null
benchi-kafka     | 	zookeeper.ssl.truststore.password = null
benchi-kafka     | 	zookeeper.ssl.truststore.type = null
benchi-kafka     |  (kafka.server.KafkaConfig)
benchi-kafka     | [2025-03-18 16:05:46,642] INFO RemoteLogManagerConfig values: 
benchi-kafka     | 	log.local.retention.bytes = -2
benchi-kafka     | 	log.local.retention.ms = -2
benchi-kafka     | 	remote.fetch.max.wait.ms = 500
benchi-kafka     | 	remote.log.index.file.cache.total.size.bytes = 1073741824
benchi-kafka     | 	remote.log.manager.copier.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.copy.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.copy.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.expiration.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.fetch.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.fetch.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.task.interval.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.max.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.ms = 500
benchi-kafka     | 	remote.log.manager.task.retry.jitter = 0.2
benchi-kafka     | 	remote.log.manager.thread.pool.size = 10
benchi-kafka     | 	remote.log.metadata.custom.metadata.max.bytes = 128
benchi-kafka     | 	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
benchi-kafka     | 	remote.log.metadata.manager.class.path = null
benchi-kafka     | 	remote.log.metadata.manager.impl.prefix = rlmm.config.
benchi-kafka     | 	remote.log.metadata.manager.listener.name = null
benchi-kafka     | 	remote.log.reader.max.pending.tasks = 100
benchi-kafka     | 	remote.log.reader.threads = 10
benchi-kafka     | 	remote.log.storage.manager.class.name = null
benchi-kafka     | 	remote.log.storage.manager.class.path = null
benchi-kafka     | 	remote.log.storage.manager.impl.prefix = rsm.config.
benchi-kafka     | 	remote.log.storage.system.enable = false
benchi-kafka     |  (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
benchi-kafka     | [2025-03-18 16:05:46,648] INFO Unexpected credentials store injected: null (io.confluent.kafkarest.servlet.KafkaRestApplicationProvider)
benchi-kafka     | [2025-03-18 16:05:46,650] INFO For rest-app with listener null, configuring custom request logging (io.confluent.kafkarest.KafkaRestApplication)
benchi-kafka     | [2025-03-18 16:05:46,651] INFO EventEmitterConfig values: 
benchi-kafka     |  (io.confluent.shaded.io.confluent.telemetry.events.EventEmitterConfig)
benchi-kafka     | [2025-03-18 16:05:46,651] INFO EventEmitterConfig values: 
benchi-kafka     |  (io.confluent.shaded.io.confluent.telemetry.events.EventEmitterConfig)
benchi-kafka     | [2025-03-18 16:05:46,652] INFO ConfluentTelemetryConfig values: 
benchi-kafka     | 	confluent.telemetry.api.key = null
benchi-kafka     | 	confluent.telemetry.api.secret = null
benchi-kafka     | 	confluent.telemetry.cluster.id = null
benchi-kafka     | 	confluent.telemetry.debug.enabled = false
benchi-kafka     | 	confluent.telemetry.enabled = false
benchi-kafka     | 	confluent.telemetry.events.collector.include = .*transaction.remove.expired.transaction.cleanup.interval.ms.*|.*transaction.state.log.load.buffer.size.*|.*transaction.state.log.min.isr.*|.*transaction.state.log.num.partitions.*|.*transaction.state.log.replication.factor.*|.*transaction.state.log.segment.bytes.*|.*transactional.id.expiration.ms.*|.*controlled.shutdown.enable.*|.*controlled.shutdown.max.retries.*|.*controlled.shutdown.retry.backoff.ms.*|.*fetch.max.bytes.*|.*fetch.purgatory.purge.interval.requests.*|.*group.initial.rebalance.delay.ms.*|.*group.max.session.timeout.ms.*|.*group.max.size.*|.*group.min.session.timeout.ms.*|.*log.cleaner.backoff.ms.*|.*log.cleaner.dedupe.buffer.size.*|.*log.cleaner.delete.retention.ms.*|.*log.cleaner.enable.*|.*log.cleaner.io.buffer.load.factor.*|.*log.cleaner.io.buffer.size.*|.*log.cleaner.io.max.bytes.per.second.*|.*log.cleaner.max.compaction.lag.ms.*|.*log.cleaner.min.cleanable.ratio.*|.*log.cleaner.min.compaction.lag.ms.*|.*log.cleaner.threads.*|.*log.cleanup.policy.*|.*log.index.interval.bytes.*|.*log.index.size.max.bytes.*|.*log.message.downconversion.enable.*|.*log.message.format.version.*|.*log.message.timestamp.difference.max.ms.*|.*log.message.timestamp.type.*|.*max.connection.creation.rate.*|.*max.connections.*|.*max.connections.per.ip.*|.*max.incremental.fetch.session.cache.slots.*|.*replica.fetch.backoff.ms.*|.*replica.fetch.max.bytes.*|.*replica.fetch.min.bytes.*|.*replica.fetch.response.max.bytes.*|.*replica.fetch.wait.max.ms.*|.*replica.high.watermark.checkpoint.interval.ms.*|.*replica.lag.time.max.ms.*|.*replica.selector.class.*|.*replica.socket.receive.buffer.bytes.*|.*replica.socket.timeout.ms.*|.*alter.config.policy.class.name.*|.*alter.log.dirs.replication.quota.window.num.*|.*alter.log.dirs.replication.quota.window.size.seconds.*|.*metrics.num.samples.*|.*metrics.recording.level.*|.*metrics.sample.window.ms.*|.*quota.window.num.*|.*quota.window.size.seconds.*|.*replication.quota.window.num.*|.*replication.quota.window.size.seconds.*|.*confluent.balancer.enable.*|.*confluent.balancer.throttle.bytes.per.second.*|.*confluent.balancer.heal.uneven.load.trigger.*|.*confluent.balancer.heal.broker.failure.threshold.ms.*|.*confluent.balancer.disk.max.load.*|.*confluent.balancer.exclude.topic.prefixes.*|.*confluent.balancer.exclude.topic.names.*|.*confluent.tier.local.hotset.bytes.*|.*confluent.tier.local.hotset.ms.*|.*confluent.tier.archiver.num.threads.*|.*confluent.tier.backend.*|.*confluent.tier.enable.*|.*confluent.tier.feature.*|.*confluent.tier.fetcher.num.threads.*|.*confluent.tier.max.partition.fetch.bytes.override.*|.*confluent.tier.metadata.replication.factor.*|.*confluent.operator.managed.*|.*confluent.ansible.managed.*|.*confluent.license.*
benchi-kafka     | 	confluent.telemetry.events.enable = true
benchi-kafka     | 	confluent.telemetry.metrics.collector.include = .*io.confluent.system/.*(process_cpu_load|max_file_descriptor_count|open_file_descriptor_count|system_cpu_load|system_load_average|free_physical_memory_size|total_physical_memory_size|disk_total_bytes|disk_usable_bytes|jvm/mem|jvm/gc).*
benchi-kafka     | 	confluent.telemetry.metrics.collector.interval.ms = 60000
benchi-kafka     | 	confluent.telemetry.metrics.collector.slo.enabled = false
benchi-kafka     | 	confluent.telemetry.proxy.password = null
benchi-kafka     | 	confluent.telemetry.proxy.url = null
benchi-kafka     | 	confluent.telemetry.proxy.username = null
benchi-kafka     |  (io.confluent.telemetry.ConfluentTelemetryConfig)
benchi-kafka     | [2025-03-18 16:05:46,652] INFO VolumeMetricsCollectorConfig values: 
benchi-kafka     | 	confluent.telemetry.metrics.collector.volume.update.ms = 15000
benchi-kafka     |  (io.confluent.telemetry.collector.VolumeMetricsCollector$VolumeMetricsCollectorConfig)
benchi-kafka     | [2025-03-18 16:05:46,652] INFO HttpClientConfig values: 
benchi-kafka     | 	api.key = null
benchi-kafka     | 	api.secret = null
benchi-kafka     | 	buffer.batch.duration.max.ms = null
benchi-kafka     | 	buffer.batch.items.max = null
benchi-kafka     | 	buffer.inflight.submissions.max = null
benchi-kafka     | 	buffer.pending.batches.max = null
benchi-kafka     | 	client.attempts.max = null
benchi-kafka     | 	client.base.url = https://collector.telemetry.confluent.cloud
benchi-kafka     | 	client.compression = null
benchi-kafka     | 	client.connect.timeout.ms = null
benchi-kafka     | 	client.metrics.path.override = /v1/metrics
benchi-kafka     | 	client.request.timeout.ms = null
benchi-kafka     | 	client.retry.delay.seconds = null
benchi-kafka     | 	proxy.password = null
benchi-kafka     | 	proxy.url = null
benchi-kafka     | 	proxy.username = null
benchi-kafka     | 	type = httpTelemetryClient
benchi-kafka     |  (io.confluent.telemetry.exporter.http.HttpClientConfig)
benchi-kafka     | [2025-03-18 16:05:46,652] INFO HttpExporterConfig values: 
benchi-kafka     | 	api.key = null
benchi-kafka     | 	api.secret = null
benchi-kafka     | 	buffer.batch.duration.max.ms = null
benchi-kafka     | 	buffer.batch.items.max = null
benchi-kafka     | 	buffer.inflight.submissions.max = null
benchi-kafka     | 	buffer.pending.batches.max = null
benchi-kafka     | 	client = _confluentClient
benchi-kafka     | 	client.attempts.max = null
benchi-kafka     | 	client.base.url = null
benchi-kafka     | 	client.compression = null
benchi-kafka     | 	client.connect.timeout.ms = null
benchi-kafka     | 	client.metrics.path.override = /v1/metrics
benchi-kafka     | 	client.request.timeout.ms = null
benchi-kafka     | 	client.retry.delay.seconds = null
benchi-kafka     | 	enabled = false
benchi-kafka     | 	events.enabled = true
benchi-kafka     | 	metrics.enabled = true
benchi-kafka     | 	metrics.include = null
benchi-kafka     | 	proxy.password = null
benchi-kafka     | 	proxy.url = null
benchi-kafka     | 	proxy.username = null
benchi-kafka     | 	remote.configurable = false
benchi-kafka     | 	type = http
benchi-kafka     |  (io.confluent.telemetry.exporter.http.HttpExporterConfig)
benchi-kafka     | [2025-03-18 16:05:46,652] INFO Configuring named client _confluentClient for exporter _confluent (io.confluent.telemetry.exporter.http.HttpExporterConfig)
benchi-kafka     | [2025-03-18 16:05:46,652] WARN no telemetry exporters are enabled (io.confluent.telemetry.ConfluentTelemetryConfig)
benchi-kafka     | [2025-03-18 16:05:46,652] INFO PollingRemoteConfigurationConfig values: 
benchi-kafka     | 	enabled = true
benchi-kafka     | 	refresh.interval.ms = 60000
benchi-kafka     |  (io.confluent.shaded.io.confluent.telemetry.config.remote.polling.PollingRemoteConfigurationConfig)
benchi-kafka     | [2025-03-18 16:05:46,652] WARN Ignoring redefinition of existing telemetry label kafka_rest.version (io.confluent.shaded.io.confluent.telemetry.ResourceBuilderFacade)
benchi-kafka     | [2025-03-18 16:05:46,653] INFO ConfluentTelemetryConfig values: 
benchi-kafka     | 	confluent.telemetry.api.key = null
benchi-kafka     | 	confluent.telemetry.api.secret = null
benchi-kafka     | 	confluent.telemetry.cluster.id = null
benchi-kafka     | 	confluent.telemetry.debug.enabled = false
benchi-kafka     | 	confluent.telemetry.enabled = false
benchi-kafka     | 	confluent.telemetry.events.collector.include = .*transaction.remove.expired.transaction.cleanup.interval.ms.*|.*transaction.state.log.load.buffer.size.*|.*transaction.state.log.min.isr.*|.*transaction.state.log.num.partitions.*|.*transaction.state.log.replication.factor.*|.*transaction.state.log.segment.bytes.*|.*transactional.id.expiration.ms.*|.*controlled.shutdown.enable.*|.*controlled.shutdown.max.retries.*|.*controlled.shutdown.retry.backoff.ms.*|.*fetch.max.bytes.*|.*fetch.purgatory.purge.interval.requests.*|.*group.initial.rebalance.delay.ms.*|.*group.max.session.timeout.ms.*|.*group.max.size.*|.*group.min.session.timeout.ms.*|.*log.cleaner.backoff.ms.*|.*log.cleaner.dedupe.buffer.size.*|.*log.cleaner.delete.retention.ms.*|.*log.cleaner.enable.*|.*log.cleaner.io.buffer.load.factor.*|.*log.cleaner.io.buffer.size.*|.*log.cleaner.io.max.bytes.per.second.*|.*log.cleaner.max.compaction.lag.ms.*|.*log.cleaner.min.cleanable.ratio.*|.*log.cleaner.min.compaction.lag.ms.*|.*log.cleaner.threads.*|.*log.cleanup.policy.*|.*log.index.interval.bytes.*|.*log.index.size.max.bytes.*|.*log.message.downconversion.enable.*|.*log.message.format.version.*|.*log.message.timestamp.difference.max.ms.*|.*log.message.timestamp.type.*|.*max.connection.creation.rate.*|.*max.connections.*|.*max.connections.per.ip.*|.*max.incremental.fetch.session.cache.slots.*|.*replica.fetch.backoff.ms.*|.*replica.fetch.max.bytes.*|.*replica.fetch.min.bytes.*|.*replica.fetch.response.max.bytes.*|.*replica.fetch.wait.max.ms.*|.*replica.high.watermark.checkpoint.interval.ms.*|.*replica.lag.time.max.ms.*|.*replica.selector.class.*|.*replica.socket.receive.buffer.bytes.*|.*replica.socket.timeout.ms.*|.*alter.config.policy.class.name.*|.*alter.log.dirs.replication.quota.window.num.*|.*alter.log.dirs.replication.quota.window.size.seconds.*|.*metrics.num.samples.*|.*metrics.recording.level.*|.*metrics.sample.window.ms.*|.*quota.window.num.*|.*quota.window.size.seconds.*|.*replication.quota.window.num.*|.*replication.quota.window.size.seconds.*|.*confluent.balancer.enable.*|.*confluent.balancer.throttle.bytes.per.second.*|.*confluent.balancer.heal.uneven.load.trigger.*|.*confluent.balancer.heal.broker.failure.threshold.ms.*|.*confluent.balancer.disk.max.load.*|.*confluent.balancer.exclude.topic.prefixes.*|.*confluent.balancer.exclude.topic.names.*|.*confluent.tier.local.hotset.bytes.*|.*confluent.tier.local.hotset.ms.*|.*confluent.tier.archiver.num.threads.*|.*confluent.tier.backend.*|.*confluent.tier.enable.*|.*confluent.tier.feature.*|.*confluent.tier.fetcher.num.threads.*|.*confluent.tier.max.partition.fetch.bytes.override.*|.*confluent.tier.metadata.replication.factor.*|.*confluent.operator.managed.*|.*confluent.ansible.managed.*|.*confluent.license.*
benchi-kafka     | 	confluent.telemetry.events.enable = true
benchi-kafka     | 	confluent.telemetry.metrics.collector.include = .*io.confluent.telemetry/.*.*|.*io.confluent.system/.*(process_cpu_load|max_file_descriptor_count|open_file_descriptor_count|system_cpu_load|system_load_average|free_physical_memory_size|total_physical_memory_size|disk_total_bytes|disk_usable_bytes|jvm/mem|jvm/gc).*|.*io.confluent.kafka.rest/.*(connections_active|connections_closed_rate|request_error_rate|request_latency_avg|request_latency_max|request_rate|response_size_avg|response_size_max).*
benchi-kafka     | 	confluent.telemetry.metrics.collector.interval.ms = 60000
benchi-kafka     | 	confluent.telemetry.metrics.collector.slo.enabled = false
benchi-kafka     | 	confluent.telemetry.proxy.password = null
benchi-kafka     | 	confluent.telemetry.proxy.url = null
benchi-kafka     | 	confluent.telemetry.proxy.username = null
benchi-kafka     |  (io.confluent.telemetry.ConfluentTelemetryConfig)
benchi-kafka     | [2025-03-18 16:05:46,653] INFO VolumeMetricsCollectorConfig values: 
benchi-kafka     | 	confluent.telemetry.metrics.collector.volume.update.ms = 15000
benchi-kafka     |  (io.confluent.telemetry.collector.VolumeMetricsCollector$VolumeMetricsCollectorConfig)
benchi-kafka     | [2025-03-18 16:05:46,653] INFO HttpClientConfig values: 
benchi-kafka     | 	api.key = null
benchi-kafka     | 	api.secret = null
benchi-kafka     | 	buffer.batch.duration.max.ms = null
benchi-kafka     | 	buffer.batch.items.max = null
benchi-kafka     | 	buffer.inflight.submissions.max = null
benchi-kafka     | 	buffer.pending.batches.max = null
benchi-kafka     | 	client.attempts.max = null
benchi-kafka     | 	client.base.url = https://collector.telemetry.confluent.cloud
benchi-kafka     | 	client.compression = null
benchi-kafka     | 	client.connect.timeout.ms = null
benchi-kafka     | 	client.metrics.path.override = /v1/metrics
benchi-kafka     | 	client.request.timeout.ms = null
benchi-kafka     | 	client.retry.delay.seconds = null
benchi-kafka     | 	proxy.password = null
benchi-kafka     | 	proxy.url = null
benchi-kafka     | 	proxy.username = null
benchi-kafka     | 	type = httpTelemetryClient
benchi-kafka     |  (io.confluent.telemetry.exporter.http.HttpClientConfig)
benchi-kafka     | [2025-03-18 16:05:46,653] INFO HttpExporterConfig values: 
benchi-kafka     | 	api.key = null
benchi-kafka     | 	api.secret = null
benchi-kafka     | 	buffer.batch.duration.max.ms = null
benchi-kafka     | 	buffer.batch.items.max = null
benchi-kafka     | 	buffer.inflight.submissions.max = null
benchi-kafka     | 	buffer.pending.batches.max = null
benchi-kafka     | 	client = _confluentClient
benchi-kafka     | 	client.attempts.max = null
benchi-kafka     | 	client.base.url = null
benchi-kafka     | 	client.compression = null
benchi-kafka     | 	client.connect.timeout.ms = null
benchi-kafka     | 	client.metrics.path.override = /v1/metrics
benchi-kafka     | 	client.request.timeout.ms = null
benchi-kafka     | 	client.retry.delay.seconds = null
benchi-kafka     | 	enabled = false
benchi-kafka     | 	events.enabled = true
benchi-kafka     | 	metrics.enabled = true
benchi-kafka     | 	metrics.include = null
benchi-kafka     | 	proxy.password = null
benchi-kafka     | 	proxy.url = null
benchi-kafka     | 	proxy.username = null
benchi-kafka     | 	remote.configurable = false
benchi-kafka     | 	type = http
benchi-kafka     |  (io.confluent.telemetry.exporter.http.HttpExporterConfig)
benchi-kafka     | [2025-03-18 16:05:46,653] INFO Configuring named client _confluentClient for exporter _confluent (io.confluent.telemetry.exporter.http.HttpExporterConfig)
benchi-kafka     | [2025-03-18 16:05:46,653] WARN no telemetry exporters are enabled (io.confluent.telemetry.ConfluentTelemetryConfig)
benchi-kafka     | [2025-03-18 16:05:46,653] INFO PollingRemoteConfigurationConfig values: 
benchi-kafka     | 	enabled = true
benchi-kafka     | 	refresh.interval.ms = 60000
benchi-kafka     |  (io.confluent.shaded.io.confluent.telemetry.config.remote.polling.PollingRemoteConfigurationConfig)
benchi-kafka     | [2025-03-18 16:05:46,653] INFO Initializing the event logger (io.confluent.telemetry.reporter.TelemetryReporter)
benchi-kafka     | [2025-03-18 16:05:46,653] INFO EventLoggerConfig values: 
benchi-kafka     | 	event.logger.cloudevent.codec = structured
benchi-kafka     | 	event.logger.exporter.class = class io.confluent.shaded.io.confluent.telemetry.events.exporter.http.EventHttpExporter
benchi-kafka     |  (io.confluent.shaded.io.confluent.telemetry.events.EventLoggerConfig)
benchi-kafka     | [2025-03-18 16:05:46,653] INFO HttpExporterConfig values: 
benchi-kafka     | 	api.key = null
benchi-kafka     | 	api.secret = null
benchi-kafka     | 	buffer.batch.duration.max.ms = null
benchi-kafka     | 	buffer.batch.items.max = null
benchi-kafka     | 	buffer.inflight.submissions.max = null
benchi-kafka     | 	buffer.pending.batches.max = null
benchi-kafka     | 	client.attempts.max = null
benchi-kafka     | 	client.base.url = https://collector.telemetry.confluent.cloud
benchi-kafka     | 	client.compression = null
benchi-kafka     | 	client.connect.timeout.ms = null
benchi-kafka     | 	client.request.timeout.ms = null
benchi-kafka     | 	client.retry.delay.seconds = null
benchi-kafka     | 	enabled = true
benchi-kafka     | 	events.enabled = true
benchi-kafka     | 	metrics.enabled = true
benchi-kafka     | 	proxy.password = null
benchi-kafka     | 	proxy.url = null
benchi-kafka     | 	proxy.username = null
benchi-kafka     | 	type = http
benchi-kafka     |  (io.confluent.shaded.io.confluent.telemetry.events.exporter.http.HttpExporterConfig)
benchi-kafka     | [2025-03-18 16:05:46,656] INFO Starting Confluent telemetry reporter with an interval of 60000 ms) (io.confluent.telemetry.reporter.TelemetryReporter)
benchi-kafka     | [2025-03-18 16:05:46,657] INFO Application provider 'KafkaRestApplicationProvider' provided 1 instance(s). (io.confluent.http.server.KafkaHttpApplicationLoader)
benchi-kafka     | [2025-03-18 16:05:46,660] INFO Initial capacity 128, increased by 64, maximum capacity 2147483647. (io.confluent.rest.ApplicationServer)
benchi-kafka     | [2025-03-18 16:05:46,698] INFO RestConfig values: 
benchi-kafka     | 	access.control.allow.headers = 
benchi-kafka     | 	access.control.allow.methods = 
benchi-kafka     | 	access.control.allow.origin = 
benchi-kafka     | 	access.control.skip.options = true
benchi-kafka     | 	authentication.method = NONE
benchi-kafka     | 	authentication.realm = 
benchi-kafka     | 	authentication.roles = [*]
benchi-kafka     | 	authentication.skip.paths = []
benchi-kafka     | 	compression.enable = true
benchi-kafka     | 	connector.connection.limit = 0
benchi-kafka     | 	csrf.prevention.enable = false
benchi-kafka     | 	csrf.prevention.token.endpoint = /csrf
benchi-kafka     | 	csrf.prevention.token.expiration.minutes = 30
benchi-kafka     | 	csrf.prevention.token.max.entries = 10000
benchi-kafka     | 	debug = false
benchi-kafka     | 	dos.filter.delay.ms = 100
benchi-kafka     | 	dos.filter.enabled = false
benchi-kafka     | 	dos.filter.insert.headers = true
benchi-kafka     | 	dos.filter.ip.whitelist = []
benchi-kafka     | 	dos.filter.managed.attr = false
benchi-kafka     | 	dos.filter.max.idle.tracker.ms = 30000
benchi-kafka     | 	dos.filter.max.requests.ms = 30000
benchi-kafka     | 	dos.filter.max.requests.per.connection.per.sec = 25
benchi-kafka     | 	dos.filter.max.requests.per.sec = 25
benchi-kafka     | 	dos.filter.max.wait.ms = 50
benchi-kafka     | 	dos.filter.throttle.ms = 30000
benchi-kafka     | 	dos.filter.throttled.requests = 5
benchi-kafka     | 	http2.enabled = true
benchi-kafka     | 	idle.timeout.ms = 30000
benchi-kafka     | 	listener.protocol.map = []
benchi-kafka     | 	listeners = []
benchi-kafka     | 	max.request.header.size = 8192
benchi-kafka     | 	max.response.header.size = 8192
benchi-kafka     | 	metric.reporters = []
benchi-kafka     | 	metrics.global.stats.request.tags.enable = false
benchi-kafka     | 	metrics.jmx.prefix = rest-utils
benchi-kafka     | 	metrics.latency.sla.ms = 50
benchi-kafka     | 	metrics.latency.slo.ms = 5
benchi-kafka     | 	metrics.latency.slo.sla.enable = false
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	metrics.tag.map = []
benchi-kafka     | 	network.traffic.rate.limit.backend = guava
benchi-kafka     | 	network.traffic.rate.limit.bytes.per.sec = 20971520
benchi-kafka     | 	network.traffic.rate.limit.enable = false
benchi-kafka     | 	nosniff.prevention.enable = false
benchi-kafka     | 	port = 8080
benchi-kafka     | 	proxy.protocol.enabled = false
benchi-kafka     | 	reject.options.request = false
benchi-kafka     | 	request.logger.name = io.confluent.rest-utils.requests
benchi-kafka     | 	request.queue.capacity = 2147483647
benchi-kafka     | 	request.queue.capacity.growby = 64
benchi-kafka     | 	request.queue.capacity.init = 128
benchi-kafka     | 	resource.extension.classes = []
benchi-kafka     | 	response.http.headers.config = 
benchi-kafka     | 	response.mediatype.default = application/json
benchi-kafka     | 	response.mediatype.preferred = [application/json]
benchi-kafka     | 	rest.servlet.initializor.classes = []
benchi-kafka     | 	server.connection.limit = 0
benchi-kafka     | 	shutdown.graceful.ms = 1000
benchi-kafka     | 	sni.check.enabled = false
benchi-kafka     | 	ssl.cipher.suites = []
benchi-kafka     | 	ssl.client.auth = false
benchi-kafka     | 	ssl.client.authentication = NONE
benchi-kafka     | 	ssl.enabled.protocols = []
benchi-kafka     | 	ssl.endpoint.identification.algorithm = null
benchi-kafka     | 	ssl.key.password = [hidden]
benchi-kafka     | 	ssl.keymanager.algorithm = 
benchi-kafka     | 	ssl.keystore.location = 
benchi-kafka     | 	ssl.keystore.password = [hidden]
benchi-kafka     | 	ssl.keystore.reload = false
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.keystore.watch.location = 
benchi-kafka     | 	ssl.protocol = TLS
benchi-kafka     | 	ssl.provider = 
benchi-kafka     | 	ssl.trustmanager.algorithm = 
benchi-kafka     | 	ssl.truststore.location = 
benchi-kafka     | 	ssl.truststore.password = [hidden]
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     | 	suppress.stack.trace.response = true
benchi-kafka     | 	thread.pool.max = 200
benchi-kafka     | 	thread.pool.min = 8
benchi-kafka     | 	websocket.path.prefix = /ws
benchi-kafka     | 	websocket.servlet.initializor.classes = []
benchi-kafka     |  (io.confluent.rest.RestConfig)
benchi-kafka     | [2025-03-18 16:05:46,698] INFO Adding listener with HTTP/2: NamedURI{uri=http://0.0.0.0:8090, name='null'} (io.confluent.rest.ApplicationServer)
benchi-kafka     | [2025-03-18 16:05:46,703] INFO [ControllerServer id=1] CreateTopics result(s): CreatableTopic(name='_confluent-link-metadata', numPartitions=50, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact'), CreateableTopicConfig(name='min.insync.replicas', value='2')], linkName=null, mirrorTopic=null, sourceTopicId=AAAAAAAAAAAAAAAAAAAAAA, mirrorStartOffsetSpec=-9223372036854775808, mirrorStartOffsets=[], stoppedSequenceNumber=0): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:05:46,703] INFO [ControllerServer id=1] Replayed TopicRecord for topic _confluent-link-metadata with topic ID FgiypwCwTt68SpYw3T9ahw. (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:05:46,704] INFO [ControllerServer id=1] Replayed ConfigRecord for ConfigResource(type=TOPIC, name='_confluent-link-metadata') which set configuration cleanup.policy to compact (org.apache.kafka.controller.ConfigurationControlManager)
benchi-kafka     | [2025-03-18 16:05:46,704] INFO [ControllerServer id=1] Replayed ConfigRecord for ConfigResource(type=TOPIC, name='_confluent-link-metadata') which set configuration min.insync.replicas to 2 (org.apache.kafka.controller.ConfigurationControlManager)
benchi-kafka     | [2025-03-18 16:05:46,704] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-0 with topic ID FgiypwCwTt68SpYw3T9ahw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:05:46,705] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-1 with topic ID FgiypwCwTt68SpYw3T9ahw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:05:46,705] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-2 with topic ID FgiypwCwTt68SpYw3T9ahw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:05:46,705] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-3 with topic ID FgiypwCwTt68SpYw3T9ahw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:05:46,705] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-4 with topic ID FgiypwCwTt68SpYw3T9ahw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:05:46,705] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-5 with topic ID FgiypwCwTt68SpYw3T9ahw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:05:46,705] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-6 with topic ID FgiypwCwTt68SpYw3T9ahw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:05:46,705] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-7 with topic ID FgiypwCwTt68SpYw3T9ahw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:05:46,705] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-8 with topic ID FgiypwCwTt68SpYw3T9ahw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:05:46,705] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-9 with topic ID FgiypwCwTt68SpYw3T9ahw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:05:46,705] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-10 with topic ID FgiypwCwTt68SpYw3T9ahw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:05:46,706] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-11 with topic ID FgiypwCwTt68SpYw3T9ahw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:05:46,706] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-12 with topic ID FgiypwCwTt68SpYw3T9ahw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:05:46,706] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-13 with topic ID FgiypwCwTt68SpYw3T9ahw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:05:46,706] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-14 with topic ID FgiypwCwTt68SpYw3T9ahw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:05:46,706] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-15 with topic ID FgiypwCwTt68SpYw3T9ahw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:05:46,706] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-16 with topic ID FgiypwCwTt68SpYw3T9ahw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:05:46,706] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-17 with topic ID FgiypwCwTt68SpYw3T9ahw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:05:46,706] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-18 with topic ID FgiypwCwTt68SpYw3T9ahw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:05:46,706] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-19 with topic ID FgiypwCwTt68SpYw3T9ahw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:05:46,706] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-20 with topic ID FgiypwCwTt68SpYw3T9ahw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:05:46,706] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-21 with topic ID FgiypwCwTt68SpYw3T9ahw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:05:46,706] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-22 with topic ID FgiypwCwTt68SpYw3T9ahw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:05:46,706] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-23 with topic ID FgiypwCwTt68SpYw3T9ahw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:05:46,707] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-24 with topic ID FgiypwCwTt68SpYw3T9ahw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:05:46,707] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-25 with topic ID FgiypwCwTt68SpYw3T9ahw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:05:46,707] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-26 with topic ID FgiypwCwTt68SpYw3T9ahw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:05:46,707] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-27 with topic ID FgiypwCwTt68SpYw3T9ahw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:05:46,707] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-28 with topic ID FgiypwCwTt68SpYw3T9ahw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:05:46,707] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-29 with topic ID FgiypwCwTt68SpYw3T9ahw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:05:46,707] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-30 with topic ID FgiypwCwTt68SpYw3T9ahw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:05:46,707] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-31 with topic ID FgiypwCwTt68SpYw3T9ahw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:05:46,707] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-32 with topic ID FgiypwCwTt68SpYw3T9ahw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:05:46,707] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-33 with topic ID FgiypwCwTt68SpYw3T9ahw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:05:46,707] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-34 with topic ID FgiypwCwTt68SpYw3T9ahw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:05:46,708] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-35 with topic ID FgiypwCwTt68SpYw3T9ahw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:05:46,708] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-36 with topic ID FgiypwCwTt68SpYw3T9ahw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:05:46,708] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-37 with topic ID FgiypwCwTt68SpYw3T9ahw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:05:46,708] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-38 with topic ID FgiypwCwTt68SpYw3T9ahw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:05:46,708] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-39 with topic ID FgiypwCwTt68SpYw3T9ahw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:05:46,708] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-40 with topic ID FgiypwCwTt68SpYw3T9ahw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:05:46,708] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-41 with topic ID FgiypwCwTt68SpYw3T9ahw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:05:46,708] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-42 with topic ID FgiypwCwTt68SpYw3T9ahw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:05:46,708] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-43 with topic ID FgiypwCwTt68SpYw3T9ahw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:05:46,708] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-44 with topic ID FgiypwCwTt68SpYw3T9ahw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:05:46,708] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-45 with topic ID FgiypwCwTt68SpYw3T9ahw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:05:46,708] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-46 with topic ID FgiypwCwTt68SpYw3T9ahw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:05:46,709] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-47 with topic ID FgiypwCwTt68SpYw3T9ahw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:05:46,709] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-48 with topic ID FgiypwCwTt68SpYw3T9ahw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:05:46,709] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-49 with topic ID FgiypwCwTt68SpYw3T9ahw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:05:46,713] INFO Loaded KafkaHttpServer implementation class io.confluent.http.server.KafkaHttpServerImpl (io.confluent.kafka.http.server.KafkaHttpServerLoader)
benchi-kafka     | [2025-03-18 16:05:46,713] INFO KafkaHttpServer transitioned from NEW to STARTING.. (io.confluent.http.server.KafkaHttpServerImpl)
benchi-kafka     | [2025-03-18 16:05:46,716] INFO Registered MetricsListener to connector of listener: null (io.confluent.rest.ApplicationServer)
benchi-kafka     | [2025-03-18 16:05:46,740] INFO SBC Event SbcMetadataUpdateEvent-11 generated 1 more events to enqueue in the following order - [SbcConfigUpdateEvent-12]. Enqueuing... (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:05:46,740] INFO Handling event SbcKraftStartupEvent-5 (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:05:46,740] INFO Balancer Status state for brokers [1] transitioned from BALANCER_EVENT_RECEIVED to STARTING due to event INITIALIZING_CRUISE_CONTROL. (io.confluent.databalancer.operation.StateMachine)
benchi-kafka     | [2025-03-18 16:05:46,740] INFO DataBalancer: Activating SBC with io.confluent.databalancer.BrokersMetadataSnapshot@4ed18343 (io.confluent.databalancer.KafkaDataBalanceManager)
benchi-kafka     | [2025-03-18 16:05:46,741] INFO [Broker id=1] Transitioning 50 partition(s) to local leaders. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,741] INFO DataBalancer: Scheduling DataBalanceEngine Startup (io.confluent.databalancer.ConfluentDataBalanceEngine)
benchi-kafka     | [2025-03-18 16:05:46,742] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-link-metadata-43, _confluent-link-metadata-10, _confluent-link-metadata-39, _confluent-link-metadata-6, _confluent-link-metadata-18, _confluent-link-metadata-47, _confluent-link-metadata-14, _confluent-link-metadata-27, _confluent-link-metadata-23, _confluent-link-metadata-35, _confluent-link-metadata-2, _confluent-link-metadata-31, _confluent-link-metadata-42, _confluent-link-metadata-13, _confluent-link-metadata-38, _confluent-link-metadata-9, _confluent-link-metadata-21, _confluent-link-metadata-46, _confluent-link-metadata-17, _confluent-link-metadata-26, _confluent-link-metadata-22, _confluent-link-metadata-34, _confluent-link-metadata-5, _confluent-link-metadata-30, _confluent-link-metadata-1, _confluent-link-metadata-45, _confluent-link-metadata-12, _confluent-link-metadata-41, _confluent-link-metadata-8, _confluent-link-metadata-20, _confluent-link-metadata-49, _confluent-link-metadata-16, _confluent-link-metadata-29, _confluent-link-metadata-25, _confluent-link-metadata-37, _confluent-link-metadata-4, _confluent-link-metadata-33, _confluent-link-metadata-0, _confluent-link-metadata-11, _confluent-link-metadata-44, _confluent-link-metadata-7, _confluent-link-metadata-40, _confluent-link-metadata-19, _confluent-link-metadata-15, _confluent-link-metadata-48, _confluent-link-metadata-28, _confluent-link-metadata-24, _confluent-link-metadata-3, _confluent-link-metadata-36, _confluent-link-metadata-32) (kafka.server.ReplicaFetcherManager)
benchi-kafka     | [2025-03-18 16:05:46,743] INFO [Broker id=1] Creating new partition _confluent-link-metadata-43 with topic id FgiypwCwTt68SpYw3T9ahw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,744] INFO Handling event SbcKraftBrokerAdditionEvent-10 (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:05:46,745] INFO Processing SbcKraftBrokerAdditionEvent-10 event with data: empty_brokers: [], new_brokers: [1] (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:05:46,745] WARN Notified of broker additions (empty broker ids [], new brokers [1]) but DataBalancer is disabled -- ignoring for now (io.confluent.databalancer.KafkaDataBalanceManager)
benchi-kafka     | [2025-03-18 16:05:46,745] INFO Handling event SbcConfigUpdateEvent-12 (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:05:46,745] INFO Balancer notified of a config change: ConfigurationsDelta(changes={ConfigResource(type=TOPIC, name='_confluent-link-metadata')=ConfigurationDelta(changedKeys=[cleanup.policy, min.insync.replicas])}) (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:05:46,745] INFO There were 0 change(s) and 0 deletion(s) to balancer configs. Changed Configs: {}, Deleted Configs: [] (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:05:46,745] INFO DataBalancer: Bootstrap server endpoint is Endpoint(listenerName='PLAINTEXT', securityProtocol=PLAINTEXT, host='broker', port=29092) (io.confluent.databalancer.startup.CruiseControlStartable)
benchi-kafka     | [2025-03-18 16:05:46,745] INFO DataBalancer: BOOTSTRAP_SERVERS determined to be broker:29092 (io.confluent.databalancer.startup.CruiseControlStartable)
benchi-kafka     | [2025-03-18 16:05:46,746] INFO KafkaCruiseControlConfig values: 
benchi-kafka     | 	alter.configs.response.timeout.ms = 30000
benchi-kafka     | 	anomaly.detection.allow.capacity.estimation = true
benchi-kafka     | 	anomaly.detection.goals = [io.confluent.cruisecontrol.analyzer.goals.ReplicaPlacementGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.RackAwareGoal, io.confluent.cruisecontrol.analyzer.goals.CellAwareGoal, io.confluent.cruisecontrol.analyzer.goals.TenantAwareGoal, io.confluent.cruisecontrol.analyzer.goals.MaxReplicaMovementParallelismGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.ReplicaCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.DiskCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.NetworkInboundCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.NetworkOutboundCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.ReplicationInboundCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.ProducerInboundCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.ReplicaDistributionGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.DiskUsageDistributionGoal]
benchi-kafka     | 	anomaly.detection.interval.ms = 60000
benchi-kafka     | 	bootstrap.servers = [broker:29092]
benchi-kafka     | 	broker.addition.elapsed.time.ms.completion.threshold = 57600000
benchi-kafka     | 	broker.addition.mean.cpu.percent.completion.threshold = 0.5
benchi-kafka     | 	broker.capacity.config.resolver.class = class com.linkedin.kafka.cruisecontrol.config.BrokerCapacityResolver
benchi-kafka     | 	broker.failure.alert.threshold.ms = 0
benchi-kafka     | 	broker.failure.exclude.recently.removed.brokers = true
benchi-kafka     | 	broker.failure.self.healing.threshold.ms = 3600000
benchi-kafka     | 	broker.metric.sample.aggregator.completeness.cache.size = 5
benchi-kafka     | 	broker.metric.sample.store.topic = _confluent_balancer_broker_samples
benchi-kafka     | 	broker.removal.shutdown.timeout.ms = 600000
benchi-kafka     | 	broker.replica.exclusion.timeout.ms = 120000
benchi-kafka     | 	bytes.cpu.contribution.weight = 0.2
benchi-kafka     | 	calculated.throttle.ratio = 0.8
benchi-kafka     | 	capacity.threshold.upper.limit = 0.95
benchi-kafka     | 	cdbe.shutdown.wait.ms = 15000
benchi-kafka     | 	cell.load.upper.bound = 0.7
benchi-kafka     | 	cell.overload.detection.interval.ms = 3600000
benchi-kafka     | 	cell.overload.duration.ms = 86400000
benchi-kafka     | 	client.id = kafka-cruise-control
benchi-kafka     | 	confluent.balancer.additional.invalidation.duration.ms = 60000
benchi-kafka     | 	confluent.balancer.plan.computation.retry.timeout.ms = 3600000
benchi-kafka     | 	confluent.cells.enable = false
benchi-kafka     | 	connections.max.idle.ms = 540000
benchi-kafka     | 	consume.out.bound.should.balance.FFF.traffic = false
benchi-kafka     | 	consumer.out.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	consumer.outbound.capacity.threshold = 0.9
benchi-kafka     | 	cpu.balance.threshold = 1.1
benchi-kafka     | 	cpu.capacity.threshold = 1.0
benchi-kafka     | 	cpu.low.utilization.threshold = 0.2
benchi-kafka     | 	cpu.low.utilization.threshold.for.broker.addition = 0.2
benchi-kafka     | 	cpu.utilization.detector.duration.ms = 600000
benchi-kafka     | 	cpu.utilization.detector.enabled = false
benchi-kafka     | 	cpu.utilization.detector.overutilization.threshold = 80.0
benchi-kafka     | 	cpu.utilization.detector.underutilization.threshold = 50.0
benchi-kafka     | 	default.api.timeout.ms = 60000
benchi-kafka     | 	default.replica.movement.strategies = [com.linkedin.kafka.cruisecontrol.executor.strategy.BaseReplicaMovementStrategy]
benchi-kafka     | 	describe.broker.exclusion.timeout.ms = 60000
benchi-kafka     | 	describe.cluster.response.timeout.ms = 30000
benchi-kafka     | 	describe.configs.batch.size = 1000
benchi-kafka     | 	describe.configs.response.timeout.ms = 30000
benchi-kafka     | 	describe.topics.response.timeout.ms = 30000
benchi-kafka     | 	disk.balance.threshold = 1.1
benchi-kafka     | 	disk.low.utilization.threshold = 0.2
benchi-kafka     | 	disk.max.load = 0.85
benchi-kafka     | 	disk.min.free.space.gb = 0
benchi-kafka     | 	disk.min.free.space.lower.limit.gb = 0
benchi-kafka     | 	disk.read.ratio = 0.2
benchi-kafka     | 	disk.utilization.detector.duration.ms = 600000
benchi-kafka     | 	disk.utilization.detector.enabled = false
benchi-kafka     | 	disk.utilization.detector.overutilization.threshold = 80.0
benchi-kafka     | 	disk.utilization.detector.reserved.capacity = 150000.0
benchi-kafka     | 	disk.utilization.detector.underutilization.threshold = 35.0
benchi-kafka     | 	dynamic.throttling.enabled = true
benchi-kafka     | 	execution.progress.check.interval.ms = 7000
benchi-kafka     | 	executor.leader.action.timeout.ms = 180000
benchi-kafka     | 	executor.notifier.class = class com.linkedin.kafka.cruisecontrol.executor.ExecutorNoopNotifier
benchi-kafka     | 	executor.reservation.refresh.time.ms = 60000
benchi-kafka     | 	follower.network.inbound.weight.for.cpu.util = 0.15
benchi-kafka     | 	goal.balancedness.priority.weight = 1.1
benchi-kafka     | 	goal.balancedness.strictness.weight = 1.5
benchi-kafka     | 	goal.violation.delay.on.new.brokers.ms = 1800000
benchi-kafka     | 	goal.violation.distribution.threshold.multiplier = 1.1
benchi-kafka     | 	goal.violation.exclude.recently.removed.brokers = true
benchi-kafka     | 	goals = [com.linkedin.kafka.cruisecontrol.analyzer.goals.MovementExclusionGoal, io.confluent.cruisecontrol.analyzer.goals.ReplicaPlacementGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.RackAwareGoal, io.confluent.cruisecontrol.analyzer.goals.CellAwareGoal, io.confluent.cruisecontrol.analyzer.goals.TenantAwareGoal, io.confluent.cruisecontrol.analyzer.goals.MaxReplicaMovementParallelismGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.ReplicaCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.DiskCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.NetworkInboundCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.NetworkOutboundCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.ReplicationInboundCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.ProducerInboundCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.MirrorInboundCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.ConsumerOutboundCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.SystemTopicEvenDistributionGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.ReplicaDistributionGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.DiskUsageDistributionGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.LeaderReplicaDistributionGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.NetworkInboundUsageDistributionGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.NetworkOutboundUsageDistributionGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.TopicReplicaDistributionGoal]
benchi-kafka     | 	hot.partition.capacity.utilization.threshold = 0.2
benchi-kafka     | 	incremental.balancing.cpu.top.proposal.tracking.enabled = true
benchi-kafka     | 	incremental.balancing.cpu.top.proposal.tracking.num.proposals = 15
benchi-kafka     | 	incremental.balancing.enabled = false
benchi-kafka     | 	incremental.balancing.goals = [com.linkedin.kafka.cruisecontrol.analyzer.goals.MovementExclusionGoal, io.confluent.cruisecontrol.analyzer.goals.ReplicaPlacementGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.RackAwareGoal, io.confluent.cruisecontrol.analyzer.goals.CellAwareGoal, io.confluent.cruisecontrol.analyzer.goals.TenantAwareGoal, io.confluent.cruisecontrol.analyzer.goals.MaxReplicaMovementParallelismGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.ReplicaCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.DiskCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.NetworkInboundCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.NetworkOutboundCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.ReplicationInboundCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.ProducerInboundCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.MirrorInboundCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.ConsumerOutboundCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.IncrementalCPUResourceDistributionGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.IncrementalTopicReplicaDistributionGoal]
benchi-kafka     | 	incremental.balancing.lower.bound = 0.02
benchi-kafka     | 	incremental.balancing.min.valid.windows = 5
benchi-kafka     | 	incremental.balancing.step.ratio = 0.2
benchi-kafka     | 	inter.cell.balancing.enabled = false
benchi-kafka     | 	invalid.replica.assignment.retry.timeout.ms = 300000
benchi-kafka     | 	leader.network.inbound.weight.for.cpu.util = 0.7
benchi-kafka     | 	leader.network.outbound.weight.for.cpu.util = 0.15
benchi-kafka     | 	leader.replica.count.balance.threshold = 1.1
benchi-kafka     | 	logdir.response.timeout.ms = 30000
benchi-kafka     | 	max.allowed.extrapolations.per.broker = 5
benchi-kafka     | 	max.allowed.extrapolations.per.partition = 5
benchi-kafka     | 	max.capacity.balancing.delta.percentage = 0.0
benchi-kafka     | 	max.replicas = 2147483647
benchi-kafka     | 	max.volume.throughput.mb = 0
benchi-kafka     | 	metadata.client.timeout.ms = 180000
benchi-kafka     | 	metadata.ttl = 10000
benchi-kafka     | 	metric.sampler.class = class io.confluent.cruisecontrol.metricsreporter.ConfluentTelemetryReporterSampler
benchi-kafka     | 	min.samples.per.partition.metrics.window = 1
benchi-kafka     | 	min.valid.partition.ratio = 0.95
benchi-kafka     | 	network.in.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	network.inbound.balance.threshold = 1.1
benchi-kafka     | 	network.inbound.capacity.threshold = 0.8
benchi-kafka     | 	network.inbound.low.utilization.threshold = 0.2
benchi-kafka     | 	network.out.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	network.outbound.balance.threshold = 1.1
benchi-kafka     | 	network.outbound.capacity.threshold = 0.8
benchi-kafka     | 	network.outbound.low.utilization.threshold = 0.2
benchi-kafka     | 	num.cached.recent.anomaly.states = 10
benchi-kafka     | 	num.concurrent.leader.movements = 1000
benchi-kafka     | 	num.concurrent.partition.movements.per.broker = 5
benchi-kafka     | 	num.metric.fetchers = 1
benchi-kafka     | 	num.partition.metrics.windows = 12
benchi-kafka     | 	partition.metric.sample.aggregator.completeness.cache.size = 5
benchi-kafka     | 	partition.metric.sample.store.topic = _confluent_balancer_partition_samples
benchi-kafka     | 	partition.metrics.window.ms = 180000
benchi-kafka     | 	populate.default.disk.capacity.from.local = true
benchi-kafka     | 	producer.in.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	producer.inbound.capacity.threshold = 0.9
benchi-kafka     | 	read.throughput.multiplier = 1.0
benchi-kafka     | 	receive.buffer.bytes = 32768
benchi-kafka     | 	reconnect.backoff.ms = 50
benchi-kafka     | 	removal.history.retention.time.ms = 86400000
benchi-kafka     | 	replica.count.balance.threshold = 1.1
benchi-kafka     | 	replica.movement.strategies = [com.linkedin.kafka.cruisecontrol.executor.strategy.PostponeUrpReplicaMovementStrategy, com.linkedin.kafka.cruisecontrol.executor.strategy.PrioritizeLargeReplicaMovementStrategy, com.linkedin.kafka.cruisecontrol.executor.strategy.PrioritizeSmallReplicaMovementStrategy, com.linkedin.kafka.cruisecontrol.executor.strategy.BaseReplicaMovementStrategy]
benchi-kafka     | 	replication.in.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	replication.inbound.capacity.threshold = 0.9
benchi-kafka     | 	request.cpu.contribution.weight = 0.8
benchi-kafka     | 	request.timeout.ms = 30000
benchi-kafka     | 	resource.utilization.detector.interval.ms = 60000
benchi-kafka     | 	sampling.allow.cpu.capacity.estimation = true
benchi-kafka     | 	sasl.client.callback.handler.class = null
benchi-kafka     | 	sasl.jaas.config = null
benchi-kafka     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
benchi-kafka     | 	sasl.kerberos.min.time.before.relogin = 60000
benchi-kafka     | 	sasl.kerberos.service.name = null
benchi-kafka     | 	sasl.kerberos.ticket.renew.jitter = 0.05
benchi-kafka     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
benchi-kafka     | 	sasl.login.callback.handler.class = null
benchi-kafka     | 	sasl.login.class = null
benchi-kafka     | 	sasl.login.connect.timeout.ms = null
benchi-kafka     | 	sasl.login.read.timeout.ms = null
benchi-kafka     | 	sasl.login.refresh.buffer.seconds = 300
benchi-kafka     | 	sasl.login.refresh.min.period.seconds = 60
benchi-kafka     | 	sasl.login.refresh.window.factor = 0.8
benchi-kafka     | 	sasl.login.refresh.window.jitter = 0.05
benchi-kafka     | 	sasl.login.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.login.retry.backoff.ms = 100
benchi-kafka     | 	sasl.mechanism = GSSAPI
benchi-kafka     | 	sasl.oauthbearer.clientassertion.audience = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.expiration = 5
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.jti.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.nbf.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.issuer = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.location = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key.passphrase = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.subject = null
benchi-kafka     | 	sasl.oauthbearer.clock.skew.seconds = 30
benchi-kafka     | 	sasl.oauthbearer.expected.audience = null
benchi-kafka     | 	sasl.oauthbearer.expected.issuer = null
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.url = null
benchi-kafka     | 	sasl.oauthbearer.scope.claim.name = scope
benchi-kafka     | 	sasl.oauthbearer.sub.claim.name = sub
benchi-kafka     | 	sasl.oauthbearer.token.endpoint.url = null
benchi-kafka     | 	sbc.metrics.parser.enabled = false
benchi-kafka     | 	security.protocol = PLAINTEXT
benchi-kafka     | 	self.healing.broker.failure.enabled = true
benchi-kafka     | 	self.healing.goal.violation.enabled = false
benchi-kafka     | 	self.healing.maximum.rounds = 1
benchi-kafka     | 	send.buffer.bytes = 131072
benchi-kafka     | 	ssl.cipher.suites = null
benchi-kafka     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	ssl.engine.factory.class = null
benchi-kafka     | 	ssl.key.password = null
benchi-kafka     | 	ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	ssl.keystore.certificate.chain = null
benchi-kafka     | 	ssl.keystore.key = null
benchi-kafka     | 	ssl.keystore.location = null
benchi-kafka     | 	ssl.keystore.password = null
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.protocol = TLSv1.3
benchi-kafka     | 	ssl.provider = null
benchi-kafka     | 	ssl.secure.random.implementation = null
benchi-kafka     | 	ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	ssl.truststore.certificates = null
benchi-kafka     | 	ssl.truststore.location = null
benchi-kafka     | 	ssl.truststore.password = null
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     | 	startup.retry.delay.minutes = 5
benchi-kafka     | 	startup.retry.max.hours = 2
benchi-kafka     | 	static.throttle.rate.override.enabled = false
benchi-kafka     | 	tenant.maximum.movements = 0
benchi-kafka     | 	tenant.suspension.ms = 86400000
benchi-kafka     | 	throttle.bytes.per.second = 10485760
benchi-kafka     | 	topic.balancing.badly.imbalanced.topic.imbalance.score.threshold = 0.3
benchi-kafka     | 	topic.balancing.balance.threshold.multiplier = 1.0
benchi-kafka     | 	topic.balancing.broker.addition.completion.percentage = 0.8
benchi-kafka     | 	topic.balancing.broker.addition.detector.with.trdg.enabled = false
benchi-kafka     | 	topic.balancing.imbalanced.score.threshold = 0.07
benchi-kafka     | 	topic.balancing.max.reassignments.per.iteration = -1
benchi-kafka     | 	topic.balancing.slightly.imbalanced.topic.imbalance.score.threshold = 0.05
benchi-kafka     | 	topic.balancing.slightly.imbalanced.topics.percentage.trigger = 0.2
benchi-kafka     | 	topic.balancing.trigger.threshold.multiplier = 3.0
benchi-kafka     | 	topic.partition.maximum.movements = 5
benchi-kafka     | 	topic.partition.movement.expiration.ms = 10800000
benchi-kafka     | 	topic.partition.suspension.ms = 18000000
benchi-kafka     | 	topics.excluded.from.partition.movement = 
benchi-kafka     | 	v2.addition.enabled = false
benchi-kafka     | 	write.throughput.multiplier = 1.0
benchi-kafka     | 	zookeeper.connect = 
benchi-kafka     | 	zookeeper.security.enabled = false
benchi-kafka     |  (com.linkedin.kafka.cruisecontrol.config.KafkaCruiseControlConfig)
benchi-kafka     | [2025-03-18 16:05:46,746] INFO DataBalancer: Instantiating DataBalanceEngine (io.confluent.databalancer.ConfluentDataBalanceEngine)
benchi-kafka     | [2025-03-18 16:05:46,748] INFO DataBalancer: Checking startup components (io.confluent.databalancer.startup.CruiseControlStartable)
benchi-kafka     | [2025-03-18 16:05:46,748] INFO DataBalancer: Checking startup component StartupComponent ConfluentTelemetryReporterSampler (io.confluent.databalancer.startup.StartupComponents)
benchi-kafka     | [2025-03-18 16:05:46,749] INFO [Broker id=1] Creating new partition _confluent-link-metadata-10 with topic id FgiypwCwTt68SpYw3T9ahw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,750] INFO [Broker id=1] Creating new partition _confluent-link-metadata-39 with topic id FgiypwCwTt68SpYw3T9ahw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,750] INFO [Broker id=1] Creating new partition _confluent-link-metadata-6 with topic id FgiypwCwTt68SpYw3T9ahw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,750] INFO [Broker id=1] Creating new partition _confluent-link-metadata-18 with topic id FgiypwCwTt68SpYw3T9ahw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,751] INFO [Broker id=1] Creating new partition _confluent-link-metadata-47 with topic id FgiypwCwTt68SpYw3T9ahw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,751] INFO [Broker id=1] Creating new partition _confluent-link-metadata-14 with topic id FgiypwCwTt68SpYw3T9ahw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,751] INFO Binding MetadataApiApplication to all listeners. (io.confluent.rest.Application)
benchi-kafka     | [2025-03-18 16:05:46,752] INFO [Broker id=1] Creating new partition _confluent-link-metadata-27 with topic id FgiypwCwTt68SpYw3T9ahw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,752] INFO [Broker id=1] Creating new partition _confluent-link-metadata-23 with topic id FgiypwCwTt68SpYw3T9ahw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,752] WARN Disabling exponential reconnect backoff because reconnect.backoff.ms is set, but reconnect.backoff.max.ms is not. (org.apache.kafka.clients.CommonClientConfigs)
benchi-kafka     | [2025-03-18 16:05:46,752] INFO ConsumerConfig values: 
benchi-kafka     | 	allow.auto.create.topics = true
benchi-kafka     | 	auto.commit.interval.ms = 5000
benchi-kafka     | 	auto.include.jmx.reporter = true
benchi-kafka     | 	auto.offset.reset = latest
benchi-kafka     | 	bootstrap.servers = [broker:29092]
benchi-kafka     | 	check.crcs = true
benchi-kafka     | 	client.dns.lookup = use_all_dns_ips
benchi-kafka     | 	client.id = kafka-cruise-control
benchi-kafka     | 	client.rack = 
benchi-kafka     | 	confluent.lkc.id = null
benchi-kafka     | 	confluent.proxy.protocol.client.address = null
benchi-kafka     | 	confluent.proxy.protocol.client.mode = PROXY
benchi-kafka     | 	confluent.proxy.protocol.client.port = null
benchi-kafka     | 	confluent.proxy.protocol.client.version = NONE
benchi-kafka     | 	connections.max.idle.ms = 540000
benchi-kafka     | 	default.api.timeout.ms = 60000
benchi-kafka     | 	enable.auto.commit = false
benchi-kafka     | 	enable.metrics.push = true
benchi-kafka     | 	exclude.internal.topics = true
benchi-kafka     | 	fetch.max.bytes = 52428800
benchi-kafka     | 	fetch.max.wait.ms = 500
benchi-kafka     | 	fetch.min.bytes = 1
benchi-kafka     | 	group.id = ConfluentTelemetryReporterSampler-6820492430475519429
benchi-kafka     | 	group.instance.id = null
benchi-kafka     | 	group.protocol = classic
benchi-kafka     | 	group.remote.assignor = null
benchi-kafka     | 	heartbeat.interval.ms = 3000
benchi-kafka     | 	interceptor.classes = []
benchi-kafka     | 	internal.leave.group.on.close = true
benchi-kafka     | 	internal.throw.on.fetch.stable.offset.unsupported = false
benchi-kafka     | 	isolation.level = read_uncommitted
benchi-kafka     | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
benchi-kafka     | 	max.partition.fetch.bytes = 1048576
benchi-kafka     | 	max.poll.interval.ms = 2147483647
benchi-kafka     | 	max.poll.records = 2147483647
benchi-kafka     | 	metadata.max.age.ms = 300000
benchi-kafka     | 	metadata.recovery.strategy = none
benchi-kafka     | 	metric.reporters = []
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.recording.level = INFO
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
benchi-kafka     | 	receive.buffer.bytes = 32768
benchi-kafka     | 	reconnect.backoff.max.ms = 50
benchi-kafka     | 	reconnect.backoff.ms = 50
benchi-kafka     | 	request.timeout.ms = 30000
benchi-kafka     | 	retry.backoff.max.ms = 1000
benchi-kafka     | 	retry.backoff.ms = 100
benchi-kafka     | 	sasl.client.callback.handler.class = null
benchi-kafka     | 	sasl.jaas.config = null
benchi-kafka     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
benchi-kafka     | 	sasl.kerberos.min.time.before.relogin = 60000
benchi-kafka     | 	sasl.kerberos.service.name = null
benchi-kafka     | 	sasl.kerberos.ticket.renew.jitter = 0.05
benchi-kafka     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
benchi-kafka     | 	sasl.login.callback.handler.class = null
benchi-kafka     | 	sasl.login.class = null
benchi-kafka     | 	sasl.login.connect.timeout.ms = null
benchi-kafka     | 	sasl.login.read.timeout.ms = null
benchi-kafka     | 	sasl.login.refresh.buffer.seconds = 300
benchi-kafka     | 	sasl.login.refresh.min.period.seconds = 60
benchi-kafka     | 	sasl.login.refresh.window.factor = 0.8
benchi-kafka     | 	sasl.login.refresh.window.jitter = 0.05
benchi-kafka     | 	sasl.login.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.login.retry.backoff.ms = 100
benchi-kafka     | 	sasl.mechanism = GSSAPI
benchi-kafka     | 	sasl.oauthbearer.clientassertion.audience = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.expiration = 5
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.jti.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.nbf.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.issuer = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.location = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key.passphrase = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.subject = null
benchi-kafka     | 	sasl.oauthbearer.clock.skew.seconds = 30
benchi-kafka     | 	sasl.oauthbearer.expected.audience = null
benchi-kafka     | 	sasl.oauthbearer.expected.issuer = null
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.url = null
benchi-kafka     | 	sasl.oauthbearer.scope.claim.name = scope
benchi-kafka     | 	sasl.oauthbearer.sub.claim.name = sub
benchi-kafka     | 	sasl.oauthbearer.token.endpoint.url = null
benchi-kafka     | 	security.protocol = PLAINTEXT
benchi-kafka     | 	security.providers = null
benchi-kafka     | 	send.buffer.bytes = 131072
benchi-kafka     | 	session.timeout.ms = 45000
benchi-kafka     | 	socket.connection.setup.timeout.max.ms = 30000
benchi-kafka     | 	socket.connection.setup.timeout.ms = 10000
benchi-kafka     | 	ssl.cipher.suites = null
benchi-kafka     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	ssl.engine.factory.class = null
benchi-kafka     | 	ssl.key.password = null
benchi-kafka     | 	ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	ssl.keystore.certificate.chain = null
benchi-kafka     | 	ssl.keystore.key = null
benchi-kafka     | 	ssl.keystore.location = null
benchi-kafka     | 	ssl.keystore.password = null
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.protocol = TLSv1.3
benchi-kafka     | 	ssl.provider = null
benchi-kafka     | 	ssl.secure.random.implementation = null
benchi-kafka     | 	ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	ssl.truststore.certificates = null
benchi-kafka     | 	ssl.truststore.location = null
benchi-kafka     | 	ssl.truststore.password = null
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
benchi-kafka     |  (org.apache.kafka.clients.consumer.ConsumerConfig)
benchi-kafka     | [2025-03-18 16:05:46,752] INFO [Broker id=1] Creating new partition _confluent-link-metadata-35 with topic id FgiypwCwTt68SpYw3T9ahw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,753] INFO [Broker id=1] Creating new partition _confluent-link-metadata-2 with topic id FgiypwCwTt68SpYw3T9ahw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,753] INFO [Broker id=1] Creating new partition _confluent-link-metadata-31 with topic id FgiypwCwTt68SpYw3T9ahw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,755] INFO [Broker id=1] Creating new partition _confluent-link-metadata-42 with topic id FgiypwCwTt68SpYw3T9ahw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,755] INFO [Broker id=1] Creating new partition _confluent-link-metadata-13 with topic id FgiypwCwTt68SpYw3T9ahw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,755] INFO [Broker id=1] Creating new partition _confluent-link-metadata-38 with topic id FgiypwCwTt68SpYw3T9ahw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,756] INFO [Broker id=1] Creating new partition _confluent-link-metadata-9 with topic id FgiypwCwTt68SpYw3T9ahw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,756] INFO [Broker id=1] Creating new partition _confluent-link-metadata-21 with topic id FgiypwCwTt68SpYw3T9ahw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,756] INFO [Broker id=1] Creating new partition _confluent-link-metadata-46 with topic id FgiypwCwTt68SpYw3T9ahw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,757] INFO [Broker id=1] Creating new partition _confluent-link-metadata-17 with topic id FgiypwCwTt68SpYw3T9ahw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,757] INFO [Broker id=1] Creating new partition _confluent-link-metadata-26 with topic id FgiypwCwTt68SpYw3T9ahw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,757] INFO [Broker id=1] Creating new partition _confluent-link-metadata-22 with topic id FgiypwCwTt68SpYw3T9ahw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,757] INFO [Broker id=1] Creating new partition _confluent-link-metadata-34 with topic id FgiypwCwTt68SpYw3T9ahw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,758] INFO [Broker id=1] Creating new partition _confluent-link-metadata-5 with topic id FgiypwCwTt68SpYw3T9ahw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,758] INFO [Broker id=1] Creating new partition _confluent-link-metadata-30 with topic id FgiypwCwTt68SpYw3T9ahw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,758] INFO initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector)
benchi-kafka     | [2025-03-18 16:05:46,758] INFO [Broker id=1] Creating new partition _confluent-link-metadata-1 with topic id FgiypwCwTt68SpYw3T9ahw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,758] INFO [Broker id=1] Creating new partition _confluent-link-metadata-45 with topic id FgiypwCwTt68SpYw3T9ahw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,759] INFO [Broker id=1] Creating new partition _confluent-link-metadata-12 with topic id FgiypwCwTt68SpYw3T9ahw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,759] INFO [Broker id=1] Creating new partition _confluent-link-metadata-41 with topic id FgiypwCwTt68SpYw3T9ahw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,759] INFO [Broker id=1] Creating new partition _confluent-link-metadata-8 with topic id FgiypwCwTt68SpYw3T9ahw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,760] INFO [Broker id=1] Creating new partition _confluent-link-metadata-20 with topic id FgiypwCwTt68SpYw3T9ahw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,760] INFO [Broker id=1] Creating new partition _confluent-link-metadata-49 with topic id FgiypwCwTt68SpYw3T9ahw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,760] INFO [Broker id=1] Creating new partition _confluent-link-metadata-16 with topic id FgiypwCwTt68SpYw3T9ahw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,760] INFO [Broker id=1] Creating new partition _confluent-link-metadata-29 with topic id FgiypwCwTt68SpYw3T9ahw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,761] INFO [Broker id=1] Creating new partition _confluent-link-metadata-25 with topic id FgiypwCwTt68SpYw3T9ahw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,761] INFO [Broker id=1] Creating new partition _confluent-link-metadata-37 with topic id FgiypwCwTt68SpYw3T9ahw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,762] INFO [Broker id=1] Creating new partition _confluent-link-metadata-4 with topic id FgiypwCwTt68SpYw3T9ahw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,762] INFO [Broker id=1] Creating new partition _confluent-link-metadata-33 with topic id FgiypwCwTt68SpYw3T9ahw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,763] INFO [Broker id=1] Creating new partition _confluent-link-metadata-0 with topic id FgiypwCwTt68SpYw3T9ahw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,763] INFO [Broker id=1] Creating new partition _confluent-link-metadata-11 with topic id FgiypwCwTt68SpYw3T9ahw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,764] INFO [Broker id=1] Creating new partition _confluent-link-metadata-44 with topic id FgiypwCwTt68SpYw3T9ahw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,764] INFO [Broker id=1] Creating new partition _confluent-link-metadata-7 with topic id FgiypwCwTt68SpYw3T9ahw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,764] INFO [Broker id=1] Creating new partition _confluent-link-metadata-40 with topic id FgiypwCwTt68SpYw3T9ahw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,765] INFO [Broker id=1] Creating new partition _confluent-link-metadata-19 with topic id FgiypwCwTt68SpYw3T9ahw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,765] INFO [Broker id=1] Creating new partition _confluent-link-metadata-15 with topic id FgiypwCwTt68SpYw3T9ahw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,765] INFO [Broker id=1] Creating new partition _confluent-link-metadata-48 with topic id FgiypwCwTt68SpYw3T9ahw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,766] INFO [Broker id=1] Creating new partition _confluent-link-metadata-28 with topic id FgiypwCwTt68SpYw3T9ahw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,766] INFO [Broker id=1] Creating new partition _confluent-link-metadata-24 with topic id FgiypwCwTt68SpYw3T9ahw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,766] INFO [Broker id=1] Creating new partition _confluent-link-metadata-3 with topic id FgiypwCwTt68SpYw3T9ahw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,767] INFO [Broker id=1] Creating new partition _confluent-link-metadata-36 with topic id FgiypwCwTt68SpYw3T9ahw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,767] INFO [Broker id=1] Creating new partition _confluent-link-metadata-32 with topic id FgiypwCwTt68SpYw3T9ahw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,770] INFO [Broker id=1] Stopped fetchers as part of become-leader transition for 50 partitions (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,780] INFO These configurations '[sasl.oauthbearer.jwks.endpoint.refresh.ms, sasl.oauthbearer.clientassertion.include.jti.claim, sasl.login.refresh.min.period.seconds, sasl.oauthbearer.scope.claim.name, sasl.login.refresh.window.factor, sasl.kerberos.ticket.renew.window.factor, sasl.login.retry.backoff.ms, sasl.oauthbearer.clientassertion.expiration, sasl.kerberos.kinit.cmd, sasl.kerberos.ticket.renew.jitter, ssl.keystore.type, ssl.trustmanager.algorithm, sasl.kerberos.min.time.before.relogin, ssl.endpoint.identification.algorithm, ssl.protocol, sasl.login.refresh.buffer.seconds, sasl.login.retry.backoff.max.ms, ssl.enabled.protocols, sasl.oauthbearer.sub.claim.name, ssl.truststore.type, sasl.oauthbearer.jwks.endpoint.retry.backoff.ms, sasl.oauthbearer.clock.skew.seconds, ssl.keymanager.algorithm, sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms, sasl.oauthbearer.clientassertion.include.nbf.claim, sasl.login.refresh.window.jitter]' were supplied but are not used yet. (org.apache.kafka.clients.consumer.ConsumerConfig)
benchi-kafka     | [2025-03-18 16:05:46,780] INFO Kafka version: 7.8.1-ce (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:05:46,780] INFO Kafka commitId: eefb51c6f2f1b888b8b03296353338afbac8e213 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:05:46,780] INFO Kafka startTimeMs: 1742313946780 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:05:46,780] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-6820492430475519429] Subscribed to pattern: '_confluent-telemetry-metrics' (org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer)
benchi-kafka     | [2025-03-18 16:05:46,783] INFO [MergedLog partition=_confluent-link-metadata-39, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:05:46,784] INFO Created log for partition _confluent-link-metadata-39 in /tmp/kraft-combined-logs/_confluent-link-metadata-39 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:05:46,784] INFO [Partition _confluent-link-metadata-39 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-39 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,785] INFO [Partition _confluent-link-metadata-39 broker=1] Log loaded for partition _confluent-link-metadata-39 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,785] INFO [MergedLog partition=_confluent-link-metadata-41, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:05:46,786] INFO Setting topicIdPartition FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-39 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:05:46,786] INFO Created log for partition _confluent-link-metadata-41 in /tmp/kraft-combined-logs/_confluent-link-metadata-41 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:05:46,786] INFO [MergedLog partition=_confluent-link-metadata-39, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-39 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:05:46,786] INFO [Partition _confluent-link-metadata-41 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-41 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,787] INFO [Partition _confluent-link-metadata-41 broker=1] Log loaded for partition _confluent-link-metadata-41 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,787] INFO Setting topicIdPartition FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-41 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:05:46,787] INFO [MergedLog partition=_confluent-link-metadata-41, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-41 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:05:46,788] INFO [MergedLog partition=_confluent-link-metadata-43, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:05:46,788] INFO Created log for partition _confluent-link-metadata-43 in /tmp/kraft-combined-logs/_confluent-link-metadata-43 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:05:46,788] INFO [Partition _confluent-link-metadata-43 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-43 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,788] INFO [Partition _confluent-link-metadata-43 broker=1] Log loaded for partition _confluent-link-metadata-43 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,788] INFO Setting topicIdPartition FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-43 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:05:46,788] INFO [MergedLog partition=_confluent-link-metadata-43, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-43 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:05:46,790] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-6820492430475519429] Cluster ID: MkU3OEVBNTcwNTJENDM2Qk (org.apache.kafka.clients.Metadata)
benchi-kafka     | [2025-03-18 16:05:46,790] INFO [MergedLog partition=_confluent-link-metadata-24, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:05:46,790] INFO [Broker id=1] Leader _confluent-link-metadata-39 with topic id Some(FgiypwCwTt68SpYw3T9ahw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,790] INFO [Broker id=1] Leader _confluent-link-metadata-41 with topic id Some(FgiypwCwTt68SpYw3T9ahw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,791] INFO Created log for partition _confluent-link-metadata-24 in /tmp/kraft-combined-logs/_confluent-link-metadata-24 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:05:46,791] INFO [Partition _confluent-link-metadata-24 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-24 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,791] INFO [Partition _confluent-link-metadata-24 broker=1] Log loaded for partition _confluent-link-metadata-24 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,791] INFO Setting topicIdPartition FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-24 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:05:46,791] INFO [MergedLog partition=_confluent-link-metadata-24, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-24 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:05:46,791] INFO [Broker id=1] Leader _confluent-link-metadata-24 with topic id Some(FgiypwCwTt68SpYw3T9ahw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,791] INFO [Broker id=1] Leader _confluent-link-metadata-43 with topic id Some(FgiypwCwTt68SpYw3T9ahw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,792] INFO Waiting for 1 seconds for metric reporter topic _confluent-telemetry-metrics to become available. (io.confluent.cruisecontrol.metricsreporter.ConfluentMetricsSamplerBase)
benchi-kafka     | [2025-03-18 16:05:46,792] INFO [MergedLog partition=_confluent-link-metadata-20, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:05:46,792] INFO Registered MetricsListener to connector of listener: null (io.confluent.rest.ApplicationServer)
benchi-kafka     | [2025-03-18 16:05:46,792] INFO Created log for partition _confluent-link-metadata-20 in /tmp/kraft-combined-logs/_confluent-link-metadata-20 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:05:46,792] INFO [Partition _confluent-link-metadata-20 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-20 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,793] INFO [Partition _confluent-link-metadata-20 broker=1] Log loaded for partition _confluent-link-metadata-20 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,793] INFO Setting topicIdPartition FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-20 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:05:46,793] INFO [MergedLog partition=_confluent-link-metadata-20, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-20 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:05:46,793] INFO [Broker id=1] Leader _confluent-link-metadata-20 with topic id Some(FgiypwCwTt68SpYw3T9ahw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,794] INFO [MergedLog partition=_confluent-link-metadata-18, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:05:46,795] INFO Created log for partition _confluent-link-metadata-18 in /tmp/kraft-combined-logs/_confluent-link-metadata-18 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:05:46,795] INFO [Partition _confluent-link-metadata-18 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-18 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,795] INFO [Partition _confluent-link-metadata-18 broker=1] Log loaded for partition _confluent-link-metadata-18 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,795] INFO Setting topicIdPartition FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-18 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:05:46,795] INFO [MergedLog partition=_confluent-link-metadata-18, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-18 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:05:46,795] INFO [Broker id=1] Leader _confluent-link-metadata-18 with topic id Some(FgiypwCwTt68SpYw3T9ahw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,797] INFO [MergedLog partition=_confluent-link-metadata-23, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:05:46,797] INFO Created log for partition _confluent-link-metadata-23 in /tmp/kraft-combined-logs/_confluent-link-metadata-23 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:05:46,797] INFO [Partition _confluent-link-metadata-23 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-23 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,797] INFO [Partition _confluent-link-metadata-23 broker=1] Log loaded for partition _confluent-link-metadata-23 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,797] INFO Setting topicIdPartition FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-23 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:05:46,797] INFO [MergedLog partition=_confluent-link-metadata-23, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-23 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:05:46,797] INFO [Broker id=1] Leader _confluent-link-metadata-23 with topic id Some(FgiypwCwTt68SpYw3T9ahw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,799] INFO [MergedLog partition=_confluent-link-metadata-21, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:05:46,799] INFO Created log for partition _confluent-link-metadata-21 in /tmp/kraft-combined-logs/_confluent-link-metadata-21 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:05:46,799] INFO [Partition _confluent-link-metadata-21 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-21 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,799] INFO [Partition _confluent-link-metadata-21 broker=1] Log loaded for partition _confluent-link-metadata-21 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,799] INFO Setting topicIdPartition FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-21 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:05:46,799] INFO [MergedLog partition=_confluent-link-metadata-21, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-21 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:05:46,799] INFO [Broker id=1] Leader _confluent-link-metadata-21 with topic id Some(FgiypwCwTt68SpYw3T9ahw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,800] INFO [MergedLog partition=_confluent-link-metadata-25, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:05:46,801] INFO Created log for partition _confluent-link-metadata-25 in /tmp/kraft-combined-logs/_confluent-link-metadata-25 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:05:46,801] INFO [Partition _confluent-link-metadata-25 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-25 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,801] INFO [Partition _confluent-link-metadata-25 broker=1] Log loaded for partition _confluent-link-metadata-25 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,801] INFO Setting topicIdPartition FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-25 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:05:46,801] INFO [MergedLog partition=_confluent-link-metadata-25, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-25 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:05:46,801] INFO [Broker id=1] Leader _confluent-link-metadata-25 with topic id Some(FgiypwCwTt68SpYw3T9ahw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,802] INFO [MergedLog partition=_confluent-link-metadata-19, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:05:46,802] INFO Created log for partition _confluent-link-metadata-19 in /tmp/kraft-combined-logs/_confluent-link-metadata-19 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:05:46,802] INFO [Partition _confluent-link-metadata-19 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-19 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,802] INFO [Partition _confluent-link-metadata-19 broker=1] Log loaded for partition _confluent-link-metadata-19 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,802] INFO Setting topicIdPartition FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-19 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:05:46,802] INFO [MergedLog partition=_confluent-link-metadata-19, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-19 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:05:46,802] INFO [Broker id=1] Leader _confluent-link-metadata-19 with topic id Some(FgiypwCwTt68SpYw3T9ahw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,804] INFO [MergedLog partition=_confluent-link-metadata-29, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:05:46,804] INFO Created log for partition _confluent-link-metadata-29 in /tmp/kraft-combined-logs/_confluent-link-metadata-29 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:05:46,804] INFO [Partition _confluent-link-metadata-29 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-29 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,804] INFO [Partition _confluent-link-metadata-29 broker=1] Log loaded for partition _confluent-link-metadata-29 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,804] INFO Setting topicIdPartition FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-29 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:05:46,804] INFO [MergedLog partition=_confluent-link-metadata-29, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-29 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:05:46,804] INFO [Broker id=1] Leader _confluent-link-metadata-29 with topic id Some(FgiypwCwTt68SpYw3T9ahw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,805] INFO [MergedLog partition=_confluent-link-metadata-46, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:05:46,806] INFO Created log for partition _confluent-link-metadata-46 in /tmp/kraft-combined-logs/_confluent-link-metadata-46 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:05:46,806] INFO [Partition _confluent-link-metadata-46 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-46 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,806] INFO [Partition _confluent-link-metadata-46 broker=1] Log loaded for partition _confluent-link-metadata-46 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,806] INFO Setting topicIdPartition FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-46 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:05:46,806] INFO [MergedLog partition=_confluent-link-metadata-46, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-46 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:05:46,806] INFO [Broker id=1] Leader _confluent-link-metadata-46 with topic id Some(FgiypwCwTt68SpYw3T9ahw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,807] INFO [MergedLog partition=_confluent-link-metadata-27, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:05:46,809] INFO Created log for partition _confluent-link-metadata-27 in /tmp/kraft-combined-logs/_confluent-link-metadata-27 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:05:46,809] INFO [Partition _confluent-link-metadata-27 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-27 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,809] INFO [Partition _confluent-link-metadata-27 broker=1] Log loaded for partition _confluent-link-metadata-27 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,809] INFO Setting topicIdPartition FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-27 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:05:46,809] INFO [MergedLog partition=_confluent-link-metadata-27, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-27 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:05:46,809] INFO [Broker id=1] Leader _confluent-link-metadata-27 with topic id Some(FgiypwCwTt68SpYw3T9ahw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,810] INFO [MergedLog partition=_confluent-link-metadata-47, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:05:46,810] INFO Created log for partition _confluent-link-metadata-47 in /tmp/kraft-combined-logs/_confluent-link-metadata-47 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:05:46,810] INFO [Partition _confluent-link-metadata-47 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-47 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,810] INFO [Partition _confluent-link-metadata-47 broker=1] Log loaded for partition _confluent-link-metadata-47 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,810] INFO Setting topicIdPartition FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-47 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:05:46,811] INFO [MergedLog partition=_confluent-link-metadata-47, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-47 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:05:46,811] INFO [Broker id=1] Leader _confluent-link-metadata-47 with topic id Some(FgiypwCwTt68SpYw3T9ahw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,812] INFO [MergedLog partition=_confluent-link-metadata-28, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:05:46,812] INFO Created log for partition _confluent-link-metadata-28 in /tmp/kraft-combined-logs/_confluent-link-metadata-28 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:05:46,812] INFO [Partition _confluent-link-metadata-28 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-28 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,812] INFO [Partition _confluent-link-metadata-28 broker=1] Log loaded for partition _confluent-link-metadata-28 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,812] INFO Setting topicIdPartition FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-28 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:05:46,812] INFO [MergedLog partition=_confluent-link-metadata-28, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-28 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:05:46,812] INFO [Broker id=1] Leader _confluent-link-metadata-28 with topic id Some(FgiypwCwTt68SpYw3T9ahw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,814] INFO [MergedLog partition=_confluent-link-metadata-49, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:05:46,814] INFO Created log for partition _confluent-link-metadata-49 in /tmp/kraft-combined-logs/_confluent-link-metadata-49 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:05:46,814] INFO [Partition _confluent-link-metadata-49 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-49 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,814] INFO [Partition _confluent-link-metadata-49 broker=1] Log loaded for partition _confluent-link-metadata-49 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,814] INFO Setting topicIdPartition FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-49 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:05:46,814] INFO [MergedLog partition=_confluent-link-metadata-49, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-49 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:05:46,814] INFO [Broker id=1] Leader _confluent-link-metadata-49 with topic id Some(FgiypwCwTt68SpYw3T9ahw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,815] INFO [MergedLog partition=_confluent-link-metadata-10, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:05:46,815] INFO Created log for partition _confluent-link-metadata-10 in /tmp/kraft-combined-logs/_confluent-link-metadata-10 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:05:46,815] INFO [Partition _confluent-link-metadata-10 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-10 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,816] INFO [Partition _confluent-link-metadata-10 broker=1] Log loaded for partition _confluent-link-metadata-10 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,816] INFO Setting topicIdPartition FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-10 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:05:46,816] INFO [MergedLog partition=_confluent-link-metadata-10, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-10 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:05:46,816] INFO [Broker id=1] Leader _confluent-link-metadata-10 with topic id Some(FgiypwCwTt68SpYw3T9ahw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,817] INFO [MergedLog partition=_confluent-link-metadata-6, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:05:46,817] INFO Created log for partition _confluent-link-metadata-6 in /tmp/kraft-combined-logs/_confluent-link-metadata-6 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:05:46,817] INFO [Partition _confluent-link-metadata-6 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-6 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,817] INFO [Partition _confluent-link-metadata-6 broker=1] Log loaded for partition _confluent-link-metadata-6 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,817] INFO Setting topicIdPartition FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-6 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:05:46,817] INFO [MergedLog partition=_confluent-link-metadata-6, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-6 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:05:46,817] INFO [Broker id=1] Leader _confluent-link-metadata-6 with topic id Some(FgiypwCwTt68SpYw3T9ahw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,818] INFO [MergedLog partition=_confluent-link-metadata-8, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:05:46,818] INFO Created log for partition _confluent-link-metadata-8 in /tmp/kraft-combined-logs/_confluent-link-metadata-8 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:05:46,818] INFO [Partition _confluent-link-metadata-8 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-8 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,818] INFO [Partition _confluent-link-metadata-8 broker=1] Log loaded for partition _confluent-link-metadata-8 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,818] INFO Setting topicIdPartition FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-8 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:05:46,818] INFO [MergedLog partition=_confluent-link-metadata-8, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-8 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:05:46,819] INFO [Broker id=1] Leader _confluent-link-metadata-8 with topic id Some(FgiypwCwTt68SpYw3T9ahw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,820] INFO [MergedLog partition=_confluent-link-metadata-34, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:05:46,820] INFO Created log for partition _confluent-link-metadata-34 in /tmp/kraft-combined-logs/_confluent-link-metadata-34 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:05:46,820] INFO [Partition _confluent-link-metadata-34 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-34 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,820] INFO [Partition _confluent-link-metadata-34 broker=1] Log loaded for partition _confluent-link-metadata-34 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,820] INFO Setting topicIdPartition FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-34 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:05:46,820] INFO [MergedLog partition=_confluent-link-metadata-34, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-34 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:05:46,820] INFO [Broker id=1] Leader _confluent-link-metadata-34 with topic id Some(FgiypwCwTt68SpYw3T9ahw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,821] INFO [MergedLog partition=_confluent-link-metadata-7, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:05:46,822] INFO Created log for partition _confluent-link-metadata-7 in /tmp/kraft-combined-logs/_confluent-link-metadata-7 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:05:46,822] INFO [Partition _confluent-link-metadata-7 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-7 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,822] INFO [Partition _confluent-link-metadata-7 broker=1] Log loaded for partition _confluent-link-metadata-7 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,822] INFO Setting topicIdPartition FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-7 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:05:46,822] INFO [MergedLog partition=_confluent-link-metadata-7, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-7 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:05:46,822] INFO [Broker id=1] Leader _confluent-link-metadata-7 with topic id Some(FgiypwCwTt68SpYw3T9ahw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,823] INFO [MergedLog partition=_confluent-link-metadata-16, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:05:46,823] INFO Created log for partition _confluent-link-metadata-16 in /tmp/kraft-combined-logs/_confluent-link-metadata-16 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:05:46,823] INFO [Partition _confluent-link-metadata-16 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-16 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,823] INFO [Partition _confluent-link-metadata-16 broker=1] Log loaded for partition _confluent-link-metadata-16 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,823] INFO Setting topicIdPartition FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-16 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:05:46,824] INFO [MergedLog partition=_confluent-link-metadata-16, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-16 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:05:46,824] INFO [Broker id=1] Leader _confluent-link-metadata-16 with topic id Some(FgiypwCwTt68SpYw3T9ahw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,825] INFO [MergedLog partition=_confluent-link-metadata-32, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:05:46,825] INFO Created log for partition _confluent-link-metadata-32 in /tmp/kraft-combined-logs/_confluent-link-metadata-32 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:05:46,825] INFO [Partition _confluent-link-metadata-32 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-32 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,825] INFO [Partition _confluent-link-metadata-32 broker=1] Log loaded for partition _confluent-link-metadata-32 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,825] INFO Setting topicIdPartition FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-32 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:05:46,825] INFO [MergedLog partition=_confluent-link-metadata-32, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-32 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:05:46,825] INFO [Broker id=1] Leader _confluent-link-metadata-32 with topic id Some(FgiypwCwTt68SpYw3T9ahw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,826] INFO [MergedLog partition=_confluent-link-metadata-14, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:05:46,826] INFO Created log for partition _confluent-link-metadata-14 in /tmp/kraft-combined-logs/_confluent-link-metadata-14 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:05:46,826] INFO [Partition _confluent-link-metadata-14 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-14 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,826] INFO [Partition _confluent-link-metadata-14 broker=1] Log loaded for partition _confluent-link-metadata-14 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,827] INFO Setting topicIdPartition FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-14 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:05:46,827] INFO [MergedLog partition=_confluent-link-metadata-14, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-14 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:05:46,827] INFO [Broker id=1] Leader _confluent-link-metadata-14 with topic id Some(FgiypwCwTt68SpYw3T9ahw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,828] INFO [MergedLog partition=_confluent-link-metadata-31, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:05:46,828] INFO Created log for partition _confluent-link-metadata-31 in /tmp/kraft-combined-logs/_confluent-link-metadata-31 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:05:46,828] INFO [Partition _confluent-link-metadata-31 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-31 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,828] INFO [Partition _confluent-link-metadata-31 broker=1] Log loaded for partition _confluent-link-metadata-31 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,828] INFO Setting topicIdPartition FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-31 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:05:46,828] INFO [MergedLog partition=_confluent-link-metadata-31, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-31 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:05:46,828] INFO [Broker id=1] Leader _confluent-link-metadata-31 with topic id Some(FgiypwCwTt68SpYw3T9ahw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,828] WARN Using default value http://localhost:8081 for config schema.registry.url. In a future release this config won't have a default value anymore. If you are using Schema Registry, please, specify schema.registry.url explicitly. Requests will fail in a future release if you try to use Schema Registry but have not specified a value for schema.registry.url. An empty value for this property means that the Schema Registry is disabled. (io.confluent.kafkarest.KafkaRestConfig)
benchi-kafka     | [2025-03-18 16:05:46,829] INFO SchemaRegistryConfig values: 
benchi-kafka     | 	auto.register.schemas = false
benchi-kafka     | 	basic.auth.credentials.source = URL
benchi-kafka     | 	basic.auth.user.info = [hidden]
benchi-kafka     | 	bearer.auth.cache.expiry.buffer.seconds = 300
benchi-kafka     | 	bearer.auth.client.id = null
benchi-kafka     | 	bearer.auth.client.secret = null
benchi-kafka     | 	bearer.auth.credentials.source = STATIC_TOKEN
benchi-kafka     | 	bearer.auth.custom.provider.class = null
benchi-kafka     | 	bearer.auth.identity.pool.id = null
benchi-kafka     | 	bearer.auth.issuer.endpoint.url = null
benchi-kafka     | 	bearer.auth.logical.cluster = null
benchi-kafka     | 	bearer.auth.scope = null
benchi-kafka     | 	bearer.auth.scope.claim.name = scope
benchi-kafka     | 	bearer.auth.sub.claim.name = sub
benchi-kafka     | 	bearer.auth.token = [hidden]
benchi-kafka     | 	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
benchi-kafka     | 	http.connect.timeout.ms = 60000
benchi-kafka     | 	http.read.timeout.ms = 60000
benchi-kafka     | 	id.compatibility.strict = true
benchi-kafka     | 	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
benchi-kafka     | 	latest.cache.size = 1000
benchi-kafka     | 	latest.cache.ttl.sec = -1
benchi-kafka     | 	latest.compatibility.strict = true
benchi-kafka     | 	max.schemas.per.subject = 1000
benchi-kafka     | 	normalize.schemas = false
benchi-kafka     | 	propagate.schema.tags = false
benchi-kafka     | 	proxy.host = 
benchi-kafka     | 	proxy.port = -1
benchi-kafka     | 	rule.actions = []
benchi-kafka     | 	rule.executors = []
benchi-kafka     | 	rule.service.loader.enable = true
benchi-kafka     | 	schema.format = null
benchi-kafka     | 	schema.reflection = false
benchi-kafka     | 	schema.registry.basic.auth.user.info = [hidden]
benchi-kafka     | 	schema.registry.ssl.cipher.suites = null
benchi-kafka     | 	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	schema.registry.ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	schema.registry.ssl.engine.factory.class = null
benchi-kafka     | 	schema.registry.ssl.key.password = null
benchi-kafka     | 	schema.registry.ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	schema.registry.ssl.keystore.certificate.chain = null
benchi-kafka     | 	schema.registry.ssl.keystore.key = null
benchi-kafka     | 	schema.registry.ssl.keystore.location = null
benchi-kafka     | 	schema.registry.ssl.keystore.password = null
benchi-kafka     | 	schema.registry.ssl.keystore.type = JKS
benchi-kafka     | 	schema.registry.ssl.protocol = TLSv1.3
benchi-kafka     | 	schema.registry.ssl.provider = null
benchi-kafka     | 	schema.registry.ssl.secure.random.implementation = null
benchi-kafka     | 	schema.registry.ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	schema.registry.ssl.truststore.certificates = null
benchi-kafka     | 	schema.registry.ssl.truststore.location = null
benchi-kafka     | 	schema.registry.ssl.truststore.password = null
benchi-kafka     | 	schema.registry.ssl.truststore.type = JKS
benchi-kafka     | 	schema.registry.url = [http://localhost:8081]
benchi-kafka     | 	use.latest.version = false
benchi-kafka     | 	use.latest.with.metadata = null
benchi-kafka     | 	use.schema.id = -1
benchi-kafka     | 	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
benchi-kafka     |  (io.confluent.kafkarest.config.SchemaRegistryConfig)
benchi-kafka     | [2025-03-18 16:05:46,830] INFO [MergedLog partition=_confluent-link-metadata-17, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:05:46,830] INFO Created log for partition _confluent-link-metadata-17 in /tmp/kraft-combined-logs/_confluent-link-metadata-17 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:05:46,830] INFO [Partition _confluent-link-metadata-17 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-17 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,830] INFO [Partition _confluent-link-metadata-17 broker=1] Log loaded for partition _confluent-link-metadata-17 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,831] INFO Setting topicIdPartition FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-17 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:05:46,831] INFO [MergedLog partition=_confluent-link-metadata-17, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-17 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:05:46,831] INFO [Broker id=1] Leader _confluent-link-metadata-17 with topic id Some(FgiypwCwTt68SpYw3T9ahw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,832] INFO [MergedLog partition=_confluent-link-metadata-33, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:05:46,832] INFO Binding EmbeddedKafkaRestApplication to all listeners. (io.confluent.rest.Application)
benchi-kafka     | [2025-03-18 16:05:46,832] INFO Created log for partition _confluent-link-metadata-33 in /tmp/kraft-combined-logs/_confluent-link-metadata-33 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:05:46,832] INFO [Partition _confluent-link-metadata-33 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-33 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,832] INFO [Partition _confluent-link-metadata-33 broker=1] Log loaded for partition _confluent-link-metadata-33 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,832] INFO Setting topicIdPartition FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-33 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:05:46,832] INFO [MergedLog partition=_confluent-link-metadata-33, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-33 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:05:46,832] INFO [Broker id=1] Leader _confluent-link-metadata-33 with topic id Some(FgiypwCwTt68SpYw3T9ahw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,833] INFO [MergedLog partition=_confluent-link-metadata-15, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:05:46,833] INFO Created log for partition _confluent-link-metadata-15 in /tmp/kraft-combined-logs/_confluent-link-metadata-15 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:05:46,833] INFO [Partition _confluent-link-metadata-15 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-15 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,833] INFO [Partition _confluent-link-metadata-15 broker=1] Log loaded for partition _confluent-link-metadata-15 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,834] INFO Setting topicIdPartition FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-15 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:05:46,834] INFO [MergedLog partition=_confluent-link-metadata-15, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-15 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:05:46,834] INFO [Broker id=1] Leader _confluent-link-metadata-15 with topic id Some(FgiypwCwTt68SpYw3T9ahw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,835] INFO [MergedLog partition=_confluent-link-metadata-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:05:46,835] INFO Created log for partition _confluent-link-metadata-0 in /tmp/kraft-combined-logs/_confluent-link-metadata-0 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:05:46,835] INFO [Partition _confluent-link-metadata-0 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,835] INFO [Partition _confluent-link-metadata-0 broker=1] Log loaded for partition _confluent-link-metadata-0 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,835] INFO Setting topicIdPartition FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-0 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:05:46,835] INFO [MergedLog partition=_confluent-link-metadata-0, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:05:46,835] INFO [Broker id=1] Leader _confluent-link-metadata-0 with topic id Some(FgiypwCwTt68SpYw3T9ahw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,837] INFO [MergedLog partition=_confluent-link-metadata-11, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:05:46,837] INFO Created log for partition _confluent-link-metadata-11 in /tmp/kraft-combined-logs/_confluent-link-metadata-11 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:05:46,837] INFO [Partition _confluent-link-metadata-11 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-11 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,837] INFO [Partition _confluent-link-metadata-11 broker=1] Log loaded for partition _confluent-link-metadata-11 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,837] INFO Setting topicIdPartition FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-11 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:05:46,837] INFO [MergedLog partition=_confluent-link-metadata-11, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-11 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:05:46,837] INFO [Broker id=1] Leader _confluent-link-metadata-11 with topic id Some(FgiypwCwTt68SpYw3T9ahw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,839] INFO [MergedLog partition=_confluent-link-metadata-38, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:05:46,839] INFO Created log for partition _confluent-link-metadata-38 in /tmp/kraft-combined-logs/_confluent-link-metadata-38 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:05:46,839] INFO [Partition _confluent-link-metadata-38 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-38 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,839] INFO [Partition _confluent-link-metadata-38 broker=1] Log loaded for partition _confluent-link-metadata-38 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,839] INFO Setting topicIdPartition FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-38 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:05:46,839] INFO [MergedLog partition=_confluent-link-metadata-38, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-38 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:05:46,839] INFO [Broker id=1] Leader _confluent-link-metadata-38 with topic id Some(FgiypwCwTt68SpYw3T9ahw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,840] INFO [MergedLog partition=_confluent-link-metadata-35, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:05:46,841] INFO Created log for partition _confluent-link-metadata-35 in /tmp/kraft-combined-logs/_confluent-link-metadata-35 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:05:46,841] INFO [Partition _confluent-link-metadata-35 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-35 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,841] INFO [Partition _confluent-link-metadata-35 broker=1] Log loaded for partition _confluent-link-metadata-35 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,845] INFO Setting topicIdPartition FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-35 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:05:46,845] INFO [MergedLog partition=_confluent-link-metadata-35, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-35 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:05:46,845] INFO [Broker id=1] Leader _confluent-link-metadata-35 with topic id Some(FgiypwCwTt68SpYw3T9ahw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,846] INFO [MergedLog partition=_confluent-link-metadata-3, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:05:46,846] INFO Created log for partition _confluent-link-metadata-3 in /tmp/kraft-combined-logs/_confluent-link-metadata-3 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:05:46,846] INFO [Partition _confluent-link-metadata-3 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-3 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,846] INFO [Partition _confluent-link-metadata-3 broker=1] Log loaded for partition _confluent-link-metadata-3 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,846] INFO Setting topicIdPartition FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-3 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:05:46,846] INFO [MergedLog partition=_confluent-link-metadata-3, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-3 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:05:46,847] INFO [Broker id=1] Leader _confluent-link-metadata-3 with topic id Some(FgiypwCwTt68SpYw3T9ahw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,848] INFO [MergedLog partition=_confluent-link-metadata-37, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:05:46,848] INFO Created log for partition _confluent-link-metadata-37 in /tmp/kraft-combined-logs/_confluent-link-metadata-37 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:05:46,848] INFO [Partition _confluent-link-metadata-37 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-37 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,848] INFO [Partition _confluent-link-metadata-37 broker=1] Log loaded for partition _confluent-link-metadata-37 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,848] INFO Setting topicIdPartition FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-37 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:05:46,849] INFO [MergedLog partition=_confluent-link-metadata-37, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-37 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:05:46,849] INFO [Broker id=1] Leader _confluent-link-metadata-37 with topic id Some(FgiypwCwTt68SpYw3T9ahw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,850] INFO [MergedLog partition=_confluent-link-metadata-40, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:05:46,850] INFO Created log for partition _confluent-link-metadata-40 in /tmp/kraft-combined-logs/_confluent-link-metadata-40 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:05:46,850] INFO [Partition _confluent-link-metadata-40 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-40 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,850] INFO [Partition _confluent-link-metadata-40 broker=1] Log loaded for partition _confluent-link-metadata-40 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,850] INFO Setting topicIdPartition FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-40 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:05:46,850] INFO [MergedLog partition=_confluent-link-metadata-40, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-40 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:05:46,850] INFO [Broker id=1] Leader _confluent-link-metadata-40 with topic id Some(FgiypwCwTt68SpYw3T9ahw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,851] INFO [MergedLog partition=_confluent-link-metadata-5, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:05:46,851] INFO Created log for partition _confluent-link-metadata-5 in /tmp/kraft-combined-logs/_confluent-link-metadata-5 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:05:46,852] INFO [Partition _confluent-link-metadata-5 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-5 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,852] INFO [Partition _confluent-link-metadata-5 broker=1] Log loaded for partition _confluent-link-metadata-5 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,852] INFO Setting topicIdPartition FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-5 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:05:46,852] INFO [MergedLog partition=_confluent-link-metadata-5, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-5 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:05:46,852] INFO [Broker id=1] Leader _confluent-link-metadata-5 with topic id Some(FgiypwCwTt68SpYw3T9ahw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,853] INFO [MergedLog partition=_confluent-link-metadata-45, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:05:46,853] INFO Created log for partition _confluent-link-metadata-45 in /tmp/kraft-combined-logs/_confluent-link-metadata-45 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:05:46,853] INFO [Partition _confluent-link-metadata-45 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-45 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,853] INFO [Partition _confluent-link-metadata-45 broker=1] Log loaded for partition _confluent-link-metadata-45 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,853] INFO Setting topicIdPartition FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-45 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:05:46,853] INFO [MergedLog partition=_confluent-link-metadata-45, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-45 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:05:46,853] INFO [Broker id=1] Leader _confluent-link-metadata-45 with topic id Some(FgiypwCwTt68SpYw3T9ahw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,855] INFO [MergedLog partition=_confluent-link-metadata-30, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:05:46,855] INFO Created log for partition _confluent-link-metadata-30 in /tmp/kraft-combined-logs/_confluent-link-metadata-30 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:05:46,855] INFO [Partition _confluent-link-metadata-30 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-30 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,855] INFO [Partition _confluent-link-metadata-30 broker=1] Log loaded for partition _confluent-link-metadata-30 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,855] INFO Setting topicIdPartition FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-30 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:05:46,855] INFO [MergedLog partition=_confluent-link-metadata-30, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-30 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:05:46,855] INFO [Broker id=1] Leader _confluent-link-metadata-30 with topic id Some(FgiypwCwTt68SpYw3T9ahw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,856] INFO [MergedLog partition=_confluent-link-metadata-22, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:05:46,856] INFO Created log for partition _confluent-link-metadata-22 in /tmp/kraft-combined-logs/_confluent-link-metadata-22 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:05:46,856] INFO [Partition _confluent-link-metadata-22 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-22 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,856] INFO [Partition _confluent-link-metadata-22 broker=1] Log loaded for partition _confluent-link-metadata-22 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,856] INFO Setting topicIdPartition FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-22 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:05:46,857] INFO [MergedLog partition=_confluent-link-metadata-22, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-22 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:05:46,857] INFO [Broker id=1] Leader _confluent-link-metadata-22 with topic id Some(FgiypwCwTt68SpYw3T9ahw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,858] INFO [MergedLog partition=_confluent-link-metadata-4, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:05:46,858] INFO Created log for partition _confluent-link-metadata-4 in /tmp/kraft-combined-logs/_confluent-link-metadata-4 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:05:46,858] INFO [Partition _confluent-link-metadata-4 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-4 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,858] INFO [Partition _confluent-link-metadata-4 broker=1] Log loaded for partition _confluent-link-metadata-4 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,858] INFO Setting topicIdPartition FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-4 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:05:46,858] INFO [MergedLog partition=_confluent-link-metadata-4, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-4 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:05:46,858] INFO [Broker id=1] Leader _confluent-link-metadata-4 with topic id Some(FgiypwCwTt68SpYw3T9ahw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,859] INFO [MergedLog partition=_confluent-link-metadata-36, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:05:46,860] INFO Created log for partition _confluent-link-metadata-36 in /tmp/kraft-combined-logs/_confluent-link-metadata-36 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:05:46,860] INFO [Partition _confluent-link-metadata-36 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-36 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,860] INFO [Partition _confluent-link-metadata-36 broker=1] Log loaded for partition _confluent-link-metadata-36 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,860] INFO Setting topicIdPartition FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-36 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:05:46,860] INFO [MergedLog partition=_confluent-link-metadata-36, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-36 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:05:46,860] INFO [Broker id=1] Leader _confluent-link-metadata-36 with topic id Some(FgiypwCwTt68SpYw3T9ahw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,861] INFO [MergedLog partition=_confluent-link-metadata-2, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:05:46,861] INFO Created log for partition _confluent-link-metadata-2 in /tmp/kraft-combined-logs/_confluent-link-metadata-2 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:05:46,861] INFO [Partition _confluent-link-metadata-2 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-2 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,861] INFO [Partition _confluent-link-metadata-2 broker=1] Log loaded for partition _confluent-link-metadata-2 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,861] INFO Setting topicIdPartition FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-2 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:05:46,861] INFO [MergedLog partition=_confluent-link-metadata-2, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-2 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:05:46,861] INFO [Broker id=1] Leader _confluent-link-metadata-2 with topic id Some(FgiypwCwTt68SpYw3T9ahw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,862] INFO jetty-9.4.54.v20240208; built: 2024-02-08T19:42:39.027Z; git: cef3fbd6d736a21e7d541a5db490381d95a2047d; jvm 17.0.13+11 (org.eclipse.jetty.server.Server)
benchi-kafka     | [2025-03-18 16:05:46,863] INFO [MergedLog partition=_confluent-link-metadata-9, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:05:46,863] INFO Created log for partition _confluent-link-metadata-9 in /tmp/kraft-combined-logs/_confluent-link-metadata-9 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:05:46,863] INFO [Partition _confluent-link-metadata-9 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-9 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,863] INFO [Partition _confluent-link-metadata-9 broker=1] Log loaded for partition _confluent-link-metadata-9 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,864] INFO Setting topicIdPartition FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-9 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:05:46,864] INFO [MergedLog partition=_confluent-link-metadata-9, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-9 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:05:46,864] INFO [Broker id=1] Leader _confluent-link-metadata-9 with topic id Some(FgiypwCwTt68SpYw3T9ahw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,864] INFO [MergedLog partition=_confluent-link-metadata-44, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:05:46,864] INFO Created log for partition _confluent-link-metadata-44 in /tmp/kraft-combined-logs/_confluent-link-metadata-44 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:05:46,864] INFO [Partition _confluent-link-metadata-44 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-44 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,864] INFO [Partition _confluent-link-metadata-44 broker=1] Log loaded for partition _confluent-link-metadata-44 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,864] INFO Setting topicIdPartition FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-44 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:05:46,865] INFO [MergedLog partition=_confluent-link-metadata-44, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-44 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:05:46,865] INFO [Broker id=1] Leader _confluent-link-metadata-44 with topic id Some(FgiypwCwTt68SpYw3T9ahw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,866] INFO [MergedLog partition=_confluent-link-metadata-42, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:05:46,866] INFO Created log for partition _confluent-link-metadata-42 in /tmp/kraft-combined-logs/_confluent-link-metadata-42 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:05:46,866] INFO [Partition _confluent-link-metadata-42 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-42 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,866] INFO [Partition _confluent-link-metadata-42 broker=1] Log loaded for partition _confluent-link-metadata-42 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,866] INFO Setting topicIdPartition FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-42 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:05:46,866] INFO [MergedLog partition=_confluent-link-metadata-42, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-42 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:05:46,866] INFO [Broker id=1] Leader _confluent-link-metadata-42 with topic id Some(FgiypwCwTt68SpYw3T9ahw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,867] INFO [MergedLog partition=_confluent-link-metadata-48, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:05:46,868] INFO Created log for partition _confluent-link-metadata-48 in /tmp/kraft-combined-logs/_confluent-link-metadata-48 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:05:46,868] INFO [Partition _confluent-link-metadata-48 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-48 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,868] INFO [Partition _confluent-link-metadata-48 broker=1] Log loaded for partition _confluent-link-metadata-48 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,868] INFO Setting topicIdPartition FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-48 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:05:46,868] INFO [MergedLog partition=_confluent-link-metadata-48, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-48 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:05:46,868] INFO [Broker id=1] Leader _confluent-link-metadata-48 with topic id Some(FgiypwCwTt68SpYw3T9ahw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,869] INFO [MergedLog partition=_confluent-link-metadata-13, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:05:46,869] INFO Created log for partition _confluent-link-metadata-13 in /tmp/kraft-combined-logs/_confluent-link-metadata-13 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:05:46,869] INFO [Partition _confluent-link-metadata-13 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-13 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,869] INFO [Partition _confluent-link-metadata-13 broker=1] Log loaded for partition _confluent-link-metadata-13 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,869] INFO Setting topicIdPartition FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-13 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:05:46,869] INFO [MergedLog partition=_confluent-link-metadata-13, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-13 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:05:46,869] INFO [Broker id=1] Leader _confluent-link-metadata-13 with topic id Some(FgiypwCwTt68SpYw3T9ahw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,870] INFO [MergedLog partition=_confluent-link-metadata-26, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:05:46,871] INFO Created log for partition _confluent-link-metadata-26 in /tmp/kraft-combined-logs/_confluent-link-metadata-26 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:05:46,871] INFO [Partition _confluent-link-metadata-26 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-26 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,871] INFO [Partition _confluent-link-metadata-26 broker=1] Log loaded for partition _confluent-link-metadata-26 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,871] INFO Setting topicIdPartition FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-26 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:05:46,871] INFO [MergedLog partition=_confluent-link-metadata-26, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-26 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:05:46,871] INFO [Broker id=1] Leader _confluent-link-metadata-26 with topic id Some(FgiypwCwTt68SpYw3T9ahw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,872] INFO [MergedLog partition=_confluent-link-metadata-1, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:05:46,872] INFO Created log for partition _confluent-link-metadata-1 in /tmp/kraft-combined-logs/_confluent-link-metadata-1 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:05:46,872] INFO [Partition _confluent-link-metadata-1 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-1 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,872] INFO [Partition _confluent-link-metadata-1 broker=1] Log loaded for partition _confluent-link-metadata-1 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,873] INFO Setting topicIdPartition FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-1 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:05:46,873] INFO [MergedLog partition=_confluent-link-metadata-1, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-1 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:05:46,873] INFO [Broker id=1] Leader _confluent-link-metadata-1 with topic id Some(FgiypwCwTt68SpYw3T9ahw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,874] INFO [MergedLog partition=_confluent-link-metadata-12, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:05:46,874] INFO Created log for partition _confluent-link-metadata-12 in /tmp/kraft-combined-logs/_confluent-link-metadata-12 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:05:46,874] INFO [Partition _confluent-link-metadata-12 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-12 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,874] INFO [Partition _confluent-link-metadata-12 broker=1] Log loaded for partition _confluent-link-metadata-12 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:46,874] INFO Setting topicIdPartition FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-12 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:05:46,874] INFO [MergedLog partition=_confluent-link-metadata-12, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-12 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:05:46,874] INFO [Broker id=1] Leader _confluent-link-metadata-12 with topic id Some(FgiypwCwTt68SpYw3T9ahw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:46,879] INFO [DynamicConfigPublisher broker id=1] Updating topic _confluent-link-metadata with new configuration : cleanup.policy -> compact,min.insync.replicas -> 2 (kafka.server.metadata.DynamicConfigPublisher)
benchi-kafka     | [2025-03-18 16:05:46,883] INFO DefaultSessionIdManager workerName=node0 (org.eclipse.jetty.server.session)
benchi-kafka     | [2025-03-18 16:05:46,883] INFO No SessionScavenger set, using defaults (org.eclipse.jetty.server.session)
benchi-kafka     | [2025-03-18 16:05:46,884] INFO node0 Scavenging every 660000ms (org.eclipse.jetty.server.session)
benchi-kafka     | [2025-03-18 16:05:46,900] INFO [Producer clientId=confluent-metrics-reporter] Cluster ID: MkU3OEVBNTcwNTJENDM2Qk (org.apache.kafka.clients.Metadata)
benchi-kafka     | [2025-03-18 16:05:46,944] INFO [Producer clientId=confluent-telemetry-reporter-local-producer] Cluster ID: MkU3OEVBNTcwNTJENDM2Qk (org.apache.kafka.clients.Metadata)
benchi-kafka     | Mar 18, 2025 4:05:46 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
benchi-kafka     | WARNING: A provider io.confluent.metadataapi.resources.MetadataResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.metadataapi.resources.MetadataResource will be ignored. 
benchi-kafka     | [2025-03-18 16:05:47,034] INFO HV000001: Hibernate Validator 6.1.7.Final (org.hibernate.validator.internal.util.Version)
benchi-kafka     | [2025-03-18 16:05:47,115] INFO Started o.e.j.s.ServletContextHandler@115ece1c{/v1/metadata,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler)
benchi-kafka     | [2025-03-18 16:05:47,129] WARN Using default value http://localhost:8081 for config schema.registry.url. In a future release this config won't have a default value anymore. If you are using Schema Registry, please, specify schema.registry.url explicitly. Requests will fail in a future release if you try to use Schema Registry but have not specified a value for schema.registry.url. An empty value for this property means that the Schema Registry is disabled. (io.confluent.kafkarest.KafkaRestConfig)
benchi-kafka     | [2025-03-18 16:05:47,130] WARN Using default value http://localhost:8081 for config schema.registry.url. In a future release this config won't have a default value anymore. If you are using Schema Registry, please, specify schema.registry.url explicitly. Requests will fail in a future release if you try to use Schema Registry but have not specified a value for schema.registry.url. An empty value for this property means that the Schema Registry is disabled. (io.confluent.kafkarest.KafkaRestConfig)
benchi-kafka     | [2025-03-18 16:05:47,134] WARN Using default value http://localhost:8081 for config schema.registry.url. In a future release this config won't have a default value anymore. If you are using Schema Registry, please, specify schema.registry.url explicitly. Requests will fail in a future release if you try to use Schema Registry but have not specified a value for schema.registry.url. An empty value for this property means that the Schema Registry is disabled. (io.confluent.kafkarest.KafkaRestConfig)
benchi-kafka     | [2025-03-18 16:05:47,138] WARN Using default value http://localhost:8081 for config schema.registry.url. In a future release this config won't have a default value anymore. If you are using Schema Registry, please, specify schema.registry.url explicitly. Requests will fail in a future release if you try to use Schema Registry but have not specified a value for schema.registry.url. An empty value for this property means that the Schema Registry is disabled. (io.confluent.kafkarest.KafkaRestConfig)
benchi-kafka     | [2025-03-18 16:05:47,325] INFO Started o.e.j.s.ServletContextHandler@73d44d87{/kafka,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler)
benchi-kafka     | [2025-03-18 16:05:47,331] INFO Started o.e.j.s.ServletContextHandler@1d59abc{/ws,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler)
benchi-kafka     | [2025-03-18 16:05:47,331] INFO Started o.e.j.s.ServletContextHandler@46ee174c{/ws,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler)
benchi-kafka     | [2025-03-18 16:05:47,338] INFO Started NetworkTrafficServerConnector@77e1dacd{HTTP/1.1, (http/1.1, h2c)}{0.0.0.0:8090} (org.eclipse.jetty.server.AbstractConnector)
benchi-kafka     | [2025-03-18 16:05:47,338] INFO Started @2452ms (org.eclipse.jetty.server.Server)
benchi-kafka     | [2025-03-18 16:05:47,338] INFO KafkaHttpServer transitioned from STARTING to RUNNING.. (io.confluent.http.server.KafkaHttpServerImpl)
benchi-kafka     | [2025-03-18 16:05:47,343] INFO LicenseConfig values: 
benchi-kafka     | 	confluent.license = [hidden]
benchi-kafka     | 	confluent.license.retry.backoff.max.ms = 100000
benchi-kafka     | 	confluent.license.retry.backoff.min.ms = 1000
benchi-kafka     | 	confluent.license.topic = _confluent-license
benchi-kafka     | 	confluent.license.topic.create.timeout.ms = 600000
benchi-kafka     | 	confluent.license.topic.replication.factor = 1
benchi-kafka     |  (io.confluent.license.validator.LicenseConfig)
benchi-kafka     | [2025-03-18 16:05:47,343] INFO LicenseConfig values: 
benchi-kafka     | 	confluent.license = [hidden]
benchi-kafka     | 	confluent.license.retry.backoff.max.ms = 100000
benchi-kafka     | 	confluent.license.retry.backoff.min.ms = 1000
benchi-kafka     | 	confluent.license.topic = _confluent-license
benchi-kafka     | 	confluent.license.topic.create.timeout.ms = 600000
benchi-kafka     | 	confluent.license.topic.replication.factor = 1
benchi-kafka     |  (io.confluent.license.validator.LicenseConfig)
benchi-kafka     | [2025-03-18 16:05:47,350] INFO AdminClientConfig values: 
benchi-kafka     | 	auto.include.jmx.reporter = true
benchi-kafka     | 	bootstrap.controllers = []
benchi-kafka     | 	bootstrap.servers = [broker:29092]
benchi-kafka     | 	client.dns.lookup = use_all_dns_ips
benchi-kafka     | 	client.id = _confluent-license-admin-1
benchi-kafka     | 	confluent.lkc.id = null
benchi-kafka     | 	confluent.metrics.reporter.bootstrap.servers = localhost:9092
benchi-kafka     | 	confluent.proxy.protocol.client.address = null
benchi-kafka     | 	confluent.proxy.protocol.client.mode = PROXY
benchi-kafka     | 	confluent.proxy.protocol.client.port = null
benchi-kafka     | 	confluent.proxy.protocol.client.version = NONE
benchi-kafka     | 	connections.max.idle.ms = 300000
benchi-kafka     | 	default.api.timeout.ms = 60000
benchi-kafka     | 	enable.metrics.push = true
benchi-kafka     | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
benchi-kafka     | 	metadata.max.age.ms = 300000
benchi-kafka     | 	metadata.recovery.strategy = none
benchi-kafka     | 	metric.reporters = []
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.recording.level = INFO
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	receive.buffer.bytes = 65536
benchi-kafka     | 	reconnect.backoff.max.ms = 1000
benchi-kafka     | 	reconnect.backoff.ms = 50
benchi-kafka     | 	request.timeout.ms = 30000
benchi-kafka     | 	retries = 2147483647
benchi-kafka     | 	retry.backoff.max.ms = 1000
benchi-kafka     | 	retry.backoff.ms = 100
benchi-kafka     | 	sasl.client.callback.handler.class = null
benchi-kafka     | 	sasl.jaas.config = null
benchi-kafka     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
benchi-kafka     | 	sasl.kerberos.min.time.before.relogin = 60000
benchi-kafka     | 	sasl.kerberos.service.name = null
benchi-kafka     | 	sasl.kerberos.ticket.renew.jitter = 0.05
benchi-kafka     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
benchi-kafka     | 	sasl.login.callback.handler.class = null
benchi-kafka     | 	sasl.login.class = null
benchi-kafka     | 	sasl.login.connect.timeout.ms = null
benchi-kafka     | 	sasl.login.read.timeout.ms = null
benchi-kafka     | 	sasl.login.refresh.buffer.seconds = 300
benchi-kafka     | 	sasl.login.refresh.min.period.seconds = 60
benchi-kafka     | 	sasl.login.refresh.window.factor = 0.8
benchi-kafka     | 	sasl.login.refresh.window.jitter = 0.05
benchi-kafka     | 	sasl.login.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.login.retry.backoff.ms = 100
benchi-kafka     | 	sasl.mechanism = GSSAPI
benchi-kafka     | 	sasl.oauthbearer.clientassertion.audience = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.expiration = 5
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.jti.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.nbf.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.issuer = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.location = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key.passphrase = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.subject = null
benchi-kafka     | 	sasl.oauthbearer.clock.skew.seconds = 30
benchi-kafka     | 	sasl.oauthbearer.expected.audience = null
benchi-kafka     | 	sasl.oauthbearer.expected.issuer = null
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.url = null
benchi-kafka     | 	sasl.oauthbearer.scope.claim.name = scope
benchi-kafka     | 	sasl.oauthbearer.sub.claim.name = sub
benchi-kafka     | 	sasl.oauthbearer.token.endpoint.url = null
benchi-kafka     | 	security.protocol = PLAINTEXT
benchi-kafka     | 	security.providers = null
benchi-kafka     | 	send.buffer.bytes = 131072
benchi-kafka     | 	socket.connection.setup.timeout.max.ms = 30000
benchi-kafka     | 	socket.connection.setup.timeout.ms = 10000
benchi-kafka     | 	ssl.cipher.suites = null
benchi-kafka     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	ssl.engine.factory.class = null
benchi-kafka     | 	ssl.key.password = null
benchi-kafka     | 	ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	ssl.keystore.certificate.chain = null
benchi-kafka     | 	ssl.keystore.key = null
benchi-kafka     | 	ssl.keystore.location = null
benchi-kafka     | 	ssl.keystore.password = null
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.protocol = TLSv1.3
benchi-kafka     | 	ssl.provider = null
benchi-kafka     | 	ssl.secure.random.implementation = null
benchi-kafka     | 	ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	ssl.truststore.certificates = null
benchi-kafka     | 	ssl.truststore.location = null
benchi-kafka     | 	ssl.truststore.password = null
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     |  (org.apache.kafka.clients.admin.AdminClientConfig)
benchi-kafka     | [2025-03-18 16:05:47,351] INFO These configurations '[confluent.metrics.topic.replicas, replication.factor, confluent.metrics.reporter.bootstrap.servers, jmx.hostname, min.insync.replicas, cluster.link.metadata.topic.replication.factor, jmx.port]' were supplied but are not used yet. (org.apache.kafka.clients.admin.AdminClientConfig)
benchi-kafka     | [2025-03-18 16:05:47,351] INFO Kafka version: 7.8.1-ce (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:05:47,351] INFO Kafka commitId: eefb51c6f2f1b888b8b03296353338afbac8e213 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:05:47,351] INFO Kafka startTimeMs: 1742313947351 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:05:47,358] INFO [AdminClient clientId=_confluent-license-admin-1] The DescribeTopicPartitions API is not supported, using Metadata API to describe topics. (org.apache.kafka.clients.admin.KafkaAdminClient)
benchi-kafka     | [2025-03-18 16:05:47,361] INFO App info kafka.admin.client for _confluent-license-admin-1 unregistered (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:05:47,365] INFO Starting License Store (io.confluent.license.LicenseStore)
benchi-kafka     | [2025-03-18 16:05:47,365] INFO Starting KafkaBasedLog with topic _confluent-command reportErrorsToCallback=false (org.apache.kafka.connect.util.KafkaBasedLog)
benchi-kafka     | [2025-03-18 16:05:47,365] INFO AdminClientConfig values: 
benchi-kafka     | 	auto.include.jmx.reporter = true
benchi-kafka     | 	bootstrap.controllers = []
benchi-kafka     | 	bootstrap.servers = [broker:29092]
benchi-kafka     | 	client.dns.lookup = use_all_dns_ips
benchi-kafka     | 	client.id = _confluent-license-admin-1
benchi-kafka     | 	confluent.lkc.id = null
benchi-kafka     | 	confluent.metrics.reporter.bootstrap.servers = localhost:9092
benchi-kafka     | 	confluent.proxy.protocol.client.address = null
benchi-kafka     | 	confluent.proxy.protocol.client.mode = PROXY
benchi-kafka     | 	confluent.proxy.protocol.client.port = null
benchi-kafka     | 	confluent.proxy.protocol.client.version = NONE
benchi-kafka     | 	connections.max.idle.ms = 300000
benchi-kafka     | 	default.api.timeout.ms = 60000
benchi-kafka     | 	enable.metrics.push = true
benchi-kafka     | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
benchi-kafka     | 	metadata.max.age.ms = 300000
benchi-kafka     | 	metadata.recovery.strategy = none
benchi-kafka     | 	metric.reporters = []
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.recording.level = INFO
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	receive.buffer.bytes = 65536
benchi-kafka     | 	reconnect.backoff.max.ms = 1000
benchi-kafka     | 	reconnect.backoff.ms = 50
benchi-kafka     | 	request.timeout.ms = 30000
benchi-kafka     | 	retries = 2147483647
benchi-kafka     | 	retry.backoff.max.ms = 1000
benchi-kafka     | 	retry.backoff.ms = 100
benchi-kafka     | 	sasl.client.callback.handler.class = null
benchi-kafka     | 	sasl.jaas.config = null
benchi-kafka     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
benchi-kafka     | 	sasl.kerberos.min.time.before.relogin = 60000
benchi-kafka     | 	sasl.kerberos.service.name = null
benchi-kafka     | 	sasl.kerberos.ticket.renew.jitter = 0.05
benchi-kafka     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
benchi-kafka     | 	sasl.login.callback.handler.class = null
benchi-kafka     | 	sasl.login.class = null
benchi-kafka     | 	sasl.login.connect.timeout.ms = null
benchi-kafka     | 	sasl.login.read.timeout.ms = null
benchi-kafka     | 	sasl.login.refresh.buffer.seconds = 300
benchi-kafka     | 	sasl.login.refresh.min.period.seconds = 60
benchi-kafka     | 	sasl.login.refresh.window.factor = 0.8
benchi-kafka     | 	sasl.login.refresh.window.jitter = 0.05
benchi-kafka     | 	sasl.login.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.login.retry.backoff.ms = 100
benchi-kafka     | 	sasl.mechanism = GSSAPI
benchi-kafka     | 	sasl.oauthbearer.clientassertion.audience = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.expiration = 5
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.jti.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.nbf.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.issuer = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.location = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key.passphrase = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.subject = null
benchi-kafka     | 	sasl.oauthbearer.clock.skew.seconds = 30
benchi-kafka     | 	sasl.oauthbearer.expected.audience = null
benchi-kafka     | 	sasl.oauthbearer.expected.issuer = null
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.url = null
benchi-kafka     | 	sasl.oauthbearer.scope.claim.name = scope
benchi-kafka     | 	sasl.oauthbearer.sub.claim.name = sub
benchi-kafka     | 	sasl.oauthbearer.token.endpoint.url = null
benchi-kafka     | 	security.protocol = PLAINTEXT
benchi-kafka     | 	security.providers = null
benchi-kafka     | 	send.buffer.bytes = 131072
benchi-kafka     | 	socket.connection.setup.timeout.max.ms = 30000
benchi-kafka     | 	socket.connection.setup.timeout.ms = 10000
benchi-kafka     | 	ssl.cipher.suites = null
benchi-kafka     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	ssl.engine.factory.class = null
benchi-kafka     | 	ssl.key.password = null
benchi-kafka     | 	ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	ssl.keystore.certificate.chain = null
benchi-kafka     | 	ssl.keystore.key = null
benchi-kafka     | 	ssl.keystore.location = null
benchi-kafka     | 	ssl.keystore.password = null
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.protocol = TLSv1.3
benchi-kafka     | 	ssl.provider = null
benchi-kafka     | 	ssl.secure.random.implementation = null
benchi-kafka     | 	ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	ssl.truststore.certificates = null
benchi-kafka     | 	ssl.truststore.location = null
benchi-kafka     | 	ssl.truststore.password = null
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     |  (org.apache.kafka.clients.admin.AdminClientConfig)
benchi-kafka     | [2025-03-18 16:05:47,366] INFO These configurations '[confluent.metrics.topic.replicas, replication.factor, confluent.metrics.reporter.bootstrap.servers, jmx.hostname, min.insync.replicas, cluster.link.metadata.topic.replication.factor, jmx.port]' were supplied but are not used yet. (org.apache.kafka.clients.admin.AdminClientConfig)
benchi-kafka     | [2025-03-18 16:05:47,366] INFO Kafka version: 7.8.1-ce (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:05:47,366] INFO Kafka commitId: eefb51c6f2f1b888b8b03296353338afbac8e213 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:05:47,366] INFO Kafka startTimeMs: 1742313947366 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:05:47,370] INFO [AdminClient clientId=_confluent-license-admin-1] The DescribeTopicPartitions API is not supported, using Metadata API to describe topics. (org.apache.kafka.clients.admin.KafkaAdminClient)
benchi-kafka     | [2025-03-18 16:05:47,373] INFO [ControllerServer id=1] CreateTopics result(s): CreatableTopic(name='_confluent-command', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact'), CreateableTopicConfig(name='min.insync.replicas', value='1')], linkName=null, mirrorTopic=null, sourceTopicId=AAAAAAAAAAAAAAAAAAAAAA, mirrorStartOffsetSpec=-9223372036854775808, mirrorStartOffsets=[], stoppedSequenceNumber=0): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:05:47,373] INFO [ControllerServer id=1] Replayed TopicRecord for topic _confluent-command with topic ID _WH1I7T7TFuBhdihyIdWew. (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:05:47,373] INFO [ControllerServer id=1] Replayed ConfigRecord for ConfigResource(type=TOPIC, name='_confluent-command') which set configuration cleanup.policy to compact (org.apache.kafka.controller.ConfigurationControlManager)
benchi-kafka     | [2025-03-18 16:05:47,373] INFO [ControllerServer id=1] Replayed ConfigRecord for ConfigResource(type=TOPIC, name='_confluent-command') which set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
benchi-kafka     | [2025-03-18 16:05:47,373] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-command-0 with topic ID _WH1I7T7TFuBhdihyIdWew and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:05:47,381] INFO SBC Event SbcMetadataUpdateEvent-14 generated 1 more events to enqueue in the following order - [SbcConfigUpdateEvent-15]. Enqueuing... (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:05:47,381] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:47,381] INFO Handling event SbcConfigUpdateEvent-15 (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:05:47,381] INFO Balancer notified of a config change: ConfigurationsDelta(changes={ConfigResource(type=TOPIC, name='_confluent-command')=ConfigurationDelta(changedKeys=[cleanup.policy, min.insync.replicas])}) (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:05:47,381] INFO There were 0 change(s) and 0 deletion(s) to balancer configs. Changed Configs: {}, Deleted Configs: [] (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:05:47,381] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-command-0) (kafka.server.ReplicaFetcherManager)
benchi-kafka     | [2025-03-18 16:05:47,381] INFO [Broker id=1] Creating new partition _confluent-command-0 with topic id _WH1I7T7TFuBhdihyIdWew. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:47,381] INFO [Broker id=1] Stopped fetchers as part of become-leader transition for 1 partitions (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:47,382] INFO App info kafka.admin.client for _confluent-license-admin-1 unregistered (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:05:47,383] INFO ConsumerConfig values: 
benchi-kafka     | 	allow.auto.create.topics = true
benchi-kafka     | 	auto.commit.interval.ms = 5000
benchi-kafka     | 	auto.include.jmx.reporter = true
benchi-kafka     | 	auto.offset.reset = latest
benchi-kafka     | 	bootstrap.servers = [broker:29092]
benchi-kafka     | 	check.crcs = true
benchi-kafka     | 	client.dns.lookup = use_all_dns_ips
benchi-kafka     | 	client.id = _confluent-license-consumer-1
benchi-kafka     | 	client.rack = 
benchi-kafka     | 	confluent.lkc.id = null
benchi-kafka     | 	confluent.proxy.protocol.client.address = null
benchi-kafka     | 	confluent.proxy.protocol.client.mode = PROXY
benchi-kafka     | 	confluent.proxy.protocol.client.port = null
benchi-kafka     | 	confluent.proxy.protocol.client.version = NONE
benchi-kafka     | 	connections.max.idle.ms = 540000
benchi-kafka     | 	default.api.timeout.ms = 60000
benchi-kafka     | 	enable.auto.commit = false
benchi-kafka     | 	enable.metrics.push = true
benchi-kafka     | 	exclude.internal.topics = true
benchi-kafka     | 	fetch.max.bytes = 52428800
benchi-kafka     | 	fetch.max.wait.ms = 500
benchi-kafka     | 	fetch.min.bytes = 1
benchi-kafka     | 	group.id = null
benchi-kafka     | 	group.instance.id = null
benchi-kafka     | 	group.protocol = classic
benchi-kafka     | 	group.remote.assignor = null
benchi-kafka     | 	heartbeat.interval.ms = 3000
benchi-kafka     | 	interceptor.classes = []
benchi-kafka     | 	internal.leave.group.on.close = true
benchi-kafka     | 	internal.throw.on.fetch.stable.offset.unsupported = false
benchi-kafka     | 	isolation.level = read_uncommitted
benchi-kafka     | 	key.deserializer = class io.confluent.license.LicenseStore$LicenseKeySerde
benchi-kafka     | 	max.partition.fetch.bytes = 1048576
benchi-kafka     | 	max.poll.interval.ms = 300000
benchi-kafka     | 	max.poll.records = 500
benchi-kafka     | 	metadata.max.age.ms = 300000
benchi-kafka     | 	metadata.recovery.strategy = none
benchi-kafka     | 	metric.reporters = []
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.recording.level = INFO
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
benchi-kafka     | 	receive.buffer.bytes = 65536
benchi-kafka     | 	reconnect.backoff.max.ms = 1000
benchi-kafka     | 	reconnect.backoff.ms = 50
benchi-kafka     | 	request.timeout.ms = 120000
benchi-kafka     | 	retry.backoff.max.ms = 1000
benchi-kafka     | 	retry.backoff.ms = 100
benchi-kafka     | 	sasl.client.callback.handler.class = null
benchi-kafka     | 	sasl.jaas.config = null
benchi-kafka     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
benchi-kafka     | 	sasl.kerberos.min.time.before.relogin = 60000
benchi-kafka     | 	sasl.kerberos.service.name = null
benchi-kafka     | 	sasl.kerberos.ticket.renew.jitter = 0.05
benchi-kafka     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
benchi-kafka     | 	sasl.login.callback.handler.class = null
benchi-kafka     | 	sasl.login.class = null
benchi-kafka     | 	sasl.login.connect.timeout.ms = null
benchi-kafka     | 	sasl.login.read.timeout.ms = null
benchi-kafka     | 	sasl.login.refresh.buffer.seconds = 300
benchi-kafka     | 	sasl.login.refresh.min.period.seconds = 60
benchi-kafka     | 	sasl.login.refresh.window.factor = 0.8
benchi-kafka     | 	sasl.login.refresh.window.jitter = 0.05
benchi-kafka     | 	sasl.login.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.login.retry.backoff.ms = 100
benchi-kafka     | 	sasl.mechanism = GSSAPI
benchi-kafka     | 	sasl.oauthbearer.clientassertion.audience = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.expiration = 5
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.jti.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.nbf.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.issuer = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.location = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key.passphrase = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.subject = null
benchi-kafka     | 	sasl.oauthbearer.clock.skew.seconds = 30
benchi-kafka     | 	sasl.oauthbearer.expected.audience = null
benchi-kafka     | 	sasl.oauthbearer.expected.issuer = null
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.url = null
benchi-kafka     | 	sasl.oauthbearer.scope.claim.name = scope
benchi-kafka     | 	sasl.oauthbearer.sub.claim.name = sub
benchi-kafka     | 	sasl.oauthbearer.token.endpoint.url = null
benchi-kafka     | 	security.protocol = PLAINTEXT
benchi-kafka     | 	security.providers = null
benchi-kafka     | 	send.buffer.bytes = 131072
benchi-kafka     | 	session.timeout.ms = 45000
benchi-kafka     | 	socket.connection.setup.timeout.max.ms = 30000
benchi-kafka     | 	socket.connection.setup.timeout.ms = 10000
benchi-kafka     | 	ssl.cipher.suites = null
benchi-kafka     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	ssl.engine.factory.class = null
benchi-kafka     | 	ssl.key.password = null
benchi-kafka     | 	ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	ssl.keystore.certificate.chain = null
benchi-kafka     | 	ssl.keystore.key = null
benchi-kafka     | 	ssl.keystore.location = null
benchi-kafka     | 	ssl.keystore.password = null
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.protocol = TLSv1.3
benchi-kafka     | 	ssl.provider = null
benchi-kafka     | 	ssl.secure.random.implementation = null
benchi-kafka     | 	ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	ssl.truststore.certificates = null
benchi-kafka     | 	ssl.truststore.location = null
benchi-kafka     | 	ssl.truststore.password = null
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     | 	value.deserializer = class io.confluent.license.LicenseStore$LicenseMessageSerde
benchi-kafka     |  (org.apache.kafka.clients.consumer.ConsumerConfig)
benchi-kafka     | [2025-03-18 16:05:47,383] INFO initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector)
benchi-kafka     | [2025-03-18 16:05:47,383] INFO [MergedLog partition=_confluent-command-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:05:47,383] INFO Created log for partition _confluent-command-0 in /tmp/kraft-combined-logs/_confluent-command-0 with properties {cleanup.policy=compact, min.insync.replicas=1} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:05:47,383] INFO [Partition _confluent-command-0 broker=1] No checkpointed highwatermark is found for partition _confluent-command-0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:47,383] INFO [Partition _confluent-command-0 broker=1] Log loaded for partition _confluent-command-0 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:05:47,384] INFO Setting topicIdPartition _WH1I7T7TFuBhdihyIdWew:_confluent-command-0 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:05:47,384] INFO [MergedLog partition=_confluent-command-0, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-command-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:05:47,384] INFO [Broker id=1] Leader _confluent-command-0 with topic id Some(_WH1I7T7TFuBhdihyIdWew) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:05:47,384] INFO [DynamicConfigPublisher broker id=1] Updating topic _confluent-command with new configuration : cleanup.policy -> compact,min.insync.replicas -> 1 (kafka.server.metadata.DynamicConfigPublisher)
benchi-kafka     | [2025-03-18 16:05:47,385] INFO These configurations '[confluent.metrics.topic.replicas, confluent.metrics.reporter.bootstrap.servers, jmx.hostname, cluster.link.metadata.topic.replication.factor, jmx.port]' were supplied but are not used yet. (org.apache.kafka.clients.consumer.ConsumerConfig)
benchi-kafka     | [2025-03-18 16:05:47,385] INFO Kafka version: 7.8.1-ce (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:05:47,385] INFO Kafka commitId: eefb51c6f2f1b888b8b03296353338afbac8e213 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:05:47,385] INFO Kafka startTimeMs: 1742313947385 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:05:47,387] INFO [Consumer clientId=_confluent-license-consumer-1, groupId=null] Cluster ID: MkU3OEVBNTcwNTJENDM2Qk (org.apache.kafka.clients.Metadata)
benchi-kafka     | [2025-03-18 16:05:47,391] INFO App info kafka.consumer for _confluent-license-consumer-1 unregistered (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:05:47,391] INFO ProducerConfig values: 
benchi-kafka     | 	acks = -1
benchi-kafka     | 	auto.include.jmx.reporter = true
benchi-kafka     | 	batch.size = 16384
benchi-kafka     | 	bootstrap.servers = [broker:29092]
benchi-kafka     | 	buffer.memory = 33554432
benchi-kafka     | 	client.dns.lookup = use_all_dns_ips
benchi-kafka     | 	client.id = _confluent-license-producer-1
benchi-kafka     | 	compression.gzip.level = -1
benchi-kafka     | 	compression.lz4.level = 9
benchi-kafka     | 	compression.type = none
benchi-kafka     | 	compression.zstd.level = 3
benchi-kafka     | 	confluent.lkc.id = null
benchi-kafka     | 	confluent.proxy.protocol.client.address = null
benchi-kafka     | 	confluent.proxy.protocol.client.mode = PROXY
benchi-kafka     | 	confluent.proxy.protocol.client.port = null
benchi-kafka     | 	confluent.proxy.protocol.client.version = NONE
benchi-kafka     | 	connections.max.idle.ms = 540000
benchi-kafka     | 	delivery.timeout.ms = 120000
benchi-kafka     | 	enable.idempotence = false
benchi-kafka     | 	enable.metrics.push = true
benchi-kafka     | 	interceptor.classes = []
benchi-kafka     | 	key.serializer = class io.confluent.license.LicenseStore$LicenseKeySerde
benchi-kafka     | 	linger.ms = 0
benchi-kafka     | 	max.block.ms = 60000
benchi-kafka     | 	max.in.flight.requests.per.connection = 1
benchi-kafka     | 	max.request.size = 1048576
benchi-kafka     | 	metadata.max.age.ms = 300000
benchi-kafka     | 	metadata.max.idle.ms = 300000
benchi-kafka     | 	metadata.recovery.strategy = none
benchi-kafka     | 	metric.reporters = []
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.recording.level = INFO
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	partitioner.adaptive.partitioning.enable = true
benchi-kafka     | 	partitioner.availability.timeout.ms = 0
benchi-kafka     | 	partitioner.class = null
benchi-kafka     | 	partitioner.ignore.keys = false
benchi-kafka     | 	receive.buffer.bytes = 32768
benchi-kafka     | 	reconnect.backoff.max.ms = 1000
benchi-kafka     | 	reconnect.backoff.ms = 50
benchi-kafka     | 	request.timeout.ms = 30000
benchi-kafka     | 	retries = 2147483647
benchi-kafka     | 	retry.backoff.max.ms = 1000
benchi-kafka     | 	retry.backoff.ms = 100
benchi-kafka     | 	sasl.client.callback.handler.class = null
benchi-kafka     | 	sasl.jaas.config = null
benchi-kafka     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
benchi-kafka     | 	sasl.kerberos.min.time.before.relogin = 60000
benchi-kafka     | 	sasl.kerberos.service.name = null
benchi-kafka     | 	sasl.kerberos.ticket.renew.jitter = 0.05
benchi-kafka     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
benchi-kafka     | 	sasl.login.callback.handler.class = null
benchi-kafka     | 	sasl.login.class = null
benchi-kafka     | 	sasl.login.connect.timeout.ms = null
benchi-kafka     | 	sasl.login.read.timeout.ms = null
benchi-kafka     | 	sasl.login.refresh.buffer.seconds = 300
benchi-kafka     | 	sasl.login.refresh.min.period.seconds = 60
benchi-kafka     | 	sasl.login.refresh.window.factor = 0.8
benchi-kafka     | 	sasl.login.refresh.window.jitter = 0.05
benchi-kafka     | 	sasl.login.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.login.retry.backoff.ms = 100
benchi-kafka     | 	sasl.mechanism = GSSAPI
benchi-kafka     | 	sasl.oauthbearer.clientassertion.audience = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.expiration = 5
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.jti.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.nbf.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.issuer = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.location = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key.passphrase = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.subject = null
benchi-kafka     | 	sasl.oauthbearer.clock.skew.seconds = 30
benchi-kafka     | 	sasl.oauthbearer.expected.audience = null
benchi-kafka     | 	sasl.oauthbearer.expected.issuer = null
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.url = null
benchi-kafka     | 	sasl.oauthbearer.scope.claim.name = scope
benchi-kafka     | 	sasl.oauthbearer.sub.claim.name = sub
benchi-kafka     | 	sasl.oauthbearer.token.endpoint.url = null
benchi-kafka     | 	security.protocol = PLAINTEXT
benchi-kafka     | 	security.providers = null
benchi-kafka     | 	send.buffer.bytes = 131072
benchi-kafka     | 	socket.connection.setup.timeout.max.ms = 30000
benchi-kafka     | 	socket.connection.setup.timeout.ms = 10000
benchi-kafka     | 	ssl.cipher.suites = null
benchi-kafka     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	ssl.engine.factory.class = null
benchi-kafka     | 	ssl.key.password = null
benchi-kafka     | 	ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	ssl.keystore.certificate.chain = null
benchi-kafka     | 	ssl.keystore.key = null
benchi-kafka     | 	ssl.keystore.location = null
benchi-kafka     | 	ssl.keystore.password = null
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.protocol = TLSv1.3
benchi-kafka     | 	ssl.provider = null
benchi-kafka     | 	ssl.secure.random.implementation = null
benchi-kafka     | 	ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	ssl.truststore.certificates = null
benchi-kafka     | 	ssl.truststore.location = null
benchi-kafka     | 	ssl.truststore.password = null
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     | 	transaction.timeout.ms = 60000
benchi-kafka     | 	transactional.id = null
benchi-kafka     | 	value.serializer = class io.confluent.license.LicenseStore$LicenseMessageSerde
benchi-kafka     |  (org.apache.kafka.clients.producer.ProducerConfig)
benchi-kafka     | [2025-03-18 16:05:47,392] INFO initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector)
benchi-kafka     | [2025-03-18 16:05:47,392] INFO These configurations '[confluent.metrics.topic.replicas, confluent.metrics.reporter.bootstrap.servers, jmx.hostname, cluster.link.metadata.topic.replication.factor, jmx.port]' were supplied but are not used yet. (org.apache.kafka.clients.producer.ProducerConfig)
benchi-kafka     | [2025-03-18 16:05:47,393] INFO Kafka version: 7.8.1-ce (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:05:47,393] INFO Kafka commitId: eefb51c6f2f1b888b8b03296353338afbac8e213 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:05:47,393] INFO Kafka startTimeMs: 1742313947393 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:05:47,393] INFO ConsumerConfig values: 
benchi-kafka     | 	allow.auto.create.topics = true
benchi-kafka     | 	auto.commit.interval.ms = 5000
benchi-kafka     | 	auto.include.jmx.reporter = true
benchi-kafka     | 	auto.offset.reset = earliest
benchi-kafka     | 	bootstrap.servers = [broker:29092]
benchi-kafka     | 	check.crcs = true
benchi-kafka     | 	client.dns.lookup = use_all_dns_ips
benchi-kafka     | 	client.id = _confluent-license-consumer-1
benchi-kafka     | 	client.rack = 
benchi-kafka     | 	confluent.lkc.id = null
benchi-kafka     | 	confluent.proxy.protocol.client.address = null
benchi-kafka     | 	confluent.proxy.protocol.client.mode = PROXY
benchi-kafka     | 	confluent.proxy.protocol.client.port = null
benchi-kafka     | 	confluent.proxy.protocol.client.version = NONE
benchi-kafka     | 	connections.max.idle.ms = 540000
benchi-kafka     | 	default.api.timeout.ms = 60000
benchi-kafka     | 	enable.auto.commit = false
benchi-kafka     | 	enable.metrics.push = true
benchi-kafka     | 	exclude.internal.topics = true
benchi-kafka     | 	fetch.max.bytes = 52428800
benchi-kafka     | 	fetch.max.wait.ms = 500
benchi-kafka     | 	fetch.min.bytes = 1
benchi-kafka     | 	group.id = null
benchi-kafka     | 	group.instance.id = null
benchi-kafka     | 	group.protocol = classic
benchi-kafka     | 	group.remote.assignor = null
benchi-kafka     | 	heartbeat.interval.ms = 3000
benchi-kafka     | 	interceptor.classes = []
benchi-kafka     | 	internal.leave.group.on.close = true
benchi-kafka     | 	internal.throw.on.fetch.stable.offset.unsupported = false
benchi-kafka     | 	isolation.level = read_uncommitted
benchi-kafka     | 	key.deserializer = class io.confluent.license.LicenseStore$LicenseKeySerde
benchi-kafka     | 	max.partition.fetch.bytes = 1048576
benchi-kafka     | 	max.poll.interval.ms = 300000
benchi-kafka     | 	max.poll.records = 500
benchi-kafka     | 	metadata.max.age.ms = 300000
benchi-kafka     | 	metadata.recovery.strategy = none
benchi-kafka     | 	metric.reporters = []
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.recording.level = INFO
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
benchi-kafka     | 	receive.buffer.bytes = 65536
benchi-kafka     | 	reconnect.backoff.max.ms = 1000
benchi-kafka     | 	reconnect.backoff.ms = 50
benchi-kafka     | 	request.timeout.ms = 120000
benchi-kafka     | 	retry.backoff.max.ms = 1000
benchi-kafka     | 	retry.backoff.ms = 100
benchi-kafka     | 	sasl.client.callback.handler.class = null
benchi-kafka     | 	sasl.jaas.config = null
benchi-kafka     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
benchi-kafka     | 	sasl.kerberos.min.time.before.relogin = 60000
benchi-kafka     | 	sasl.kerberos.service.name = null
benchi-kafka     | 	sasl.kerberos.ticket.renew.jitter = 0.05
benchi-kafka     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
benchi-kafka     | 	sasl.login.callback.handler.class = null
benchi-kafka     | 	sasl.login.class = null
benchi-kafka     | 	sasl.login.connect.timeout.ms = null
benchi-kafka     | 	sasl.login.read.timeout.ms = null
benchi-kafka     | 	sasl.login.refresh.buffer.seconds = 300
benchi-kafka     | 	sasl.login.refresh.min.period.seconds = 60
benchi-kafka     | 	sasl.login.refresh.window.factor = 0.8
benchi-kafka     | 	sasl.login.refresh.window.jitter = 0.05
benchi-kafka     | 	sasl.login.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.login.retry.backoff.ms = 100
benchi-kafka     | 	sasl.mechanism = GSSAPI
benchi-kafka     | 	sasl.oauthbearer.clientassertion.audience = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.expiration = 5
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.jti.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.nbf.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.issuer = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.location = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key.passphrase = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.subject = null
benchi-kafka     | 	sasl.oauthbearer.clock.skew.seconds = 30
benchi-kafka     | 	sasl.oauthbearer.expected.audience = null
benchi-kafka     | 	sasl.oauthbearer.expected.issuer = null
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.url = null
benchi-kafka     | 	sasl.oauthbearer.scope.claim.name = scope
benchi-kafka     | 	sasl.oauthbearer.sub.claim.name = sub
benchi-kafka     | 	sasl.oauthbearer.token.endpoint.url = null
benchi-kafka     | 	security.protocol = PLAINTEXT
benchi-kafka     | 	security.providers = null
benchi-kafka     | 	send.buffer.bytes = 131072
benchi-kafka     | 	session.timeout.ms = 45000
benchi-kafka     | 	socket.connection.setup.timeout.max.ms = 30000
benchi-kafka     | 	socket.connection.setup.timeout.ms = 10000
benchi-kafka     | 	ssl.cipher.suites = null
benchi-kafka     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	ssl.engine.factory.class = null
benchi-kafka     | 	ssl.key.password = null
benchi-kafka     | 	ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	ssl.keystore.certificate.chain = null
benchi-kafka     | 	ssl.keystore.key = null
benchi-kafka     | 	ssl.keystore.location = null
benchi-kafka     | 	ssl.keystore.password = null
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.protocol = TLSv1.3
benchi-kafka     | 	ssl.provider = null
benchi-kafka     | 	ssl.secure.random.implementation = null
benchi-kafka     | 	ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	ssl.truststore.certificates = null
benchi-kafka     | 	ssl.truststore.location = null
benchi-kafka     | 	ssl.truststore.password = null
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     | 	value.deserializer = class io.confluent.license.LicenseStore$LicenseMessageSerde
benchi-kafka     |  (org.apache.kafka.clients.consumer.ConsumerConfig)
benchi-kafka     | [2025-03-18 16:05:47,393] INFO initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector)
benchi-kafka     | [2025-03-18 16:05:47,394] INFO These configurations '[confluent.metrics.topic.replicas, confluent.metrics.reporter.bootstrap.servers, jmx.hostname, cluster.link.metadata.topic.replication.factor, jmx.port]' were supplied but are not used yet. (org.apache.kafka.clients.consumer.ConsumerConfig)
benchi-kafka     | [2025-03-18 16:05:47,394] INFO Kafka version: 7.8.1-ce (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:05:47,394] INFO Kafka commitId: eefb51c6f2f1b888b8b03296353338afbac8e213 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:05:47,394] INFO Kafka startTimeMs: 1742313947394 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:05:47,395] INFO [Producer clientId=_confluent-license-producer-1] Cluster ID: MkU3OEVBNTcwNTJENDM2Qk (org.apache.kafka.clients.Metadata)
benchi-kafka     | [2025-03-18 16:05:47,397] INFO [Consumer clientId=_confluent-license-consumer-1, groupId=null] Cluster ID: MkU3OEVBNTcwNTJENDM2Qk (org.apache.kafka.clients.Metadata)
benchi-kafka     | [2025-03-18 16:05:47,398] INFO [Consumer clientId=_confluent-license-consumer-1, groupId=null] Assigned to partition(s): _confluent-command-0 (org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer)
benchi-kafka     | [2025-03-18 16:05:47,399] INFO [Consumer clientId=_confluent-license-consumer-1, groupId=null] Seeking to earliest offset of partition _confluent-command-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
benchi-kafka     | [2025-03-18 16:05:47,412] INFO Finished reading KafkaBasedLog for topic _confluent-command (org.apache.kafka.connect.util.KafkaBasedLog)
benchi-kafka     | [2025-03-18 16:05:47,412] INFO Started KafkaBasedLog for topic _confluent-command (org.apache.kafka.connect.util.KafkaBasedLog)
benchi-kafka     | [2025-03-18 16:05:47,412] INFO Started License Store (io.confluent.license.LicenseStore)
benchi-kafka     | [2025-03-18 16:05:47,805] INFO Waiting for 2 seconds for metric reporter topic _confluent-telemetry-metrics to become available. (io.confluent.cruisecontrol.metricsreporter.ConfluentMetricsSamplerBase)
benchi-kafka     | [2025-03-18 16:05:47,944] INFO AdminClientConfig values: 
benchi-kafka     | 	auto.include.jmx.reporter = true
benchi-kafka     | 	bootstrap.controllers = []
benchi-kafka     | 	bootstrap.servers = [broker:29092]
benchi-kafka     | 	client.dns.lookup = use_all_dns_ips
benchi-kafka     | 	client.id = _confluent-license-admin-1
benchi-kafka     | 	confluent.lkc.id = null
benchi-kafka     | 	confluent.metrics.reporter.bootstrap.servers = localhost:9092
benchi-kafka     | 	confluent.proxy.protocol.client.address = null
benchi-kafka     | 	confluent.proxy.protocol.client.mode = PROXY
benchi-kafka     | 	confluent.proxy.protocol.client.port = null
benchi-kafka     | 	confluent.proxy.protocol.client.version = NONE
benchi-kafka     | 	connections.max.idle.ms = 300000
benchi-kafka     | 	default.api.timeout.ms = 60000
benchi-kafka     | 	enable.metrics.push = true
benchi-kafka     | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
benchi-kafka     | 	metadata.max.age.ms = 300000
benchi-kafka     | 	metadata.recovery.strategy = none
benchi-kafka     | 	metric.reporters = []
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.recording.level = INFO
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	receive.buffer.bytes = 65536
benchi-kafka     | 	reconnect.backoff.max.ms = 1000
benchi-kafka     | 	reconnect.backoff.ms = 50
benchi-kafka     | 	request.timeout.ms = 30000
benchi-kafka     | 	retries = 2147483647
benchi-kafka     | 	retry.backoff.max.ms = 1000
benchi-kafka     | 	retry.backoff.ms = 100
benchi-kafka     | 	sasl.client.callback.handler.class = null
benchi-kafka     | 	sasl.jaas.config = null
benchi-kafka     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
benchi-kafka     | 	sasl.kerberos.min.time.before.relogin = 60000
benchi-kafka     | 	sasl.kerberos.service.name = null
benchi-kafka     | 	sasl.kerberos.ticket.renew.jitter = 0.05
benchi-kafka     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
benchi-kafka     | 	sasl.login.callback.handler.class = null
benchi-kafka     | 	sasl.login.class = null
benchi-kafka     | 	sasl.login.connect.timeout.ms = null
benchi-kafka     | 	sasl.login.read.timeout.ms = null
benchi-kafka     | 	sasl.login.refresh.buffer.seconds = 300
benchi-kafka     | 	sasl.login.refresh.min.period.seconds = 60
benchi-kafka     | 	sasl.login.refresh.window.factor = 0.8
benchi-kafka     | 	sasl.login.refresh.window.jitter = 0.05
benchi-kafka     | 	sasl.login.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.login.retry.backoff.ms = 100
benchi-kafka     | 	sasl.mechanism = GSSAPI
benchi-kafka     | 	sasl.oauthbearer.clientassertion.audience = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.expiration = 5
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.jti.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.nbf.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.issuer = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.location = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key.passphrase = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.subject = null
benchi-kafka     | 	sasl.oauthbearer.clock.skew.seconds = 30
benchi-kafka     | 	sasl.oauthbearer.expected.audience = null
benchi-kafka     | 	sasl.oauthbearer.expected.issuer = null
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.url = null
benchi-kafka     | 	sasl.oauthbearer.scope.claim.name = scope
benchi-kafka     | 	sasl.oauthbearer.sub.claim.name = sub
benchi-kafka     | 	sasl.oauthbearer.token.endpoint.url = null
benchi-kafka     | 	security.protocol = PLAINTEXT
benchi-kafka     | 	security.providers = null
benchi-kafka     | 	send.buffer.bytes = 131072
benchi-kafka     | 	socket.connection.setup.timeout.max.ms = 30000
benchi-kafka     | 	socket.connection.setup.timeout.ms = 10000
benchi-kafka     | 	ssl.cipher.suites = null
benchi-kafka     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	ssl.engine.factory.class = null
benchi-kafka     | 	ssl.key.password = null
benchi-kafka     | 	ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	ssl.keystore.certificate.chain = null
benchi-kafka     | 	ssl.keystore.key = null
benchi-kafka     | 	ssl.keystore.location = null
benchi-kafka     | 	ssl.keystore.password = null
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.protocol = TLSv1.3
benchi-kafka     | 	ssl.provider = null
benchi-kafka     | 	ssl.secure.random.implementation = null
benchi-kafka     | 	ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	ssl.truststore.certificates = null
benchi-kafka     | 	ssl.truststore.location = null
benchi-kafka     | 	ssl.truststore.password = null
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     |  (org.apache.kafka.clients.admin.AdminClientConfig)
benchi-kafka     | [2025-03-18 16:05:47,945] INFO These configurations '[confluent.metrics.topic.replicas, replication.factor, confluent.metrics.reporter.bootstrap.servers, jmx.hostname, min.insync.replicas, cluster.link.metadata.topic.replication.factor, jmx.port]' were supplied but are not used yet. (org.apache.kafka.clients.admin.AdminClientConfig)
benchi-kafka     | [2025-03-18 16:05:47,945] INFO Kafka version: 7.8.1-ce (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:05:47,945] INFO Kafka commitId: eefb51c6f2f1b888b8b03296353338afbac8e213 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:05:47,945] INFO Kafka startTimeMs: 1742313947945 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:05:47,951] INFO App info kafka.admin.client for _confluent-license-admin-1 unregistered (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:05:47,986] INFO AdminClientConfig values: 
benchi-kafka     | 	auto.include.jmx.reporter = true
benchi-kafka     | 	bootstrap.controllers = []
benchi-kafka     | 	bootstrap.servers = [broker:29092]
benchi-kafka     | 	client.dns.lookup = use_all_dns_ips
benchi-kafka     | 	client.id = _confluent-license-admin-1
benchi-kafka     | 	confluent.lkc.id = null
benchi-kafka     | 	confluent.metrics.reporter.bootstrap.servers = localhost:9092
benchi-kafka     | 	confluent.proxy.protocol.client.address = null
benchi-kafka     | 	confluent.proxy.protocol.client.mode = PROXY
benchi-kafka     | 	confluent.proxy.protocol.client.port = null
benchi-kafka     | 	confluent.proxy.protocol.client.version = NONE
benchi-kafka     | 	connections.max.idle.ms = 300000
benchi-kafka     | 	default.api.timeout.ms = 60000
benchi-kafka     | 	enable.metrics.push = true
benchi-kafka     | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
benchi-kafka     | 	metadata.max.age.ms = 300000
benchi-kafka     | 	metadata.recovery.strategy = none
benchi-kafka     | 	metric.reporters = []
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.recording.level = INFO
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	receive.buffer.bytes = 65536
benchi-kafka     | 	reconnect.backoff.max.ms = 1000
benchi-kafka     | 	reconnect.backoff.ms = 50
benchi-kafka     | 	request.timeout.ms = 30000
benchi-kafka     | 	retries = 2147483647
benchi-kafka     | 	retry.backoff.max.ms = 1000
benchi-kafka     | 	retry.backoff.ms = 100
benchi-kafka     | 	sasl.client.callback.handler.class = null
benchi-kafka     | 	sasl.jaas.config = null
benchi-kafka     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
benchi-kafka     | 	sasl.kerberos.min.time.before.relogin = 60000
benchi-kafka     | 	sasl.kerberos.service.name = null
benchi-kafka     | 	sasl.kerberos.ticket.renew.jitter = 0.05
benchi-kafka     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
benchi-kafka     | 	sasl.login.callback.handler.class = null
benchi-kafka     | 	sasl.login.class = null
benchi-kafka     | 	sasl.login.connect.timeout.ms = null
benchi-kafka     | 	sasl.login.read.timeout.ms = null
benchi-kafka     | 	sasl.login.refresh.buffer.seconds = 300
benchi-kafka     | 	sasl.login.refresh.min.period.seconds = 60
benchi-kafka     | 	sasl.login.refresh.window.factor = 0.8
benchi-kafka     | 	sasl.login.refresh.window.jitter = 0.05
benchi-kafka     | 	sasl.login.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.login.retry.backoff.ms = 100
benchi-kafka     | 	sasl.mechanism = GSSAPI
benchi-kafka     | 	sasl.oauthbearer.clientassertion.audience = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.expiration = 5
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.jti.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.nbf.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.issuer = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.location = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key.passphrase = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.subject = null
benchi-kafka     | 	sasl.oauthbearer.clock.skew.seconds = 30
benchi-kafka     | 	sasl.oauthbearer.expected.audience = null
benchi-kafka     | 	sasl.oauthbearer.expected.issuer = null
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.url = null
benchi-kafka     | 	sasl.oauthbearer.scope.claim.name = scope
benchi-kafka     | 	sasl.oauthbearer.sub.claim.name = sub
benchi-kafka     | 	sasl.oauthbearer.token.endpoint.url = null
benchi-kafka     | 	security.protocol = PLAINTEXT
benchi-kafka     | 	security.providers = null
benchi-kafka     | 	send.buffer.bytes = 131072
benchi-kafka     | 	socket.connection.setup.timeout.max.ms = 30000
benchi-kafka     | 	socket.connection.setup.timeout.ms = 10000
benchi-kafka     | 	ssl.cipher.suites = null
benchi-kafka     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	ssl.engine.factory.class = null
benchi-kafka     | 	ssl.key.password = null
benchi-kafka     | 	ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	ssl.keystore.certificate.chain = null
benchi-kafka     | 	ssl.keystore.key = null
benchi-kafka     | 	ssl.keystore.location = null
benchi-kafka     | 	ssl.keystore.password = null
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.protocol = TLSv1.3
benchi-kafka     | 	ssl.provider = null
benchi-kafka     | 	ssl.secure.random.implementation = null
benchi-kafka     | 	ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	ssl.truststore.certificates = null
benchi-kafka     | 	ssl.truststore.location = null
benchi-kafka     | 	ssl.truststore.password = null
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     |  (org.apache.kafka.clients.admin.AdminClientConfig)
benchi-kafka     | [2025-03-18 16:05:47,987] INFO These configurations '[confluent.metrics.topic.replicas, replication.factor, confluent.metrics.reporter.bootstrap.servers, jmx.hostname, min.insync.replicas, cluster.link.metadata.topic.replication.factor, jmx.port]' were supplied but are not used yet. (org.apache.kafka.clients.admin.AdminClientConfig)
benchi-kafka     | [2025-03-18 16:05:47,987] INFO Kafka version: 7.8.1-ce (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:05:47,987] INFO Kafka commitId: eefb51c6f2f1b888b8b03296353338afbac8e213 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:05:47,987] INFO Kafka startTimeMs: 1742313947987 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:05:47,991] INFO App info kafka.admin.client for _confluent-license-admin-1 unregistered (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:05:47,992] INFO License for single cluster, single node (io.confluent.license.LicenseManager)
benchi-kafka     | [2025-03-18 16:05:47,995] INFO [BrokerServer id=1] Transition from STARTING to STARTED (kafka.server.BrokerServer)
benchi-kafka     | [2025-03-18 16:05:47,995] INFO Kafka version: 7.8.1-ce (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:05:47,995] INFO Kafka commitId: eefb51c6f2f1b888b8b03296353338afbac8e213 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:05:47,995] INFO Kafka startTimeMs: 1742313947995 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:05:47,995] INFO [KafkaRaftServer nodeId=1] Kafka Server started (kafka.server.KafkaRaftServer)
benchi-kafka     | [2025-03-18 16:05:49,811] INFO Waiting for 4 seconds for metric reporter topic _confluent-telemetry-metrics to become available. (io.confluent.cruisecontrol.metricsreporter.ConfluentMetricsSamplerBase)
benchi-kafka     | [2025-03-18 16:05:53,815] INFO Waiting for 8 seconds for metric reporter topic _confluent-telemetry-metrics to become available. (io.confluent.cruisecontrol.metricsreporter.ConfluentMetricsSamplerBase)
benchi-kafka     | [2025-03-18 16:06:01,522] INFO AdminClientConfig values: 
benchi-kafka     | 	auto.include.jmx.reporter = true
benchi-kafka     | 	bootstrap.controllers = []
benchi-kafka     | 	bootstrap.servers = [localhost:9092]
benchi-kafka     | 	client.dns.lookup = use_all_dns_ips
benchi-kafka     | 	client.id = 
benchi-kafka     | 	confluent.lkc.id = null
benchi-kafka     | 	confluent.metrics.reporter.bootstrap.servers = kafka-0:9071
benchi-kafka     | 	confluent.proxy.protocol.client.address = null
benchi-kafka     | 	confluent.proxy.protocol.client.mode = PROXY
benchi-kafka     | 	confluent.proxy.protocol.client.port = null
benchi-kafka     | 	confluent.proxy.protocol.client.version = NONE
benchi-kafka     | 	connections.max.idle.ms = 300000
benchi-kafka     | 	default.api.timeout.ms = 60000
benchi-kafka     | 	enable.metrics.push = true
benchi-kafka     | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
benchi-kafka     | 	metadata.max.age.ms = 300000
benchi-kafka     | 	metadata.recovery.strategy = none
benchi-kafka     | 	metric.reporters = []
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.recording.level = INFO
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	receive.buffer.bytes = 65536
benchi-kafka     | 	reconnect.backoff.max.ms = 1000
benchi-kafka     | 	reconnect.backoff.ms = 50
benchi-kafka     | 	request.timeout.ms = 30000
benchi-kafka     | 	retries = 2147483647
benchi-kafka     | 	retry.backoff.max.ms = 1000
benchi-kafka     | 	retry.backoff.ms = 100
benchi-kafka     | 	sasl.client.callback.handler.class = null
benchi-kafka     | 	sasl.jaas.config = null
benchi-kafka     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
benchi-kafka     | 	sasl.kerberos.min.time.before.relogin = 60000
benchi-kafka     | 	sasl.kerberos.service.name = null
benchi-kafka     | 	sasl.kerberos.ticket.renew.jitter = 0.05
benchi-kafka     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
benchi-kafka     | 	sasl.login.callback.handler.class = null
benchi-kafka     | 	sasl.login.class = null
benchi-kafka     | 	sasl.login.connect.timeout.ms = null
benchi-kafka     | 	sasl.login.read.timeout.ms = null
benchi-kafka     | 	sasl.login.refresh.buffer.seconds = 300
benchi-kafka     | 	sasl.login.refresh.min.period.seconds = 60
benchi-kafka     | 	sasl.login.refresh.window.factor = 0.8
benchi-kafka     | 	sasl.login.refresh.window.jitter = 0.05
benchi-kafka     | 	sasl.login.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.login.retry.backoff.ms = 100
benchi-kafka     | 	sasl.mechanism = GSSAPI
benchi-kafka     | 	sasl.oauthbearer.clientassertion.audience = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.expiration = 5
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.jti.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.nbf.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.issuer = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.location = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key.passphrase = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.subject = null
benchi-kafka     | 	sasl.oauthbearer.clock.skew.seconds = 30
benchi-kafka     | 	sasl.oauthbearer.expected.audience = null
benchi-kafka     | 	sasl.oauthbearer.expected.issuer = null
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.url = null
benchi-kafka     | 	sasl.oauthbearer.scope.claim.name = scope
benchi-kafka     | 	sasl.oauthbearer.sub.claim.name = sub
benchi-kafka     | 	sasl.oauthbearer.token.endpoint.url = null
benchi-kafka     | 	security.protocol = PLAINTEXT
benchi-kafka     | 	security.providers = null
benchi-kafka     | 	send.buffer.bytes = 131072
benchi-kafka     | 	socket.connection.setup.timeout.max.ms = 30000
benchi-kafka     | 	socket.connection.setup.timeout.ms = 10000
benchi-kafka     | 	ssl.cipher.suites = null
benchi-kafka     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	ssl.engine.factory.class = null
benchi-kafka     | 	ssl.key.password = null
benchi-kafka     | 	ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	ssl.keystore.certificate.chain = null
benchi-kafka     | 	ssl.keystore.key = null
benchi-kafka     | 	ssl.keystore.location = null
benchi-kafka     | 	ssl.keystore.password = null
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.protocol = TLSv1.3
benchi-kafka     | 	ssl.provider = null
benchi-kafka     | 	ssl.secure.random.implementation = null
benchi-kafka     | 	ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	ssl.truststore.certificates = null
benchi-kafka     | 	ssl.truststore.location = null
benchi-kafka     | 	ssl.truststore.password = null
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     |  (org.apache.kafka.clients.admin.AdminClientConfig)
benchi-kafka     | [2025-03-18 16:06:01,533] INFO Kafka version: 7.8.1-ce (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:06:01,533] INFO Kafka commitId: eefb51c6f2f1b888b8b03296353338afbac8e213 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:06:01,533] INFO Kafka startTimeMs: 1742313961533 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:06:01,540] INFO [AdminClient clientId=adminclient-1] The DescribeTopicPartitions API is not supported, using Metadata API to describe topics. (org.apache.kafka.clients.admin.KafkaAdminClient)
benchi-kafka     | [2025-03-18 16:06:01,543] INFO [ControllerServer id=1] CreateTopics result(s): CreatableTopic(name='_confluent-metrics', numPartitions=12, replicationFactor=3, assignments=[], configs=[CreateableTopicConfig(name='max.message.bytes', value='10485760'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='259200000'), CreateableTopicConfig(name='segment.ms', value='14400000'), CreateableTopicConfig(name='retention.bytes', value='-1')], linkName=null, mirrorTopic=null, sourceTopicId=AAAAAAAAAAAAAAAAAAAAAA, mirrorStartOffsetSpec=-9223372036854775808, mirrorStartOffsets=[], stoppedSequenceNumber=0): INVALID_REPLICATION_FACTOR (Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.) (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:06:01,544] INFO App info kafka.admin.client for adminclient-1 unregistered (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:06:01,546] ERROR Error checking or creating metrics topic (io.confluent.metrics.reporter.ConfluentMetricsReporter)
benchi-kafka     | org.apache.kafka.common.errors.InvalidReplicationFactorException: Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.
benchi-kafka     | [2025-03-18 16:06:01,818] INFO Waiting for 16 seconds for metric reporter topic _confluent-telemetry-metrics to become available. (io.confluent.cruisecontrol.metricsreporter.ConfluentMetricsSamplerBase)
benchi-kafka     | [2025-03-18 16:06:16,524] INFO AdminClientConfig values: 
benchi-kafka     | 	auto.include.jmx.reporter = true
benchi-kafka     | 	bootstrap.controllers = []
benchi-kafka     | 	bootstrap.servers = [localhost:9092]
benchi-kafka     | 	client.dns.lookup = use_all_dns_ips
benchi-kafka     | 	client.id = 
benchi-kafka     | 	confluent.lkc.id = null
benchi-kafka     | 	confluent.metrics.reporter.bootstrap.servers = kafka-0:9071
benchi-kafka     | 	confluent.proxy.protocol.client.address = null
benchi-kafka     | 	confluent.proxy.protocol.client.mode = PROXY
benchi-kafka     | 	confluent.proxy.protocol.client.port = null
benchi-kafka     | 	confluent.proxy.protocol.client.version = NONE
benchi-kafka     | 	connections.max.idle.ms = 300000
benchi-kafka     | 	default.api.timeout.ms = 60000
benchi-kafka     | 	enable.metrics.push = true
benchi-kafka     | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
benchi-kafka     | 	metadata.max.age.ms = 300000
benchi-kafka     | 	metadata.recovery.strategy = none
benchi-kafka     | 	metric.reporters = []
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.recording.level = INFO
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	receive.buffer.bytes = 65536
benchi-kafka     | 	reconnect.backoff.max.ms = 1000
benchi-kafka     | 	reconnect.backoff.ms = 50
benchi-kafka     | 	request.timeout.ms = 30000
benchi-kafka     | 	retries = 2147483647
benchi-kafka     | 	retry.backoff.max.ms = 1000
benchi-kafka     | 	retry.backoff.ms = 100
benchi-kafka     | 	sasl.client.callback.handler.class = null
benchi-kafka     | 	sasl.jaas.config = null
benchi-kafka     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
benchi-kafka     | 	sasl.kerberos.min.time.before.relogin = 60000
benchi-kafka     | 	sasl.kerberos.service.name = null
benchi-kafka     | 	sasl.kerberos.ticket.renew.jitter = 0.05
benchi-kafka     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
benchi-kafka     | 	sasl.login.callback.handler.class = null
benchi-kafka     | 	sasl.login.class = null
benchi-kafka     | 	sasl.login.connect.timeout.ms = null
benchi-kafka     | 	sasl.login.read.timeout.ms = null
benchi-kafka     | 	sasl.login.refresh.buffer.seconds = 300
benchi-kafka     | 	sasl.login.refresh.min.period.seconds = 60
benchi-kafka     | 	sasl.login.refresh.window.factor = 0.8
benchi-kafka     | 	sasl.login.refresh.window.jitter = 0.05
benchi-kafka     | 	sasl.login.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.login.retry.backoff.ms = 100
benchi-kafka     | 	sasl.mechanism = GSSAPI
benchi-kafka     | 	sasl.oauthbearer.clientassertion.audience = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.expiration = 5
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.jti.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.nbf.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.issuer = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.location = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key.passphrase = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.subject = null
benchi-kafka     | 	sasl.oauthbearer.clock.skew.seconds = 30
benchi-kafka     | 	sasl.oauthbearer.expected.audience = null
benchi-kafka     | 	sasl.oauthbearer.expected.issuer = null
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.url = null
benchi-kafka     | 	sasl.oauthbearer.scope.claim.name = scope
benchi-kafka     | 	sasl.oauthbearer.sub.claim.name = sub
benchi-kafka     | 	sasl.oauthbearer.token.endpoint.url = null
benchi-kafka     | 	security.protocol = PLAINTEXT
benchi-kafka     | 	security.providers = null
benchi-kafka     | 	send.buffer.bytes = 131072
benchi-kafka     | 	socket.connection.setup.timeout.max.ms = 30000
benchi-kafka     | 	socket.connection.setup.timeout.ms = 10000
benchi-kafka     | 	ssl.cipher.suites = null
benchi-kafka     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	ssl.engine.factory.class = null
benchi-kafka     | 	ssl.key.password = null
benchi-kafka     | 	ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	ssl.keystore.certificate.chain = null
benchi-kafka     | 	ssl.keystore.key = null
benchi-kafka     | 	ssl.keystore.location = null
benchi-kafka     | 	ssl.keystore.password = null
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.protocol = TLSv1.3
benchi-kafka     | 	ssl.provider = null
benchi-kafka     | 	ssl.secure.random.implementation = null
benchi-kafka     | 	ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	ssl.truststore.certificates = null
benchi-kafka     | 	ssl.truststore.location = null
benchi-kafka     | 	ssl.truststore.password = null
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     |  (org.apache.kafka.clients.admin.AdminClientConfig)
benchi-kafka     | [2025-03-18 16:06:16,527] INFO Kafka version: 7.8.1-ce (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:06:16,527] INFO Kafka commitId: eefb51c6f2f1b888b8b03296353338afbac8e213 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:06:16,527] INFO Kafka startTimeMs: 1742313976527 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:06:16,531] INFO Beginning log roller... (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:16,532] INFO Log roller completed in 0 seconds (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:16,539] INFO [AdminClient clientId=adminclient-2] The DescribeTopicPartitions API is not supported, using Metadata API to describe topics. (org.apache.kafka.clients.admin.KafkaAdminClient)
benchi-kafka     | [2025-03-18 16:06:16,542] INFO [ControllerServer id=1] CreateTopics result(s): CreatableTopic(name='_confluent-metrics', numPartitions=12, replicationFactor=3, assignments=[], configs=[CreateableTopicConfig(name='max.message.bytes', value='10485760'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='259200000'), CreateableTopicConfig(name='segment.ms', value='14400000'), CreateableTopicConfig(name='retention.bytes', value='-1')], linkName=null, mirrorTopic=null, sourceTopicId=AAAAAAAAAAAAAAAAAAAAAA, mirrorStartOffsetSpec=-9223372036854775808, mirrorStartOffsets=[], stoppedSequenceNumber=0): INVALID_REPLICATION_FACTOR (Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.) (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:06:16,543] INFO App info kafka.admin.client for adminclient-2 unregistered (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:06:16,544] ERROR Error checking or creating metrics topic (io.confluent.metrics.reporter.ConfluentMetricsReporter)
benchi-kafka     | org.apache.kafka.common.errors.InvalidReplicationFactorException: Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.
benchi-kafka     | [2025-03-18 16:06:17,825] INFO Waiting for 32 seconds for metric reporter topic _confluent-telemetry-metrics to become available. (io.confluent.cruisecontrol.metricsreporter.ConfluentMetricsSamplerBase)
benchi-kafka     | [2025-03-18 16:06:31,519] INFO AdminClientConfig values: 
benchi-kafka     | 	auto.include.jmx.reporter = true
benchi-kafka     | 	bootstrap.controllers = []
benchi-kafka     | 	bootstrap.servers = [localhost:9092]
benchi-kafka     | 	client.dns.lookup = use_all_dns_ips
benchi-kafka     | 	client.id = 
benchi-kafka     | 	confluent.lkc.id = null
benchi-kafka     | 	confluent.metrics.reporter.bootstrap.servers = kafka-0:9071
benchi-kafka     | 	confluent.proxy.protocol.client.address = null
benchi-kafka     | 	confluent.proxy.protocol.client.mode = PROXY
benchi-kafka     | 	confluent.proxy.protocol.client.port = null
benchi-kafka     | 	confluent.proxy.protocol.client.version = NONE
benchi-kafka     | 	connections.max.idle.ms = 300000
benchi-kafka     | 	default.api.timeout.ms = 60000
benchi-kafka     | 	enable.metrics.push = true
benchi-kafka     | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
benchi-kafka     | 	metadata.max.age.ms = 300000
benchi-kafka     | 	metadata.recovery.strategy = none
benchi-kafka     | 	metric.reporters = []
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.recording.level = INFO
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	receive.buffer.bytes = 65536
benchi-kafka     | 	reconnect.backoff.max.ms = 1000
benchi-kafka     | 	reconnect.backoff.ms = 50
benchi-kafka     | 	request.timeout.ms = 30000
benchi-kafka     | 	retries = 2147483647
benchi-kafka     | 	retry.backoff.max.ms = 1000
benchi-kafka     | 	retry.backoff.ms = 100
benchi-kafka     | 	sasl.client.callback.handler.class = null
benchi-kafka     | 	sasl.jaas.config = null
benchi-kafka     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
benchi-kafka     | 	sasl.kerberos.min.time.before.relogin = 60000
benchi-kafka     | 	sasl.kerberos.service.name = null
benchi-kafka     | 	sasl.kerberos.ticket.renew.jitter = 0.05
benchi-kafka     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
benchi-kafka     | 	sasl.login.callback.handler.class = null
benchi-kafka     | 	sasl.login.class = null
benchi-kafka     | 	sasl.login.connect.timeout.ms = null
benchi-kafka     | 	sasl.login.read.timeout.ms = null
benchi-kafka     | 	sasl.login.refresh.buffer.seconds = 300
benchi-kafka     | 	sasl.login.refresh.min.period.seconds = 60
benchi-kafka     | 	sasl.login.refresh.window.factor = 0.8
benchi-kafka     | 	sasl.login.refresh.window.jitter = 0.05
benchi-kafka     | 	sasl.login.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.login.retry.backoff.ms = 100
benchi-kafka     | 	sasl.mechanism = GSSAPI
benchi-kafka     | 	sasl.oauthbearer.clientassertion.audience = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.expiration = 5
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.jti.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.nbf.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.issuer = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.location = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key.passphrase = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.subject = null
benchi-kafka     | 	sasl.oauthbearer.clock.skew.seconds = 30
benchi-kafka     | 	sasl.oauthbearer.expected.audience = null
benchi-kafka     | 	sasl.oauthbearer.expected.issuer = null
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.url = null
benchi-kafka     | 	sasl.oauthbearer.scope.claim.name = scope
benchi-kafka     | 	sasl.oauthbearer.sub.claim.name = sub
benchi-kafka     | 	sasl.oauthbearer.token.endpoint.url = null
benchi-kafka     | 	security.protocol = PLAINTEXT
benchi-kafka     | 	security.providers = null
benchi-kafka     | 	send.buffer.bytes = 131072
benchi-kafka     | 	socket.connection.setup.timeout.max.ms = 30000
benchi-kafka     | 	socket.connection.setup.timeout.ms = 10000
benchi-kafka     | 	ssl.cipher.suites = null
benchi-kafka     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	ssl.engine.factory.class = null
benchi-kafka     | 	ssl.key.password = null
benchi-kafka     | 	ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	ssl.keystore.certificate.chain = null
benchi-kafka     | 	ssl.keystore.key = null
benchi-kafka     | 	ssl.keystore.location = null
benchi-kafka     | 	ssl.keystore.password = null
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.protocol = TLSv1.3
benchi-kafka     | 	ssl.provider = null
benchi-kafka     | 	ssl.secure.random.implementation = null
benchi-kafka     | 	ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	ssl.truststore.certificates = null
benchi-kafka     | 	ssl.truststore.location = null
benchi-kafka     | 	ssl.truststore.password = null
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     |  (org.apache.kafka.clients.admin.AdminClientConfig)
benchi-kafka     | [2025-03-18 16:06:31,522] INFO Kafka version: 7.8.1-ce (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:06:31,522] INFO Kafka commitId: eefb51c6f2f1b888b8b03296353338afbac8e213 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:06:31,522] INFO Kafka startTimeMs: 1742313991521 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:06:31,528] INFO [AdminClient clientId=adminclient-3] The DescribeTopicPartitions API is not supported, using Metadata API to describe topics. (org.apache.kafka.clients.admin.KafkaAdminClient)
benchi-kafka     | [2025-03-18 16:06:31,530] INFO [ControllerServer id=1] CreateTopics result(s): CreatableTopic(name='_confluent-metrics', numPartitions=12, replicationFactor=3, assignments=[], configs=[CreateableTopicConfig(name='max.message.bytes', value='10485760'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='259200000'), CreateableTopicConfig(name='segment.ms', value='14400000'), CreateableTopicConfig(name='retention.bytes', value='-1')], linkName=null, mirrorTopic=null, sourceTopicId=AAAAAAAAAAAAAAAAAAAAAA, mirrorStartOffsetSpec=-9223372036854775808, mirrorStartOffsets=[], stoppedSequenceNumber=0): INVALID_REPLICATION_FACTOR (Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.) (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:06:31,531] INFO App info kafka.admin.client for adminclient-3 unregistered (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:06:31,531] ERROR Error checking or creating metrics topic (io.confluent.metrics.reporter.ConfluentMetricsReporter)
benchi-kafka     | org.apache.kafka.common.errors.InvalidReplicationFactorException: Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.
benchi-kafka     | [2025-03-18 16:06:45,887] INFO [CelltControllerMetricsPublisher id=1] No cells found. Skipping cell metrics refresh. (org.apache.kafka.controller.metrics.CellControllerMetricsPublisher)
benchi-kafka     | [2025-03-18 16:06:46,519] INFO AdminClientConfig values: 
benchi-kafka     | 	auto.include.jmx.reporter = true
benchi-kafka     | 	bootstrap.controllers = []
benchi-kafka     | 	bootstrap.servers = [localhost:9092]
benchi-kafka     | 	client.dns.lookup = use_all_dns_ips
benchi-kafka     | 	client.id = 
benchi-kafka     | 	confluent.lkc.id = null
benchi-kafka     | 	confluent.metrics.reporter.bootstrap.servers = kafka-0:9071
benchi-kafka     | 	confluent.proxy.protocol.client.address = null
benchi-kafka     | 	confluent.proxy.protocol.client.mode = PROXY
benchi-kafka     | 	confluent.proxy.protocol.client.port = null
benchi-kafka     | 	confluent.proxy.protocol.client.version = NONE
benchi-kafka     | 	connections.max.idle.ms = 300000
benchi-kafka     | 	default.api.timeout.ms = 60000
benchi-kafka     | 	enable.metrics.push = true
benchi-kafka     | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
benchi-kafka     | 	metadata.max.age.ms = 300000
benchi-kafka     | 	metadata.recovery.strategy = none
benchi-kafka     | 	metric.reporters = []
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.recording.level = INFO
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	receive.buffer.bytes = 65536
benchi-kafka     | 	reconnect.backoff.max.ms = 1000
benchi-kafka     | 	reconnect.backoff.ms = 50
benchi-kafka     | 	request.timeout.ms = 30000
benchi-kafka     | 	retries = 2147483647
benchi-kafka     | 	retry.backoff.max.ms = 1000
benchi-kafka     | 	retry.backoff.ms = 100
benchi-kafka     | 	sasl.client.callback.handler.class = null
benchi-kafka     | 	sasl.jaas.config = null
benchi-kafka     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
benchi-kafka     | 	sasl.kerberos.min.time.before.relogin = 60000
benchi-kafka     | 	sasl.kerberos.service.name = null
benchi-kafka     | 	sasl.kerberos.ticket.renew.jitter = 0.05
benchi-kafka     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
benchi-kafka     | 	sasl.login.callback.handler.class = null
benchi-kafka     | 	sasl.login.class = null
benchi-kafka     | 	sasl.login.connect.timeout.ms = null
benchi-kafka     | 	sasl.login.read.timeout.ms = null
benchi-kafka     | 	sasl.login.refresh.buffer.seconds = 300
benchi-kafka     | 	sasl.login.refresh.min.period.seconds = 60
benchi-kafka     | 	sasl.login.refresh.window.factor = 0.8
benchi-kafka     | 	sasl.login.refresh.window.jitter = 0.05
benchi-kafka     | 	sasl.login.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.login.retry.backoff.ms = 100
benchi-kafka     | 	sasl.mechanism = GSSAPI
benchi-kafka     | 	sasl.oauthbearer.clientassertion.audience = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.expiration = 5
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.jti.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.nbf.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.issuer = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.location = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key.passphrase = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.subject = null
benchi-kafka     | 	sasl.oauthbearer.clock.skew.seconds = 30
benchi-kafka     | 	sasl.oauthbearer.expected.audience = null
benchi-kafka     | 	sasl.oauthbearer.expected.issuer = null
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.url = null
benchi-kafka     | 	sasl.oauthbearer.scope.claim.name = scope
benchi-kafka     | 	sasl.oauthbearer.sub.claim.name = sub
benchi-kafka     | 	sasl.oauthbearer.token.endpoint.url = null
benchi-kafka     | 	security.protocol = PLAINTEXT
benchi-kafka     | 	security.providers = null
benchi-kafka     | 	send.buffer.bytes = 131072
benchi-kafka     | 	socket.connection.setup.timeout.max.ms = 30000
benchi-kafka     | 	socket.connection.setup.timeout.ms = 10000
benchi-kafka     | 	ssl.cipher.suites = null
benchi-kafka     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	ssl.engine.factory.class = null
benchi-kafka     | 	ssl.key.password = null
benchi-kafka     | 	ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	ssl.keystore.certificate.chain = null
benchi-kafka     | 	ssl.keystore.key = null
benchi-kafka     | 	ssl.keystore.location = null
benchi-kafka     | 	ssl.keystore.password = null
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.protocol = TLSv1.3
benchi-kafka     | 	ssl.provider = null
benchi-kafka     | 	ssl.secure.random.implementation = null
benchi-kafka     | 	ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	ssl.truststore.certificates = null
benchi-kafka     | 	ssl.truststore.location = null
benchi-kafka     | 	ssl.truststore.password = null
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     |  (org.apache.kafka.clients.admin.AdminClientConfig)
benchi-kafka     | [2025-03-18 16:06:46,520] INFO Kafka version: 7.8.1-ce (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:06:46,520] INFO Kafka commitId: eefb51c6f2f1b888b8b03296353338afbac8e213 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:06:46,520] INFO Kafka startTimeMs: 1742314006520 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:06:46,524] INFO [AdminClient clientId=adminclient-4] The DescribeTopicPartitions API is not supported, using Metadata API to describe topics. (org.apache.kafka.clients.admin.KafkaAdminClient)
benchi-kafka     | [2025-03-18 16:06:46,527] INFO [ControllerServer id=1] CreateTopics result(s): CreatableTopic(name='_confluent-metrics', numPartitions=12, replicationFactor=3, assignments=[], configs=[CreateableTopicConfig(name='max.message.bytes', value='10485760'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='259200000'), CreateableTopicConfig(name='segment.ms', value='14400000'), CreateableTopicConfig(name='retention.bytes', value='-1')], linkName=null, mirrorTopic=null, sourceTopicId=AAAAAAAAAAAAAAAAAAAAAA, mirrorStartOffsetSpec=-9223372036854775808, mirrorStartOffsets=[], stoppedSequenceNumber=0): INVALID_REPLICATION_FACTOR (Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.) (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:06:46,528] INFO App info kafka.admin.client for adminclient-4 unregistered (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:06:46,528] ERROR Error checking or creating metrics topic (io.confluent.metrics.reporter.ConfluentMetricsReporter)
benchi-kafka     | org.apache.kafka.common.errors.InvalidReplicationFactorException: Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.
benchi-kafka     | [2025-03-18 16:06:46,546] INFO AdminClientConfig values: 
benchi-kafka     | 	auto.include.jmx.reporter = true
benchi-kafka     | 	bootstrap.controllers = []
benchi-kafka     | 	bootstrap.servers = [broker:29092]
benchi-kafka     | 	client.dns.lookup = use_all_dns_ips
benchi-kafka     | 	client.id = confluent-telemetry-reporter-local-producer
benchi-kafka     | 	confluent.lkc.id = null
benchi-kafka     | 	confluent.metrics.reporter.bootstrap.servers = localhost:9092
benchi-kafka     | 	confluent.proxy.protocol.client.address = null
benchi-kafka     | 	confluent.proxy.protocol.client.mode = PROXY
benchi-kafka     | 	confluent.proxy.protocol.client.port = null
benchi-kafka     | 	confluent.proxy.protocol.client.version = NONE
benchi-kafka     | 	connections.max.idle.ms = 300000
benchi-kafka     | 	default.api.timeout.ms = 60000
benchi-kafka     | 	enable.metrics.push = true
benchi-kafka     | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
benchi-kafka     | 	metadata.max.age.ms = 300000
benchi-kafka     | 	metadata.recovery.strategy = none
benchi-kafka     | 	metric.reporters = []
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.recording.level = INFO
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	receive.buffer.bytes = 65536
benchi-kafka     | 	reconnect.backoff.max.ms = 1000
benchi-kafka     | 	reconnect.backoff.ms = 50
benchi-kafka     | 	request.timeout.ms = 30000
benchi-kafka     | 	retries = 2147483647
benchi-kafka     | 	retry.backoff.max.ms = 1000
benchi-kafka     | 	retry.backoff.ms = 500
benchi-kafka     | 	sasl.client.callback.handler.class = null
benchi-kafka     | 	sasl.jaas.config = null
benchi-kafka     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
benchi-kafka     | 	sasl.kerberos.min.time.before.relogin = 60000
benchi-kafka     | 	sasl.kerberos.service.name = null
benchi-kafka     | 	sasl.kerberos.ticket.renew.jitter = 0.05
benchi-kafka     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
benchi-kafka     | 	sasl.login.callback.handler.class = null
benchi-kafka     | 	sasl.login.class = null
benchi-kafka     | 	sasl.login.connect.timeout.ms = null
benchi-kafka     | 	sasl.login.read.timeout.ms = null
benchi-kafka     | 	sasl.login.refresh.buffer.seconds = 300
benchi-kafka     | 	sasl.login.refresh.min.period.seconds = 60
benchi-kafka     | 	sasl.login.refresh.window.factor = 0.8
benchi-kafka     | 	sasl.login.refresh.window.jitter = 0.05
benchi-kafka     | 	sasl.login.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.login.retry.backoff.ms = 100
benchi-kafka     | 	sasl.mechanism = GSSAPI
benchi-kafka     | 	sasl.oauthbearer.clientassertion.audience = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.expiration = 5
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.jti.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.nbf.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.issuer = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.location = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key.passphrase = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.subject = null
benchi-kafka     | 	sasl.oauthbearer.clock.skew.seconds = 30
benchi-kafka     | 	sasl.oauthbearer.expected.audience = null
benchi-kafka     | 	sasl.oauthbearer.expected.issuer = null
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.url = null
benchi-kafka     | 	sasl.oauthbearer.scope.claim.name = scope
benchi-kafka     | 	sasl.oauthbearer.sub.claim.name = sub
benchi-kafka     | 	sasl.oauthbearer.token.endpoint.url = null
benchi-kafka     | 	security.protocol = PLAINTEXT
benchi-kafka     | 	security.providers = null
benchi-kafka     | 	send.buffer.bytes = 131072
benchi-kafka     | 	socket.connection.setup.timeout.max.ms = 30000
benchi-kafka     | 	socket.connection.setup.timeout.ms = 10000
benchi-kafka     | 	ssl.cipher.suites = null
benchi-kafka     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	ssl.engine.factory.class = null
benchi-kafka     | 	ssl.key.password = null
benchi-kafka     | 	ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	ssl.keystore.certificate.chain = null
benchi-kafka     | 	ssl.keystore.key = null
benchi-kafka     | 	ssl.keystore.location = null
benchi-kafka     | 	ssl.keystore.password = null
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.protocol = TLSv1.3
benchi-kafka     | 	ssl.provider = null
benchi-kafka     | 	ssl.secure.random.implementation = null
benchi-kafka     | 	ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	ssl.truststore.certificates = null
benchi-kafka     | 	ssl.truststore.location = null
benchi-kafka     | 	ssl.truststore.password = null
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     |  (org.apache.kafka.clients.admin.AdminClientConfig)
benchi-kafka     | [2025-03-18 16:06:46,546] INFO These configurations '[compression.type, confluent.metrics.reporter.bootstrap.servers, enable.idempotence, acks, key.serializer, max.request.size, value.serializer, partitioner.class, interceptor.classes, max.in.flight.requests.per.connection, linger.ms]' were supplied but are not used yet. (org.apache.kafka.clients.admin.AdminClientConfig)
benchi-kafka     | [2025-03-18 16:06:46,546] INFO Kafka version: 7.8.1-ce (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:06:46,546] INFO Kafka commitId: eefb51c6f2f1b888b8b03296353338afbac8e213 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:06:46,546] INFO Kafka startTimeMs: 1742314006546 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:06:46,549] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] The DescribeTopicPartitions API is not supported, using Metadata API to describe topics. (org.apache.kafka.clients.admin.KafkaAdminClient)
benchi-kafka     | [2025-03-18 16:06:46,552] INFO [ControllerServer id=1] CreateTopics result(s): CreatableTopic(name='_confluent-telemetry-metrics', numPartitions=12, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='max.message.bytes', value='10485760'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='259200000'), CreateableTopicConfig(name='segment.ms', value='14400000'), CreateableTopicConfig(name='retention.bytes', value='-1')], linkName=null, mirrorTopic=null, sourceTopicId=AAAAAAAAAAAAAAAAAAAAAA, mirrorStartOffsetSpec=-9223372036854775808, mirrorStartOffsets=[], stoppedSequenceNumber=0): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:06:46,552] INFO [ControllerServer id=1] Replayed TopicRecord for topic _confluent-telemetry-metrics with topic ID ED-u4HVARJua1x3-BA6aMQ. (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:06:46,552] INFO [ControllerServer id=1] Replayed ConfigRecord for ConfigResource(type=TOPIC, name='_confluent-telemetry-metrics') which set configuration max.message.bytes to 10485760 (org.apache.kafka.controller.ConfigurationControlManager)
benchi-kafka     | [2025-03-18 16:06:46,552] INFO [ControllerServer id=1] Replayed ConfigRecord for ConfigResource(type=TOPIC, name='_confluent-telemetry-metrics') which set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
benchi-kafka     | [2025-03-18 16:06:46,552] INFO [ControllerServer id=1] Replayed ConfigRecord for ConfigResource(type=TOPIC, name='_confluent-telemetry-metrics') which set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
benchi-kafka     | [2025-03-18 16:06:46,552] INFO [ControllerServer id=1] Replayed ConfigRecord for ConfigResource(type=TOPIC, name='_confluent-telemetry-metrics') which set configuration retention.ms to 259200000 (org.apache.kafka.controller.ConfigurationControlManager)
benchi-kafka     | [2025-03-18 16:06:46,552] INFO [ControllerServer id=1] Replayed ConfigRecord for ConfigResource(type=TOPIC, name='_confluent-telemetry-metrics') which set configuration segment.ms to 14400000 (org.apache.kafka.controller.ConfigurationControlManager)
benchi-kafka     | [2025-03-18 16:06:46,552] INFO [ControllerServer id=1] Replayed ConfigRecord for ConfigResource(type=TOPIC, name='_confluent-telemetry-metrics') which set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
benchi-kafka     | [2025-03-18 16:06:46,552] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-telemetry-metrics-0 with topic ID ED-u4HVARJua1x3-BA6aMQ and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:06:46,552] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-telemetry-metrics-1 with topic ID ED-u4HVARJua1x3-BA6aMQ and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:06:46,552] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-telemetry-metrics-2 with topic ID ED-u4HVARJua1x3-BA6aMQ and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:06:46,552] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-telemetry-metrics-3 with topic ID ED-u4HVARJua1x3-BA6aMQ and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:06:46,552] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-telemetry-metrics-4 with topic ID ED-u4HVARJua1x3-BA6aMQ and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:06:46,552] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-telemetry-metrics-5 with topic ID ED-u4HVARJua1x3-BA6aMQ and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:06:46,552] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-telemetry-metrics-6 with topic ID ED-u4HVARJua1x3-BA6aMQ and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:06:46,552] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-telemetry-metrics-7 with topic ID ED-u4HVARJua1x3-BA6aMQ and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:06:46,552] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-telemetry-metrics-8 with topic ID ED-u4HVARJua1x3-BA6aMQ and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:06:46,552] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-telemetry-metrics-9 with topic ID ED-u4HVARJua1x3-BA6aMQ and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:06:46,552] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-telemetry-metrics-10 with topic ID ED-u4HVARJua1x3-BA6aMQ and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:06:46,553] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-telemetry-metrics-11 with topic ID ED-u4HVARJua1x3-BA6aMQ and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:06:46,580] INFO SBC Event SbcMetadataUpdateEvent-133 generated 1 more events to enqueue in the following order - [SbcConfigUpdateEvent-134]. Enqueuing... (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:06:46,580] INFO Handling event SbcConfigUpdateEvent-134 (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:06:46,580] INFO [Broker id=1] Transitioning 12 partition(s) to local leaders. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:46,580] INFO Balancer notified of a config change: ConfigurationsDelta(changes={ConfigResource(type=TOPIC, name='_confluent-telemetry-metrics')=ConfigurationDelta(changedKeys=[max.message.bytes, message.timestamp.type, min.insync.replicas, retention.ms, segment.ms, retention.bytes])}) (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:06:46,580] INFO There were 0 change(s) and 0 deletion(s) to balancer configs. Changed Configs: {}, Deleted Configs: [] (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:06:46,580] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-telemetry-metrics-3, _confluent-telemetry-metrics-4, _confluent-telemetry-metrics-5, _confluent-telemetry-metrics-6, _confluent-telemetry-metrics-7, _confluent-telemetry-metrics-8, _confluent-telemetry-metrics-9, _confluent-telemetry-metrics-10, _confluent-telemetry-metrics-11, _confluent-telemetry-metrics-0, _confluent-telemetry-metrics-1, _confluent-telemetry-metrics-2) (kafka.server.ReplicaFetcherManager)
benchi-kafka     | [2025-03-18 16:06:46,580] INFO [Broker id=1] Creating new partition _confluent-telemetry-metrics-3 with topic id ED-u4HVARJua1x3-BA6aMQ. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:46,581] INFO [Broker id=1] Creating new partition _confluent-telemetry-metrics-4 with topic id ED-u4HVARJua1x3-BA6aMQ. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:46,581] INFO Created telemetry topic _confluent-telemetry-metrics (io.confluent.telemetry.exporter.kafka.KafkaExporter)
benchi-kafka     | [2025-03-18 16:06:46,581] INFO App info kafka.admin.client for confluent-telemetry-reporter-local-producer unregistered (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:06:46,581] INFO [Broker id=1] Creating new partition _confluent-telemetry-metrics-5 with topic id ED-u4HVARJua1x3-BA6aMQ. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:46,582] INFO [Broker id=1] Creating new partition _confluent-telemetry-metrics-6 with topic id ED-u4HVARJua1x3-BA6aMQ. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:46,582] INFO [Broker id=1] Creating new partition _confluent-telemetry-metrics-7 with topic id ED-u4HVARJua1x3-BA6aMQ. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:46,582] INFO [Broker id=1] Creating new partition _confluent-telemetry-metrics-8 with topic id ED-u4HVARJua1x3-BA6aMQ. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:46,582] INFO [Broker id=1] Creating new partition _confluent-telemetry-metrics-9 with topic id ED-u4HVARJua1x3-BA6aMQ. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:46,582] INFO [Broker id=1] Creating new partition _confluent-telemetry-metrics-10 with topic id ED-u4HVARJua1x3-BA6aMQ. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:46,583] INFO [Broker id=1] Creating new partition _confluent-telemetry-metrics-11 with topic id ED-u4HVARJua1x3-BA6aMQ. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:46,583] INFO [Broker id=1] Creating new partition _confluent-telemetry-metrics-0 with topic id ED-u4HVARJua1x3-BA6aMQ. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:46,583] INFO [Broker id=1] Creating new partition _confluent-telemetry-metrics-1 with topic id ED-u4HVARJua1x3-BA6aMQ. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:46,583] INFO [Broker id=1] Creating new partition _confluent-telemetry-metrics-2 with topic id ED-u4HVARJua1x3-BA6aMQ. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:46,584] INFO [Broker id=1] Stopped fetchers as part of become-leader transition for 12 partitions (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:46,585] INFO [MergedLog partition=_confluent-telemetry-metrics-7, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:06:46,586] INFO Created log for partition _confluent-telemetry-metrics-7 in /tmp/kraft-combined-logs/_confluent-telemetry-metrics-7 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:46,586] INFO [Partition _confluent-telemetry-metrics-7 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-7 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:46,586] INFO [Partition _confluent-telemetry-metrics-7 broker=1] Log loaded for partition _confluent-telemetry-metrics-7 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:46,586] INFO Setting topicIdPartition ED-u4HVARJua1x3-BA6aMQ:_confluent-telemetry-metrics-7 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:06:46,586] INFO [MergedLog partition=_confluent-telemetry-metrics-7, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-telemetry-metrics-7 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:06:46,586] INFO [Broker id=1] Leader _confluent-telemetry-metrics-7 with topic id Some(ED-u4HVARJua1x3-BA6aMQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:46,587] INFO [MergedLog partition=_confluent-telemetry-metrics-3, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:06:46,587] INFO Created log for partition _confluent-telemetry-metrics-3 in /tmp/kraft-combined-logs/_confluent-telemetry-metrics-3 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:46,587] INFO [Partition _confluent-telemetry-metrics-3 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-3 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:46,587] INFO [Partition _confluent-telemetry-metrics-3 broker=1] Log loaded for partition _confluent-telemetry-metrics-3 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:46,587] INFO Setting topicIdPartition ED-u4HVARJua1x3-BA6aMQ:_confluent-telemetry-metrics-3 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:06:46,587] INFO [MergedLog partition=_confluent-telemetry-metrics-3, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-telemetry-metrics-3 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:06:46,587] INFO [Broker id=1] Leader _confluent-telemetry-metrics-3 with topic id Some(ED-u4HVARJua1x3-BA6aMQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:46,588] INFO [MergedLog partition=_confluent-telemetry-metrics-11, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:06:46,588] INFO Created log for partition _confluent-telemetry-metrics-11 in /tmp/kraft-combined-logs/_confluent-telemetry-metrics-11 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:46,588] INFO [Partition _confluent-telemetry-metrics-11 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-11 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:46,588] INFO [Partition _confluent-telemetry-metrics-11 broker=1] Log loaded for partition _confluent-telemetry-metrics-11 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:46,589] INFO Setting topicIdPartition ED-u4HVARJua1x3-BA6aMQ:_confluent-telemetry-metrics-11 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:06:46,589] INFO [MergedLog partition=_confluent-telemetry-metrics-11, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-telemetry-metrics-11 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:06:46,589] INFO [Broker id=1] Leader _confluent-telemetry-metrics-11 with topic id Some(ED-u4HVARJua1x3-BA6aMQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:46,589] INFO [MergedLog partition=_confluent-telemetry-metrics-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:06:46,590] INFO Created log for partition _confluent-telemetry-metrics-0 in /tmp/kraft-combined-logs/_confluent-telemetry-metrics-0 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:46,590] INFO [Partition _confluent-telemetry-metrics-0 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:46,590] INFO [Partition _confluent-telemetry-metrics-0 broker=1] Log loaded for partition _confluent-telemetry-metrics-0 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:46,590] INFO Setting topicIdPartition ED-u4HVARJua1x3-BA6aMQ:_confluent-telemetry-metrics-0 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:06:46,590] INFO [MergedLog partition=_confluent-telemetry-metrics-0, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-telemetry-metrics-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:06:46,590] INFO [Broker id=1] Leader _confluent-telemetry-metrics-0 with topic id Some(ED-u4HVARJua1x3-BA6aMQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:46,591] INFO [MergedLog partition=_confluent-telemetry-metrics-4, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:06:46,591] INFO Created log for partition _confluent-telemetry-metrics-4 in /tmp/kraft-combined-logs/_confluent-telemetry-metrics-4 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:46,591] INFO [Partition _confluent-telemetry-metrics-4 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-4 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:46,591] INFO [Partition _confluent-telemetry-metrics-4 broker=1] Log loaded for partition _confluent-telemetry-metrics-4 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:46,591] INFO Setting topicIdPartition ED-u4HVARJua1x3-BA6aMQ:_confluent-telemetry-metrics-4 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:06:46,591] INFO [MergedLog partition=_confluent-telemetry-metrics-4, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-telemetry-metrics-4 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:06:46,591] INFO [Broker id=1] Leader _confluent-telemetry-metrics-4 with topic id Some(ED-u4HVARJua1x3-BA6aMQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:46,591] INFO Partitioner has null list of partitions to produce to. Calculating partitions to produce to (io.confluent.shaded.io.confluent.telemetry.events.exporter.kafka.RandomBrokerPartitionSubsetPartitioner)
benchi-kafka     | [2025-03-18 16:06:46,592] INFO Kafka Producer producing to the following subset partitions: {_confluent-telemetry-metrics=[5, 6]} (io.confluent.shaded.io.confluent.telemetry.events.exporter.kafka.RandomBrokerPartitionSubsetPartitioner)
benchi-kafka     | [2025-03-18 16:06:46,592] INFO [MergedLog partition=_confluent-telemetry-metrics-8, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:06:46,593] INFO Created log for partition _confluent-telemetry-metrics-8 in /tmp/kraft-combined-logs/_confluent-telemetry-metrics-8 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:46,593] INFO [Partition _confluent-telemetry-metrics-8 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-8 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:46,593] INFO [Partition _confluent-telemetry-metrics-8 broker=1] Log loaded for partition _confluent-telemetry-metrics-8 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:46,593] INFO Setting topicIdPartition ED-u4HVARJua1x3-BA6aMQ:_confluent-telemetry-metrics-8 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:06:46,593] INFO [MergedLog partition=_confluent-telemetry-metrics-8, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-telemetry-metrics-8 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:06:46,593] INFO [Broker id=1] Leader _confluent-telemetry-metrics-8 with topic id Some(ED-u4HVARJua1x3-BA6aMQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:46,594] INFO [MergedLog partition=_confluent-telemetry-metrics-5, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:06:46,594] INFO Created log for partition _confluent-telemetry-metrics-5 in /tmp/kraft-combined-logs/_confluent-telemetry-metrics-5 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:46,594] INFO [Partition _confluent-telemetry-metrics-5 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-5 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:46,594] INFO [Partition _confluent-telemetry-metrics-5 broker=1] Log loaded for partition _confluent-telemetry-metrics-5 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:46,594] INFO Setting topicIdPartition ED-u4HVARJua1x3-BA6aMQ:_confluent-telemetry-metrics-5 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:06:46,595] INFO [MergedLog partition=_confluent-telemetry-metrics-5, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-telemetry-metrics-5 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:06:46,595] INFO [Broker id=1] Leader _confluent-telemetry-metrics-5 with topic id Some(ED-u4HVARJua1x3-BA6aMQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:46,596] INFO [MergedLog partition=_confluent-telemetry-metrics-1, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:06:46,597] INFO Created log for partition _confluent-telemetry-metrics-1 in /tmp/kraft-combined-logs/_confluent-telemetry-metrics-1 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:46,597] INFO [Partition _confluent-telemetry-metrics-1 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-1 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:46,597] INFO [Partition _confluent-telemetry-metrics-1 broker=1] Log loaded for partition _confluent-telemetry-metrics-1 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:46,597] INFO Setting topicIdPartition ED-u4HVARJua1x3-BA6aMQ:_confluent-telemetry-metrics-1 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:06:46,597] INFO [MergedLog partition=_confluent-telemetry-metrics-1, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-telemetry-metrics-1 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:06:46,597] INFO [Broker id=1] Leader _confluent-telemetry-metrics-1 with topic id Some(ED-u4HVARJua1x3-BA6aMQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:46,598] INFO [MergedLog partition=_confluent-telemetry-metrics-6, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:06:46,598] INFO Created log for partition _confluent-telemetry-metrics-6 in /tmp/kraft-combined-logs/_confluent-telemetry-metrics-6 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:46,598] INFO [Partition _confluent-telemetry-metrics-6 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-6 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:46,598] INFO [Partition _confluent-telemetry-metrics-6 broker=1] Log loaded for partition _confluent-telemetry-metrics-6 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:46,598] INFO Setting topicIdPartition ED-u4HVARJua1x3-BA6aMQ:_confluent-telemetry-metrics-6 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:06:46,598] INFO [MergedLog partition=_confluent-telemetry-metrics-6, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-telemetry-metrics-6 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:06:46,598] INFO [Broker id=1] Leader _confluent-telemetry-metrics-6 with topic id Some(ED-u4HVARJua1x3-BA6aMQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:46,600] INFO [MergedLog partition=_confluent-telemetry-metrics-9, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:06:46,600] INFO Created log for partition _confluent-telemetry-metrics-9 in /tmp/kraft-combined-logs/_confluent-telemetry-metrics-9 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:46,600] INFO [Partition _confluent-telemetry-metrics-9 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-9 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:46,600] INFO [Partition _confluent-telemetry-metrics-9 broker=1] Log loaded for partition _confluent-telemetry-metrics-9 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:46,600] INFO Setting topicIdPartition ED-u4HVARJua1x3-BA6aMQ:_confluent-telemetry-metrics-9 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:06:46,600] INFO [MergedLog partition=_confluent-telemetry-metrics-9, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-telemetry-metrics-9 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:06:46,600] INFO [Broker id=1] Leader _confluent-telemetry-metrics-9 with topic id Some(ED-u4HVARJua1x3-BA6aMQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:46,601] INFO [MergedLog partition=_confluent-telemetry-metrics-2, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:06:46,602] INFO Created log for partition _confluent-telemetry-metrics-2 in /tmp/kraft-combined-logs/_confluent-telemetry-metrics-2 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:46,602] INFO [Partition _confluent-telemetry-metrics-2 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-2 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:46,602] INFO [Partition _confluent-telemetry-metrics-2 broker=1] Log loaded for partition _confluent-telemetry-metrics-2 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:46,602] INFO Setting topicIdPartition ED-u4HVARJua1x3-BA6aMQ:_confluent-telemetry-metrics-2 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:06:46,602] INFO [MergedLog partition=_confluent-telemetry-metrics-2, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-telemetry-metrics-2 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:06:46,602] INFO [Broker id=1] Leader _confluent-telemetry-metrics-2 with topic id Some(ED-u4HVARJua1x3-BA6aMQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:46,603] INFO [MergedLog partition=_confluent-telemetry-metrics-10, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:06:46,603] INFO Created log for partition _confluent-telemetry-metrics-10 in /tmp/kraft-combined-logs/_confluent-telemetry-metrics-10 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:46,603] INFO [Partition _confluent-telemetry-metrics-10 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-10 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:46,603] INFO [Partition _confluent-telemetry-metrics-10 broker=1] Log loaded for partition _confluent-telemetry-metrics-10 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:46,603] INFO Setting topicIdPartition ED-u4HVARJua1x3-BA6aMQ:_confluent-telemetry-metrics-10 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:06:46,603] INFO [MergedLog partition=_confluent-telemetry-metrics-10, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-telemetry-metrics-10 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:06:46,603] INFO [Broker id=1] Leader _confluent-telemetry-metrics-10 with topic id Some(ED-u4HVARJua1x3-BA6aMQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:46,604] INFO [DynamicConfigPublisher broker id=1] Updating topic _confluent-telemetry-metrics with new configuration : max.message.bytes -> 10485760,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,retention.ms -> 259200000,segment.ms -> 14400000,retention.bytes -> -1 (kafka.server.metadata.DynamicConfigPublisher)
benchi-kafka     | [2025-03-18 16:06:46,619] INFO Broker Addition context is yet to be initialized, hence BrokerAddCount metrics will be reported as 0. (io.confluent.databalancer.ConfluentDataBalanceEngine)
benchi-kafka     | [2025-03-18 16:06:46,650] INFO [Partition _confluent-telemetry-metrics-5 broker=1] roll: _confluent-telemetry-metrics-5: first produce received, lastOffset: 22, leaderEpoch: 0, numMessages:23, time diff: 59 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:46,651] INFO [Partition _confluent-telemetry-metrics-5 broker=1] roll: _confluent-telemetry-metrics-5: first HWM advanced, leaderEpoch: 0, old HW: 0, new HW: 23, become leader time: 1742314006591 ms, time diff: 60 ms (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:46,652] INFO [Partition _confluent-telemetry-metrics-6 broker=1] roll: _confluent-telemetry-metrics-6: first produce received, lastOffset: 22, leaderEpoch: 0, numMessages:23, time diff: 57 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:46,652] INFO [Partition _confluent-telemetry-metrics-6 broker=1] roll: _confluent-telemetry-metrics-6: first HWM advanced, leaderEpoch: 0, old HW: 0, new HW: 23, become leader time: 1742314006595 ms, time diff: 57 ms (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:49,832] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-6820492430475519429] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
benchi-kafka     | [2025-03-18 16:06:49,832] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-6820492430475519429] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
benchi-kafka     | [2025-03-18 16:06:49,836] INFO App info kafka.consumer for kafka-cruise-control unregistered (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:06:49,836] INFO Metric Reporter Sampler ready to start. (io.confluent.cruisecontrol.metricsreporter.ConfluentMetricsSamplerBase)
benchi-kafka     | [2025-03-18 16:06:49,836] INFO DataBalancer: Startup component StartupComponent ConfluentTelemetryReporterSampler ready to proceed (io.confluent.databalancer.startup.StartupComponents)
benchi-kafka     | [2025-03-18 16:06:49,836] INFO DataBalancer: Checking startup component StartupComponent SampleStoreTopicCleanUp (io.confluent.databalancer.startup.StartupComponents)
benchi-kafka     | [2025-03-18 16:06:49,836] INFO AdminClientConfig values: 
benchi-kafka     | 	auto.include.jmx.reporter = true
benchi-kafka     | 	bootstrap.controllers = []
benchi-kafka     | 	bootstrap.servers = [broker:29092]
benchi-kafka     | 	client.dns.lookup = use_all_dns_ips
benchi-kafka     | 	client.id = kafka-cruise-control
benchi-kafka     | 	confluent.lkc.id = null
benchi-kafka     | 	confluent.metrics.reporter.bootstrap.servers = localhost:9092
benchi-kafka     | 	confluent.proxy.protocol.client.address = null
benchi-kafka     | 	confluent.proxy.protocol.client.mode = PROXY
benchi-kafka     | 	confluent.proxy.protocol.client.port = null
benchi-kafka     | 	confluent.proxy.protocol.client.version = NONE
benchi-kafka     | 	connections.max.idle.ms = 540000
benchi-kafka     | 	default.api.timeout.ms = 60000
benchi-kafka     | 	enable.metrics.push = true
benchi-kafka     | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
benchi-kafka     | 	metadata.max.age.ms = 300000
benchi-kafka     | 	metadata.recovery.strategy = none
benchi-kafka     | 	metric.reporters = []
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.recording.level = INFO
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	receive.buffer.bytes = 32768
benchi-kafka     | 	reconnect.backoff.max.ms = 5000
benchi-kafka     | 	reconnect.backoff.ms = 500
benchi-kafka     | 	request.timeout.ms = 30000
benchi-kafka     | 	retries = 2147483647
benchi-kafka     | 	retry.backoff.max.ms = 1000
benchi-kafka     | 	retry.backoff.ms = 500
benchi-kafka     | 	sasl.client.callback.handler.class = null
benchi-kafka     | 	sasl.jaas.config = null
benchi-kafka     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
benchi-kafka     | 	sasl.kerberos.min.time.before.relogin = 60000
benchi-kafka     | 	sasl.kerberos.service.name = null
benchi-kafka     | 	sasl.kerberos.ticket.renew.jitter = 0.05
benchi-kafka     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
benchi-kafka     | 	sasl.login.callback.handler.class = null
benchi-kafka     | 	sasl.login.class = null
benchi-kafka     | 	sasl.login.connect.timeout.ms = null
benchi-kafka     | 	sasl.login.read.timeout.ms = null
benchi-kafka     | 	sasl.login.refresh.buffer.seconds = 300
benchi-kafka     | 	sasl.login.refresh.min.period.seconds = 60
benchi-kafka     | 	sasl.login.refresh.window.factor = 0.8
benchi-kafka     | 	sasl.login.refresh.window.jitter = 0.05
benchi-kafka     | 	sasl.login.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.login.retry.backoff.ms = 100
benchi-kafka     | 	sasl.mechanism = GSSAPI
benchi-kafka     | 	sasl.oauthbearer.clientassertion.audience = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.expiration = 5
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.jti.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.nbf.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.issuer = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.location = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key.passphrase = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.subject = null
benchi-kafka     | 	sasl.oauthbearer.clock.skew.seconds = 30
benchi-kafka     | 	sasl.oauthbearer.expected.audience = null
benchi-kafka     | 	sasl.oauthbearer.expected.issuer = null
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.url = null
benchi-kafka     | 	sasl.oauthbearer.scope.claim.name = scope
benchi-kafka     | 	sasl.oauthbearer.sub.claim.name = sub
benchi-kafka     | 	sasl.oauthbearer.token.endpoint.url = null
benchi-kafka     | 	security.protocol = PLAINTEXT
benchi-kafka     | 	security.providers = null
benchi-kafka     | 	send.buffer.bytes = 131072
benchi-kafka     | 	socket.connection.setup.timeout.max.ms = 30000
benchi-kafka     | 	socket.connection.setup.timeout.ms = 10000
benchi-kafka     | 	ssl.cipher.suites = null
benchi-kafka     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	ssl.engine.factory.class = null
benchi-kafka     | 	ssl.key.password = null
benchi-kafka     | 	ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	ssl.keystore.certificate.chain = null
benchi-kafka     | 	ssl.keystore.key = null
benchi-kafka     | 	ssl.keystore.location = null
benchi-kafka     | 	ssl.keystore.password = null
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.protocol = TLSv1.3
benchi-kafka     | 	ssl.provider = null
benchi-kafka     | 	ssl.secure.random.implementation = null
benchi-kafka     | 	ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	ssl.truststore.certificates = null
benchi-kafka     | 	ssl.truststore.location = null
benchi-kafka     | 	ssl.truststore.password = null
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     |  (org.apache.kafka.clients.admin.AdminClientConfig)
benchi-kafka     | [2025-03-18 16:06:49,837] INFO These configurations '[sasl.oauthbearer.jwks.endpoint.refresh.ms, sasl.oauthbearer.clientassertion.include.jti.claim, sasl.login.refresh.min.period.seconds, sasl.oauthbearer.scope.claim.name, sasl.login.refresh.window.factor, sasl.kerberos.ticket.renew.window.factor, sasl.login.retry.backoff.ms, sasl.oauthbearer.clientassertion.expiration, sasl.kerberos.kinit.cmd, sasl.kerberos.ticket.renew.jitter, ssl.keystore.type, ssl.trustmanager.algorithm, sasl.kerberos.min.time.before.relogin, ssl.endpoint.identification.algorithm, ssl.protocol, confluent.metrics.reporter.bootstrap.servers, sasl.login.refresh.buffer.seconds, sasl.login.retry.backoff.max.ms, ssl.enabled.protocols, sasl.oauthbearer.sub.claim.name, ssl.truststore.type, sasl.oauthbearer.jwks.endpoint.retry.backoff.ms, sasl.oauthbearer.clock.skew.seconds, ssl.keymanager.algorithm, sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms, sasl.oauthbearer.clientassertion.include.nbf.claim, sasl.login.refresh.window.jitter]' were supplied but are not used yet. (org.apache.kafka.clients.admin.AdminClientConfig)
benchi-kafka     | [2025-03-18 16:06:49,837] INFO Kafka version: 7.8.1-ce (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:06:49,837] INFO Kafka commitId: eefb51c6f2f1b888b8b03296353338afbac8e213 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:06:49,837] INFO Kafka startTimeMs: 1742314009837 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:06:49,845] INFO DataBalancer: No topics to be deleted. (com.linkedin.kafka.cruisecontrol.SbkTopicUtils)
benchi-kafka     | [2025-03-18 16:06:49,845] INFO App info kafka.admin.client for kafka-cruise-control unregistered (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:06:49,846] INFO DataBalancer: Startup component StartupComponent SampleStoreTopicCleanUp ready to proceed (io.confluent.databalancer.startup.StartupComponents)
benchi-kafka     | [2025-03-18 16:06:49,846] INFO DataBalancer: Checking startup component StartupComponent ApiStatePersistenceStore (io.confluent.databalancer.startup.StartupComponents)
benchi-kafka     | [2025-03-18 16:06:49,846] INFO AdminClientConfig values: 
benchi-kafka     | 	auto.include.jmx.reporter = true
benchi-kafka     | 	bootstrap.controllers = []
benchi-kafka     | 	bootstrap.servers = [broker:29092]
benchi-kafka     | 	client.dns.lookup = use_all_dns_ips
benchi-kafka     | 	client.id = kafka-cruise-control
benchi-kafka     | 	confluent.lkc.id = null
benchi-kafka     | 	confluent.metrics.reporter.bootstrap.servers = localhost:9092
benchi-kafka     | 	confluent.proxy.protocol.client.address = null
benchi-kafka     | 	confluent.proxy.protocol.client.mode = PROXY
benchi-kafka     | 	confluent.proxy.protocol.client.port = null
benchi-kafka     | 	confluent.proxy.protocol.client.version = NONE
benchi-kafka     | 	connections.max.idle.ms = 540000
benchi-kafka     | 	default.api.timeout.ms = 60000
benchi-kafka     | 	enable.metrics.push = true
benchi-kafka     | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
benchi-kafka     | 	metadata.max.age.ms = 300000
benchi-kafka     | 	metadata.recovery.strategy = none
benchi-kafka     | 	metric.reporters = []
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.recording.level = INFO
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	receive.buffer.bytes = 32768
benchi-kafka     | 	reconnect.backoff.max.ms = 5000
benchi-kafka     | 	reconnect.backoff.ms = 500
benchi-kafka     | 	request.timeout.ms = 30000
benchi-kafka     | 	retries = 2147483647
benchi-kafka     | 	retry.backoff.max.ms = 1000
benchi-kafka     | 	retry.backoff.ms = 500
benchi-kafka     | 	sasl.client.callback.handler.class = null
benchi-kafka     | 	sasl.jaas.config = null
benchi-kafka     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
benchi-kafka     | 	sasl.kerberos.min.time.before.relogin = 60000
benchi-kafka     | 	sasl.kerberos.service.name = null
benchi-kafka     | 	sasl.kerberos.ticket.renew.jitter = 0.05
benchi-kafka     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
benchi-kafka     | 	sasl.login.callback.handler.class = null
benchi-kafka     | 	sasl.login.class = null
benchi-kafka     | 	sasl.login.connect.timeout.ms = null
benchi-kafka     | 	sasl.login.read.timeout.ms = null
benchi-kafka     | 	sasl.login.refresh.buffer.seconds = 300
benchi-kafka     | 	sasl.login.refresh.min.period.seconds = 60
benchi-kafka     | 	sasl.login.refresh.window.factor = 0.8
benchi-kafka     | 	sasl.login.refresh.window.jitter = 0.05
benchi-kafka     | 	sasl.login.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.login.retry.backoff.ms = 100
benchi-kafka     | 	sasl.mechanism = GSSAPI
benchi-kafka     | 	sasl.oauthbearer.clientassertion.audience = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.expiration = 5
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.jti.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.nbf.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.issuer = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.location = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key.passphrase = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.subject = null
benchi-kafka     | 	sasl.oauthbearer.clock.skew.seconds = 30
benchi-kafka     | 	sasl.oauthbearer.expected.audience = null
benchi-kafka     | 	sasl.oauthbearer.expected.issuer = null
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.url = null
benchi-kafka     | 	sasl.oauthbearer.scope.claim.name = scope
benchi-kafka     | 	sasl.oauthbearer.sub.claim.name = sub
benchi-kafka     | 	sasl.oauthbearer.token.endpoint.url = null
benchi-kafka     | 	security.protocol = PLAINTEXT
benchi-kafka     | 	security.providers = null
benchi-kafka     | 	send.buffer.bytes = 131072
benchi-kafka     | 	socket.connection.setup.timeout.max.ms = 30000
benchi-kafka     | 	socket.connection.setup.timeout.ms = 10000
benchi-kafka     | 	ssl.cipher.suites = null
benchi-kafka     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	ssl.engine.factory.class = null
benchi-kafka     | 	ssl.key.password = null
benchi-kafka     | 	ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	ssl.keystore.certificate.chain = null
benchi-kafka     | 	ssl.keystore.key = null
benchi-kafka     | 	ssl.keystore.location = null
benchi-kafka     | 	ssl.keystore.password = null
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.protocol = TLSv1.3
benchi-kafka     | 	ssl.provider = null
benchi-kafka     | 	ssl.secure.random.implementation = null
benchi-kafka     | 	ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	ssl.truststore.certificates = null
benchi-kafka     | 	ssl.truststore.location = null
benchi-kafka     | 	ssl.truststore.password = null
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     |  (org.apache.kafka.clients.admin.AdminClientConfig)
benchi-kafka     | [2025-03-18 16:06:49,847] INFO These configurations '[sasl.oauthbearer.jwks.endpoint.refresh.ms, sasl.oauthbearer.clientassertion.include.jti.claim, sasl.login.refresh.min.period.seconds, sasl.oauthbearer.scope.claim.name, sasl.login.refresh.window.factor, sasl.kerberos.ticket.renew.window.factor, sasl.login.retry.backoff.ms, sasl.oauthbearer.clientassertion.expiration, sasl.kerberos.kinit.cmd, sasl.kerberos.ticket.renew.jitter, ssl.keystore.type, ssl.trustmanager.algorithm, sasl.kerberos.min.time.before.relogin, ssl.endpoint.identification.algorithm, ssl.protocol, confluent.metrics.reporter.bootstrap.servers, sasl.login.refresh.buffer.seconds, sasl.login.retry.backoff.max.ms, ssl.enabled.protocols, sasl.oauthbearer.sub.claim.name, ssl.truststore.type, sasl.oauthbearer.jwks.endpoint.retry.backoff.ms, sasl.oauthbearer.clock.skew.seconds, ssl.keymanager.algorithm, sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms, sasl.oauthbearer.clientassertion.include.nbf.claim, sasl.login.refresh.window.jitter]' were supplied but are not used yet. (org.apache.kafka.clients.admin.AdminClientConfig)
benchi-kafka     | [2025-03-18 16:06:49,847] INFO Kafka version: 7.8.1-ce (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:06:49,847] INFO Kafka commitId: eefb51c6f2f1b888b8b03296353338afbac8e213 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:06:49,847] INFO Kafka startTimeMs: 1742314009847 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:06:49,853] INFO DataBalancer: Creating topic _confluent_balancer_api_state  (com.linkedin.kafka.cruisecontrol.SbkTopicUtils)
benchi-kafka     | [2025-03-18 16:06:49,855] INFO [ControllerServer id=1] CreateTopics result(s): CreatableTopic(name='_confluent_balancer_api_state', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='delete,compact'), CreateableTopicConfig(name='retention.ms', value='2592000000')], linkName=null, mirrorTopic=null, sourceTopicId=AAAAAAAAAAAAAAAAAAAAAA, mirrorStartOffsetSpec=-9223372036854775808, mirrorStartOffsets=[], stoppedSequenceNumber=0): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:06:49,855] INFO [ControllerServer id=1] Replayed TopicRecord for topic _confluent_balancer_api_state with topic ID 312Wt30jSxCsJ4PMdsfwvw. (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:06:49,855] INFO [ControllerServer id=1] Replayed ConfigRecord for ConfigResource(type=TOPIC, name='_confluent_balancer_api_state') which set configuration cleanup.policy to delete,compact (org.apache.kafka.controller.ConfigurationControlManager)
benchi-kafka     | [2025-03-18 16:06:49,855] INFO [ControllerServer id=1] Replayed ConfigRecord for ConfigResource(type=TOPIC, name='_confluent_balancer_api_state') which set configuration retention.ms to 2592000000 (org.apache.kafka.controller.ConfigurationControlManager)
benchi-kafka     | [2025-03-18 16:06:49,855] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent_balancer_api_state-0 with topic ID 312Wt30jSxCsJ4PMdsfwvw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:06:49,882] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:49,882] INFO SBC Event SbcMetadataUpdateEvent-142 generated 1 more events to enqueue in the following order - [SbcConfigUpdateEvent-143]. Enqueuing... (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:06:49,882] INFO Handling event SbcConfigUpdateEvent-143 (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:06:49,882] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent_balancer_api_state-0) (kafka.server.ReplicaFetcherManager)
benchi-kafka     | [2025-03-18 16:06:49,882] INFO Balancer notified of a config change: ConfigurationsDelta(changes={ConfigResource(type=TOPIC, name='_confluent_balancer_api_state')=ConfigurationDelta(changedKeys=[cleanup.policy, retention.ms])}) (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:06:49,882] INFO There were 0 change(s) and 0 deletion(s) to balancer configs. Changed Configs: {}, Deleted Configs: [] (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:06:49,882] INFO [Broker id=1] Creating new partition _confluent_balancer_api_state-0 with topic id 312Wt30jSxCsJ4PMdsfwvw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:49,883] INFO [Broker id=1] Stopped fetchers as part of become-leader transition for 1 partitions (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:49,884] INFO App info kafka.admin.client for kafka-cruise-control unregistered (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:06:49,885] INFO Waiting for 1 seconds to ensure that api persistent store topic is created/exists. (io.confluent.databalancer.persistence.ApiStatePersistenceStore)
benchi-kafka     | [2025-03-18 16:06:49,885] INFO [MergedLog partition=_confluent_balancer_api_state-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:06:49,885] INFO Created log for partition _confluent_balancer_api_state-0 in /tmp/kraft-combined-logs/_confluent_balancer_api_state-0 with properties {cleanup.policy=delete,compact, retention.ms=2592000000} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:49,886] INFO [Partition _confluent_balancer_api_state-0 broker=1] No checkpointed highwatermark is found for partition _confluent_balancer_api_state-0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:49,886] INFO [Partition _confluent_balancer_api_state-0 broker=1] Log loaded for partition _confluent_balancer_api_state-0 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:49,886] INFO Setting topicIdPartition 312Wt30jSxCsJ4PMdsfwvw:_confluent_balancer_api_state-0 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:06:49,886] INFO [MergedLog partition=_confluent_balancer_api_state-0, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent_balancer_api_state-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:06:49,886] INFO [Broker id=1] Leader _confluent_balancer_api_state-0 with topic id Some(312Wt30jSxCsJ4PMdsfwvw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:49,886] INFO [DynamicConfigPublisher broker id=1] Updating topic _confluent_balancer_api_state with new configuration : cleanup.policy -> delete,compact,retention.ms -> 2592000000 (kafka.server.metadata.DynamicConfigPublisher)
benchi-kafka     | [2025-03-18 16:06:50,888] INFO AdminClientConfig values: 
benchi-kafka     | 	auto.include.jmx.reporter = true
benchi-kafka     | 	bootstrap.controllers = []
benchi-kafka     | 	bootstrap.servers = [broker:29092]
benchi-kafka     | 	client.dns.lookup = use_all_dns_ips
benchi-kafka     | 	client.id = kafka-cruise-control
benchi-kafka     | 	confluent.lkc.id = null
benchi-kafka     | 	confluent.metrics.reporter.bootstrap.servers = localhost:9092
benchi-kafka     | 	confluent.proxy.protocol.client.address = null
benchi-kafka     | 	confluent.proxy.protocol.client.mode = PROXY
benchi-kafka     | 	confluent.proxy.protocol.client.port = null
benchi-kafka     | 	confluent.proxy.protocol.client.version = NONE
benchi-kafka     | 	connections.max.idle.ms = 540000
benchi-kafka     | 	default.api.timeout.ms = 60000
benchi-kafka     | 	enable.metrics.push = true
benchi-kafka     | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
benchi-kafka     | 	metadata.max.age.ms = 300000
benchi-kafka     | 	metadata.recovery.strategy = none
benchi-kafka     | 	metric.reporters = []
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.recording.level = INFO
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	receive.buffer.bytes = 32768
benchi-kafka     | 	reconnect.backoff.max.ms = 5000
benchi-kafka     | 	reconnect.backoff.ms = 500
benchi-kafka     | 	request.timeout.ms = 30000
benchi-kafka     | 	retries = 2147483647
benchi-kafka     | 	retry.backoff.max.ms = 1000
benchi-kafka     | 	retry.backoff.ms = 500
benchi-kafka     | 	sasl.client.callback.handler.class = null
benchi-kafka     | 	sasl.jaas.config = null
benchi-kafka     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
benchi-kafka     | 	sasl.kerberos.min.time.before.relogin = 60000
benchi-kafka     | 	sasl.kerberos.service.name = null
benchi-kafka     | 	sasl.kerberos.ticket.renew.jitter = 0.05
benchi-kafka     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
benchi-kafka     | 	sasl.login.callback.handler.class = null
benchi-kafka     | 	sasl.login.class = null
benchi-kafka     | 	sasl.login.connect.timeout.ms = null
benchi-kafka     | 	sasl.login.read.timeout.ms = null
benchi-kafka     | 	sasl.login.refresh.buffer.seconds = 300
benchi-kafka     | 	sasl.login.refresh.min.period.seconds = 60
benchi-kafka     | 	sasl.login.refresh.window.factor = 0.8
benchi-kafka     | 	sasl.login.refresh.window.jitter = 0.05
benchi-kafka     | 	sasl.login.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.login.retry.backoff.ms = 100
benchi-kafka     | 	sasl.mechanism = GSSAPI
benchi-kafka     | 	sasl.oauthbearer.clientassertion.audience = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.expiration = 5
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.jti.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.nbf.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.issuer = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.location = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key.passphrase = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.subject = null
benchi-kafka     | 	sasl.oauthbearer.clock.skew.seconds = 30
benchi-kafka     | 	sasl.oauthbearer.expected.audience = null
benchi-kafka     | 	sasl.oauthbearer.expected.issuer = null
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.url = null
benchi-kafka     | 	sasl.oauthbearer.scope.claim.name = scope
benchi-kafka     | 	sasl.oauthbearer.sub.claim.name = sub
benchi-kafka     | 	sasl.oauthbearer.token.endpoint.url = null
benchi-kafka     | 	security.protocol = PLAINTEXT
benchi-kafka     | 	security.providers = null
benchi-kafka     | 	send.buffer.bytes = 131072
benchi-kafka     | 	socket.connection.setup.timeout.max.ms = 30000
benchi-kafka     | 	socket.connection.setup.timeout.ms = 10000
benchi-kafka     | 	ssl.cipher.suites = null
benchi-kafka     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	ssl.engine.factory.class = null
benchi-kafka     | 	ssl.key.password = null
benchi-kafka     | 	ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	ssl.keystore.certificate.chain = null
benchi-kafka     | 	ssl.keystore.key = null
benchi-kafka     | 	ssl.keystore.location = null
benchi-kafka     | 	ssl.keystore.password = null
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.protocol = TLSv1.3
benchi-kafka     | 	ssl.provider = null
benchi-kafka     | 	ssl.secure.random.implementation = null
benchi-kafka     | 	ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	ssl.truststore.certificates = null
benchi-kafka     | 	ssl.truststore.location = null
benchi-kafka     | 	ssl.truststore.password = null
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     |  (org.apache.kafka.clients.admin.AdminClientConfig)
benchi-kafka     | [2025-03-18 16:06:50,890] INFO These configurations '[sasl.oauthbearer.jwks.endpoint.refresh.ms, sasl.oauthbearer.clientassertion.include.jti.claim, sasl.login.refresh.min.period.seconds, sasl.oauthbearer.scope.claim.name, sasl.login.refresh.window.factor, sasl.kerberos.ticket.renew.window.factor, sasl.login.retry.backoff.ms, sasl.oauthbearer.clientassertion.expiration, sasl.kerberos.kinit.cmd, sasl.kerberos.ticket.renew.jitter, ssl.keystore.type, ssl.trustmanager.algorithm, sasl.kerberos.min.time.before.relogin, ssl.endpoint.identification.algorithm, ssl.protocol, confluent.metrics.reporter.bootstrap.servers, sasl.login.refresh.buffer.seconds, sasl.login.retry.backoff.max.ms, ssl.enabled.protocols, sasl.oauthbearer.sub.claim.name, ssl.truststore.type, sasl.oauthbearer.jwks.endpoint.retry.backoff.ms, sasl.oauthbearer.clock.skew.seconds, ssl.keymanager.algorithm, sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms, sasl.oauthbearer.clientassertion.include.nbf.claim, sasl.login.refresh.window.jitter]' were supplied but are not used yet. (org.apache.kafka.clients.admin.AdminClientConfig)
benchi-kafka     | [2025-03-18 16:06:50,890] INFO Kafka version: 7.8.1-ce (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:06:50,890] INFO Kafka commitId: eefb51c6f2f1b888b8b03296353338afbac8e213 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:06:50,890] INFO Kafka startTimeMs: 1742314010890 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:06:50,902] INFO DataBalancer: Adjusting topic _confluent_balancer_api_state configuration (com.linkedin.kafka.cruisecontrol.SbkTopicUtils)
benchi-kafka     | [2025-03-18 16:06:50,923] INFO [AdminClient clientId=kafka-cruise-control] The DescribeTopicPartitions API is not supported, using Metadata API to describe topics. (org.apache.kafka.clients.admin.KafkaAdminClient)
benchi-kafka     | [2025-03-18 16:06:50,925] INFO App info kafka.admin.client for kafka-cruise-control unregistered (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:06:50,926] INFO Confirmed that topic _confluent_balancer_api_state exists. (io.confluent.databalancer.persistence.ApiStatePersistenceStore)
benchi-kafka     | [2025-03-18 16:06:50,926] INFO DataBalancer: Startup component StartupComponent ApiStatePersistenceStore ready to proceed (io.confluent.databalancer.startup.StartupComponents)
benchi-kafka     | [2025-03-18 16:06:50,926] INFO DataBalancer: Startup checking succeeded, proceeding to full validation. (io.confluent.databalancer.startup.StartupComponents)
benchi-kafka     | [2025-03-18 16:06:50,926] INFO DataBalancer: Creating CruiseControl (io.confluent.databalancer.startup.CruiseControlStartable)
benchi-kafka     | [2025-03-18 16:06:50,935] INFO DataBalancer: Bootstrap server endpoint is Endpoint(listenerName='PLAINTEXT', securityProtocol=PLAINTEXT, host='broker', port=29092) (io.confluent.databalancer.startup.CruiseControlStartable)
benchi-kafka     | [2025-03-18 16:06:50,936] INFO AdminClientConfig values: 
benchi-kafka     | 	auto.include.jmx.reporter = true
benchi-kafka     | 	bootstrap.controllers = []
benchi-kafka     | 	bootstrap.servers = [broker:29092]
benchi-kafka     | 	client.dns.lookup = use_all_dns_ips
benchi-kafka     | 	client.id = null-admin-1
benchi-kafka     | 	confluent.lkc.id = null
benchi-kafka     | 	confluent.metrics.reporter.bootstrap.servers = localhost:9092
benchi-kafka     | 	confluent.proxy.protocol.client.address = null
benchi-kafka     | 	confluent.proxy.protocol.client.mode = PROXY
benchi-kafka     | 	confluent.proxy.protocol.client.port = null
benchi-kafka     | 	confluent.proxy.protocol.client.version = NONE
benchi-kafka     | 	connections.max.idle.ms = 300000
benchi-kafka     | 	default.api.timeout.ms = 60000
benchi-kafka     | 	enable.metrics.push = true
benchi-kafka     | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
benchi-kafka     | 	metadata.max.age.ms = 300000
benchi-kafka     | 	metadata.recovery.strategy = none
benchi-kafka     | 	metric.reporters = []
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.recording.level = INFO
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	receive.buffer.bytes = 65536
benchi-kafka     | 	reconnect.backoff.max.ms = 5000
benchi-kafka     | 	reconnect.backoff.ms = 500
benchi-kafka     | 	request.timeout.ms = 30000
benchi-kafka     | 	retries = 2147483647
benchi-kafka     | 	retry.backoff.max.ms = 1000
benchi-kafka     | 	retry.backoff.ms = 500
benchi-kafka     | 	sasl.client.callback.handler.class = null
benchi-kafka     | 	sasl.jaas.config = null
benchi-kafka     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
benchi-kafka     | 	sasl.kerberos.min.time.before.relogin = 60000
benchi-kafka     | 	sasl.kerberos.service.name = null
benchi-kafka     | 	sasl.kerberos.ticket.renew.jitter = 0.05
benchi-kafka     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
benchi-kafka     | 	sasl.login.callback.handler.class = null
benchi-kafka     | 	sasl.login.class = null
benchi-kafka     | 	sasl.login.connect.timeout.ms = null
benchi-kafka     | 	sasl.login.read.timeout.ms = null
benchi-kafka     | 	sasl.login.refresh.buffer.seconds = 300
benchi-kafka     | 	sasl.login.refresh.min.period.seconds = 60
benchi-kafka     | 	sasl.login.refresh.window.factor = 0.8
benchi-kafka     | 	sasl.login.refresh.window.jitter = 0.05
benchi-kafka     | 	sasl.login.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.login.retry.backoff.ms = 100
benchi-kafka     | 	sasl.mechanism = GSSAPI
benchi-kafka     | 	sasl.oauthbearer.clientassertion.audience = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.expiration = 5
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.jti.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.nbf.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.issuer = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.location = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key.passphrase = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.subject = null
benchi-kafka     | 	sasl.oauthbearer.clock.skew.seconds = 30
benchi-kafka     | 	sasl.oauthbearer.expected.audience = null
benchi-kafka     | 	sasl.oauthbearer.expected.issuer = null
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.url = null
benchi-kafka     | 	sasl.oauthbearer.scope.claim.name = scope
benchi-kafka     | 	sasl.oauthbearer.sub.claim.name = sub
benchi-kafka     | 	sasl.oauthbearer.token.endpoint.url = null
benchi-kafka     | 	security.protocol = PLAINTEXT
benchi-kafka     | 	security.providers = null
benchi-kafka     | 	send.buffer.bytes = 131072
benchi-kafka     | 	socket.connection.setup.timeout.max.ms = 30000
benchi-kafka     | 	socket.connection.setup.timeout.ms = 10000
benchi-kafka     | 	ssl.cipher.suites = null
benchi-kafka     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	ssl.engine.factory.class = null
benchi-kafka     | 	ssl.key.password = null
benchi-kafka     | 	ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	ssl.keystore.certificate.chain = null
benchi-kafka     | 	ssl.keystore.key = null
benchi-kafka     | 	ssl.keystore.location = null
benchi-kafka     | 	ssl.keystore.password = null
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.protocol = TLSv1.3
benchi-kafka     | 	ssl.provider = null
benchi-kafka     | 	ssl.secure.random.implementation = null
benchi-kafka     | 	ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	ssl.truststore.certificates = null
benchi-kafka     | 	ssl.truststore.location = null
benchi-kafka     | 	ssl.truststore.password = null
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     |  (org.apache.kafka.clients.admin.AdminClientConfig)
benchi-kafka     | [2025-03-18 16:06:50,937] INFO These configurations '[confluent.metrics.reporter.bootstrap.servers]' were supplied but are not used yet. (org.apache.kafka.clients.admin.AdminClientConfig)
benchi-kafka     | [2025-03-18 16:06:50,937] INFO Kafka version: 7.8.1-ce (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:06:50,937] INFO Kafka commitId: eefb51c6f2f1b888b8b03296353338afbac8e213 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:06:50,937] INFO Kafka startTimeMs: 1742314010937 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:06:50,938] INFO Starting KafkaBasedLog with topic _confluent_balancer_api_state reportErrorsToCallback=false (org.apache.kafka.connect.util.KafkaBasedLog)
benchi-kafka     | [2025-03-18 16:06:50,938] INFO ProducerConfig values: 
benchi-kafka     | 	acks = -1
benchi-kafka     | 	auto.include.jmx.reporter = true
benchi-kafka     | 	batch.size = 16384
benchi-kafka     | 	bootstrap.servers = [broker:29092]
benchi-kafka     | 	buffer.memory = 33554432
benchi-kafka     | 	client.dns.lookup = use_all_dns_ips
benchi-kafka     | 	client.id = _confluent_balancer_api_state-producer-1
benchi-kafka     | 	compression.gzip.level = -1
benchi-kafka     | 	compression.lz4.level = 9
benchi-kafka     | 	compression.type = none
benchi-kafka     | 	compression.zstd.level = 3
benchi-kafka     | 	confluent.lkc.id = null
benchi-kafka     | 	confluent.proxy.protocol.client.address = null
benchi-kafka     | 	confluent.proxy.protocol.client.mode = PROXY
benchi-kafka     | 	confluent.proxy.protocol.client.port = null
benchi-kafka     | 	confluent.proxy.protocol.client.version = NONE
benchi-kafka     | 	connections.max.idle.ms = 540000
benchi-kafka     | 	delivery.timeout.ms = 120000
benchi-kafka     | 	enable.idempotence = false
benchi-kafka     | 	enable.metrics.push = true
benchi-kafka     | 	interceptor.classes = []
benchi-kafka     | 	key.serializer = class io.confluent.databalancer.persistence.ApiStatePersistenceStore$SbkApiStatusKeySerde
benchi-kafka     | 	linger.ms = 0
benchi-kafka     | 	max.block.ms = 60000
benchi-kafka     | 	max.in.flight.requests.per.connection = 1
benchi-kafka     | 	max.request.size = 1048576
benchi-kafka     | 	metadata.max.age.ms = 300000
benchi-kafka     | 	metadata.max.idle.ms = 300000
benchi-kafka     | 	metadata.recovery.strategy = none
benchi-kafka     | 	metric.reporters = []
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.recording.level = INFO
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	partitioner.adaptive.partitioning.enable = true
benchi-kafka     | 	partitioner.availability.timeout.ms = 0
benchi-kafka     | 	partitioner.class = null
benchi-kafka     | 	partitioner.ignore.keys = false
benchi-kafka     | 	receive.buffer.bytes = 32768
benchi-kafka     | 	reconnect.backoff.max.ms = 1000
benchi-kafka     | 	reconnect.backoff.ms = 50
benchi-kafka     | 	request.timeout.ms = 30000
benchi-kafka     | 	retries = 2147483647
benchi-kafka     | 	retry.backoff.max.ms = 1000
benchi-kafka     | 	retry.backoff.ms = 100
benchi-kafka     | 	sasl.client.callback.handler.class = null
benchi-kafka     | 	sasl.jaas.config = null
benchi-kafka     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
benchi-kafka     | 	sasl.kerberos.min.time.before.relogin = 60000
benchi-kafka     | 	sasl.kerberos.service.name = null
benchi-kafka     | 	sasl.kerberos.ticket.renew.jitter = 0.05
benchi-kafka     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
benchi-kafka     | 	sasl.login.callback.handler.class = null
benchi-kafka     | 	sasl.login.class = null
benchi-kafka     | 	sasl.login.connect.timeout.ms = null
benchi-kafka     | 	sasl.login.read.timeout.ms = null
benchi-kafka     | 	sasl.login.refresh.buffer.seconds = 300
benchi-kafka     | 	sasl.login.refresh.min.period.seconds = 60
benchi-kafka     | 	sasl.login.refresh.window.factor = 0.8
benchi-kafka     | 	sasl.login.refresh.window.jitter = 0.05
benchi-kafka     | 	sasl.login.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.login.retry.backoff.ms = 100
benchi-kafka     | 	sasl.mechanism = GSSAPI
benchi-kafka     | 	sasl.oauthbearer.clientassertion.audience = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.expiration = 5
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.jti.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.nbf.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.issuer = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.location = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key.passphrase = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.subject = null
benchi-kafka     | 	sasl.oauthbearer.clock.skew.seconds = 30
benchi-kafka     | 	sasl.oauthbearer.expected.audience = null
benchi-kafka     | 	sasl.oauthbearer.expected.issuer = null
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.url = null
benchi-kafka     | 	sasl.oauthbearer.scope.claim.name = scope
benchi-kafka     | 	sasl.oauthbearer.sub.claim.name = sub
benchi-kafka     | 	sasl.oauthbearer.token.endpoint.url = null
benchi-kafka     | 	security.protocol = PLAINTEXT
benchi-kafka     | 	security.providers = null
benchi-kafka     | 	send.buffer.bytes = 131072
benchi-kafka     | 	socket.connection.setup.timeout.max.ms = 30000
benchi-kafka     | 	socket.connection.setup.timeout.ms = 10000
benchi-kafka     | 	ssl.cipher.suites = null
benchi-kafka     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	ssl.engine.factory.class = null
benchi-kafka     | 	ssl.key.password = null
benchi-kafka     | 	ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	ssl.keystore.certificate.chain = null
benchi-kafka     | 	ssl.keystore.key = null
benchi-kafka     | 	ssl.keystore.location = null
benchi-kafka     | 	ssl.keystore.password = null
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.protocol = TLSv1.3
benchi-kafka     | 	ssl.provider = null
benchi-kafka     | 	ssl.secure.random.implementation = null
benchi-kafka     | 	ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	ssl.truststore.certificates = null
benchi-kafka     | 	ssl.truststore.location = null
benchi-kafka     | 	ssl.truststore.password = null
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     | 	transaction.timeout.ms = 60000
benchi-kafka     | 	transactional.id = null
benchi-kafka     | 	value.serializer = class io.confluent.databalancer.persistence.ApiStatePersistenceStore$SbkApiStatusMessageSerde
benchi-kafka     |  (org.apache.kafka.clients.producer.ProducerConfig)
benchi-kafka     | [2025-03-18 16:06:50,939] INFO initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector)
benchi-kafka     | [2025-03-18 16:06:50,942] INFO Kafka version: 7.8.1-ce (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:06:50,942] INFO Kafka commitId: eefb51c6f2f1b888b8b03296353338afbac8e213 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:06:50,942] INFO Kafka startTimeMs: 1742314010942 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:06:50,942] INFO ConsumerConfig values: 
benchi-kafka     | 	allow.auto.create.topics = true
benchi-kafka     | 	auto.commit.interval.ms = 5000
benchi-kafka     | 	auto.include.jmx.reporter = true
benchi-kafka     | 	auto.offset.reset = earliest
benchi-kafka     | 	bootstrap.servers = [broker:29092]
benchi-kafka     | 	check.crcs = true
benchi-kafka     | 	client.dns.lookup = use_all_dns_ips
benchi-kafka     | 	client.id = _confluent_balancer_api_state-consumer-1
benchi-kafka     | 	client.rack = 
benchi-kafka     | 	confluent.lkc.id = null
benchi-kafka     | 	confluent.proxy.protocol.client.address = null
benchi-kafka     | 	confluent.proxy.protocol.client.mode = PROXY
benchi-kafka     | 	confluent.proxy.protocol.client.port = null
benchi-kafka     | 	confluent.proxy.protocol.client.version = NONE
benchi-kafka     | 	connections.max.idle.ms = 540000
benchi-kafka     | 	default.api.timeout.ms = 60000
benchi-kafka     | 	enable.auto.commit = false
benchi-kafka     | 	enable.metrics.push = true
benchi-kafka     | 	exclude.internal.topics = true
benchi-kafka     | 	fetch.max.bytes = 52428800
benchi-kafka     | 	fetch.max.wait.ms = 500
benchi-kafka     | 	fetch.min.bytes = 1
benchi-kafka     | 	group.id = null
benchi-kafka     | 	group.instance.id = null
benchi-kafka     | 	group.protocol = classic
benchi-kafka     | 	group.remote.assignor = null
benchi-kafka     | 	heartbeat.interval.ms = 3000
benchi-kafka     | 	interceptor.classes = []
benchi-kafka     | 	internal.leave.group.on.close = true
benchi-kafka     | 	internal.throw.on.fetch.stable.offset.unsupported = false
benchi-kafka     | 	isolation.level = read_uncommitted
benchi-kafka     | 	key.deserializer = class io.confluent.databalancer.persistence.ApiStatePersistenceStore$SbkApiStatusKeySerde
benchi-kafka     | 	max.partition.fetch.bytes = 1048576
benchi-kafka     | 	max.poll.interval.ms = 300000
benchi-kafka     | 	max.poll.records = 500
benchi-kafka     | 	metadata.max.age.ms = 300000
benchi-kafka     | 	metadata.recovery.strategy = none
benchi-kafka     | 	metric.reporters = []
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.recording.level = INFO
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
benchi-kafka     | 	receive.buffer.bytes = 65536
benchi-kafka     | 	reconnect.backoff.max.ms = 1000
benchi-kafka     | 	reconnect.backoff.ms = 50
benchi-kafka     | 	request.timeout.ms = 30000
benchi-kafka     | 	retry.backoff.max.ms = 1000
benchi-kafka     | 	retry.backoff.ms = 100
benchi-kafka     | 	sasl.client.callback.handler.class = null
benchi-kafka     | 	sasl.jaas.config = null
benchi-kafka     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
benchi-kafka     | 	sasl.kerberos.min.time.before.relogin = 60000
benchi-kafka     | 	sasl.kerberos.service.name = null
benchi-kafka     | 	sasl.kerberos.ticket.renew.jitter = 0.05
benchi-kafka     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
benchi-kafka     | 	sasl.login.callback.handler.class = null
benchi-kafka     | 	sasl.login.class = null
benchi-kafka     | 	sasl.login.connect.timeout.ms = null
benchi-kafka     | 	sasl.login.read.timeout.ms = null
benchi-kafka     | 	sasl.login.refresh.buffer.seconds = 300
benchi-kafka     | 	sasl.login.refresh.min.period.seconds = 60
benchi-kafka     | 	sasl.login.refresh.window.factor = 0.8
benchi-kafka     | 	sasl.login.refresh.window.jitter = 0.05
benchi-kafka     | 	sasl.login.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.login.retry.backoff.ms = 100
benchi-kafka     | 	sasl.mechanism = GSSAPI
benchi-kafka     | 	sasl.oauthbearer.clientassertion.audience = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.expiration = 5
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.jti.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.nbf.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.issuer = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.location = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key.passphrase = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.subject = null
benchi-kafka     | 	sasl.oauthbearer.clock.skew.seconds = 30
benchi-kafka     | 	sasl.oauthbearer.expected.audience = null
benchi-kafka     | 	sasl.oauthbearer.expected.issuer = null
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.url = null
benchi-kafka     | 	sasl.oauthbearer.scope.claim.name = scope
benchi-kafka     | 	sasl.oauthbearer.sub.claim.name = sub
benchi-kafka     | 	sasl.oauthbearer.token.endpoint.url = null
benchi-kafka     | 	security.protocol = PLAINTEXT
benchi-kafka     | 	security.providers = null
benchi-kafka     | 	send.buffer.bytes = 131072
benchi-kafka     | 	session.timeout.ms = 45000
benchi-kafka     | 	socket.connection.setup.timeout.max.ms = 30000
benchi-kafka     | 	socket.connection.setup.timeout.ms = 10000
benchi-kafka     | 	ssl.cipher.suites = null
benchi-kafka     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	ssl.engine.factory.class = null
benchi-kafka     | 	ssl.key.password = null
benchi-kafka     | 	ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	ssl.keystore.certificate.chain = null
benchi-kafka     | 	ssl.keystore.key = null
benchi-kafka     | 	ssl.keystore.location = null
benchi-kafka     | 	ssl.keystore.password = null
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.protocol = TLSv1.3
benchi-kafka     | 	ssl.provider = null
benchi-kafka     | 	ssl.secure.random.implementation = null
benchi-kafka     | 	ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	ssl.truststore.certificates = null
benchi-kafka     | 	ssl.truststore.location = null
benchi-kafka     | 	ssl.truststore.password = null
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     | 	value.deserializer = class io.confluent.databalancer.persistence.ApiStatePersistenceStore$SbkApiStatusMessageSerde
benchi-kafka     |  (org.apache.kafka.clients.consumer.ConsumerConfig)
benchi-kafka     | [2025-03-18 16:06:50,942] INFO initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector)
benchi-kafka     | [2025-03-18 16:06:50,943] INFO Kafka version: 7.8.1-ce (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:06:50,943] INFO Kafka commitId: eefb51c6f2f1b888b8b03296353338afbac8e213 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:06:50,943] INFO Kafka startTimeMs: 1742314010943 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:06:50,943] INFO [Producer clientId=_confluent_balancer_api_state-producer-1] Cluster ID: MkU3OEVBNTcwNTJENDM2Qk (org.apache.kafka.clients.Metadata)
benchi-kafka     | [2025-03-18 16:06:50,944] INFO [Consumer clientId=_confluent_balancer_api_state-consumer-1, groupId=null] Cluster ID: MkU3OEVBNTcwNTJENDM2Qk (org.apache.kafka.clients.Metadata)
benchi-kafka     | [2025-03-18 16:06:50,945] INFO [Consumer clientId=_confluent_balancer_api_state-consumer-1, groupId=null] Assigned to partition(s): _confluent_balancer_api_state-0 (org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer)
benchi-kafka     | [2025-03-18 16:06:50,945] INFO [Consumer clientId=_confluent_balancer_api_state-consumer-1, groupId=null] Seeking to earliest offset of partition _confluent_balancer_api_state-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
benchi-kafka     | [2025-03-18 16:06:50,955] INFO Finished reading KafkaBasedLog for topic _confluent_balancer_api_state (org.apache.kafka.connect.util.KafkaBasedLog)
benchi-kafka     | [2025-03-18 16:06:50,955] INFO Started KafkaBasedLog for topic _confluent_balancer_api_state (org.apache.kafka.connect.util.KafkaBasedLog)
benchi-kafka     | [2025-03-18 16:06:50,955] INFO Started DataBalancer Api State Persistence Store (io.confluent.databalancer.persistence.ApiStatePersistenceStore)
benchi-kafka     | [2025-03-18 16:06:50,957] INFO Starting Kafka Cruise Control... (com.linkedin.kafka.cruisecontrol.KafkaCruiseControl)
benchi-kafka     | [2025-03-18 16:06:50,957] INFO Initializing DataBalancer with goals UpdatableSbcGoalsConfig{rebalancingGoals=GoalsConfig{requirements=(requiredNumWindows=6, minMonitoredPartitionPercentage=0.950, includedAllTopics=true, fetchTopicPlacements=true), goals=[MovementExclusionGoal, ReplicaPlacementGoal, RackAwareGoal, CellAwareGoal, TenantAwareGoal, MaxReplicaMovementParallelismGoal, ReplicaCapacityGoal, DiskCapacityGoal, NetworkInboundCapacityGoal, NetworkOutboundCapacityGoal, ReplicationInboundCapacityGoal, ProducerInboundCapacityGoal, MirrorInboundCapacityGoal, ConsumerOutboundCapacityGoal, SystemTopicEvenDistributionGoal, ReplicaDistributionGoal, DiskUsageDistributionGoal, LeaderReplicaDistributionGoal, NetworkInboundUsageDistributionGoal, NetworkOutboundUsageDistributionGoal, TopicReplicaDistributionGoal]}, triggeringGoals=GoalsConfig{requirements=(requiredNumWindows=1, minMonitoredPartitionPercentage=0.950, includedAllTopics=true, fetchTopicPlacements=true), goals=[ReplicaPlacementGoal, RackAwareGoal, CellAwareGoal, TenantAwareGoal, MaxReplicaMovementParallelismGoal, ReplicaCapacityGoal, DiskCapacityGoal, NetworkInboundCapacityGoal, NetworkOutboundCapacityGoal, ReplicationInboundCapacityGoal, ProducerInboundCapacityGoal, ReplicaDistributionGoal, DiskUsageDistributionGoal]}, incrementalBalancingEnabled=false, incrementalBalancingGoals=GoalsConfig{requirements=(requiredNumWindows=5, minMonitoredPartitionPercentage=0.950, includedAllTopics=true, fetchTopicPlacements=true), goals=[MovementExclusionGoal, ReplicaPlacementGoal, RackAwareGoal, CellAwareGoal, TenantAwareGoal, MaxReplicaMovementParallelismGoal, ReplicaCapacityGoal, DiskCapacityGoal, NetworkInboundCapacityGoal, NetworkOutboundCapacityGoal, ReplicationInboundCapacityGoal, ProducerInboundCapacityGoal, MirrorInboundCapacityGoal, ConsumerOutboundCapacityGoal, IncrementalCPUResourceDistributionGoal, IncrementalTopicReplicaDistributionGoal]}} (com.linkedin.kafka.cruisecontrol.KafkaCruiseControl)
benchi-kafka     | [2025-03-18 16:06:50,958] INFO AdminClientConfig values: 
benchi-kafka     | 	auto.include.jmx.reporter = true
benchi-kafka     | 	bootstrap.controllers = []
benchi-kafka     | 	bootstrap.servers = [broker:29092]
benchi-kafka     | 	client.dns.lookup = use_all_dns_ips
benchi-kafka     | 	client.id = 
benchi-kafka     | 	confluent.lkc.id = null
benchi-kafka     | 	confluent.metrics.reporter.bootstrap.servers = localhost:9092
benchi-kafka     | 	confluent.proxy.protocol.client.address = null
benchi-kafka     | 	confluent.proxy.protocol.client.mode = PROXY
benchi-kafka     | 	confluent.proxy.protocol.client.port = null
benchi-kafka     | 	confluent.proxy.protocol.client.version = NONE
benchi-kafka     | 	connections.max.idle.ms = 300000
benchi-kafka     | 	default.api.timeout.ms = 60000
benchi-kafka     | 	enable.metrics.push = true
benchi-kafka     | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
benchi-kafka     | 	metadata.max.age.ms = 300000
benchi-kafka     | 	metadata.recovery.strategy = none
benchi-kafka     | 	metric.reporters = []
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.recording.level = INFO
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	receive.buffer.bytes = 65536
benchi-kafka     | 	reconnect.backoff.max.ms = 5000
benchi-kafka     | 	reconnect.backoff.ms = 500
benchi-kafka     | 	request.timeout.ms = 30000
benchi-kafka     | 	retries = 2147483647
benchi-kafka     | 	retry.backoff.max.ms = 1000
benchi-kafka     | 	retry.backoff.ms = 500
benchi-kafka     | 	sasl.client.callback.handler.class = null
benchi-kafka     | 	sasl.jaas.config = null
benchi-kafka     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
benchi-kafka     | 	sasl.kerberos.min.time.before.relogin = 60000
benchi-kafka     | 	sasl.kerberos.service.name = null
benchi-kafka     | 	sasl.kerberos.ticket.renew.jitter = 0.05
benchi-kafka     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
benchi-kafka     | 	sasl.login.callback.handler.class = null
benchi-kafka     | 	sasl.login.class = null
benchi-kafka     | 	sasl.login.connect.timeout.ms = null
benchi-kafka     | 	sasl.login.read.timeout.ms = null
benchi-kafka     | 	sasl.login.refresh.buffer.seconds = 300
benchi-kafka     | 	sasl.login.refresh.min.period.seconds = 60
benchi-kafka     | 	sasl.login.refresh.window.factor = 0.8
benchi-kafka     | 	sasl.login.refresh.window.jitter = 0.05
benchi-kafka     | 	sasl.login.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.login.retry.backoff.ms = 100
benchi-kafka     | 	sasl.mechanism = GSSAPI
benchi-kafka     | 	sasl.oauthbearer.clientassertion.audience = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.expiration = 5
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.jti.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.nbf.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.issuer = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.location = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key.passphrase = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.subject = null
benchi-kafka     | 	sasl.oauthbearer.clock.skew.seconds = 30
benchi-kafka     | 	sasl.oauthbearer.expected.audience = null
benchi-kafka     | 	sasl.oauthbearer.expected.issuer = null
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.url = null
benchi-kafka     | 	sasl.oauthbearer.scope.claim.name = scope
benchi-kafka     | 	sasl.oauthbearer.sub.claim.name = sub
benchi-kafka     | 	sasl.oauthbearer.token.endpoint.url = null
benchi-kafka     | 	security.protocol = PLAINTEXT
benchi-kafka     | 	security.providers = null
benchi-kafka     | 	send.buffer.bytes = 131072
benchi-kafka     | 	socket.connection.setup.timeout.max.ms = 30000
benchi-kafka     | 	socket.connection.setup.timeout.ms = 10000
benchi-kafka     | 	ssl.cipher.suites = null
benchi-kafka     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	ssl.engine.factory.class = null
benchi-kafka     | 	ssl.key.password = null
benchi-kafka     | 	ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	ssl.keystore.certificate.chain = null
benchi-kafka     | 	ssl.keystore.key = null
benchi-kafka     | 	ssl.keystore.location = null
benchi-kafka     | 	ssl.keystore.password = null
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.protocol = TLSv1.3
benchi-kafka     | 	ssl.provider = null
benchi-kafka     | 	ssl.secure.random.implementation = null
benchi-kafka     | 	ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	ssl.truststore.certificates = null
benchi-kafka     | 	ssl.truststore.location = null
benchi-kafka     | 	ssl.truststore.password = null
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     |  (org.apache.kafka.clients.admin.AdminClientConfig)
benchi-kafka     | [2025-03-18 16:06:50,958] INFO These configurations '[confluent.metrics.reporter.bootstrap.servers]' were supplied but are not used yet. (org.apache.kafka.clients.admin.AdminClientConfig)
benchi-kafka     | [2025-03-18 16:06:50,958] INFO Kafka version: 7.8.1-ce (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:06:50,958] INFO Kafka commitId: eefb51c6f2f1b888b8b03296353338afbac8e213 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:06:50,958] INFO Kafka startTimeMs: 1742314010958 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:06:50,962] INFO CruiseControl: Attempting to configure Broker Capacity from config properties (com.linkedin.kafka.cruisecontrol.config.BrokerCapacityResolver)
benchi-kafka     | [2025-03-18 16:06:50,981] INFO [AdminClient clientId=adminclient-5] The DescribeTopicPartitions API is not supported, using Metadata API to describe topics. (org.apache.kafka.clients.admin.KafkaAdminClient)
benchi-kafka     | [2025-03-18 16:06:50,993] INFO Notifying listeners about metadata change. (com.linkedin.kafka.cruisecontrol.common.MetadataClient)
benchi-kafka     | [2025-03-18 16:06:51,009] INFO [Partition _confluent_balancer_api_state-0 broker=1] roll: _confluent_balancer_api_state-0: first produce received, lastOffset: 0, leaderEpoch: 0, numMessages:1, time diff: 1126 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,009] INFO [Partition _confluent_balancer_api_state-0 broker=1] roll: _confluent_balancer_api_state-0: first HWM advanced, leaderEpoch: 0, old HW: 0, new HW: 1, become leader time: 1742314009883 ms, time diff: 1126 ms (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,023] INFO Loaded an even cluster load state record EvenClusterLoadStateRecord{ currentState=null, currentStateCreatedAt=0, currentStateLastUpdatedAt=0, currentStateException=null, previousState=null, previousStateCreatedAt=0, previousStateLastUpdatedAt=0, previousStateException=null} (io.confluent.databalancer.persistence.ApiStatePersistenceStore)
benchi-kafka     | [2025-03-18 16:06:51,040] INFO Set throttle rate 10485760. Will not override static throttles when setting the rate. (com.linkedin.kafka.cruisecontrol.executor.ReplicationThrottleHelper)
benchi-kafka     | [2025-03-18 16:06:51,059] INFO Removed throttled replicas config for topics: [_confluent_balancer_api_state, _confluent-command, _confluent-telemetry-metrics, _confluent-link-metadata] (com.linkedin.kafka.cruisecontrol.executor.ReplicationThrottleHelper)
benchi-kafka     | [2025-03-18 16:06:51,068] INFO Removed throttle rate config from 0 brokers (com.linkedin.kafka.cruisecontrol.executor.ReplicationThrottleHelper)
benchi-kafka     | [2025-03-18 16:06:51,069] INFO Starting anomaly detector. (com.linkedin.kafka.cruisecontrol.detector.AnomalyDetector)
benchi-kafka     | [2025-03-18 16:06:51,069] INFO Starting metric sampling task. (com.linkedin.kafka.cruisecontrol.monitor.task.MetricSamplingTask)
benchi-kafka     | [2025-03-18 16:06:51,069] INFO [SBK_BrokerFailureDetector]: Starting (com.linkedin.kafka.cruisecontrol.detector.BrokerFailureDetector)
benchi-kafka     | [2025-03-18 16:06:51,070] INFO Starting anomaly handler (com.linkedin.kafka.cruisecontrol.detector.AnomalyDetector)
benchi-kafka     | [2025-03-18 16:06:51,070] INFO KafkaCruiseControlConfig values: 
benchi-kafka     | 	alter.configs.response.timeout.ms = 30000
benchi-kafka     | 	anomaly.detection.allow.capacity.estimation = true
benchi-kafka     | 	anomaly.detection.goals = [io.confluent.cruisecontrol.analyzer.goals.ReplicaPlacementGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.RackAwareGoal, io.confluent.cruisecontrol.analyzer.goals.CellAwareGoal, io.confluent.cruisecontrol.analyzer.goals.TenantAwareGoal, io.confluent.cruisecontrol.analyzer.goals.MaxReplicaMovementParallelismGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.ReplicaCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.DiskCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.NetworkInboundCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.NetworkOutboundCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.ReplicationInboundCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.ProducerInboundCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.ReplicaDistributionGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.DiskUsageDistributionGoal]
benchi-kafka     | 	anomaly.detection.interval.ms = 60000
benchi-kafka     | 	bootstrap.servers = [broker:29092]
benchi-kafka     | 	broker.addition.elapsed.time.ms.completion.threshold = 57600000
benchi-kafka     | 	broker.addition.mean.cpu.percent.completion.threshold = 0.5
benchi-kafka     | 	broker.capacity.config.resolver.class = class com.linkedin.kafka.cruisecontrol.config.BrokerCapacityResolver
benchi-kafka     | 	broker.failure.alert.threshold.ms = 0
benchi-kafka     | 	broker.failure.exclude.recently.removed.brokers = true
benchi-kafka     | 	broker.failure.self.healing.threshold.ms = 3600000
benchi-kafka     | 	broker.metric.sample.aggregator.completeness.cache.size = 5
benchi-kafka     | 	broker.metric.sample.store.topic = _confluent_balancer_broker_samples
benchi-kafka     | 	broker.removal.shutdown.timeout.ms = 600000
benchi-kafka     | 	broker.replica.exclusion.timeout.ms = 120000
benchi-kafka     | 	bytes.cpu.contribution.weight = 0.2
benchi-kafka     | 	calculated.throttle.ratio = 0.8
benchi-kafka     | 	capacity.threshold.upper.limit = 0.95
benchi-kafka     | 	cdbe.shutdown.wait.ms = 15000
benchi-kafka     | 	cell.load.upper.bound = 0.7
benchi-kafka     | 	cell.overload.detection.interval.ms = 3600000
benchi-kafka     | 	cell.overload.duration.ms = 86400000
benchi-kafka     | 	client.id = kafka-cruise-control
benchi-kafka     | 	confluent.balancer.additional.invalidation.duration.ms = 60000
benchi-kafka     | 	confluent.balancer.plan.computation.retry.timeout.ms = 3600000
benchi-kafka     | 	confluent.cells.enable = false
benchi-kafka     | 	connections.max.idle.ms = 540000
benchi-kafka     | 	consume.out.bound.should.balance.FFF.traffic = false
benchi-kafka     | 	consumer.out.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	consumer.outbound.capacity.threshold = 0.9
benchi-kafka     | 	cpu.balance.threshold = 1.1
benchi-kafka     | 	cpu.capacity.threshold = 1.0
benchi-kafka     | 	cpu.low.utilization.threshold = 0.2
benchi-kafka     | 	cpu.low.utilization.threshold.for.broker.addition = 0.2
benchi-kafka     | 	cpu.utilization.detector.duration.ms = 600000
benchi-kafka     | 	cpu.utilization.detector.enabled = false
benchi-kafka     | 	cpu.utilization.detector.overutilization.threshold = 80.0
benchi-kafka     | 	cpu.utilization.detector.underutilization.threshold = 50.0
benchi-kafka     | 	default.api.timeout.ms = 60000
benchi-kafka     | 	default.replica.movement.strategies = [com.linkedin.kafka.cruisecontrol.executor.strategy.BaseReplicaMovementStrategy]
benchi-kafka     | 	describe.broker.exclusion.timeout.ms = 60000
benchi-kafka     | 	describe.cluster.response.timeout.ms = 30000
benchi-kafka     | 	describe.configs.batch.size = 1000
benchi-kafka     | 	describe.configs.response.timeout.ms = 30000
benchi-kafka     | 	describe.topics.response.timeout.ms = 30000
benchi-kafka     | 	disk.balance.threshold = 1.1
benchi-kafka     | 	disk.low.utilization.threshold = 0.2
benchi-kafka     | 	disk.max.load = 0.85
benchi-kafka     | 	disk.min.free.space.gb = 0
benchi-kafka     | 	disk.min.free.space.lower.limit.gb = 0
benchi-kafka     | 	disk.read.ratio = 0.2
benchi-kafka     | 	disk.utilization.detector.duration.ms = 600000
benchi-kafka     | 	disk.utilization.detector.enabled = false
benchi-kafka     | 	disk.utilization.detector.overutilization.threshold = 80.0
benchi-kafka     | 	disk.utilization.detector.reserved.capacity = 150000.0
benchi-kafka     | 	disk.utilization.detector.underutilization.threshold = 35.0
benchi-kafka     | 	dynamic.throttling.enabled = true
benchi-kafka     | 	execution.progress.check.interval.ms = 7000
benchi-kafka     | 	executor.leader.action.timeout.ms = 180000
benchi-kafka     | 	executor.notifier.class = class com.linkedin.kafka.cruisecontrol.executor.ExecutorNoopNotifier
benchi-kafka     | 	executor.reservation.refresh.time.ms = 60000
benchi-kafka     | 	follower.network.inbound.weight.for.cpu.util = 0.15
benchi-kafka     | 	goal.balancedness.priority.weight = 1.1
benchi-kafka     | 	goal.balancedness.strictness.weight = 1.5
benchi-kafka     | 	goal.violation.delay.on.new.brokers.ms = 1800000
benchi-kafka     | 	goal.violation.distribution.threshold.multiplier = 1.1
benchi-kafka     | 	goal.violation.exclude.recently.removed.brokers = true
benchi-kafka     | 	goals = [com.linkedin.kafka.cruisecontrol.analyzer.goals.MovementExclusionGoal, io.confluent.cruisecontrol.analyzer.goals.ReplicaPlacementGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.RackAwareGoal, io.confluent.cruisecontrol.analyzer.goals.CellAwareGoal, io.confluent.cruisecontrol.analyzer.goals.TenantAwareGoal, io.confluent.cruisecontrol.analyzer.goals.MaxReplicaMovementParallelismGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.ReplicaCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.DiskCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.NetworkInboundCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.NetworkOutboundCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.ReplicationInboundCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.ProducerInboundCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.MirrorInboundCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.ConsumerOutboundCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.SystemTopicEvenDistributionGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.ReplicaDistributionGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.DiskUsageDistributionGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.LeaderReplicaDistributionGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.NetworkInboundUsageDistributionGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.NetworkOutboundUsageDistributionGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.TopicReplicaDistributionGoal]
benchi-kafka     | 	hot.partition.capacity.utilization.threshold = 0.2
benchi-kafka     | 	incremental.balancing.cpu.top.proposal.tracking.enabled = true
benchi-kafka     | 	incremental.balancing.cpu.top.proposal.tracking.num.proposals = 15
benchi-kafka     | 	incremental.balancing.enabled = false
benchi-kafka     | 	incremental.balancing.goals = [com.linkedin.kafka.cruisecontrol.analyzer.goals.MovementExclusionGoal, io.confluent.cruisecontrol.analyzer.goals.ReplicaPlacementGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.RackAwareGoal, io.confluent.cruisecontrol.analyzer.goals.CellAwareGoal, io.confluent.cruisecontrol.analyzer.goals.TenantAwareGoal, io.confluent.cruisecontrol.analyzer.goals.MaxReplicaMovementParallelismGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.ReplicaCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.DiskCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.NetworkInboundCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.NetworkOutboundCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.ReplicationInboundCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.ProducerInboundCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.MirrorInboundCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.ConsumerOutboundCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.IncrementalCPUResourceDistributionGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.IncrementalTopicReplicaDistributionGoal]
benchi-kafka     | 	incremental.balancing.lower.bound = 0.02
benchi-kafka     | 	incremental.balancing.min.valid.windows = 5
benchi-kafka     | 	incremental.balancing.step.ratio = 0.2
benchi-kafka     | 	inter.cell.balancing.enabled = false
benchi-kafka     | 	invalid.replica.assignment.retry.timeout.ms = 300000
benchi-kafka     | 	leader.network.inbound.weight.for.cpu.util = 0.7
benchi-kafka     | 	leader.network.outbound.weight.for.cpu.util = 0.15
benchi-kafka     | 	leader.replica.count.balance.threshold = 1.1
benchi-kafka     | 	logdir.response.timeout.ms = 30000
benchi-kafka     | 	max.allowed.extrapolations.per.broker = 5
benchi-kafka     | 	max.allowed.extrapolations.per.partition = 5
benchi-kafka     | 	max.capacity.balancing.delta.percentage = 0.0
benchi-kafka     | 	max.replicas = 2147483647
benchi-kafka     | 	max.volume.throughput.mb = 0
benchi-kafka     | 	metadata.client.timeout.ms = 180000
benchi-kafka     | 	metadata.ttl = 10000
benchi-kafka     | 	metric.sampler.class = class io.confluent.cruisecontrol.metricsreporter.ConfluentTelemetryReporterSampler
benchi-kafka     | 	min.samples.per.partition.metrics.window = 1
benchi-kafka     | 	min.valid.partition.ratio = 0.95
benchi-kafka     | 	network.in.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	network.inbound.balance.threshold = 1.1
benchi-kafka     | 	network.inbound.capacity.threshold = 0.8
benchi-kafka     | 	network.inbound.low.utilization.threshold = 0.2
benchi-kafka     | 	network.out.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	network.outbound.balance.threshold = 1.1
benchi-kafka     | 	network.outbound.capacity.threshold = 0.8
benchi-kafka     | 	network.outbound.low.utilization.threshold = 0.2
benchi-kafka     | 	num.cached.recent.anomaly.states = 10
benchi-kafka     | 	num.concurrent.leader.movements = 1000
benchi-kafka     | 	num.concurrent.partition.movements.per.broker = 5
benchi-kafka     | 	num.metric.fetchers = 1
benchi-kafka     | 	num.partition.metrics.windows = 12
benchi-kafka     | 	partition.metric.sample.aggregator.completeness.cache.size = 5
benchi-kafka     | 	partition.metric.sample.store.topic = _confluent_balancer_partition_samples
benchi-kafka     | 	partition.metrics.window.ms = 180000
benchi-kafka     | 	populate.default.disk.capacity.from.local = true
benchi-kafka     | 	producer.in.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	producer.inbound.capacity.threshold = 0.9
benchi-kafka     | 	read.throughput.multiplier = 1.0
benchi-kafka     | 	receive.buffer.bytes = 32768
benchi-kafka     | 	reconnect.backoff.ms = 50
benchi-kafka     | 	removal.history.retention.time.ms = 86400000
benchi-kafka     | 	replica.count.balance.threshold = 1.1
benchi-kafka     | 	replica.movement.strategies = [com.linkedin.kafka.cruisecontrol.executor.strategy.PostponeUrpReplicaMovementStrategy, com.linkedin.kafka.cruisecontrol.executor.strategy.PrioritizeLargeReplicaMovementStrategy, com.linkedin.kafka.cruisecontrol.executor.strategy.PrioritizeSmallReplicaMovementStrategy, com.linkedin.kafka.cruisecontrol.executor.strategy.BaseReplicaMovementStrategy]
benchi-kafka     | 	replication.in.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	replication.inbound.capacity.threshold = 0.9
benchi-kafka     | 	request.cpu.contribution.weight = 0.8
benchi-kafka     | 	request.timeout.ms = 30000
benchi-kafka     | 	resource.utilization.detector.interval.ms = 60000
benchi-kafka     | 	sampling.allow.cpu.capacity.estimation = true
benchi-kafka     | 	sasl.client.callback.handler.class = null
benchi-kafka     | 	sasl.jaas.config = null
benchi-kafka     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
benchi-kafka     | 	sasl.kerberos.min.time.before.relogin = 60000
benchi-kafka     | 	sasl.kerberos.service.name = null
benchi-kafka     | 	sasl.kerberos.ticket.renew.jitter = 0.05
benchi-kafka     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
benchi-kafka     | 	sasl.login.callback.handler.class = null
benchi-kafka     | 	sasl.login.class = null
benchi-kafka     | 	sasl.login.connect.timeout.ms = null
benchi-kafka     | 	sasl.login.read.timeout.ms = null
benchi-kafka     | 	sasl.login.refresh.buffer.seconds = 300
benchi-kafka     | 	sasl.login.refresh.min.period.seconds = 60
benchi-kafka     | 	sasl.login.refresh.window.factor = 0.8
benchi-kafka     | 	sasl.login.refresh.window.jitter = 0.05
benchi-kafka     | 	sasl.login.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.login.retry.backoff.ms = 100
benchi-kafka     | 	sasl.mechanism = GSSAPI
benchi-kafka     | 	sasl.oauthbearer.clientassertion.audience = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.expiration = 5
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.jti.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.nbf.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.issuer = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.location = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key.passphrase = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.subject = null
benchi-kafka     | 	sasl.oauthbearer.clock.skew.seconds = 30
benchi-kafka     | 	sasl.oauthbearer.expected.audience = null
benchi-kafka     | 	sasl.oauthbearer.expected.issuer = null
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.url = null
benchi-kafka     | 	sasl.oauthbearer.scope.claim.name = scope
benchi-kafka     | 	sasl.oauthbearer.sub.claim.name = sub
benchi-kafka     | 	sasl.oauthbearer.token.endpoint.url = null
benchi-kafka     | 	sbc.metrics.parser.enabled = false
benchi-kafka     | 	security.protocol = PLAINTEXT
benchi-kafka     | 	self.healing.broker.failure.enabled = true
benchi-kafka     | 	self.healing.goal.violation.enabled = false
benchi-kafka     | 	self.healing.maximum.rounds = 1
benchi-kafka     | 	send.buffer.bytes = 131072
benchi-kafka     | 	ssl.cipher.suites = null
benchi-kafka     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	ssl.engine.factory.class = null
benchi-kafka     | 	ssl.key.password = null
benchi-kafka     | 	ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	ssl.keystore.certificate.chain = null
benchi-kafka     | 	ssl.keystore.key = null
benchi-kafka     | 	ssl.keystore.location = null
benchi-kafka     | 	ssl.keystore.password = null
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.protocol = TLSv1.3
benchi-kafka     | 	ssl.provider = null
benchi-kafka     | 	ssl.secure.random.implementation = null
benchi-kafka     | 	ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	ssl.truststore.certificates = null
benchi-kafka     | 	ssl.truststore.location = null
benchi-kafka     | 	ssl.truststore.password = null
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     | 	startup.retry.delay.minutes = 5
benchi-kafka     | 	startup.retry.max.hours = 2
benchi-kafka     | 	static.throttle.rate.override.enabled = false
benchi-kafka     | 	tenant.maximum.movements = 0
benchi-kafka     | 	tenant.suspension.ms = 86400000
benchi-kafka     | 	throttle.bytes.per.second = 10485760
benchi-kafka     | 	topic.balancing.badly.imbalanced.topic.imbalance.score.threshold = 0.3
benchi-kafka     | 	topic.balancing.balance.threshold.multiplier = 1.0
benchi-kafka     | 	topic.balancing.broker.addition.completion.percentage = 0.8
benchi-kafka     | 	topic.balancing.broker.addition.detector.with.trdg.enabled = false
benchi-kafka     | 	topic.balancing.imbalanced.score.threshold = 0.07
benchi-kafka     | 	topic.balancing.max.reassignments.per.iteration = -1
benchi-kafka     | 	topic.balancing.slightly.imbalanced.topic.imbalance.score.threshold = 0.05
benchi-kafka     | 	topic.balancing.slightly.imbalanced.topics.percentage.trigger = 0.2
benchi-kafka     | 	topic.balancing.trigger.threshold.multiplier = 3.0
benchi-kafka     | 	topic.partition.maximum.movements = 5
benchi-kafka     | 	topic.partition.movement.expiration.ms = 10800000
benchi-kafka     | 	topic.partition.suspension.ms = 18000000
benchi-kafka     | 	topics.excluded.from.partition.movement = 
benchi-kafka     | 	v2.addition.enabled = false
benchi-kafka     | 	write.throughput.multiplier = 1.0
benchi-kafka     | 	zookeeper.connect = 
benchi-kafka     | 	zookeeper.security.enabled = false
benchi-kafka     |  (com.linkedin.kafka.cruisecontrol.config.KafkaCruiseControlConfig)
benchi-kafka     | [2025-03-18 16:06:51,070] INFO Kafka Cruise Control started. (com.linkedin.kafka.cruisecontrol.KafkaCruiseControl)
benchi-kafka     | [2025-03-18 16:06:51,070] INFO Alive brokers: [1], failed brokers: [] (com.linkedin.kafka.cruisecontrol.detector.BrokerFailureDetector)
benchi-kafka     | [2025-03-18 16:06:51,070] WARN Disabling exponential reconnect backoff because reconnect.backoff.ms is set, but reconnect.backoff.max.ms is not. (org.apache.kafka.clients.CommonClientConfigs)
benchi-kafka     | [2025-03-18 16:06:51,070] INFO Updated list of failed broker: {} (com.linkedin.kafka.cruisecontrol.detector.BrokerFailureDetector)
benchi-kafka     | [2025-03-18 16:06:51,070] INFO ConsumerConfig values: 
benchi-kafka     | 	allow.auto.create.topics = true
benchi-kafka     | 	auto.commit.interval.ms = 5000
benchi-kafka     | 	auto.include.jmx.reporter = true
benchi-kafka     | 	auto.offset.reset = latest
benchi-kafka     | 	bootstrap.servers = [broker:29092]
benchi-kafka     | 	check.crcs = true
benchi-kafka     | 	client.dns.lookup = use_all_dns_ips
benchi-kafka     | 	client.id = kafka-cruise-control
benchi-kafka     | 	client.rack = 
benchi-kafka     | 	confluent.lkc.id = null
benchi-kafka     | 	confluent.proxy.protocol.client.address = null
benchi-kafka     | 	confluent.proxy.protocol.client.mode = PROXY
benchi-kafka     | 	confluent.proxy.protocol.client.port = null
benchi-kafka     | 	confluent.proxy.protocol.client.version = NONE
benchi-kafka     | 	connections.max.idle.ms = 540000
benchi-kafka     | 	default.api.timeout.ms = 60000
benchi-kafka     | 	enable.auto.commit = false
benchi-kafka     | 	enable.metrics.push = true
benchi-kafka     | 	exclude.internal.topics = true
benchi-kafka     | 	fetch.max.bytes = 52428800
benchi-kafka     | 	fetch.max.wait.ms = 500
benchi-kafka     | 	fetch.min.bytes = 1
benchi-kafka     | 	group.id = ConfluentTelemetryReporterSampler--2946986101702943459
benchi-kafka     | 	group.instance.id = null
benchi-kafka     | 	group.protocol = classic
benchi-kafka     | 	group.remote.assignor = null
benchi-kafka     | 	heartbeat.interval.ms = 3000
benchi-kafka     | 	interceptor.classes = []
benchi-kafka     | 	internal.leave.group.on.close = true
benchi-kafka     | 	internal.throw.on.fetch.stable.offset.unsupported = false
benchi-kafka     | 	isolation.level = read_uncommitted
benchi-kafka     | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
benchi-kafka     | 	max.partition.fetch.bytes = 1048576
benchi-kafka     | 	max.poll.interval.ms = 2147483647
benchi-kafka     | 	max.poll.records = 2147483647
benchi-kafka     | 	metadata.max.age.ms = 300000
benchi-kafka     | 	metadata.recovery.strategy = none
benchi-kafka     | 	metric.reporters = []
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.recording.level = INFO
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
benchi-kafka     | 	receive.buffer.bytes = 32768
benchi-kafka     | 	reconnect.backoff.max.ms = 50
benchi-kafka     | 	reconnect.backoff.ms = 50
benchi-kafka     | 	request.timeout.ms = 30000
benchi-kafka     | 	retry.backoff.max.ms = 1000
benchi-kafka     | 	retry.backoff.ms = 100
benchi-kafka     | 	sasl.client.callback.handler.class = null
benchi-kafka     | 	sasl.jaas.config = null
benchi-kafka     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
benchi-kafka     | 	sasl.kerberos.min.time.before.relogin = 60000
benchi-kafka     | 	sasl.kerberos.service.name = null
benchi-kafka     | 	sasl.kerberos.ticket.renew.jitter = 0.05
benchi-kafka     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
benchi-kafka     | 	sasl.login.callback.handler.class = null
benchi-kafka     | 	sasl.login.class = null
benchi-kafka     | 	sasl.login.connect.timeout.ms = null
benchi-kafka     | 	sasl.login.read.timeout.ms = null
benchi-kafka     | 	sasl.login.refresh.buffer.seconds = 300
benchi-kafka     | 	sasl.login.refresh.min.period.seconds = 60
benchi-kafka     | 	sasl.login.refresh.window.factor = 0.8
benchi-kafka     | 	sasl.login.refresh.window.jitter = 0.05
benchi-kafka     | 	sasl.login.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.login.retry.backoff.ms = 100
benchi-kafka     | 	sasl.mechanism = GSSAPI
benchi-kafka     | 	sasl.oauthbearer.clientassertion.audience = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.expiration = 5
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.jti.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.nbf.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.issuer = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.location = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key.passphrase = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.subject = null
benchi-kafka     | 	sasl.oauthbearer.clock.skew.seconds = 30
benchi-kafka     | 	sasl.oauthbearer.expected.audience = null
benchi-kafka     | 	sasl.oauthbearer.expected.issuer = null
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.url = null
benchi-kafka     | 	sasl.oauthbearer.scope.claim.name = scope
benchi-kafka     | 	sasl.oauthbearer.sub.claim.name = sub
benchi-kafka     | 	sasl.oauthbearer.token.endpoint.url = null
benchi-kafka     | 	security.protocol = PLAINTEXT
benchi-kafka     | 	security.providers = null
benchi-kafka     | 	send.buffer.bytes = 131072
benchi-kafka     | 	session.timeout.ms = 45000
benchi-kafka     | 	socket.connection.setup.timeout.max.ms = 30000
benchi-kafka     | 	socket.connection.setup.timeout.ms = 10000
benchi-kafka     | 	ssl.cipher.suites = null
benchi-kafka     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	ssl.engine.factory.class = null
benchi-kafka     | 	ssl.key.password = null
benchi-kafka     | 	ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	ssl.keystore.certificate.chain = null
benchi-kafka     | 	ssl.keystore.key = null
benchi-kafka     | 	ssl.keystore.location = null
benchi-kafka     | 	ssl.keystore.password = null
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.protocol = TLSv1.3
benchi-kafka     | 	ssl.provider = null
benchi-kafka     | 	ssl.secure.random.implementation = null
benchi-kafka     | 	ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	ssl.truststore.certificates = null
benchi-kafka     | 	ssl.truststore.location = null
benchi-kafka     | 	ssl.truststore.password = null
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
benchi-kafka     |  (org.apache.kafka.clients.consumer.ConsumerConfig)
benchi-kafka     | [2025-03-18 16:06:51,070] INFO initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector)
benchi-kafka     | [2025-03-18 16:06:51,070] INFO No pending DataBalancer operations found at startup. (io.confluent.databalancer.ConfluentDataBalanceEngine)
benchi-kafka     | [2025-03-18 16:06:51,070] INFO Balancer Status state for brokers [1] transitioned from STARTING to RUNNING due to event CRUISE_CONTROL_INITIALIZATION_COMPLETED. (io.confluent.databalancer.operation.StateMachine)
benchi-kafka     | [2025-03-18 16:06:51,070] INFO DataBalancer: Scheduling DataBalanceEngine auto-heal update (setting to false) (io.confluent.databalancer.ConfluentDataBalanceEngine)
benchi-kafka     | [2025-03-18 16:06:51,070] INFO DataBalancer: DataBalanceEngine started (io.confluent.databalancer.ConfluentDataBalanceEngine)
benchi-kafka     | [2025-03-18 16:06:51,070] INFO Databalancer: Updating auto-heal mode to (false) (io.confluent.databalancer.ConfluentDataBalanceEngine)
benchi-kafka     | [2025-03-18 16:06:51,070] INFO Changing GOAL_VIOLATION anomaly self-healing actions to false (io.confluent.databalancer.ConfluentDataBalanceEngine)
benchi-kafka     | [2025-03-18 16:06:51,070] INFO Goal violation self-healing left disabled (no change) (com.linkedin.kafka.cruisecontrol.KafkaCruiseControl)
benchi-kafka     | [2025-03-18 16:06:51,071] INFO These configurations '[sasl.oauthbearer.jwks.endpoint.refresh.ms, sasl.oauthbearer.clientassertion.include.jti.claim, sasl.login.refresh.min.period.seconds, sasl.oauthbearer.scope.claim.name, sasl.login.refresh.window.factor, sasl.kerberos.ticket.renew.window.factor, sasl.login.retry.backoff.ms, sasl.oauthbearer.clientassertion.expiration, sasl.kerberos.kinit.cmd, sasl.kerberos.ticket.renew.jitter, ssl.keystore.type, ssl.trustmanager.algorithm, sasl.kerberos.min.time.before.relogin, ssl.endpoint.identification.algorithm, ssl.protocol, sasl.login.refresh.buffer.seconds, sasl.login.retry.backoff.max.ms, ssl.enabled.protocols, sasl.oauthbearer.sub.claim.name, ssl.truststore.type, sasl.oauthbearer.jwks.endpoint.retry.backoff.ms, sasl.oauthbearer.clock.skew.seconds, ssl.keymanager.algorithm, sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms, sasl.oauthbearer.clientassertion.include.nbf.claim, sasl.login.refresh.window.jitter]' were supplied but are not used yet. (org.apache.kafka.clients.consumer.ConsumerConfig)
benchi-kafka     | [2025-03-18 16:06:51,071] INFO Kafka version: 7.8.1-ce (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:06:51,071] INFO Kafka commitId: eefb51c6f2f1b888b8b03296353338afbac8e213 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:06:51,071] INFO Kafka startTimeMs: 1742314011071 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:06:51,071] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Subscribed to pattern: '_confluent-telemetry-metrics' (org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer)
benchi-kafka     | [2025-03-18 16:06:51,073] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Cluster ID: MkU3OEVBNTcwNTJENDM2Qk (org.apache.kafka.clients.Metadata)
benchi-kafka     | [2025-03-18 16:06:51,075] INFO Beginning to sample MetricsWindow{sizeMs=180000, startMs=1742313780000, endMs=1742313960000, endMsInclusive=1742313959999, index=9679522, baseTimestamp=0}(16:03:00 - 16:05:59.999) (com.linkedin.kafka.cruisecontrol.monitor.task.MetricSamplingTask)
benchi-kafka     | [2025-03-18 16:06:51,079] INFO Sent auto-creation request for Set(__consumer_offsets) to the active controller. (kafka.server.DefaultAutoTopicCreationManager)
benchi-kafka     | [2025-03-18 16:06:51,080] INFO [ControllerServer id=1] CreateTopics result(s): CreatableTopic(name='__consumer_offsets', numPartitions=50, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='compression.type', value='producer'), CreateableTopicConfig(name='cleanup.policy', value='compact'), CreateableTopicConfig(name='max.compaction.lag.ms', value='9223372036854775807'), CreateableTopicConfig(name='segment.bytes', value='104857600'), CreateableTopicConfig(name='confluent.placement.constraints', value=''), CreateableTopicConfig(name='min.cleanable.dirty.ratio', value='0.5'), CreateableTopicConfig(name='delete.retention.ms', value='86400000')], linkName=null, mirrorTopic=null, sourceTopicId=AAAAAAAAAAAAAAAAAAAAAA, mirrorStartOffsetSpec=-9223372036854775808, mirrorStartOffsets=[], stoppedSequenceNumber=0): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:06:51,080] INFO [ControllerServer id=1] Replayed TopicRecord for topic __consumer_offsets with topic ID TDrV9QLDQhGxmWrWM4O8uw. (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:06:51,080] INFO [ControllerServer id=1] Replayed ConfigRecord for ConfigResource(type=TOPIC, name='__consumer_offsets') which set configuration compression.type to producer (org.apache.kafka.controller.ConfigurationControlManager)
benchi-kafka     | [2025-03-18 16:06:51,080] INFO [ControllerServer id=1] Replayed ConfigRecord for ConfigResource(type=TOPIC, name='__consumer_offsets') which set configuration cleanup.policy to compact (org.apache.kafka.controller.ConfigurationControlManager)
benchi-kafka     | [2025-03-18 16:06:51,080] INFO [ControllerServer id=1] Replayed ConfigRecord for ConfigResource(type=TOPIC, name='__consumer_offsets') which set configuration max.compaction.lag.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
benchi-kafka     | [2025-03-18 16:06:51,081] INFO [ControllerServer id=1] Replayed ConfigRecord for ConfigResource(type=TOPIC, name='__consumer_offsets') which set configuration segment.bytes to 104857600 (org.apache.kafka.controller.ConfigurationControlManager)
benchi-kafka     | [2025-03-18 16:06:51,081] INFO [ControllerServer id=1] Replayed ConfigRecord for ConfigResource(type=TOPIC, name='__consumer_offsets') which set configuration confluent.placement.constraints to  (org.apache.kafka.controller.ConfigurationControlManager)
benchi-kafka     | [2025-03-18 16:06:51,081] INFO [ControllerServer id=1] Replayed ConfigRecord for ConfigResource(type=TOPIC, name='__consumer_offsets') which set configuration min.cleanable.dirty.ratio to 0.5 (org.apache.kafka.controller.ConfigurationControlManager)
benchi-kafka     | [2025-03-18 16:06:51,081] INFO [ControllerServer id=1] Replayed ConfigRecord for ConfigResource(type=TOPIC, name='__consumer_offsets') which set configuration delete.retention.ms to 86400000 (org.apache.kafka.controller.ConfigurationControlManager)
benchi-kafka     | [2025-03-18 16:06:51,081] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-0 with topic ID TDrV9QLDQhGxmWrWM4O8uw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:06:51,081] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-1 with topic ID TDrV9QLDQhGxmWrWM4O8uw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:06:51,081] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-2 with topic ID TDrV9QLDQhGxmWrWM4O8uw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:06:51,081] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-3 with topic ID TDrV9QLDQhGxmWrWM4O8uw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:06:51,081] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-4 with topic ID TDrV9QLDQhGxmWrWM4O8uw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:06:51,081] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-5 with topic ID TDrV9QLDQhGxmWrWM4O8uw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:06:51,081] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-6 with topic ID TDrV9QLDQhGxmWrWM4O8uw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:06:51,081] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-7 with topic ID TDrV9QLDQhGxmWrWM4O8uw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:06:51,081] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-8 with topic ID TDrV9QLDQhGxmWrWM4O8uw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:06:51,081] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-9 with topic ID TDrV9QLDQhGxmWrWM4O8uw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:06:51,081] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-10 with topic ID TDrV9QLDQhGxmWrWM4O8uw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:06:51,081] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-11 with topic ID TDrV9QLDQhGxmWrWM4O8uw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:06:51,081] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-12 with topic ID TDrV9QLDQhGxmWrWM4O8uw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:06:51,081] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-13 with topic ID TDrV9QLDQhGxmWrWM4O8uw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:06:51,081] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-14 with topic ID TDrV9QLDQhGxmWrWM4O8uw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:06:51,081] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-15 with topic ID TDrV9QLDQhGxmWrWM4O8uw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:06:51,081] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-16 with topic ID TDrV9QLDQhGxmWrWM4O8uw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:06:51,081] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-17 with topic ID TDrV9QLDQhGxmWrWM4O8uw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:06:51,081] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-18 with topic ID TDrV9QLDQhGxmWrWM4O8uw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:06:51,081] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-19 with topic ID TDrV9QLDQhGxmWrWM4O8uw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:06:51,081] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-20 with topic ID TDrV9QLDQhGxmWrWM4O8uw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:06:51,082] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-21 with topic ID TDrV9QLDQhGxmWrWM4O8uw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:06:51,082] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-22 with topic ID TDrV9QLDQhGxmWrWM4O8uw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:06:51,082] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-23 with topic ID TDrV9QLDQhGxmWrWM4O8uw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:06:51,082] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-24 with topic ID TDrV9QLDQhGxmWrWM4O8uw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:06:51,082] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-25 with topic ID TDrV9QLDQhGxmWrWM4O8uw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:06:51,082] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-26 with topic ID TDrV9QLDQhGxmWrWM4O8uw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:06:51,082] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-27 with topic ID TDrV9QLDQhGxmWrWM4O8uw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:06:51,082] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-28 with topic ID TDrV9QLDQhGxmWrWM4O8uw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:06:51,082] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-29 with topic ID TDrV9QLDQhGxmWrWM4O8uw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:06:51,082] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-30 with topic ID TDrV9QLDQhGxmWrWM4O8uw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:06:51,082] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-31 with topic ID TDrV9QLDQhGxmWrWM4O8uw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:06:51,082] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-32 with topic ID TDrV9QLDQhGxmWrWM4O8uw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:06:51,082] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-33 with topic ID TDrV9QLDQhGxmWrWM4O8uw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:06:51,082] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-34 with topic ID TDrV9QLDQhGxmWrWM4O8uw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:06:51,082] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-35 with topic ID TDrV9QLDQhGxmWrWM4O8uw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:06:51,082] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-36 with topic ID TDrV9QLDQhGxmWrWM4O8uw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:06:51,082] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-37 with topic ID TDrV9QLDQhGxmWrWM4O8uw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:06:51,082] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-38 with topic ID TDrV9QLDQhGxmWrWM4O8uw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:06:51,082] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-39 with topic ID TDrV9QLDQhGxmWrWM4O8uw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:06:51,082] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-40 with topic ID TDrV9QLDQhGxmWrWM4O8uw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:06:51,082] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-41 with topic ID TDrV9QLDQhGxmWrWM4O8uw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:06:51,082] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-42 with topic ID TDrV9QLDQhGxmWrWM4O8uw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:06:51,082] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-43 with topic ID TDrV9QLDQhGxmWrWM4O8uw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:06:51,082] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-44 with topic ID TDrV9QLDQhGxmWrWM4O8uw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:06:51,082] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-45 with topic ID TDrV9QLDQhGxmWrWM4O8uw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:06:51,082] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-46 with topic ID TDrV9QLDQhGxmWrWM4O8uw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:06:51,082] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-47 with topic ID TDrV9QLDQhGxmWrWM4O8uw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:06:51,082] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-48 with topic ID TDrV9QLDQhGxmWrWM4O8uw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:06:51,082] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-49 with topic ID TDrV9QLDQhGxmWrWM4O8uw and PartitionRegistration(replicas=[1], observers=[], directories=[v3Z8JLzy5QaOAeyEyCPGQA], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:06:51,112] INFO SBC Event SbcMetadataUpdateEvent-146 generated 1 more events to enqueue in the following order - [SbcConfigUpdateEvent-147]. Enqueuing... (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:06:51,112] INFO [Broker id=1] Transitioning 50 partition(s) to local leaders. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,112] INFO Handling event SbcConfigUpdateEvent-147 (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:06:51,112] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(__consumer_offsets-13, __consumer_offsets-46, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-21, __consumer_offsets-17, __consumer_offsets-30, __consumer_offsets-26, __consumer_offsets-5, __consumer_offsets-38, __consumer_offsets-1, __consumer_offsets-34, __consumer_offsets-16, __consumer_offsets-45, __consumer_offsets-12, __consumer_offsets-41, __consumer_offsets-24, __consumer_offsets-20, __consumer_offsets-49, __consumer_offsets-0, __consumer_offsets-29, __consumer_offsets-25, __consumer_offsets-8, __consumer_offsets-37, __consumer_offsets-4, __consumer_offsets-33, __consumer_offsets-15, __consumer_offsets-48, __consumer_offsets-11, __consumer_offsets-44, __consumer_offsets-23, __consumer_offsets-19, __consumer_offsets-32, __consumer_offsets-28, __consumer_offsets-7, __consumer_offsets-40, __consumer_offsets-3, __consumer_offsets-36, __consumer_offsets-47, __consumer_offsets-14, __consumer_offsets-43, __consumer_offsets-10, __consumer_offsets-22, __consumer_offsets-18, __consumer_offsets-31, __consumer_offsets-27, __consumer_offsets-39, __consumer_offsets-6, __consumer_offsets-35, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
benchi-kafka     | [2025-03-18 16:06:51,112] INFO Balancer notified of a config change: ConfigurationsDelta(changes={ConfigResource(type=TOPIC, name='__consumer_offsets')=ConfigurationDelta(changedKeys=[compression.type, cleanup.policy, max.compaction.lag.ms, segment.bytes, confluent.placement.constraints, min.cleanable.dirty.ratio, delete.retention.ms])}) (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:06:51,112] INFO [Broker id=1] Creating new partition __consumer_offsets-13 with topic id TDrV9QLDQhGxmWrWM4O8uw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,112] INFO There were 0 change(s) and 0 deletion(s) to balancer configs. Changed Configs: {}, Deleted Configs: [] (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:06:51,112] INFO [Broker id=1] Creating new partition __consumer_offsets-46 with topic id TDrV9QLDQhGxmWrWM4O8uw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,112] INFO [Broker id=1] Creating new partition __consumer_offsets-9 with topic id TDrV9QLDQhGxmWrWM4O8uw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,113] INFO [Broker id=1] Creating new partition __consumer_offsets-42 with topic id TDrV9QLDQhGxmWrWM4O8uw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,113] INFO [Broker id=1] Creating new partition __consumer_offsets-21 with topic id TDrV9QLDQhGxmWrWM4O8uw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,113] INFO [Broker id=1] Creating new partition __consumer_offsets-17 with topic id TDrV9QLDQhGxmWrWM4O8uw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,114] INFO [Broker id=1] Creating new partition __consumer_offsets-30 with topic id TDrV9QLDQhGxmWrWM4O8uw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,114] INFO [Broker id=1] Creating new partition __consumer_offsets-26 with topic id TDrV9QLDQhGxmWrWM4O8uw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,114] INFO [Broker id=1] Creating new partition __consumer_offsets-5 with topic id TDrV9QLDQhGxmWrWM4O8uw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,115] INFO [Broker id=1] Creating new partition __consumer_offsets-38 with topic id TDrV9QLDQhGxmWrWM4O8uw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,115] INFO [Broker id=1] Creating new partition __consumer_offsets-1 with topic id TDrV9QLDQhGxmWrWM4O8uw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,115] INFO [Broker id=1] Creating new partition __consumer_offsets-34 with topic id TDrV9QLDQhGxmWrWM4O8uw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,115] INFO [Broker id=1] Creating new partition __consumer_offsets-16 with topic id TDrV9QLDQhGxmWrWM4O8uw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,115] INFO [Broker id=1] Creating new partition __consumer_offsets-45 with topic id TDrV9QLDQhGxmWrWM4O8uw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,115] INFO [Broker id=1] Creating new partition __consumer_offsets-12 with topic id TDrV9QLDQhGxmWrWM4O8uw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,116] INFO [Broker id=1] Creating new partition __consumer_offsets-41 with topic id TDrV9QLDQhGxmWrWM4O8uw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,116] INFO [Broker id=1] Creating new partition __consumer_offsets-24 with topic id TDrV9QLDQhGxmWrWM4O8uw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,116] INFO [Broker id=1] Creating new partition __consumer_offsets-20 with topic id TDrV9QLDQhGxmWrWM4O8uw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,116] INFO [Broker id=1] Creating new partition __consumer_offsets-49 with topic id TDrV9QLDQhGxmWrWM4O8uw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,116] INFO [Broker id=1] Creating new partition __consumer_offsets-0 with topic id TDrV9QLDQhGxmWrWM4O8uw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,116] INFO [Broker id=1] Creating new partition __consumer_offsets-29 with topic id TDrV9QLDQhGxmWrWM4O8uw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,116] INFO [Broker id=1] Creating new partition __consumer_offsets-25 with topic id TDrV9QLDQhGxmWrWM4O8uw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,117] INFO [Broker id=1] Creating new partition __consumer_offsets-8 with topic id TDrV9QLDQhGxmWrWM4O8uw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,117] INFO [Broker id=1] Creating new partition __consumer_offsets-37 with topic id TDrV9QLDQhGxmWrWM4O8uw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,117] INFO [Broker id=1] Creating new partition __consumer_offsets-4 with topic id TDrV9QLDQhGxmWrWM4O8uw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,117] INFO [Broker id=1] Creating new partition __consumer_offsets-33 with topic id TDrV9QLDQhGxmWrWM4O8uw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,117] INFO [Broker id=1] Creating new partition __consumer_offsets-15 with topic id TDrV9QLDQhGxmWrWM4O8uw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,117] INFO [Broker id=1] Creating new partition __consumer_offsets-48 with topic id TDrV9QLDQhGxmWrWM4O8uw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,118] INFO [Broker id=1] Creating new partition __consumer_offsets-11 with topic id TDrV9QLDQhGxmWrWM4O8uw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,118] INFO [Broker id=1] Creating new partition __consumer_offsets-44 with topic id TDrV9QLDQhGxmWrWM4O8uw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,118] INFO [Broker id=1] Creating new partition __consumer_offsets-23 with topic id TDrV9QLDQhGxmWrWM4O8uw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,118] INFO [Broker id=1] Creating new partition __consumer_offsets-19 with topic id TDrV9QLDQhGxmWrWM4O8uw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,118] INFO [Broker id=1] Creating new partition __consumer_offsets-32 with topic id TDrV9QLDQhGxmWrWM4O8uw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,118] INFO [Broker id=1] Creating new partition __consumer_offsets-28 with topic id TDrV9QLDQhGxmWrWM4O8uw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,118] INFO [Broker id=1] Creating new partition __consumer_offsets-7 with topic id TDrV9QLDQhGxmWrWM4O8uw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,119] INFO [Broker id=1] Creating new partition __consumer_offsets-40 with topic id TDrV9QLDQhGxmWrWM4O8uw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,119] INFO [Broker id=1] Creating new partition __consumer_offsets-3 with topic id TDrV9QLDQhGxmWrWM4O8uw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,119] INFO [Broker id=1] Creating new partition __consumer_offsets-36 with topic id TDrV9QLDQhGxmWrWM4O8uw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,119] INFO [Broker id=1] Creating new partition __consumer_offsets-47 with topic id TDrV9QLDQhGxmWrWM4O8uw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,119] INFO [Broker id=1] Creating new partition __consumer_offsets-14 with topic id TDrV9QLDQhGxmWrWM4O8uw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,120] INFO [Broker id=1] Creating new partition __consumer_offsets-43 with topic id TDrV9QLDQhGxmWrWM4O8uw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,120] INFO [Broker id=1] Creating new partition __consumer_offsets-10 with topic id TDrV9QLDQhGxmWrWM4O8uw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,120] INFO [Broker id=1] Creating new partition __consumer_offsets-22 with topic id TDrV9QLDQhGxmWrWM4O8uw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,120] INFO [Broker id=1] Creating new partition __consumer_offsets-18 with topic id TDrV9QLDQhGxmWrWM4O8uw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,120] INFO [Broker id=1] Creating new partition __consumer_offsets-31 with topic id TDrV9QLDQhGxmWrWM4O8uw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,120] INFO [Broker id=1] Creating new partition __consumer_offsets-27 with topic id TDrV9QLDQhGxmWrWM4O8uw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,121] INFO [Broker id=1] Creating new partition __consumer_offsets-39 with topic id TDrV9QLDQhGxmWrWM4O8uw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,121] INFO [Broker id=1] Creating new partition __consumer_offsets-6 with topic id TDrV9QLDQhGxmWrWM4O8uw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,121] INFO [Broker id=1] Creating new partition __consumer_offsets-35 with topic id TDrV9QLDQhGxmWrWM4O8uw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,121] INFO [Broker id=1] Creating new partition __consumer_offsets-2 with topic id TDrV9QLDQhGxmWrWM4O8uw. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,121] INFO [Broker id=1] Stopped fetchers as part of become-leader transition for 50 partitions (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,123] INFO [MergedLog partition=__consumer_offsets-24, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:06:51,123] INFO Created log for partition __consumer_offsets-24 in /tmp/kraft-combined-logs/__consumer_offsets-24 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,123] INFO [Partition __consumer_offsets-24 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,123] INFO [Partition __consumer_offsets-24 broker=1] Log loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,123] INFO Setting topicIdPartition TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-24 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:06:51,124] INFO [MergedLog partition=__consumer_offsets-24, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-24 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:06:51,124] INFO [Broker id=1] Leader __consumer_offsets-24 with topic id Some(TDrV9QLDQhGxmWrWM4O8uw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,124] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
benchi-kafka     | [2025-03-18 16:06:51,125] INFO [MergedLog partition=__consumer_offsets-3, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:06:51,125] INFO Created log for partition __consumer_offsets-3 in /tmp/kraft-combined-logs/__consumer_offsets-3 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,125] INFO [Partition __consumer_offsets-3 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,125] INFO [Partition __consumer_offsets-3 broker=1] Log loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,125] INFO Setting topicIdPartition TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-3 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:06:51,125] INFO [MergedLog partition=__consumer_offsets-3, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-3 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:06:51,125] INFO [Broker id=1] Leader __consumer_offsets-3 with topic id Some(TDrV9QLDQhGxmWrWM4O8uw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,126] INFO [MergedLog partition=__consumer_offsets-1, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:06:51,126] INFO Created log for partition __consumer_offsets-1 in /tmp/kraft-combined-logs/__consumer_offsets-1 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,126] INFO [Partition __consumer_offsets-1 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,126] INFO [Partition __consumer_offsets-1 broker=1] Log loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,126] INFO Setting topicIdPartition TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-1 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:06:51,126] INFO [MergedLog partition=__consumer_offsets-1, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-1 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:06:51,126] INFO [Broker id=1] Leader __consumer_offsets-1 with topic id Some(TDrV9QLDQhGxmWrWM4O8uw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,127] INFO [MergedLog partition=__consumer_offsets-8, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:06:51,127] INFO Created log for partition __consumer_offsets-8 in /tmp/kraft-combined-logs/__consumer_offsets-8 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,128] INFO [Partition __consumer_offsets-8 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,128] INFO [Partition __consumer_offsets-8 broker=1] Log loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,128] INFO Setting topicIdPartition TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-8 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:06:51,128] INFO [MergedLog partition=__consumer_offsets-8, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-8 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:06:51,128] INFO [Broker id=1] Leader __consumer_offsets-8 with topic id Some(TDrV9QLDQhGxmWrWM4O8uw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,129] INFO [MergedLog partition=__consumer_offsets-9, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:06:51,129] INFO Created log for partition __consumer_offsets-9 in /tmp/kraft-combined-logs/__consumer_offsets-9 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,129] INFO [Partition __consumer_offsets-9 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,129] INFO [Partition __consumer_offsets-9 broker=1] Log loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,129] INFO Setting topicIdPartition TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-9 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:06:51,129] INFO [MergedLog partition=__consumer_offsets-9, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-9 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:06:51,129] INFO [Broker id=1] Leader __consumer_offsets-9 with topic id Some(TDrV9QLDQhGxmWrWM4O8uw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,130] INFO [MergedLog partition=__consumer_offsets-23, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:06:51,130] INFO Created log for partition __consumer_offsets-23 in /tmp/kraft-combined-logs/__consumer_offsets-23 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,130] INFO [Partition __consumer_offsets-23 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,130] INFO [Partition __consumer_offsets-23 broker=1] Log loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,130] INFO Setting topicIdPartition TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-23 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:06:51,130] INFO [MergedLog partition=__consumer_offsets-23, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-23 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:06:51,130] INFO [Broker id=1] Leader __consumer_offsets-23 with topic id Some(TDrV9QLDQhGxmWrWM4O8uw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,131] INFO [MergedLog partition=__consumer_offsets-21, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:06:51,131] INFO Created log for partition __consumer_offsets-21 in /tmp/kraft-combined-logs/__consumer_offsets-21 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,131] INFO [Partition __consumer_offsets-21 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,131] INFO [Partition __consumer_offsets-21 broker=1] Log loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,131] INFO Setting topicIdPartition TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-21 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:06:51,131] INFO [MergedLog partition=__consumer_offsets-21, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-21 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:06:51,131] INFO [Broker id=1] Leader __consumer_offsets-21 with topic id Some(TDrV9QLDQhGxmWrWM4O8uw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,132] INFO [MergedLog partition=__consumer_offsets-27, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:06:51,132] INFO Created log for partition __consumer_offsets-27 in /tmp/kraft-combined-logs/__consumer_offsets-27 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,132] INFO [Partition __consumer_offsets-27 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,132] INFO [Partition __consumer_offsets-27 broker=1] Log loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,132] INFO Setting topicIdPartition TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-27 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:06:51,133] INFO [MergedLog partition=__consumer_offsets-27, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-27 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:06:51,133] INFO [Broker id=1] Leader __consumer_offsets-27 with topic id Some(TDrV9QLDQhGxmWrWM4O8uw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,134] INFO [MergedLog partition=__consumer_offsets-26, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:06:51,134] INFO Created log for partition __consumer_offsets-26 in /tmp/kraft-combined-logs/__consumer_offsets-26 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,134] INFO [Partition __consumer_offsets-26 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,134] INFO [Partition __consumer_offsets-26 broker=1] Log loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,134] INFO Setting topicIdPartition TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-26 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:06:51,134] INFO [MergedLog partition=__consumer_offsets-26, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-26 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:06:51,134] INFO [Broker id=1] Leader __consumer_offsets-26 with topic id Some(TDrV9QLDQhGxmWrWM4O8uw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,135] INFO [MergedLog partition=__consumer_offsets-28, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:06:51,135] INFO Created log for partition __consumer_offsets-28 in /tmp/kraft-combined-logs/__consumer_offsets-28 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,135] INFO [Partition __consumer_offsets-28 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,135] INFO [Partition __consumer_offsets-28 broker=1] Log loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,135] INFO Setting topicIdPartition TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-28 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:06:51,135] INFO [MergedLog partition=__consumer_offsets-28, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-28 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:06:51,135] INFO [Broker id=1] Leader __consumer_offsets-28 with topic id Some(TDrV9QLDQhGxmWrWM4O8uw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,136] INFO [MergedLog partition=__consumer_offsets-30, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:06:51,137] INFO Created log for partition __consumer_offsets-30 in /tmp/kraft-combined-logs/__consumer_offsets-30 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,137] INFO [Partition __consumer_offsets-30 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,137] INFO [Partition __consumer_offsets-30 broker=1] Log loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,137] INFO Setting topicIdPartition TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-30 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:06:51,137] INFO [MergedLog partition=__consumer_offsets-30, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-30 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:06:51,137] INFO [Broker id=1] Leader __consumer_offsets-30 with topic id Some(TDrV9QLDQhGxmWrWM4O8uw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,138] INFO [MergedLog partition=__consumer_offsets-31, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:06:51,138] INFO Created log for partition __consumer_offsets-31 in /tmp/kraft-combined-logs/__consumer_offsets-31 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,138] INFO [Partition __consumer_offsets-31 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,138] INFO [Partition __consumer_offsets-31 broker=1] Log loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,138] INFO Setting topicIdPartition TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-31 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:06:51,138] INFO [MergedLog partition=__consumer_offsets-31, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-31 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:06:51,138] INFO [Broker id=1] Leader __consumer_offsets-31 with topic id Some(TDrV9QLDQhGxmWrWM4O8uw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,139] INFO [MergedLog partition=__consumer_offsets-17, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:06:51,139] INFO Created log for partition __consumer_offsets-17 in /tmp/kraft-combined-logs/__consumer_offsets-17 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,139] INFO [Partition __consumer_offsets-17 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,139] INFO [Partition __consumer_offsets-17 broker=1] Log loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,139] INFO Setting topicIdPartition TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-17 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:06:51,139] INFO [MergedLog partition=__consumer_offsets-17, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-17 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:06:51,139] INFO [Broker id=1] Leader __consumer_offsets-17 with topic id Some(TDrV9QLDQhGxmWrWM4O8uw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,140] INFO [MergedLog partition=__consumer_offsets-19, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:06:51,140] INFO Created log for partition __consumer_offsets-19 in /tmp/kraft-combined-logs/__consumer_offsets-19 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,140] INFO [Partition __consumer_offsets-19 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,140] INFO [Partition __consumer_offsets-19 broker=1] Log loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,140] INFO Setting topicIdPartition TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-19 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:06:51,140] INFO [MergedLog partition=__consumer_offsets-19, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-19 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:06:51,141] INFO [Broker id=1] Leader __consumer_offsets-19 with topic id Some(TDrV9QLDQhGxmWrWM4O8uw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,141] INFO [MergedLog partition=__consumer_offsets-42, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:06:51,141] INFO Created log for partition __consumer_offsets-42 in /tmp/kraft-combined-logs/__consumer_offsets-42 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,141] INFO [Partition __consumer_offsets-42 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,141] INFO [Partition __consumer_offsets-42 broker=1] Log loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,141] INFO Setting topicIdPartition TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-42 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:06:51,141] INFO [MergedLog partition=__consumer_offsets-42, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-42 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:06:51,141] INFO [Broker id=1] Leader __consumer_offsets-42 with topic id Some(TDrV9QLDQhGxmWrWM4O8uw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,142] INFO [MergedLog partition=__consumer_offsets-37, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:06:51,142] INFO Created log for partition __consumer_offsets-37 in /tmp/kraft-combined-logs/__consumer_offsets-37 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,142] INFO [Partition __consumer_offsets-37 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,142] INFO [Partition __consumer_offsets-37 broker=1] Log loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,142] INFO Setting topicIdPartition TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-37 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:06:51,142] INFO [MergedLog partition=__consumer_offsets-37, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-37 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:06:51,142] INFO [Broker id=1] Leader __consumer_offsets-37 with topic id Some(TDrV9QLDQhGxmWrWM4O8uw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,143] INFO [MergedLog partition=__consumer_offsets-34, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:06:51,144] INFO Created log for partition __consumer_offsets-34 in /tmp/kraft-combined-logs/__consumer_offsets-34 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,144] INFO [Partition __consumer_offsets-34 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,144] INFO [Partition __consumer_offsets-34 broker=1] Log loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,144] INFO Setting topicIdPartition TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-34 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:06:51,144] INFO [MergedLog partition=__consumer_offsets-34, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-34 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:06:51,144] INFO [Broker id=1] Leader __consumer_offsets-34 with topic id Some(TDrV9QLDQhGxmWrWM4O8uw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,144] INFO [MergedLog partition=__consumer_offsets-36, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:06:51,144] INFO Created log for partition __consumer_offsets-36 in /tmp/kraft-combined-logs/__consumer_offsets-36 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,145] INFO [Partition __consumer_offsets-36 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,145] INFO [Partition __consumer_offsets-36 broker=1] Log loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,145] INFO Setting topicIdPartition TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-36 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:06:51,145] INFO [MergedLog partition=__consumer_offsets-36, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-36 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:06:51,145] INFO [Broker id=1] Leader __consumer_offsets-36 with topic id Some(TDrV9QLDQhGxmWrWM4O8uw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,145] INFO [MergedLog partition=__consumer_offsets-20, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:06:51,145] INFO Created log for partition __consumer_offsets-20 in /tmp/kraft-combined-logs/__consumer_offsets-20 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,145] INFO [Partition __consumer_offsets-20 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,145] INFO [Partition __consumer_offsets-20 broker=1] Log loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,145] INFO Setting topicIdPartition TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-20 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:06:51,146] INFO [MergedLog partition=__consumer_offsets-20, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-20 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:06:51,146] INFO [Broker id=1] Leader __consumer_offsets-20 with topic id Some(TDrV9QLDQhGxmWrWM4O8uw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,146] INFO [MergedLog partition=__consumer_offsets-7, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:06:51,146] INFO Created log for partition __consumer_offsets-7 in /tmp/kraft-combined-logs/__consumer_offsets-7 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,146] INFO [Partition __consumer_offsets-7 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,147] INFO [Partition __consumer_offsets-7 broker=1] Log loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,147] INFO Setting topicIdPartition TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-7 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:06:51,147] INFO [MergedLog partition=__consumer_offsets-7, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-7 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:06:51,147] INFO [Broker id=1] Leader __consumer_offsets-7 with topic id Some(TDrV9QLDQhGxmWrWM4O8uw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,147] INFO [MergedLog partition=__consumer_offsets-5, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:06:51,148] INFO Created log for partition __consumer_offsets-5 in /tmp/kraft-combined-logs/__consumer_offsets-5 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,148] INFO [Partition __consumer_offsets-5 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,148] INFO [Partition __consumer_offsets-5 broker=1] Log loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,148] INFO Setting topicIdPartition TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-5 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:06:51,148] INFO [MergedLog partition=__consumer_offsets-5, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-5 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:06:51,148] INFO [Broker id=1] Leader __consumer_offsets-5 with topic id Some(TDrV9QLDQhGxmWrWM4O8uw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,149] INFO [MergedLog partition=__consumer_offsets-4, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:06:51,149] INFO Created log for partition __consumer_offsets-4 in /tmp/kraft-combined-logs/__consumer_offsets-4 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,149] INFO [Partition __consumer_offsets-4 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,149] INFO [Partition __consumer_offsets-4 broker=1] Log loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,149] INFO Setting topicIdPartition TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-4 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:06:51,149] INFO [MergedLog partition=__consumer_offsets-4, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-4 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:06:51,149] INFO [Broker id=1] Leader __consumer_offsets-4 with topic id Some(TDrV9QLDQhGxmWrWM4O8uw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,150] INFO [MergedLog partition=__consumer_offsets-13, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:06:51,150] INFO Created log for partition __consumer_offsets-13 in /tmp/kraft-combined-logs/__consumer_offsets-13 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,150] INFO [Partition __consumer_offsets-13 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,150] INFO [Partition __consumer_offsets-13 broker=1] Log loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,150] INFO Setting topicIdPartition TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-13 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:06:51,150] INFO [MergedLog partition=__consumer_offsets-13, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-13 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:06:51,150] INFO [Broker id=1] Leader __consumer_offsets-13 with topic id Some(TDrV9QLDQhGxmWrWM4O8uw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,151] INFO [MergedLog partition=__consumer_offsets-11, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:06:51,151] INFO Created log for partition __consumer_offsets-11 in /tmp/kraft-combined-logs/__consumer_offsets-11 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,151] INFO [Partition __consumer_offsets-11 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,151] INFO [Partition __consumer_offsets-11 broker=1] Log loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,151] INFO Setting topicIdPartition TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-11 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:06:51,151] INFO [MergedLog partition=__consumer_offsets-11, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-11 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:06:51,151] INFO [Broker id=1] Leader __consumer_offsets-11 with topic id Some(TDrV9QLDQhGxmWrWM4O8uw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,152] INFO [MergedLog partition=__consumer_offsets-16, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:06:51,152] INFO Created log for partition __consumer_offsets-16 in /tmp/kraft-combined-logs/__consumer_offsets-16 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,152] INFO [Partition __consumer_offsets-16 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,152] INFO [Partition __consumer_offsets-16 broker=1] Log loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,152] INFO Setting topicIdPartition TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-16 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:06:51,152] INFO [MergedLog partition=__consumer_offsets-16, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-16 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:06:51,152] INFO [Broker id=1] Leader __consumer_offsets-16 with topic id Some(TDrV9QLDQhGxmWrWM4O8uw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,153] INFO [MergedLog partition=__consumer_offsets-35, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:06:51,153] INFO Created log for partition __consumer_offsets-35 in /tmp/kraft-combined-logs/__consumer_offsets-35 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,153] INFO [Partition __consumer_offsets-35 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,153] INFO [Partition __consumer_offsets-35 broker=1] Log loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,153] INFO Setting topicIdPartition TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-35 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:06:51,153] INFO [MergedLog partition=__consumer_offsets-35, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-35 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:06:51,153] INFO [Broker id=1] Leader __consumer_offsets-35 with topic id Some(TDrV9QLDQhGxmWrWM4O8uw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,154] INFO [MergedLog partition=__consumer_offsets-12, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:06:51,154] INFO Created log for partition __consumer_offsets-12 in /tmp/kraft-combined-logs/__consumer_offsets-12 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,154] INFO [Partition __consumer_offsets-12 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,154] INFO [Partition __consumer_offsets-12 broker=1] Log loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,154] INFO Setting topicIdPartition TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-12 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:06:51,155] INFO [MergedLog partition=__consumer_offsets-12, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-12 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:06:51,155] INFO [Broker id=1] Leader __consumer_offsets-12 with topic id Some(TDrV9QLDQhGxmWrWM4O8uw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,155] INFO [MergedLog partition=__consumer_offsets-32, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:06:51,155] INFO Created log for partition __consumer_offsets-32 in /tmp/kraft-combined-logs/__consumer_offsets-32 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,156] INFO [Partition __consumer_offsets-32 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,156] INFO [Partition __consumer_offsets-32 broker=1] Log loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,156] INFO Setting topicIdPartition TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-32 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:06:51,156] INFO [MergedLog partition=__consumer_offsets-32, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-32 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:06:51,156] INFO [Broker id=1] Leader __consumer_offsets-32 with topic id Some(TDrV9QLDQhGxmWrWM4O8uw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,157] INFO [MergedLog partition=__consumer_offsets-41, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:06:51,157] INFO Created log for partition __consumer_offsets-41 in /tmp/kraft-combined-logs/__consumer_offsets-41 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,157] INFO [Partition __consumer_offsets-41 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,157] INFO [Partition __consumer_offsets-41 broker=1] Log loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,157] INFO Setting topicIdPartition TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-41 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:06:51,157] INFO [MergedLog partition=__consumer_offsets-41, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-41 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:06:51,157] INFO [Broker id=1] Leader __consumer_offsets-41 with topic id Some(TDrV9QLDQhGxmWrWM4O8uw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,158] INFO [MergedLog partition=__consumer_offsets-2, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:06:51,158] INFO Created log for partition __consumer_offsets-2 in /tmp/kraft-combined-logs/__consumer_offsets-2 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,158] INFO [Partition __consumer_offsets-2 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,158] INFO [Partition __consumer_offsets-2 broker=1] Log loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,158] INFO Setting topicIdPartition TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-2 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:06:51,158] INFO [MergedLog partition=__consumer_offsets-2, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-2 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:06:51,158] INFO [Broker id=1] Leader __consumer_offsets-2 with topic id Some(TDrV9QLDQhGxmWrWM4O8uw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,159] INFO [MergedLog partition=__consumer_offsets-45, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:06:51,159] INFO Created log for partition __consumer_offsets-45 in /tmp/kraft-combined-logs/__consumer_offsets-45 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,159] INFO [Partition __consumer_offsets-45 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,159] INFO [Partition __consumer_offsets-45 broker=1] Log loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,159] INFO Setting topicIdPartition TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-45 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:06:51,160] INFO [MergedLog partition=__consumer_offsets-45, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-45 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:06:51,160] INFO [Broker id=1] Leader __consumer_offsets-45 with topic id Some(TDrV9QLDQhGxmWrWM4O8uw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,161] INFO [MergedLog partition=__consumer_offsets-44, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:06:51,161] INFO Created log for partition __consumer_offsets-44 in /tmp/kraft-combined-logs/__consumer_offsets-44 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,161] INFO [Partition __consumer_offsets-44 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,161] INFO [Partition __consumer_offsets-44 broker=1] Log loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,161] INFO Setting topicIdPartition TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-44 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:06:51,161] INFO [MergedLog partition=__consumer_offsets-44, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-44 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:06:51,161] INFO [Broker id=1] Leader __consumer_offsets-44 with topic id Some(TDrV9QLDQhGxmWrWM4O8uw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,162] INFO [MergedLog partition=__consumer_offsets-46, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:06:51,162] INFO Created log for partition __consumer_offsets-46 in /tmp/kraft-combined-logs/__consumer_offsets-46 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,162] INFO [Partition __consumer_offsets-46 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,162] INFO [Partition __consumer_offsets-46 broker=1] Log loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,162] INFO Setting topicIdPartition TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-46 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:06:51,162] INFO [MergedLog partition=__consumer_offsets-46, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-46 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:06:51,162] INFO [Broker id=1] Leader __consumer_offsets-46 with topic id Some(TDrV9QLDQhGxmWrWM4O8uw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,163] INFO [MergedLog partition=__consumer_offsets-33, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:06:51,163] INFO Created log for partition __consumer_offsets-33 in /tmp/kraft-combined-logs/__consumer_offsets-33 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,163] INFO [Partition __consumer_offsets-33 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,163] INFO [Partition __consumer_offsets-33 broker=1] Log loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,164] INFO Setting topicIdPartition TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-33 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:06:51,164] INFO [MergedLog partition=__consumer_offsets-33, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-33 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:06:51,164] INFO [Broker id=1] Leader __consumer_offsets-33 with topic id Some(TDrV9QLDQhGxmWrWM4O8uw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,164] INFO [MergedLog partition=__consumer_offsets-38, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:06:51,164] INFO Created log for partition __consumer_offsets-38 in /tmp/kraft-combined-logs/__consumer_offsets-38 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,164] INFO [Partition __consumer_offsets-38 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,164] INFO [Partition __consumer_offsets-38 broker=1] Log loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,164] INFO Setting topicIdPartition TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-38 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:06:51,165] INFO [MergedLog partition=__consumer_offsets-38, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-38 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:06:51,165] INFO [Broker id=1] Leader __consumer_offsets-38 with topic id Some(TDrV9QLDQhGxmWrWM4O8uw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,165] INFO [MergedLog partition=__consumer_offsets-40, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:06:51,165] INFO Created log for partition __consumer_offsets-40 in /tmp/kraft-combined-logs/__consumer_offsets-40 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,165] INFO [Partition __consumer_offsets-40 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,165] INFO [Partition __consumer_offsets-40 broker=1] Log loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,165] INFO Setting topicIdPartition TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-40 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:06:51,165] INFO [MergedLog partition=__consumer_offsets-40, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-40 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:06:51,166] INFO [Broker id=1] Leader __consumer_offsets-40 with topic id Some(TDrV9QLDQhGxmWrWM4O8uw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,166] INFO [MergedLog partition=__consumer_offsets-49, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:06:51,166] INFO Created log for partition __consumer_offsets-49 in /tmp/kraft-combined-logs/__consumer_offsets-49 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,166] INFO [Partition __consumer_offsets-49 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,166] INFO [Partition __consumer_offsets-49 broker=1] Log loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,167] INFO Setting topicIdPartition TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-49 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:06:51,167] INFO [MergedLog partition=__consumer_offsets-49, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-49 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:06:51,167] INFO [Broker id=1] Leader __consumer_offsets-49 with topic id Some(TDrV9QLDQhGxmWrWM4O8uw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,167] INFO [MergedLog partition=__consumer_offsets-18, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:06:51,167] INFO Created log for partition __consumer_offsets-18 in /tmp/kraft-combined-logs/__consumer_offsets-18 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,168] INFO [Partition __consumer_offsets-18 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,168] INFO [Partition __consumer_offsets-18 broker=1] Log loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,168] INFO Setting topicIdPartition TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-18 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:06:51,168] INFO [MergedLog partition=__consumer_offsets-18, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-18 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:06:51,168] INFO [Broker id=1] Leader __consumer_offsets-18 with topic id Some(TDrV9QLDQhGxmWrWM4O8uw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,169] INFO [MergedLog partition=__consumer_offsets-47, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:06:51,169] INFO Created log for partition __consumer_offsets-47 in /tmp/kraft-combined-logs/__consumer_offsets-47 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,169] INFO [Partition __consumer_offsets-47 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,169] INFO [Partition __consumer_offsets-47 broker=1] Log loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,169] INFO Setting topicIdPartition TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-47 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:06:51,169] INFO [MergedLog partition=__consumer_offsets-47, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-47 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:06:51,169] INFO [Broker id=1] Leader __consumer_offsets-47 with topic id Some(TDrV9QLDQhGxmWrWM4O8uw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,170] INFO [MergedLog partition=__consumer_offsets-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:06:51,170] INFO Created log for partition __consumer_offsets-0 in /tmp/kraft-combined-logs/__consumer_offsets-0 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,170] INFO [Partition __consumer_offsets-0 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,170] INFO [Partition __consumer_offsets-0 broker=1] Log loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,170] INFO Setting topicIdPartition TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-0 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:06:51,170] INFO [MergedLog partition=__consumer_offsets-0, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:06:51,170] INFO [Broker id=1] Leader __consumer_offsets-0 with topic id Some(TDrV9QLDQhGxmWrWM4O8uw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,171] INFO [MergedLog partition=__consumer_offsets-43, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:06:51,171] INFO Created log for partition __consumer_offsets-43 in /tmp/kraft-combined-logs/__consumer_offsets-43 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,171] INFO [Partition __consumer_offsets-43 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,171] INFO [Partition __consumer_offsets-43 broker=1] Log loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,171] INFO Setting topicIdPartition TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-43 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:06:51,171] INFO [MergedLog partition=__consumer_offsets-43, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-43 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:06:51,171] INFO [Broker id=1] Leader __consumer_offsets-43 with topic id Some(TDrV9QLDQhGxmWrWM4O8uw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,172] INFO [MergedLog partition=__consumer_offsets-25, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:06:51,172] INFO Created log for partition __consumer_offsets-25 in /tmp/kraft-combined-logs/__consumer_offsets-25 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,172] INFO [Partition __consumer_offsets-25 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,172] INFO [Partition __consumer_offsets-25 broker=1] Log loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,172] INFO Setting topicIdPartition TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-25 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:06:51,172] INFO [MergedLog partition=__consumer_offsets-25, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-25 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:06:51,172] INFO [Broker id=1] Leader __consumer_offsets-25 with topic id Some(TDrV9QLDQhGxmWrWM4O8uw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,173] INFO [MergedLog partition=__consumer_offsets-39, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:06:51,173] INFO Created log for partition __consumer_offsets-39 in /tmp/kraft-combined-logs/__consumer_offsets-39 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,173] INFO [Partition __consumer_offsets-39 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,173] INFO [Partition __consumer_offsets-39 broker=1] Log loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,173] INFO Setting topicIdPartition TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-39 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:06:51,173] INFO [MergedLog partition=__consumer_offsets-39, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-39 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:06:51,173] INFO [Broker id=1] Leader __consumer_offsets-39 with topic id Some(TDrV9QLDQhGxmWrWM4O8uw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,175] INFO [MergedLog partition=__consumer_offsets-15, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:06:51,175] INFO Created log for partition __consumer_offsets-15 in /tmp/kraft-combined-logs/__consumer_offsets-15 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,175] INFO [Partition __consumer_offsets-15 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,175] INFO [Partition __consumer_offsets-15 broker=1] Log loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,175] INFO Setting topicIdPartition TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-15 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:06:51,175] INFO [MergedLog partition=__consumer_offsets-15, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-15 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:06:51,175] INFO [Broker id=1] Leader __consumer_offsets-15 with topic id Some(TDrV9QLDQhGxmWrWM4O8uw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,177] INFO [MergedLog partition=__consumer_offsets-22, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:06:51,177] INFO Created log for partition __consumer_offsets-22 in /tmp/kraft-combined-logs/__consumer_offsets-22 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,177] INFO [Partition __consumer_offsets-22 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,177] INFO [Partition __consumer_offsets-22 broker=1] Log loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,177] INFO Setting topicIdPartition TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-22 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:06:51,177] INFO [MergedLog partition=__consumer_offsets-22, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-22 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:06:51,177] INFO [Broker id=1] Leader __consumer_offsets-22 with topic id Some(TDrV9QLDQhGxmWrWM4O8uw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,178] INFO [MergedLog partition=__consumer_offsets-48, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:06:51,178] INFO Created log for partition __consumer_offsets-48 in /tmp/kraft-combined-logs/__consumer_offsets-48 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,179] INFO [Partition __consumer_offsets-48 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,179] INFO [Partition __consumer_offsets-48 broker=1] Log loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,179] INFO Setting topicIdPartition TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-48 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:06:51,179] INFO [MergedLog partition=__consumer_offsets-48, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-48 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:06:51,179] INFO [Broker id=1] Leader __consumer_offsets-48 with topic id Some(TDrV9QLDQhGxmWrWM4O8uw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,180] INFO [MergedLog partition=__consumer_offsets-6, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:06:51,180] INFO Created log for partition __consumer_offsets-6 in /tmp/kraft-combined-logs/__consumer_offsets-6 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,180] INFO [Partition __consumer_offsets-6 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,180] INFO [Partition __consumer_offsets-6 broker=1] Log loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,180] INFO Setting topicIdPartition TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-6 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:06:51,180] INFO [MergedLog partition=__consumer_offsets-6, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-6 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:06:51,180] INFO [Broker id=1] Leader __consumer_offsets-6 with topic id Some(TDrV9QLDQhGxmWrWM4O8uw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,181] INFO [MergedLog partition=__consumer_offsets-10, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:06:51,181] INFO Created log for partition __consumer_offsets-10 in /tmp/kraft-combined-logs/__consumer_offsets-10 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,181] INFO [Partition __consumer_offsets-10 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,181] INFO [Partition __consumer_offsets-10 broker=1] Log loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,181] INFO Setting topicIdPartition TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-10 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:06:51,181] INFO [MergedLog partition=__consumer_offsets-10, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-10 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:06:51,181] INFO [Broker id=1] Leader __consumer_offsets-10 with topic id Some(TDrV9QLDQhGxmWrWM4O8uw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,182] INFO [MergedLog partition=__consumer_offsets-29, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:06:51,183] INFO Created log for partition __consumer_offsets-29 in /tmp/kraft-combined-logs/__consumer_offsets-29 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,183] INFO [Partition __consumer_offsets-29 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,183] INFO [Partition __consumer_offsets-29 broker=1] Log loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,183] INFO Setting topicIdPartition TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-29 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:06:51,183] INFO [MergedLog partition=__consumer_offsets-29, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-29 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:06:51,183] INFO [Broker id=1] Leader __consumer_offsets-29 with topic id Some(TDrV9QLDQhGxmWrWM4O8uw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,183] INFO [MergedLog partition=__consumer_offsets-14, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:06:51,183] INFO Created log for partition __consumer_offsets-14 in /tmp/kraft-combined-logs/__consumer_offsets-14 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,184] INFO [Partition __consumer_offsets-14 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,184] INFO [Partition __consumer_offsets-14 broker=1] Log loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,184] INFO Setting topicIdPartition TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-14 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:06:51,184] INFO [MergedLog partition=__consumer_offsets-14, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-14 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:06:51,184] INFO [Broker id=1] Leader __consumer_offsets-14 with topic id Some(TDrV9QLDQhGxmWrWM4O8uw) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:06:51,184] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 13 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:06:51,185] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-13 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,185] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 46 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:06:51,185] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-46 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,185] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 9 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:06:51,185] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-9 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,186] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 42 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:06:51,186] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-42 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,186] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 21 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:06:51,186] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-21 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,186] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 17 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:06:51,186] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-17 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,186] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 30 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:06:51,186] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-30 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,186] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 26 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:06:51,186] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-26 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,186] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 5 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:06:51,186] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-5 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,186] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 38 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:06:51,187] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-38 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,187] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 1 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:06:51,187] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-1 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,187] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 34 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:06:51,187] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-34 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,187] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 16 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:06:51,187] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-16 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,187] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 45 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:06:51,187] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-45 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,187] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 12 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:06:51,187] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-12 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,187] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 41 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:06:51,187] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-41 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,188] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 24 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:06:51,188] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-24 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,188] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 20 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:06:51,188] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-20 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,188] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 49 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:06:51,188] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-49 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,188] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 0 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:06:51,188] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-0 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,188] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 29 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:06:51,188] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-29 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,188] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 25 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:06:51,188] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-25 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,189] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 8 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:06:51,189] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-8 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,189] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 37 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:06:51,189] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-37 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,189] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 4 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:06:51,189] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-4 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,189] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 33 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:06:51,189] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-33 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,189] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 15 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:06:51,189] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-15 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,189] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 48 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:06:51,189] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-48 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,189] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 11 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:06:51,189] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-11 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,190] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 44 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:06:51,190] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-44 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,190] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 23 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:06:51,190] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-23 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,190] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 19 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:06:51,190] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-19 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,190] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 32 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:06:51,190] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-32 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,190] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 28 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:06:51,190] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-28 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,190] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 7 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:06:51,190] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-7 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,190] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 40 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:06:51,190] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-40 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,190] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 3 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:06:51,190] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-3 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,190] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 36 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:06:51,190] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-36 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,190] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 47 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:06:51,190] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-47 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,190] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 14 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:06:51,190] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-14 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,190] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 43 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:06:51,190] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-43 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,190] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 10 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:06:51,190] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-10 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,190] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 22 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:06:51,190] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-22 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,190] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 18 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:06:51,190] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-18 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,190] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 31 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:06:51,190] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-31 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,190] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 27 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:06:51,190] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-27 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,190] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 39 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:06:51,190] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-39 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,190] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 6 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:06:51,190] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-6 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,190] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 35 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:06:51,190] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-35 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,190] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 2 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:06:51,190] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-2 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,190] INFO [DynamicConfigPublisher broker id=1] Updating topic __consumer_offsets with new configuration : compression.type -> producer,cleanup.policy -> compact,max.compaction.lag.ms -> 9223372036854775807,segment.bytes -> 104857600,confluent.placement.constraints -> ,min.cleanable.dirty.ratio -> 0.5,delete.retention.ms -> 86400000 (kafka.server.metadata.DynamicConfigPublisher)
benchi-kafka     | [2025-03-18 16:06:51,192] INFO The cleaning for partition __consumer_offsets-13 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,192] INFO The cleaning for partition __consumer_offsets-20 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,192] INFO The cleaning for partition __consumer_offsets-41 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,192] INFO The cleaning for partition __consumer_offsets-15 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,192] INFO The cleaning for partition __consumer_offsets-5 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,192] INFO The cleaning for partition __consumer_offsets-26 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,192] INFO The cleaning for partition __consumer_offsets-38 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,192] INFO The cleaning for partition __consumer_offsets-33 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,192] INFO The cleaning for partition __consumer_offsets-44 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,192] INFO The cleaning for partition __consumer_offsets-48 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,192] INFO The cleaning for partition __consumer_offsets-0 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,192] INFO The cleaning for partition __consumer_offsets-4 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,192] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
benchi-kafka     | [2025-03-18 16:06:51,192] INFO The cleaning for partition __consumer_offsets-29 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,192] INFO The cleaning for partition __consumer_offsets-1 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,193] INFO The cleaning for partition __consumer_offsets-37 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,193] INFO The cleaning for partition __consumer_offsets-23 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,192] INFO The cleaning for partition __consumer_offsets-49 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,193] INFO The cleaning for partition __consumer_offsets-11 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,192] INFO The cleaning for partition __consumer_offsets-17 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,193] INFO The cleaning for partition __consumer_offsets-8 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,192] INFO The cleaning for partition __consumer_offsets-21 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,192] INFO The cleaning for partition __consumer_offsets-45 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,192] INFO The cleaning for partition __consumer_offsets-42 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,192] INFO The cleaning for partition __consumer_offsets-24 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,192] INFO The cleaning for partition __consumer_offsets-30 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,192] INFO The cleaning for partition __consumer_offsets-34 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,192] INFO The cleaning for partition __consumer_offsets-46 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,192] INFO The cleaning for partition __consumer_offsets-9 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,192] INFO The cleaning for partition __consumer_offsets-12 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,192] INFO The cleaning for partition __consumer_offsets-25 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,192] INFO The cleaning for partition __consumer_offsets-16 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,192] INFO The cleaning for partition __consumer_offsets-19 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,196] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-21 in 10 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,196] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-17 in 9 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,196] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-41 in 9 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,196] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-37 in 7 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,197] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-44 in 7 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,197] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-9 in 12 milliseconds for epoch 0, of which 9 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,197] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-11 in 8 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,197] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-38 in 10 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,197] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-15 in 7 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,196] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-1 in 9 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,197] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-45 in 10 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,197] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-33 in 8 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,197] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-8 in 8 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,197] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-4 in 8 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,197] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-48 in 8 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,197] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-0 in 8 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,197] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-20 in 9 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,197] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-26 in 9 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,198] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-29 in 7 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,198] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-23 in 8 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,198] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-16 in 11 milliseconds for epoch 0, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,198] INFO Cleaning for partition __consumer_offsets-15 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,198] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-46 in 13 milliseconds for epoch 0, of which 9 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,198] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-42 in 10 milliseconds for epoch 0, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,198] INFO Cleaning for partition __consumer_offsets-42 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,198] INFO Cleaning for partition __consumer_offsets-11 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,198] INFO Cleaning for partition __consumer_offsets-38 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,198] INFO Cleaning for partition __consumer_offsets-26 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,198] INFO Cleaning for partition __consumer_offsets-8 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,198] INFO Cleaning for partition __consumer_offsets-16 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,198] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-5 in 10 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,198] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-24 in 8 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,198] INFO Cleaning for partition __consumer_offsets-24 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,198] INFO Cleaning for partition __consumer_offsets-46 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,198] INFO Cleaning for partition __consumer_offsets-5 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,198] INFO Cleaning for partition __consumer_offsets-21 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,198] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-12 in 8 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,199] INFO Cleaning for partition __consumer_offsets-12 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,199] INFO Cleaning for partition __consumer_offsets-45 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,199] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-13 in 14 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,199] INFO Cleaning for partition __consumer_offsets-13 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,199] INFO Cleaning for partition __consumer_offsets-48 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,199] INFO Cleaning for partition __consumer_offsets-23 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,199] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-49 in 7 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,199] INFO Cleaning for partition __consumer_offsets-49 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,199] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-34 in 12 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,199] INFO Cleaning for partition __consumer_offsets-34 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,199] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-19 in 9 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,199] INFO Cleaning for partition __consumer_offsets-19 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,199] INFO Cleaning for partition __consumer_offsets-20 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,199] INFO Cleaning for partition __consumer_offsets-29 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,200] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-30 in 10 milliseconds for epoch 0, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,200] INFO Cleaning for partition __consumer_offsets-30 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,200] INFO Cleaning for partition __consumer_offsets-44 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,200] INFO Cleaning for partition __consumer_offsets-4 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,200] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-25 in 12 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,200] INFO Cleaning for partition __consumer_offsets-25 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,200] INFO Cleaning for partition __consumer_offsets-17 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,200] INFO Cleaning for partition __consumer_offsets-37 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,201] INFO Cleaning for partition __consumer_offsets-33 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,201] INFO Cleaning for partition __consumer_offsets-0 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,201] INFO Cleaning for partition __consumer_offsets-41 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,201] INFO Cleaning for partition __consumer_offsets-9 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,201] INFO Cleaning for partition __consumer_offsets-1 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,204] INFO The cleaning for partition __consumer_offsets-28 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,204] INFO The cleaning for partition __consumer_offsets-40 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,204] INFO The cleaning for partition __consumer_offsets-3 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,204] INFO The cleaning for partition __consumer_offsets-47 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,205] INFO The cleaning for partition __consumer_offsets-14 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,205] INFO The cleaning for partition __consumer_offsets-32 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,205] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-40 in 15 milliseconds for epoch 0, of which 14 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,205] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-47 in 15 milliseconds for epoch 0, of which 15 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,204] INFO The cleaning for partition __consumer_offsets-36 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,205] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-14 in 15 milliseconds for epoch 0, of which 15 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,205] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-32 in 15 milliseconds for epoch 0, of which 15 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,205] INFO Cleaning for partition __consumer_offsets-14 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,205] INFO Cleaning for partition __consumer_offsets-32 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,205] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-36 in 15 milliseconds for epoch 0, of which 15 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,204] INFO The cleaning for partition __consumer_offsets-7 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,205] INFO The cleaning for partition __consumer_offsets-31 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,205] INFO The cleaning for partition __consumer_offsets-43 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,205] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-31 in 15 milliseconds for epoch 0, of which 15 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,205] INFO The cleaning for partition __consumer_offsets-27 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,205] INFO The cleaning for partition __consumer_offsets-22 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,205] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-27 in 15 milliseconds for epoch 0, of which 15 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,205] INFO Cleaning for partition __consumer_offsets-27 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,205] INFO The cleaning for partition __consumer_offsets-39 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,205] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-43 in 15 milliseconds for epoch 0, of which 15 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,205] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-39 in 15 milliseconds for epoch 0, of which 15 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,205] INFO Cleaning for partition __consumer_offsets-39 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,205] INFO The cleaning for partition __consumer_offsets-6 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,205] INFO The cleaning for partition __consumer_offsets-18 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,205] INFO The cleaning for partition __consumer_offsets-35 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,205] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-6 in 15 milliseconds for epoch 0, of which 15 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,205] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-35 in 15 milliseconds for epoch 0, of which 15 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,205] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-18 in 15 milliseconds for epoch 0, of which 15 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,205] INFO Cleaning for partition __consumer_offsets-36 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,205] INFO The cleaning for partition __consumer_offsets-10 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,205] INFO Cleaning for partition __consumer_offsets-47 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,205] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-10 in 15 milliseconds for epoch 0, of which 15 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,205] INFO Cleaning for partition __consumer_offsets-10 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,205] INFO Cleaning for partition __consumer_offsets-40 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,205] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-3 in 15 milliseconds for epoch 0, of which 14 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,206] INFO Cleaning for partition __consumer_offsets-3 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,205] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-28 in 15 milliseconds for epoch 0, of which 14 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,206] INFO Cleaning for partition __consumer_offsets-28 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,205] INFO Cleaning for partition __consumer_offsets-18 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,205] INFO Cleaning for partition __consumer_offsets-35 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,205] INFO Cleaning for partition __consumer_offsets-6 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,205] INFO Cleaning for partition __consumer_offsets-43 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,205] INFO The cleaning for partition __consumer_offsets-2 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,205] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-22 in 15 milliseconds for epoch 0, of which 15 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,206] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-2 in 16 milliseconds for epoch 0, of which 16 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,206] INFO Cleaning for partition __consumer_offsets-22 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,205] INFO Cleaning for partition __consumer_offsets-31 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,205] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-7 in 15 milliseconds for epoch 0, of which 15 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:06:51,206] INFO Cleaning for partition __consumer_offsets-7 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,206] INFO Cleaning for partition __consumer_offsets-2 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:06:51,209] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group ConfluentTelemetryReporterSampler--2946986101702943459 in Empty state. Created a new member id kafka-cruise-control-43b44cb7-612b-4981-bdaa-225d386e677d and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:06:51,210] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Request joining group due to: need to re-join with the given member-id: kafka-cruise-control-43b44cb7-612b-4981-bdaa-225d386e677d (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
benchi-kafka     | [2025-03-18 16:06:51,210] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
benchi-kafka     | [2025-03-18 16:06:51,212] INFO [GroupCoordinator 1]: Preparing to rebalance group ConfluentTelemetryReporterSampler--2946986101702943459 in state PreparingRebalance with old generation 0 (__consumer_offsets-23) (reason: Adding new member kafka-cruise-control-43b44cb7-612b-4981-bdaa-225d386e677d with group instance id None; client reason: need to re-join with the given member-id: kafka-cruise-control-43b44cb7-612b-4981-bdaa-225d386e677d) (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:06:51,214] INFO [GroupCoordinator 1]: Stabilized group ConfluentTelemetryReporterSampler--2946986101702943459 generation 1 (__consumer_offsets-23) with 1 members (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:06:51,215] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Successfully joined group with generation Generation{generationId=1, memberId='kafka-cruise-control-43b44cb7-612b-4981-bdaa-225d386e677d', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
benchi-kafka     | [2025-03-18 16:06:51,218] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Finished assignment for group at generation 1: {kafka-cruise-control-43b44cb7-612b-4981-bdaa-225d386e677d=Assignment(partitions=[_confluent-telemetry-metrics-0, _confluent-telemetry-metrics-1, _confluent-telemetry-metrics-2, _confluent-telemetry-metrics-3, _confluent-telemetry-metrics-4, _confluent-telemetry-metrics-5, _confluent-telemetry-metrics-6, _confluent-telemetry-metrics-7, _confluent-telemetry-metrics-8, _confluent-telemetry-metrics-9, _confluent-telemetry-metrics-10, _confluent-telemetry-metrics-11])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
benchi-kafka     | [2025-03-18 16:06:51,220] INFO [GroupCoordinator 1]: Assignment received from leader kafka-cruise-control-43b44cb7-612b-4981-bdaa-225d386e677d for group ConfluentTelemetryReporterSampler--2946986101702943459 for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:06:51,224] INFO [Partition __consumer_offsets-23 broker=1] roll: __consumer_offsets-23: first produce received, lastOffset: 0, leaderEpoch: 0, numMessages:1, time diff: 102 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,224] INFO [Partition __consumer_offsets-23 broker=1] roll: __consumer_offsets-23: first HWM advanced, leaderEpoch: 0, old HW: 0, new HW: 1, become leader time: 1742314011122 ms, time diff: 102 ms (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:06:51,225] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Successfully synced group in generation Generation{generationId=1, memberId='kafka-cruise-control-43b44cb7-612b-4981-bdaa-225d386e677d', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
benchi-kafka     | [2025-03-18 16:06:51,226] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Notifying assignor about the new Assignment(partitions=[_confluent-telemetry-metrics-0, _confluent-telemetry-metrics-1, _confluent-telemetry-metrics-2, _confluent-telemetry-metrics-3, _confluent-telemetry-metrics-4, _confluent-telemetry-metrics-5, _confluent-telemetry-metrics-6, _confluent-telemetry-metrics-7, _confluent-telemetry-metrics-8, _confluent-telemetry-metrics-9, _confluent-telemetry-metrics-10, _confluent-telemetry-metrics-11]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
benchi-kafka     | [2025-03-18 16:06:51,226] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Adding newly assigned partitions: _confluent-telemetry-metrics-0, _confluent-telemetry-metrics-1, _confluent-telemetry-metrics-2, _confluent-telemetry-metrics-3, _confluent-telemetry-metrics-4, _confluent-telemetry-metrics-5, _confluent-telemetry-metrics-6, _confluent-telemetry-metrics-7, _confluent-telemetry-metrics-8, _confluent-telemetry-metrics-9, _confluent-telemetry-metrics-10, _confluent-telemetry-metrics-11 (org.apache.kafka.clients.consumer.internals.ConsumerRebalanceListenerInvoker)
benchi-kafka     | [2025-03-18 16:06:51,229] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Found no committed offset for partition _confluent-telemetry-metrics-3 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
benchi-kafka     | [2025-03-18 16:06:51,229] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Found no committed offset for partition _confluent-telemetry-metrics-4 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
benchi-kafka     | [2025-03-18 16:06:51,229] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Found no committed offset for partition _confluent-telemetry-metrics-5 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
benchi-kafka     | [2025-03-18 16:06:51,229] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Found no committed offset for partition _confluent-telemetry-metrics-6 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
benchi-kafka     | [2025-03-18 16:06:51,229] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Found no committed offset for partition _confluent-telemetry-metrics-7 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
benchi-kafka     | [2025-03-18 16:06:51,229] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Found no committed offset for partition _confluent-telemetry-metrics-8 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
benchi-kafka     | [2025-03-18 16:06:51,229] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Found no committed offset for partition _confluent-telemetry-metrics-9 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
benchi-kafka     | [2025-03-18 16:06:51,229] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Found no committed offset for partition _confluent-telemetry-metrics-10 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
benchi-kafka     | [2025-03-18 16:06:51,229] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Found no committed offset for partition _confluent-telemetry-metrics-11 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
benchi-kafka     | [2025-03-18 16:06:51,229] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Found no committed offset for partition _confluent-telemetry-metrics-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
benchi-kafka     | [2025-03-18 16:06:51,229] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Found no committed offset for partition _confluent-telemetry-metrics-1 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
benchi-kafka     | [2025-03-18 16:06:51,229] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Found no committed offset for partition _confluent-telemetry-metrics-2 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
benchi-kafka     | [2025-03-18 16:06:51,235] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Seeking to offset 0 for partition _confluent-telemetry-metrics-3 (org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer)
benchi-kafka     | [2025-03-18 16:06:51,235] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Seeking to offset 0 for partition _confluent-telemetry-metrics-4 (org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer)
benchi-kafka     | [2025-03-18 16:06:51,235] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Seeking to offset 0 for partition _confluent-telemetry-metrics-5 (org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer)
benchi-kafka     | [2025-03-18 16:06:51,235] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Seeking to offset 0 for partition _confluent-telemetry-metrics-6 (org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer)
benchi-kafka     | [2025-03-18 16:06:51,235] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Seeking to offset 0 for partition _confluent-telemetry-metrics-7 (org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer)
benchi-kafka     | [2025-03-18 16:06:51,235] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Seeking to offset 0 for partition _confluent-telemetry-metrics-8 (org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer)
benchi-kafka     | [2025-03-18 16:06:51,235] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Seeking to offset 0 for partition _confluent-telemetry-metrics-9 (org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer)
benchi-kafka     | [2025-03-18 16:06:51,235] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Seeking to offset 0 for partition _confluent-telemetry-metrics-10 (org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer)
benchi-kafka     | [2025-03-18 16:06:51,235] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Seeking to offset 0 for partition _confluent-telemetry-metrics-11 (org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer)
benchi-kafka     | [2025-03-18 16:06:51,235] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Seeking to offset 0 for partition _confluent-telemetry-metrics-0 (org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer)
benchi-kafka     | [2025-03-18 16:06:51,235] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Seeking to offset 0 for partition _confluent-telemetry-metrics-1 (org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer)
benchi-kafka     | [2025-03-18 16:06:51,235] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Seeking to offset 0 for partition _confluent-telemetry-metrics-2 (org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer)
benchi-kafka     | [2025-03-18 16:06:51,264] INFO Finished sampling for 12 partitions - processed 79 metrics over 1 polls for the time range [16:03:00,16:05:59.999], with 0 of them added to the metrics processor, 79 of them being later than the desired end time and 0 being earlier than the desired start time. (io.confluent.cruisecontrol.metricsreporter.ConfluentMetricsSamplerBase)
benchi-kafka     | [2025-03-18 16:06:51,264] WARN Zero replica samples were added! Collected 0 (0 discarded, 0 added) replica metric samples for 0 replicas. Total partition assigned: 64. Total unrecognized replicas: 0 (com.linkedin.kafka.cruisecontrol.monitor.sampling.MetricFetcherManager)
benchi-kafka     | [2025-03-18 16:06:51,264] WARN Zero partition samples were added! Collected 0 (0 discarded, 0 added) partition metric samples for 0 partitions. Total partition assigned: 64. Total unrecognized partitions: 0 (com.linkedin.kafka.cruisecontrol.monitor.sampling.MetricFetcherManager)
benchi-kafka     | [2025-03-18 16:06:51,264] INFO Successfully finished metric sampling for time period 16:03:00 to 16:05:59.999 (1742313780000 to 1742313959999). (com.linkedin.kafka.cruisecontrol.monitor.task.MetricSamplingTask)
benchi-kafka     | [2025-03-18 16:06:51,264] INFO Sleeping the SamplingScheduler until 16:08:59.999 (for 128735ms) as instructed due to reason The last eligible window for sampling was already sampled. Sleeping until the end of the current window...  (lastSampledWindow: (index: 9679522, 16:03:00 - 16:05:59.999), currentWindow: (index: 9679523, 16:06:00 - 16:08:59.999)) (com.linkedin.kafka.cruisecontrol.monitor.task.MetricSamplingTask)
benchi-kafka     | [2025-03-18 16:07:01,521] INFO AdminClientConfig values: 
benchi-kafka     | 	auto.include.jmx.reporter = true
benchi-kafka     | 	bootstrap.controllers = []
benchi-kafka     | 	bootstrap.servers = [localhost:9092]
benchi-kafka     | 	client.dns.lookup = use_all_dns_ips
benchi-kafka     | 	client.id = 
benchi-kafka     | 	confluent.lkc.id = null
benchi-kafka     | 	confluent.metrics.reporter.bootstrap.servers = kafka-0:9071
benchi-kafka     | 	confluent.proxy.protocol.client.address = null
benchi-kafka     | 	confluent.proxy.protocol.client.mode = PROXY
benchi-kafka     | 	confluent.proxy.protocol.client.port = null
benchi-kafka     | 	confluent.proxy.protocol.client.version = NONE
benchi-kafka     | 	connections.max.idle.ms = 300000
benchi-kafka     | 	default.api.timeout.ms = 60000
benchi-kafka     | 	enable.metrics.push = true
benchi-kafka     | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
benchi-kafka     | 	metadata.max.age.ms = 300000
benchi-kafka     | 	metadata.recovery.strategy = none
benchi-kafka     | 	metric.reporters = []
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.recording.level = INFO
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	receive.buffer.bytes = 65536
benchi-kafka     | 	reconnect.backoff.max.ms = 1000
benchi-kafka     | 	reconnect.backoff.ms = 50
benchi-kafka     | 	request.timeout.ms = 30000
benchi-kafka     | 	retries = 2147483647
benchi-kafka     | 	retry.backoff.max.ms = 1000
benchi-kafka     | 	retry.backoff.ms = 100
benchi-kafka     | 	sasl.client.callback.handler.class = null
benchi-kafka     | 	sasl.jaas.config = null
benchi-kafka     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
benchi-kafka     | 	sasl.kerberos.min.time.before.relogin = 60000
benchi-kafka     | 	sasl.kerberos.service.name = null
benchi-kafka     | 	sasl.kerberos.ticket.renew.jitter = 0.05
benchi-kafka     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
benchi-kafka     | 	sasl.login.callback.handler.class = null
benchi-kafka     | 	sasl.login.class = null
benchi-kafka     | 	sasl.login.connect.timeout.ms = null
benchi-kafka     | 	sasl.login.read.timeout.ms = null
benchi-kafka     | 	sasl.login.refresh.buffer.seconds = 300
benchi-kafka     | 	sasl.login.refresh.min.period.seconds = 60
benchi-kafka     | 	sasl.login.refresh.window.factor = 0.8
benchi-kafka     | 	sasl.login.refresh.window.jitter = 0.05
benchi-kafka     | 	sasl.login.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.login.retry.backoff.ms = 100
benchi-kafka     | 	sasl.mechanism = GSSAPI
benchi-kafka     | 	sasl.oauthbearer.clientassertion.audience = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.expiration = 5
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.jti.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.nbf.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.issuer = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.location = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key.passphrase = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.subject = null
benchi-kafka     | 	sasl.oauthbearer.clock.skew.seconds = 30
benchi-kafka     | 	sasl.oauthbearer.expected.audience = null
benchi-kafka     | 	sasl.oauthbearer.expected.issuer = null
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.url = null
benchi-kafka     | 	sasl.oauthbearer.scope.claim.name = scope
benchi-kafka     | 	sasl.oauthbearer.sub.claim.name = sub
benchi-kafka     | 	sasl.oauthbearer.token.endpoint.url = null
benchi-kafka     | 	security.protocol = PLAINTEXT
benchi-kafka     | 	security.providers = null
benchi-kafka     | 	send.buffer.bytes = 131072
benchi-kafka     | 	socket.connection.setup.timeout.max.ms = 30000
benchi-kafka     | 	socket.connection.setup.timeout.ms = 10000
benchi-kafka     | 	ssl.cipher.suites = null
benchi-kafka     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	ssl.engine.factory.class = null
benchi-kafka     | 	ssl.key.password = null
benchi-kafka     | 	ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	ssl.keystore.certificate.chain = null
benchi-kafka     | 	ssl.keystore.key = null
benchi-kafka     | 	ssl.keystore.location = null
benchi-kafka     | 	ssl.keystore.password = null
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.protocol = TLSv1.3
benchi-kafka     | 	ssl.provider = null
benchi-kafka     | 	ssl.secure.random.implementation = null
benchi-kafka     | 	ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	ssl.truststore.certificates = null
benchi-kafka     | 	ssl.truststore.location = null
benchi-kafka     | 	ssl.truststore.password = null
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     |  (org.apache.kafka.clients.admin.AdminClientConfig)
benchi-kafka     | [2025-03-18 16:07:01,524] INFO Kafka version: 7.8.1-ce (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:07:01,524] INFO Kafka commitId: eefb51c6f2f1b888b8b03296353338afbac8e213 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:07:01,524] INFO Kafka startTimeMs: 1742314021524 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:07:01,529] INFO [AdminClient clientId=adminclient-6] The DescribeTopicPartitions API is not supported, using Metadata API to describe topics. (org.apache.kafka.clients.admin.KafkaAdminClient)
benchi-kafka     | [2025-03-18 16:07:01,532] INFO [ControllerServer id=1] CreateTopics result(s): CreatableTopic(name='_confluent-metrics', numPartitions=12, replicationFactor=3, assignments=[], configs=[CreateableTopicConfig(name='max.message.bytes', value='10485760'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='259200000'), CreateableTopicConfig(name='segment.ms', value='14400000'), CreateableTopicConfig(name='retention.bytes', value='-1')], linkName=null, mirrorTopic=null, sourceTopicId=AAAAAAAAAAAAAAAAAAAAAA, mirrorStartOffsetSpec=-9223372036854775808, mirrorStartOffsets=[], stoppedSequenceNumber=0): INVALID_REPLICATION_FACTOR (Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.) (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:07:01,533] INFO App info kafka.admin.client for adminclient-6 unregistered (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:07:01,534] ERROR Error checking or creating metrics topic (io.confluent.metrics.reporter.ConfluentMetricsReporter)
benchi-kafka     | org.apache.kafka.common.errors.InvalidReplicationFactorException: Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.
benchi-kafka     | [2025-03-18 16:07:16,524] INFO AdminClientConfig values: 
benchi-kafka     | 	auto.include.jmx.reporter = true
benchi-kafka     | 	bootstrap.controllers = []
benchi-kafka     | 	bootstrap.servers = [localhost:9092]
benchi-kafka     | 	client.dns.lookup = use_all_dns_ips
benchi-kafka     | 	client.id = 
benchi-kafka     | 	confluent.lkc.id = null
benchi-kafka     | 	confluent.metrics.reporter.bootstrap.servers = kafka-0:9071
benchi-kafka     | 	confluent.proxy.protocol.client.address = null
benchi-kafka     | 	confluent.proxy.protocol.client.mode = PROXY
benchi-kafka     | 	confluent.proxy.protocol.client.port = null
benchi-kafka     | 	confluent.proxy.protocol.client.version = NONE
benchi-kafka     | 	connections.max.idle.ms = 300000
benchi-kafka     | 	default.api.timeout.ms = 60000
benchi-kafka     | 	enable.metrics.push = true
benchi-kafka     | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
benchi-kafka     | 	metadata.max.age.ms = 300000
benchi-kafka     | 	metadata.recovery.strategy = none
benchi-kafka     | 	metric.reporters = []
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.recording.level = INFO
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	receive.buffer.bytes = 65536
benchi-kafka     | 	reconnect.backoff.max.ms = 1000
benchi-kafka     | 	reconnect.backoff.ms = 50
benchi-kafka     | 	request.timeout.ms = 30000
benchi-kafka     | 	retries = 2147483647
benchi-kafka     | 	retry.backoff.max.ms = 1000
benchi-kafka     | 	retry.backoff.ms = 100
benchi-kafka     | 	sasl.client.callback.handler.class = null
benchi-kafka     | 	sasl.jaas.config = null
benchi-kafka     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
benchi-kafka     | 	sasl.kerberos.min.time.before.relogin = 60000
benchi-kafka     | 	sasl.kerberos.service.name = null
benchi-kafka     | 	sasl.kerberos.ticket.renew.jitter = 0.05
benchi-kafka     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
benchi-kafka     | 	sasl.login.callback.handler.class = null
benchi-kafka     | 	sasl.login.class = null
benchi-kafka     | 	sasl.login.connect.timeout.ms = null
benchi-kafka     | 	sasl.login.read.timeout.ms = null
benchi-kafka     | 	sasl.login.refresh.buffer.seconds = 300
benchi-kafka     | 	sasl.login.refresh.min.period.seconds = 60
benchi-kafka     | 	sasl.login.refresh.window.factor = 0.8
benchi-kafka     | 	sasl.login.refresh.window.jitter = 0.05
benchi-kafka     | 	sasl.login.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.login.retry.backoff.ms = 100
benchi-kafka     | 	sasl.mechanism = GSSAPI
benchi-kafka     | 	sasl.oauthbearer.clientassertion.audience = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.expiration = 5
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.jti.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.nbf.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.issuer = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.location = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key.passphrase = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.subject = null
benchi-kafka     | 	sasl.oauthbearer.clock.skew.seconds = 30
benchi-kafka     | 	sasl.oauthbearer.expected.audience = null
benchi-kafka     | 	sasl.oauthbearer.expected.issuer = null
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.url = null
benchi-kafka     | 	sasl.oauthbearer.scope.claim.name = scope
benchi-kafka     | 	sasl.oauthbearer.sub.claim.name = sub
benchi-kafka     | 	sasl.oauthbearer.token.endpoint.url = null
benchi-kafka     | 	security.protocol = PLAINTEXT
benchi-kafka     | 	security.providers = null
benchi-kafka     | 	send.buffer.bytes = 131072
benchi-kafka     | 	socket.connection.setup.timeout.max.ms = 30000
benchi-kafka     | 	socket.connection.setup.timeout.ms = 10000
benchi-kafka     | 	ssl.cipher.suites = null
benchi-kafka     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	ssl.engine.factory.class = null
benchi-kafka     | 	ssl.key.password = null
benchi-kafka     | 	ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	ssl.keystore.certificate.chain = null
benchi-kafka     | 	ssl.keystore.key = null
benchi-kafka     | 	ssl.keystore.location = null
benchi-kafka     | 	ssl.keystore.password = null
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.protocol = TLSv1.3
benchi-kafka     | 	ssl.provider = null
benchi-kafka     | 	ssl.secure.random.implementation = null
benchi-kafka     | 	ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	ssl.truststore.certificates = null
benchi-kafka     | 	ssl.truststore.location = null
benchi-kafka     | 	ssl.truststore.password = null
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     |  (org.apache.kafka.clients.admin.AdminClientConfig)
benchi-kafka     | [2025-03-18 16:07:16,526] INFO Kafka version: 7.8.1-ce (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:07:16,526] INFO Kafka commitId: eefb51c6f2f1b888b8b03296353338afbac8e213 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:07:16,526] INFO Kafka startTimeMs: 1742314036526 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:07:16,535] INFO [AdminClient clientId=adminclient-7] The DescribeTopicPartitions API is not supported, using Metadata API to describe topics. (org.apache.kafka.clients.admin.KafkaAdminClient)
benchi-kafka     | [2025-03-18 16:07:16,538] INFO [ControllerServer id=1] CreateTopics result(s): CreatableTopic(name='_confluent-metrics', numPartitions=12, replicationFactor=3, assignments=[], configs=[CreateableTopicConfig(name='max.message.bytes', value='10485760'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='259200000'), CreateableTopicConfig(name='segment.ms', value='14400000'), CreateableTopicConfig(name='retention.bytes', value='-1')], linkName=null, mirrorTopic=null, sourceTopicId=AAAAAAAAAAAAAAAAAAAAAA, mirrorStartOffsetSpec=-9223372036854775808, mirrorStartOffsets=[], stoppedSequenceNumber=0): INVALID_REPLICATION_FACTOR (Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.) (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:07:16,539] INFO App info kafka.admin.client for adminclient-7 unregistered (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:07:16,540] ERROR Error checking or creating metrics topic (io.confluent.metrics.reporter.ConfluentMetricsReporter)
benchi-kafka     | org.apache.kafka.common.errors.InvalidReplicationFactorException: Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.
benchi-kafka     | [2025-03-18 16:07:26,688] INFO [AdminClient clientId=adminclient-5] The DescribeTopicPartitions API is not supported, using Metadata API to describe topics. (org.apache.kafka.clients.admin.KafkaAdminClient)
benchi-kafka     | [2025-03-18 16:07:26,689] INFO Notifying listeners about metadata change. (com.linkedin.kafka.cruisecontrol.common.MetadataClient)
benchi-kafka     | [2025-03-18 16:07:26,690] INFO Skipping goal violation detection due to previous new broker change - will resume at 16:36:50.978 (1742315810978) (com.linkedin.kafka.cruisecontrol.detector.GoalViolationDetector)
benchi-kafka     | [2025-03-18 16:07:26,690] INFO Skipping goal violation detection due to previous new broker change - will resume at 16:36:50.978 (1742315810978) (com.linkedin.kafka.cruisecontrol.detector.GoalViolationDetector)
benchi-kafka     | [2025-03-18 16:07:31,522] INFO AdminClientConfig values: 
benchi-kafka     | 	auto.include.jmx.reporter = true
benchi-kafka     | 	bootstrap.controllers = []
benchi-kafka     | 	bootstrap.servers = [localhost:9092]
benchi-kafka     | 	client.dns.lookup = use_all_dns_ips
benchi-kafka     | 	client.id = 
benchi-kafka     | 	confluent.lkc.id = null
benchi-kafka     | 	confluent.metrics.reporter.bootstrap.servers = kafka-0:9071
benchi-kafka     | 	confluent.proxy.protocol.client.address = null
benchi-kafka     | 	confluent.proxy.protocol.client.mode = PROXY
benchi-kafka     | 	confluent.proxy.protocol.client.port = null
benchi-kafka     | 	confluent.proxy.protocol.client.version = NONE
benchi-kafka     | 	connections.max.idle.ms = 300000
benchi-kafka     | 	default.api.timeout.ms = 60000
benchi-kafka     | 	enable.metrics.push = true
benchi-kafka     | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
benchi-kafka     | 	metadata.max.age.ms = 300000
benchi-kafka     | 	metadata.recovery.strategy = none
benchi-kafka     | 	metric.reporters = []
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.recording.level = INFO
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	receive.buffer.bytes = 65536
benchi-kafka     | 	reconnect.backoff.max.ms = 1000
benchi-kafka     | 	reconnect.backoff.ms = 50
benchi-kafka     | 	request.timeout.ms = 30000
benchi-kafka     | 	retries = 2147483647
benchi-kafka     | 	retry.backoff.max.ms = 1000
benchi-kafka     | 	retry.backoff.ms = 100
benchi-kafka     | 	sasl.client.callback.handler.class = null
benchi-kafka     | 	sasl.jaas.config = null
benchi-kafka     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
benchi-kafka     | 	sasl.kerberos.min.time.before.relogin = 60000
benchi-kafka     | 	sasl.kerberos.service.name = null
benchi-kafka     | 	sasl.kerberos.ticket.renew.jitter = 0.05
benchi-kafka     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
benchi-kafka     | 	sasl.login.callback.handler.class = null
benchi-kafka     | 	sasl.login.class = null
benchi-kafka     | 	sasl.login.connect.timeout.ms = null
benchi-kafka     | 	sasl.login.read.timeout.ms = null
benchi-kafka     | 	sasl.login.refresh.buffer.seconds = 300
benchi-kafka     | 	sasl.login.refresh.min.period.seconds = 60
benchi-kafka     | 	sasl.login.refresh.window.factor = 0.8
benchi-kafka     | 	sasl.login.refresh.window.jitter = 0.05
benchi-kafka     | 	sasl.login.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.login.retry.backoff.ms = 100
benchi-kafka     | 	sasl.mechanism = GSSAPI
benchi-kafka     | 	sasl.oauthbearer.clientassertion.audience = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.expiration = 5
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.jti.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.nbf.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.issuer = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.location = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key.passphrase = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.subject = null
benchi-kafka     | 	sasl.oauthbearer.clock.skew.seconds = 30
benchi-kafka     | 	sasl.oauthbearer.expected.audience = null
benchi-kafka     | 	sasl.oauthbearer.expected.issuer = null
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.url = null
benchi-kafka     | 	sasl.oauthbearer.scope.claim.name = scope
benchi-kafka     | 	sasl.oauthbearer.sub.claim.name = sub
benchi-kafka     | 	sasl.oauthbearer.token.endpoint.url = null
benchi-kafka     | 	security.protocol = PLAINTEXT
benchi-kafka     | 	security.providers = null
benchi-kafka     | 	send.buffer.bytes = 131072
benchi-kafka     | 	socket.connection.setup.timeout.max.ms = 30000
benchi-kafka     | 	socket.connection.setup.timeout.ms = 10000
benchi-kafka     | 	ssl.cipher.suites = null
benchi-kafka     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	ssl.engine.factory.class = null
benchi-kafka     | 	ssl.key.password = null
benchi-kafka     | 	ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	ssl.keystore.certificate.chain = null
benchi-kafka     | 	ssl.keystore.key = null
benchi-kafka     | 	ssl.keystore.location = null
benchi-kafka     | 	ssl.keystore.password = null
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.protocol = TLSv1.3
benchi-kafka     | 	ssl.provider = null
benchi-kafka     | 	ssl.secure.random.implementation = null
benchi-kafka     | 	ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	ssl.truststore.certificates = null
benchi-kafka     | 	ssl.truststore.location = null
benchi-kafka     | 	ssl.truststore.password = null
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     |  (org.apache.kafka.clients.admin.AdminClientConfig)
benchi-kafka     | [2025-03-18 16:07:31,524] INFO Kafka version: 7.8.1-ce (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:07:31,524] INFO Kafka commitId: eefb51c6f2f1b888b8b03296353338afbac8e213 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:07:31,524] INFO Kafka startTimeMs: 1742314051524 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:07:31,529] INFO [AdminClient clientId=adminclient-8] The DescribeTopicPartitions API is not supported, using Metadata API to describe topics. (org.apache.kafka.clients.admin.KafkaAdminClient)
benchi-kafka     | [2025-03-18 16:07:31,531] INFO [ControllerServer id=1] CreateTopics result(s): CreatableTopic(name='_confluent-metrics', numPartitions=12, replicationFactor=3, assignments=[], configs=[CreateableTopicConfig(name='max.message.bytes', value='10485760'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='259200000'), CreateableTopicConfig(name='segment.ms', value='14400000'), CreateableTopicConfig(name='retention.bytes', value='-1')], linkName=null, mirrorTopic=null, sourceTopicId=AAAAAAAAAAAAAAAAAAAAAA, mirrorStartOffsetSpec=-9223372036854775808, mirrorStartOffsets=[], stoppedSequenceNumber=0): INVALID_REPLICATION_FACTOR (Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.) (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:07:31,532] INFO App info kafka.admin.client for adminclient-8 unregistered (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:07:31,533] ERROR Error checking or creating metrics topic (io.confluent.metrics.reporter.ConfluentMetricsReporter)
benchi-kafka     | org.apache.kafka.common.errors.InvalidReplicationFactorException: Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.
benchi-kafka     | [2025-03-18 16:07:45,891] INFO [CelltControllerMetricsPublisher id=1] No cells found. Skipping cell metrics refresh. (org.apache.kafka.controller.metrics.CellControllerMetricsPublisher)
benchi-kafka     | [2025-03-18 16:07:46,519] INFO AdminClientConfig values: 
benchi-kafka     | 	auto.include.jmx.reporter = true
benchi-kafka     | 	bootstrap.controllers = []
benchi-kafka     | 	bootstrap.servers = [localhost:9092]
benchi-kafka     | 	client.dns.lookup = use_all_dns_ips
benchi-kafka     | 	client.id = 
benchi-kafka     | 	confluent.lkc.id = null
benchi-kafka     | 	confluent.metrics.reporter.bootstrap.servers = kafka-0:9071
benchi-kafka     | 	confluent.proxy.protocol.client.address = null
benchi-kafka     | 	confluent.proxy.protocol.client.mode = PROXY
benchi-kafka     | 	confluent.proxy.protocol.client.port = null
benchi-kafka     | 	confluent.proxy.protocol.client.version = NONE
benchi-kafka     | 	connections.max.idle.ms = 300000
benchi-kafka     | 	default.api.timeout.ms = 60000
benchi-kafka     | 	enable.metrics.push = true
benchi-kafka     | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
benchi-kafka     | 	metadata.max.age.ms = 300000
benchi-kafka     | 	metadata.recovery.strategy = none
benchi-kafka     | 	metric.reporters = []
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.recording.level = INFO
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	receive.buffer.bytes = 65536
benchi-kafka     | 	reconnect.backoff.max.ms = 1000
benchi-kafka     | 	reconnect.backoff.ms = 50
benchi-kafka     | 	request.timeout.ms = 30000
benchi-kafka     | 	retries = 2147483647
benchi-kafka     | 	retry.backoff.max.ms = 1000
benchi-kafka     | 	retry.backoff.ms = 100
benchi-kafka     | 	sasl.client.callback.handler.class = null
benchi-kafka     | 	sasl.jaas.config = null
benchi-kafka     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
benchi-kafka     | 	sasl.kerberos.min.time.before.relogin = 60000
benchi-kafka     | 	sasl.kerberos.service.name = null
benchi-kafka     | 	sasl.kerberos.ticket.renew.jitter = 0.05
benchi-kafka     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
benchi-kafka     | 	sasl.login.callback.handler.class = null
benchi-kafka     | 	sasl.login.class = null
benchi-kafka     | 	sasl.login.connect.timeout.ms = null
benchi-kafka     | 	sasl.login.read.timeout.ms = null
benchi-kafka     | 	sasl.login.refresh.buffer.seconds = 300
benchi-kafka     | 	sasl.login.refresh.min.period.seconds = 60
benchi-kafka     | 	sasl.login.refresh.window.factor = 0.8
benchi-kafka     | 	sasl.login.refresh.window.jitter = 0.05
benchi-kafka     | 	sasl.login.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.login.retry.backoff.ms = 100
benchi-kafka     | 	sasl.mechanism = GSSAPI
benchi-kafka     | 	sasl.oauthbearer.clientassertion.audience = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.expiration = 5
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.jti.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.nbf.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.issuer = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.location = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key.passphrase = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.subject = null
benchi-kafka     | 	sasl.oauthbearer.clock.skew.seconds = 30
benchi-kafka     | 	sasl.oauthbearer.expected.audience = null
benchi-kafka     | 	sasl.oauthbearer.expected.issuer = null
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.url = null
benchi-kafka     | 	sasl.oauthbearer.scope.claim.name = scope
benchi-kafka     | 	sasl.oauthbearer.sub.claim.name = sub
benchi-kafka     | 	sasl.oauthbearer.token.endpoint.url = null
benchi-kafka     | 	security.protocol = PLAINTEXT
benchi-kafka     | 	security.providers = null
benchi-kafka     | 	send.buffer.bytes = 131072
benchi-kafka     | 	socket.connection.setup.timeout.max.ms = 30000
benchi-kafka     | 	socket.connection.setup.timeout.ms = 10000
benchi-kafka     | 	ssl.cipher.suites = null
benchi-kafka     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	ssl.engine.factory.class = null
benchi-kafka     | 	ssl.key.password = null
benchi-kafka     | 	ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	ssl.keystore.certificate.chain = null
benchi-kafka     | 	ssl.keystore.key = null
benchi-kafka     | 	ssl.keystore.location = null
benchi-kafka     | 	ssl.keystore.password = null
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.protocol = TLSv1.3
benchi-kafka     | 	ssl.provider = null
benchi-kafka     | 	ssl.secure.random.implementation = null
benchi-kafka     | 	ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	ssl.truststore.certificates = null
benchi-kafka     | 	ssl.truststore.location = null
benchi-kafka     | 	ssl.truststore.password = null
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     |  (org.apache.kafka.clients.admin.AdminClientConfig)
benchi-kafka     | [2025-03-18 16:07:46,520] INFO Kafka version: 7.8.1-ce (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:07:46,520] INFO Kafka commitId: eefb51c6f2f1b888b8b03296353338afbac8e213 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:07:46,520] INFO Kafka startTimeMs: 1742314066520 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:07:46,524] INFO [AdminClient clientId=adminclient-9] The DescribeTopicPartitions API is not supported, using Metadata API to describe topics. (org.apache.kafka.clients.admin.KafkaAdminClient)
benchi-kafka     | [2025-03-18 16:07:46,526] INFO [ControllerServer id=1] CreateTopics result(s): CreatableTopic(name='_confluent-metrics', numPartitions=12, replicationFactor=3, assignments=[], configs=[CreateableTopicConfig(name='max.message.bytes', value='10485760'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='259200000'), CreateableTopicConfig(name='segment.ms', value='14400000'), CreateableTopicConfig(name='retention.bytes', value='-1')], linkName=null, mirrorTopic=null, sourceTopicId=AAAAAAAAAAAAAAAAAAAAAA, mirrorStartOffsetSpec=-9223372036854775808, mirrorStartOffsets=[], stoppedSequenceNumber=0): INVALID_REPLICATION_FACTOR (Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.) (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:07:46,538] INFO App info kafka.admin.client for adminclient-9 unregistered (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:07:46,539] ERROR Error checking or creating metrics topic (io.confluent.metrics.reporter.ConfluentMetricsReporter)
benchi-kafka     | org.apache.kafka.common.errors.InvalidReplicationFactorException: Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.
benchi-postgres  | 2025-03-18 16:07:55.230 UTC [1] LOG:  received fast shutdown request
benchi-kafka     | [2025-03-18 16:07:55,231] INFO Terminating process due to signal SIGTERM (org.apache.kafka.common.utils.LoggingSignalHandler)
benchi-kafka     | [2025-03-18 16:07:55,232] INFO [BrokerServer id=1] Transition from STARTED to SHUTTING_DOWN (kafka.server.BrokerServer)
benchi-postgres  | 2025-03-18 16:07:55.232 UTC [1] LOG:  aborting any active transactions
benchi-kafka     | [2025-03-18 16:07:55,232] INFO [BrokerServer id=1] shutting down (kafka.server.BrokerServer)
benchi-kafka     | [2025-03-18 16:07:55,233] INFO Closing License Store (io.confluent.license.LicenseStore)
benchi-kafka     | [2025-03-18 16:07:55,233] INFO Stopping KafkaBasedLog for topic _confluent-command (org.apache.kafka.connect.util.KafkaBasedLog)
benchi-postgres  | 2025-03-18 16:07:55.233 UTC [1] LOG:  background worker "logical replication launcher" (PID 69) exited with exit code 1
benchi-postgres  | 2025-03-18 16:07:55.236 UTC [64] LOG:  shutting down
benchi-kafka     | [2025-03-18 16:07:55,236] INFO [Producer clientId=_confluent-license-producer-1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer)
benchi-postgres  | 2025-03-18 16:07:55.243 UTC [64] LOG:  checkpoint starting: shutdown immediate
benchi-kafka     | [2025-03-18 16:07:55,245] INFO Stopped NetworkTrafficServerConnector@77e1dacd{HTTP/1.1, (http/1.1, h2c)}{0.0.0.0:8090} (org.eclipse.jetty.server.AbstractConnector)
benchi-kafka     | [2025-03-18 16:07:55,245] INFO node0 Stopped scavenging (org.eclipse.jetty.server.session)
benchi-kafka     | [2025-03-18 16:07:55,246] INFO Stopped o.e.j.s.ServletContextHandler@46ee174c{/ws,null,STOPPED} (org.eclipse.jetty.server.handler.ContextHandler)
benchi-kafka     | [2025-03-18 16:07:55,246] INFO Stopped o.e.j.s.ServletContextHandler@1d59abc{/ws,null,STOPPED} (org.eclipse.jetty.server.handler.ContextHandler)
benchi-kafka     | [2025-03-18 16:07:55,249] INFO Stopped o.e.j.s.ServletContextHandler@73d44d87{/kafka,null,STOPPED} (org.eclipse.jetty.server.handler.ContextHandler)
benchi-kafka     | [2025-03-18 16:07:55,249] INFO Stopped o.e.j.s.ServletContextHandler@115ece1c{/v1/metadata,null,STOPPED} (org.eclipse.jetty.server.handler.ContextHandler)
benchi-kafka     | [2025-03-18 16:07:55,253] INFO Stopping TelemetryReporter collectorTask (io.confluent.telemetry.reporter.TelemetryReporter)
benchi-kafka     | [2025-03-18 16:07:55,254] INFO Closing the event logger (io.confluent.telemetry.reporter.TelemetryReporter)
benchi-kafka     | [2025-03-18 16:07:55,259] INFO App info kafka.producer for _confluent-license-producer-1 unregistered (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:07:55,260] INFO Stopping TelemetryReporter remoteConfigTask (io.confluent.telemetry.reporter.TelemetryReporter)
benchi-postgres  | 2025-03-18 16:07:55.273 UTC [64] LOG:  checkpoint complete: wrote 3701 buffers (22.6%); 0 WAL file(s) added, 0 removed, 4 recycled; write=0.013 s, sync=0.009 s, total=0.037 s; sync files=21, longest=0.008 s, average=0.001 s; distance=58930 kB, estimate=58930 kB; lsn=0/5365B90, redo lsn=0/5365B90
benchi-postgres  | 2025-03-18 16:07:55.276 UTC [1] LOG:  database system is shut down
[Kbenchi-postgres exited with code 0
benchi-kafka     | [2025-03-18 16:07:55,567] INFO App info kafka.consumer for _confluent-license-consumer-1 unregistered (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:07:55,567] INFO Stopped KafkaBasedLog for topic _confluent-command (org.apache.kafka.connect.util.KafkaBasedLog)
benchi-kafka     | [2025-03-18 16:07:55,567] INFO Closed License Store (io.confluent.license.LicenseStore)
benchi-kafka     | [2025-03-18 16:07:55,568] INFO [BrokerLifecycleManager id=1] Beginning controlled shutdown. (kafka.server.BrokerLifecycleManager)
benchi-kafka     | [2025-03-18 16:07:55,568] INFO [ControllerServer id=1] Unfenced broker 1 has requested and been granted a controlled shutdown. (org.apache.kafka.controller.BrokerHeartbeatManager)
benchi-kafka     | [2025-03-18 16:07:55,568] INFO [ControllerServer id=1] Replayed BrokerRegistrationChangeRecord modifying the registration for broker 1: BrokerRegistrationChangeRecord(brokerId=1, brokerEpoch=6, fenced=0, inControlledShutdown=1, degradedComponents=null, logDirs=[]) (org.apache.kafka.controller.ClusterControlManager)
benchi-kafka     | [2025-03-18 16:07:55,569] INFO [ControllerServer id=1] Marking broker 1 as shutting down in heartbeat manager at offset 404 (org.apache.kafka.controller.BrokerHeartbeatManager)
benchi-kafka     | [2025-03-18 16:07:55,571] INFO [ControllerServer id=1] checkLastBatchForBroker[1]: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:07:55,585] INFO [BrokerLifecycleManager id=1] The broker is in PENDING_CONTROLLED_SHUTDOWN state, still waiting for the active controller. (kafka.server.BrokerLifecycleManager)
benchi-kafka     | [2025-03-18 16:07:55,637] INFO [ControllerServer id=1] Broker 1 is in controlled shutdown state, but can not shut down because more leaders still need to be moved. (org.apache.kafka.controller.BrokerHeartbeatManager)
benchi-kafka     | [2025-03-18 16:07:55,638] INFO [BrokerLifecycleManager id=1] The broker is in PENDING_CONTROLLED_SHUTDOWN state, still waiting for the active controller. (kafka.server.BrokerLifecycleManager)
benchi-kafka     | [2025-03-18 16:07:55,675] INFO [ControllerServer id=1] partitionChangeInControlledShutdownForBroker[1]: changing 114 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:07:55,675] INFO [ControllerServer id=1] maybeScheduleNextPartitionChangeShuttingDownBrokers: generated 114 partition change records for broker 1. (org.apache.kafka.controller.QuorumController)
benchi-kafka     | [2025-03-18 16:07:55,706] INFO [Broker id=1] Transitioning 114 partition(s) to local followers. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,716] INFO [Broker id=1] Follower __consumer_offsets-23 starts at leader epoch 1 from offset 1 with partition epoch 1 and high watermark 1. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,716] INFO [Broker id=1] Follower _confluent-telemetry-metrics-11 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,716] INFO [Broker id=1] Follower __consumer_offsets-24 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,716] INFO [Broker id=1] Follower _confluent-link-metadata-27 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,716] INFO [Broker id=1] Follower _confluent-link-metadata-24 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,716] INFO [Broker id=1] Follower __consumer_offsets-20 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,716] INFO [Broker id=1] Follower __consumer_offsets-28 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,716] INFO [Broker id=1] Follower _confluent-link-metadata-23 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,716] INFO [Broker id=1] Follower _confluent-link-metadata-20 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,716] INFO [Broker id=1] Follower _confluent_balancer_api_state-0 starts at leader epoch 1 from offset 1 with partition epoch 1 and high watermark 1. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,717] INFO [Broker id=1] Follower _confluent-link-metadata-49 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,717] INFO [Broker id=1] Follower __consumer_offsets-30 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,717] INFO [Broker id=1] Follower _confluent-link-metadata-21 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,717] INFO [Broker id=1] Follower _confluent-link-metadata-16 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,717] INFO [Broker id=1] Follower _confluent-link-metadata-29 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,717] INFO [Broker id=1] Follower _confluent-link-metadata-46 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,717] INFO [Broker id=1] Follower _confluent-link-metadata-17 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,717] INFO [Broker id=1] Follower _confluent-telemetry-metrics-1 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,717] INFO [Broker id=1] Follower __consumer_offsets-19 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,717] INFO [Broker id=1] Follower __consumer_offsets-49 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,717] INFO [Broker id=1] Follower _confluent-link-metadata-19 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,717] INFO [Broker id=1] Follower _confluent-link-metadata-35 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,717] INFO [Broker id=1] Follower __consumer_offsets-3 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,717] INFO [Broker id=1] Follower _confluent-link-metadata-2 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,717] INFO [Broker id=1] Follower __consumer_offsets-26 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,716] INFO [Broker id=1] Follower _confluent-link-metadata-25 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,717] INFO [Broker id=1] Follower __consumer_offsets-5 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,717] INFO [Broker id=1] Follower _confluent-link-metadata-31 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,717] INFO [Broker id=1] Follower __consumer_offsets-36 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,717] INFO [Broker id=1] Follower __consumer_offsets-32 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,717] INFO [Broker id=1] Follower __consumer_offsets-1 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,717] INFO [Broker id=1] Follower __consumer_offsets-11 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,717] INFO [Broker id=1] Follower _confluent-link-metadata-37 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,717] INFO [Broker id=1] Follower __consumer_offsets-34 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,717] INFO [Broker id=1] Follower _confluent-link-metadata-33 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,717] INFO [Broker id=1] Follower _confluent-link-metadata-38 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,717] INFO [Broker id=1] Follower __consumer_offsets-44 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,717] INFO [Broker id=1] Follower _confluent-command-0 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,717] INFO [Broker id=1] Follower _confluent-telemetry-metrics-4 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,717] INFO [Broker id=1] Follower __consumer_offsets-18 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,717] INFO [Broker id=1] Follower __consumer_offsets-12 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,717] INFO [Broker id=1] Follower __consumer_offsets-41 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,717] INFO [Broker id=1] Follower __consumer_offsets-43 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,717] INFO [Broker id=1] Follower __consumer_offsets-31 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,717] INFO [Broker id=1] Follower __consumer_offsets-10 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,717] INFO [Broker id=1] Follower _confluent-telemetry-metrics-8 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,717] INFO [Broker id=1] Follower _confluent-link-metadata-32 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,717] INFO [Broker id=1] Follower _confluent-telemetry-metrics-3 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,718] INFO [Broker id=1] Follower _confluent-link-metadata-41 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,718] INFO [Broker id=1] Follower __consumer_offsets-39 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,718] INFO [Broker id=1] Follower _confluent-link-metadata-11 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,717] INFO [Broker id=1] Follower _confluent-telemetry-metrics-9 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,718] INFO [Broker id=1] Follower _confluent-link-metadata-8 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,718] INFO [Broker id=1] Follower __consumer_offsets-6 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,717] INFO [Broker id=1] Follower __consumer_offsets-16 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,718] INFO [Broker id=1] Follower __consumer_offsets-22 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,717] INFO [Broker id=1] Follower _confluent-link-metadata-45 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,718] INFO [Broker id=1] Follower _confluent-link-metadata-15 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,718] INFO [Broker id=1] Follower _confluent-telemetry-metrics-5 starts at leader epoch 1 from offset 132 with partition epoch 1 and high watermark 132. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,717] INFO [Broker id=1] Follower _confluent-link-metadata-47 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,718] INFO [Broker id=1] Follower _confluent-link-metadata-13 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,717] INFO [Broker id=1] Follower _confluent-link-metadata-0 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,718] INFO [Broker id=1] Follower __consumer_offsets-8 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,717] INFO [Broker id=1] Follower _confluent-link-metadata-4 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,718] INFO [Broker id=1] Follower _confluent-link-metadata-5 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,718] INFO [Broker id=1] Follower _confluent-link-metadata-30 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,718] INFO [Broker id=1] Follower __consumer_offsets-7 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,717] INFO [Broker id=1] Follower __consumer_offsets-38 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,718] INFO [Broker id=1] Follower __consumer_offsets-40 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,718] INFO [Broker id=1] Follower __consumer_offsets-33 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,719] INFO [Broker id=1] Follower _confluent-link-metadata-18 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,719] INFO [Broker id=1] Follower _confluent-link-metadata-22 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,718] INFO [Broker id=1] Follower _confluent-link-metadata-3 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,719] INFO [Broker id=1] Follower __consumer_offsets-21 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,719] INFO [Broker id=1] Follower __consumer_offsets-27 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,718] INFO [Broker id=1] Follower _confluent-link-metadata-14 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,719] INFO [Broker id=1] Follower _confluent-link-metadata-36 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,719] INFO [Broker id=1] Follower __consumer_offsets-17 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,719] INFO [Broker id=1] Follower __consumer_offsets-35 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,718] INFO [Broker id=1] Follower _confluent-link-metadata-12 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,719] INFO [Broker id=1] Follower _confluent-telemetry-metrics-2 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,719] INFO [Broker id=1] Follower __consumer_offsets-2 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,719] INFO [Broker id=1] Follower _confluent-link-metadata-34 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,719] INFO [Broker id=1] Follower __consumer_offsets-37 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,719] INFO [Broker id=1] Follower _confluent-telemetry-metrics-0 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,719] INFO [Broker id=1] Follower __consumer_offsets-4 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,719] INFO [Broker id=1] Follower _confluent-link-metadata-1 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,718] INFO [Broker id=1] Follower _confluent-link-metadata-48 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,719] INFO [Broker id=1] Follower _confluent-link-metadata-26 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,719] INFO [Broker id=1] Follower _confluent-link-metadata-43 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,719] INFO [Broker id=1] Follower __consumer_offsets-29 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,719] INFO [Broker id=1] Follower _confluent-link-metadata-10 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,719] INFO [Broker id=1] Follower _confluent-link-metadata-42 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,719] INFO [Broker id=1] Follower __consumer_offsets-45 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,719] INFO [Broker id=1] Follower _confluent-telemetry-metrics-6 starts at leader epoch 1 from offset 121 with partition epoch 1 and high watermark 121. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,718] INFO [Broker id=1] Follower _confluent-link-metadata-9 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,720] INFO [Broker id=1] Follower __consumer_offsets-0 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,718] INFO [Broker id=1] Follower __consumer_offsets-15 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,720] INFO [Broker id=1] Follower _confluent-telemetry-metrics-7 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,720] INFO [Broker id=1] Follower _confluent-link-metadata-7 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,718] INFO [Broker id=1] Follower _confluent-link-metadata-44 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,720] INFO [Broker id=1] Follower __consumer_offsets-47 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,720] INFO [Broker id=1] Follower _confluent-link-metadata-40 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,719] INFO [Broker id=1] Follower __consumer_offsets-13 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,719] INFO [Broker id=1] Follower _confluent-link-metadata-39 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,719] INFO [Broker id=1] Follower _confluent-link-metadata-28 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,719] INFO [Broker id=1] Follower _confluent-telemetry-metrics-10 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,720] INFO [Broker id=1] Follower _confluent-link-metadata-6 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,720] INFO [Broker id=1] Follower __consumer_offsets-46 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,719] INFO [Broker id=1] Follower __consumer_offsets-25 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,720] INFO [Broker id=1] Follower __consumer_offsets-9 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,720] INFO [Broker id=1] Follower __consumer_offsets-14 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,720] INFO [Broker id=1] Follower __consumer_offsets-42 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,720] INFO [Broker id=1] Follower __consumer_offsets-48 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,721] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions HashSet(__consumer_offsets-13, _confluent-link-metadata-14, _confluent-telemetry-metrics-9, __consumer_offsets-22, _confluent-link-metadata-8, _confluent-link-metadata-24, _confluent-link-metadata-16, __consumer_offsets-30, _confluent-telemetry-metrics-5, _confluent-link-metadata-45, __consumer_offsets-8, __consumer_offsets-21, __consumer_offsets-4, _confluent-link-metadata-3, _confluent-telemetry-metrics-10, _confluent-link-metadata-32, _confluent-link-metadata-2, _confluent-link-metadata-21, _confluent-link-metadata-39, __consumer_offsets-27, __consumer_offsets-7, _confluent-link-metadata-12, __consumer_offsets-9, __consumer_offsets-46, _confluent-link-metadata-20, _confluent-link-metadata-42, __consumer_offsets-25, __consumer_offsets-35, _confluent-link-metadata-28, __consumer_offsets-41, _confluent-link-metadata-6, __consumer_offsets-33, _confluent-link-metadata-35, __consumer_offsets-49, __consumer_offsets-23, _confluent-link-metadata-15, _confluent-link-metadata-22, _confluent-telemetry-metrics-4, __consumer_offsets-47, __consumer_offsets-16, _confluent-link-metadata-46, __consumer_offsets-28, _confluent-telemetry-metrics-7, __consumer_offsets-31, __consumer_offsets-36, _confluent-link-metadata-1, _confluent-link-metadata-27, __consumer_offsets-42, _confluent-link-metadata-19, _confluent-link-metadata-31, _confluent-link-metadata-23, _confluent-telemetry-metrics-11, __consumer_offsets-18, _confluent-link-metadata-11, __consumer_offsets-3, _confluent-link-metadata-49, __consumer_offsets-37, __consumer_offsets-15, __consumer_offsets-24, _confluent-link-metadata-7, _confluent-command-0, _confluent-link-metadata-41, _confluent-link-metadata-5, _confluent-link-metadata-29, _confluent-link-metadata-13, _confluent_balancer_api_state-0, _confluent-telemetry-metrics-8, _confluent-link-metadata-47, __consumer_offsets-38, __consumer_offsets-17, _confluent-link-metadata-40, __consumer_offsets-48, _confluent-link-metadata-48, _confluent-link-metadata-37, _confluent-telemetry-metrics-6, _confluent-link-metadata-0, __consumer_offsets-19, _confluent-telemetry-metrics-3, _confluent-link-metadata-18, __consumer_offsets-11, _confluent-link-metadata-26, _confluent-link-metadata-10, __consumer_offsets-2, __consumer_offsets-43, __consumer_offsets-6, __consumer_offsets-14, _confluent-link-metadata-36, _confluent-link-metadata-44, _confluent-link-metadata-34, _confluent-telemetry-metrics-0, _confluent-link-metadata-4, _confluent-link-metadata-9, __consumer_offsets-20, __consumer_offsets-0, __consumer_offsets-44, _confluent-link-metadata-38, _confluent-telemetry-metrics-2, __consumer_offsets-39, _confluent-link-metadata-30, __consumer_offsets-12, __consumer_offsets-45, _confluent-link-metadata-33, __consumer_offsets-1, __consumer_offsets-5, __consumer_offsets-26, _confluent-link-metadata-17, __consumer_offsets-29, _confluent-telemetry-metrics-1, __consumer_offsets-34, __consumer_offsets-10, __consumer_offsets-32, __consumer_offsets-40, _confluent-link-metadata-25, _confluent-link-metadata-43) (kafka.server.ReplicaFetcherManager)
benchi-kafka     | [2025-03-18 16:07:55,722] INFO [ReplicaAlterLogDirsManager on broker 1] Removed fetcher for partitions HashSet(__consumer_offsets-13, _confluent-link-metadata-14, _confluent-telemetry-metrics-9, __consumer_offsets-22, _confluent-link-metadata-8, _confluent-link-metadata-24, _confluent-link-metadata-16, __consumer_offsets-30, _confluent-telemetry-metrics-5, _confluent-link-metadata-45, __consumer_offsets-8, __consumer_offsets-21, __consumer_offsets-4, _confluent-link-metadata-3, _confluent-telemetry-metrics-10, _confluent-link-metadata-32, _confluent-link-metadata-2, _confluent-link-metadata-21, _confluent-link-metadata-39, __consumer_offsets-27, __consumer_offsets-7, _confluent-link-metadata-12, __consumer_offsets-9, __consumer_offsets-46, _confluent-link-metadata-20, _confluent-link-metadata-42, __consumer_offsets-25, __consumer_offsets-35, _confluent-link-metadata-28, __consumer_offsets-41, _confluent-link-metadata-6, __consumer_offsets-33, _confluent-link-metadata-35, __consumer_offsets-49, __consumer_offsets-23, _confluent-link-metadata-15, _confluent-link-metadata-22, _confluent-telemetry-metrics-4, __consumer_offsets-47, __consumer_offsets-16, _confluent-link-metadata-46, __consumer_offsets-28, _confluent-telemetry-metrics-7, __consumer_offsets-31, __consumer_offsets-36, _confluent-link-metadata-1, _confluent-link-metadata-27, __consumer_offsets-42, _confluent-link-metadata-19, _confluent-link-metadata-31, _confluent-link-metadata-23, _confluent-telemetry-metrics-11, __consumer_offsets-18, _confluent-link-metadata-11, __consumer_offsets-3, _confluent-link-metadata-49, __consumer_offsets-37, __consumer_offsets-15, __consumer_offsets-24, _confluent-link-metadata-7, _confluent-command-0, _confluent-link-metadata-41, _confluent-link-metadata-5, _confluent-link-metadata-29, _confluent-link-metadata-13, _confluent_balancer_api_state-0, _confluent-telemetry-metrics-8, _confluent-link-metadata-47, __consumer_offsets-38, __consumer_offsets-17, _confluent-link-metadata-40, __consumer_offsets-48, _confluent-link-metadata-48, _confluent-link-metadata-37, _confluent-telemetry-metrics-6, _confluent-link-metadata-0, __consumer_offsets-19, _confluent-telemetry-metrics-3, _confluent-link-metadata-18, __consumer_offsets-11, _confluent-link-metadata-26, _confluent-link-metadata-10, __consumer_offsets-2, __consumer_offsets-43, __consumer_offsets-6, __consumer_offsets-14, _confluent-link-metadata-36, _confluent-link-metadata-44, _confluent-link-metadata-34, _confluent-telemetry-metrics-0, _confluent-link-metadata-4, _confluent-link-metadata-9, __consumer_offsets-20, __consumer_offsets-0, __consumer_offsets-44, _confluent-link-metadata-38, _confluent-telemetry-metrics-2, __consumer_offsets-39, _confluent-link-metadata-30, __consumer_offsets-12, __consumer_offsets-45, _confluent-link-metadata-33, __consumer_offsets-1, __consumer_offsets-5, __consumer_offsets-26, _confluent-link-metadata-17, __consumer_offsets-29, _confluent-telemetry-metrics-1, __consumer_offsets-34, __consumer_offsets-10, __consumer_offsets-32, __consumer_offsets-40, _confluent-link-metadata-25, _confluent-link-metadata-43) (kafka.server.ReplicaAlterLogDirsManager)
benchi-kafka     | [2025-03-18 16:07:55,723] INFO [Broker id=1] Stopped fetchers as part of controlled shutdown for 114 partitions (state.change.logger)
benchi-kafka     | [2025-03-18 16:07:55,724] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:07:55,724] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,724] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:07:55,724] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,725] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:07:55,725] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,725] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:07:55,725] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,725] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:07:55,725] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,725] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:07:55,725] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,725] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:07:55,725] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,726] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,726] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,726] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,726] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,726] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:07:55,726] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,726] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,726] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,726] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,726] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:07:55,726] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,726] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,726] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:07:55,726] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,726] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,726] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:07:55,726] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,726] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,727] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:07:55,727] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,727] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,727] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:07:55,727] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,727] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,727] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:07:55,727] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,727] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,727] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:07:55,727] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,727] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,727] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:07:55,727] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,728] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,728] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:07:55,728] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,728] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,728] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:07:55,728] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,728] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,728] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:07:55,728] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,728] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,728] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:07:55,728] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,728] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,728] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:07:55,728] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,729] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,729] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:07:55,729] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,729] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,729] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:07:55,729] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,729] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,729] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:07:55,729] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,729] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,729] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:07:55,729] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,729] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,729] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:07:55,729] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,730] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,730] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:07:55,730] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,730] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,730] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:07:55,730] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,730] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,730] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:07:55,730] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,730] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,730] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:07:55,730] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,730] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,731] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:07:55,731] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,731] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,731] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:07:55,731] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,731] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:07:55,731] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,731] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:07:55,731] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,731] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:07:55,731] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,731] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:07:55,731] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,731] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:07:55,731] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,731] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:07:55,731] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,731] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:07:55,731] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,731] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:07:55,731] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,731] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:07:55,731] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,731] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:07:55,731] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,731] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:07:55,731] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,731] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:07:55,731] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,731] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:07:55,731] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,731] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:07:55,731] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,731] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:07:55,731] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,731] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:07:55,731] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,731] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:07:55,731] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,731] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:07:55,731] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,731] INFO [GroupCoordinator 1]: Unloading group metadata for ConfluentTelemetryReporterSampler--2946986101702943459 with generation 1 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:07:55,732] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 1 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,732] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,732] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,732] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,732] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,732] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,732] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,732] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,732] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,732] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,732] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,733] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,733] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,733] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,733] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,733] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,733] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,733] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,733] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:55,733] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:07:57,639] INFO [ControllerServer id=1] The request from broker 1 to shut down has been granted since the lowest active offset 9223372036854775807 is now greater than the broker's controlled shutdown offset 518. (org.apache.kafka.controller.BrokerHeartbeatManager)
benchi-kafka     | [2025-03-18 16:07:57,640] INFO [ControllerServer id=1] Replayed BrokerRegistrationChangeRecord modifying the registration for broker 1: BrokerRegistrationChangeRecord(brokerId=1, brokerEpoch=6, fenced=1, inControlledShutdown=0, degradedComponents=null, logDirs=[]) (org.apache.kafka.controller.ClusterControlManager)
benchi-kafka     | [2025-03-18 16:07:57,666] INFO SBC Event SbcMetadataUpdateEvent-282 generated 1 more events to enqueue in the following order - [SbcBrokerFailureEvent-283]. Enqueuing... (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:07:57,666] INFO Handling event SbcBrokerFailureEvent-283 (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:07:57,666] INFO Notify broker failure: [1] (io.confluent.databalancer.KafkaDataBalanceManager)
benchi-kafka     | [2025-03-18 16:07:57,666] INFO [BrokerLifecycleManager id=1] The controller has asked us to exit controlled shutdown. (kafka.server.BrokerLifecycleManager)
benchi-kafka     | [2025-03-18 16:07:57,666] INFO [BrokerLifecycleManager id=1] beginShutdown: shutting down event queue. (org.apache.kafka.queue.KafkaEventQueue)
benchi-kafka     | [2025-03-18 16:07:57,667] INFO Notify DEAD_BROKER event for brokers: [1] (io.confluent.databalancer.ConfluentDataBalanceEngine)
benchi-kafka     | [2025-03-18 16:07:57,667] INFO [BrokerLifecycleManager id=1] Transitioning from PENDING_CONTROLLED_SHUTDOWN to SHUTTING_DOWN. (kafka.server.BrokerLifecycleManager)
benchi-kafka     | [2025-03-18 16:07:57,667] INFO [broker-1-to-controller-heartbeat-channel-manager]: Shutting down (kafka.server.NodeToControllerRequestThread)
benchi-kafka     | [2025-03-18 16:07:57,667] INFO [broker-1-to-controller-heartbeat-channel-manager]: Stopped (kafka.server.NodeToControllerRequestThread)
benchi-kafka     | [2025-03-18 16:07:57,667] INFO [broker-1-to-controller-heartbeat-channel-manager]: Shutdown completed (kafka.server.NodeToControllerRequestThread)
benchi-kafka     | [2025-03-18 16:07:57,667] INFO Databalancer: Working on tracking DEAD_BROKER event for brokers ([1]) (io.confluent.databalancer.ConfluentDataBalanceEngine)
benchi-kafka     | [2025-03-18 16:07:57,667] INFO Notify broker removal: [1] (com.linkedin.kafka.cruisecontrol.detector.BrokerFailureDetector)
benchi-kafka     | [2025-03-18 16:07:57,667] INFO [BrokerServer id=1] Controlled shutdown initiation completed successfully in 2.1 seconds (kafka.server.BrokerServer)
benchi-kafka     | [2025-03-18 16:07:57,668] INFO Scheduled check for broker failure detection triggered (com.linkedin.kafka.cruisecontrol.detector.BrokerFailureDetector)
benchi-kafka     | [2025-03-18 16:07:57,668] INFO GoalViolationDetector's view on DEAD brokers are updated from [] to [1]. (com.linkedin.kafka.cruisecontrol.detector.GoalViolationDetector)
benchi-kafka     | [2025-03-18 16:07:57,668] INFO Broker change event(s) detected: [Dead brokers: [1]] (com.linkedin.kafka.cruisecontrol.detector.BrokerFailureDetector)
benchi-kafka     | [2025-03-18 16:07:57,668] INFO KafkaHttpServer transitioned from RUNNING to STOPPING.. (io.confluent.http.server.KafkaHttpServerImpl)
benchi-kafka     | [2025-03-18 16:07:57,668] INFO KafkaHttpServer transitioned from STOPPING to TERMINATED.. (io.confluent.http.server.KafkaHttpServerImpl)
benchi-kafka     | [2025-03-18 16:07:57,668] INFO Node to controller channel manager for heartbeat shutdown (kafka.server.NodeToControllerChannelManagerImpl)
benchi-kafka     | [2025-03-18 16:07:57,668] INFO Broker Load Metric closed. (kafka.metrics.BrokerLoad)
benchi-kafka     | [2025-03-18 16:07:57,669] INFO [SocketServer listenerType=BROKER, nodeId=1] Stopping socket server request processors (kafka.network.SocketServer)
benchi-kafka     | [2025-03-18 16:07:57,670] INFO [Producer clientId=confluent-telemetry-reporter-local-producer] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:07:57,670] INFO [Consumer clientId=_confluent_balancer_api_state-consumer-1, groupId=null] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:07:57,670] INFO [Producer clientId=_confluent_balancer_api_state-producer-1] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:07:57,671] INFO [AdminClient clientId=adminclient-5] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:07:57,671] INFO [AdminClient clientId=cluster-link--local-admin-1] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:07:57,671] INFO [Producer clientId=confluent-metrics-reporter] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:07:57,671] INFO [AdminClient clientId=null-admin-1] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:07:57,673] INFO [AdminClient clientId=cluster-link--local-admin-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:07:57,673] INFO [Consumer clientId=_confluent_balancer_api_state-consumer-1, groupId=null] Cancelled in-flight FETCH request with correlation id 10947 due to node 1 being disconnected (elapsed time since creation: 1ms, elapsed time since send: 1ms, throttle time: 0ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:07:57,673] INFO [Consumer clientId=_confluent_balancer_api_state-consumer-1, groupId=null] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:07:57,673] INFO [AdminClient clientId=adminclient-5] Cancelled in-flight METADATA request with correlation id 28 due to node 1 being disconnected (elapsed time since creation: 2ms, elapsed time since send: 2ms, throttle time: 0ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:07:57,673] INFO [Producer clientId=confluent-metrics-reporter] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:07:57,673] INFO [AdminClient clientId=adminclient-5] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:07:57,673] INFO [Producer clientId=_confluent_balancer_api_state-producer-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:07:57,673] INFO [Producer clientId=confluent-telemetry-reporter-local-producer] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:07:57,673] INFO [AdminClient clientId=null-admin-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:07:57,673] INFO [Consumer clientId=_confluent_balancer_api_state-consumer-1, groupId=null] Error sending fetch request (sessionId=392188395, epoch=10934) to node 1: (org.apache.kafka.clients.FetchSessionHandler)
benchi-kafka     | org.apache.kafka.common.errors.DisconnectException
benchi-kafka     | [2025-03-18 16:07:57,676] INFO [SocketServer listenerType=BROKER, nodeId=1] Stopped socket server request processors (kafka.network.SocketServer)
benchi-kafka     | [2025-03-18 16:07:57,677] INFO [data-plane Kafka Request Handler on Broker 1], shutting down (kafka.server.KafkaRequestHandlerPool)
benchi-kafka     | [2025-03-18 16:07:57,678] INFO [data-plane Kafka Request Handler on Broker 1], shut down completely (kafka.server.KafkaRequestHandlerPool)
benchi-kafka     | [2025-03-18 16:07:57,678] INFO [ExpirationReaper-1-AlterAcls]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:07:57,678] INFO [ExpirationReaper-1-AlterAcls]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:07:57,678] INFO [ExpirationReaper-1-AlterAcls]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:07:57,678] INFO [KafkaApi-1] Shutdown complete. (kafka.server.KafkaApis)
benchi-kafka     | [2025-03-18 16:07:57,680] INFO [TransactionCoordinator id=1] Shutting down. (kafka.coordinator.transaction.TransactionCoordinator)
benchi-kafka     | [2025-03-18 16:07:57,680] INFO [Transaction State Manager 1]: Shutdown complete (kafka.coordinator.transaction.TransactionStateManager)
benchi-kafka     | [2025-03-18 16:07:57,680] INFO [TxnMarkerSenderThread-1]: Shutting down (kafka.coordinator.transaction.TransactionMarkerChannelManager)
benchi-kafka     | [2025-03-18 16:07:57,680] INFO [TxnMarkerSenderThread-1]: Stopped (kafka.coordinator.transaction.TransactionMarkerChannelManager)
benchi-kafka     | [2025-03-18 16:07:57,680] INFO [TxnMarkerSenderThread-1]: Shutdown completed (kafka.coordinator.transaction.TransactionMarkerChannelManager)
benchi-kafka     | [2025-03-18 16:07:57,681] INFO [TransactionCoordinator id=1] Shutdown complete. (kafka.coordinator.transaction.TransactionCoordinator)
benchi-kafka     | [2025-03-18 16:07:57,681] INFO [GroupCoordinator 1]: Shutting down. (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:07:57,681] INFO [ExpirationReaper-1-Heartbeat]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:07:57,681] INFO [ExpirationReaper-1-Heartbeat]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:07:57,681] INFO [ExpirationReaper-1-Heartbeat]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:07:57,681] INFO [ExpirationReaper-1-Rebalance]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:07:57,681] INFO [ExpirationReaper-1-Rebalance]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:07:57,681] INFO [ExpirationReaper-1-Rebalance]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:07:57,682] INFO [GroupCoordinator 1]: Shutdown complete. (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:07:57,682] INFO [AssignmentsManager id=1]KafkaEventQueue#close: shutting down event queue. (org.apache.kafka.queue.KafkaEventQueue)
benchi-kafka     | [2025-03-18 16:07:57,682] INFO [broker-1-to-controller-directory-assignments-channel-manager]: Shutting down (kafka.server.NodeToControllerRequestThread)
benchi-kafka     | [2025-03-18 16:07:57,682] INFO [broker-1-to-controller-directory-assignments-channel-manager]: Stopped (kafka.server.NodeToControllerRequestThread)
benchi-kafka     | [2025-03-18 16:07:57,682] INFO [broker-1-to-controller-directory-assignments-channel-manager]: Shutdown completed (kafka.server.NodeToControllerRequestThread)
benchi-kafka     | [2025-03-18 16:07:57,683] INFO Node to controller channel manager for directory-assignments shutdown (kafka.server.NodeToControllerChannelManagerImpl)
benchi-kafka     | [2025-03-18 16:07:57,683] INFO [AssignmentsManager id=1]closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
benchi-kafka     | [2025-03-18 16:07:57,684] INFO [ReplicaManager broker=1] Shutting down (kafka.server.ReplicaManager)
benchi-kafka     | [2025-03-18 16:07:57,685] INFO [LogDirFailureHandler]: Shutting down (kafka.server.ReplicaManager$LogDirFailureHandler)
benchi-kafka     | [2025-03-18 16:07:57,685] INFO [LogDirFailureHandler]: Stopped (kafka.server.ReplicaManager$LogDirFailureHandler)
benchi-kafka     | [2025-03-18 16:07:57,685] INFO [LogDirFailureHandler]: Shutdown completed (kafka.server.ReplicaManager$LogDirFailureHandler)
benchi-kafka     | [2025-03-18 16:07:57,685] INFO [ReplicaFetcherManager on broker 1] shutting down (kafka.server.ReplicaFetcherManager)
benchi-kafka     | [2025-03-18 16:07:57,685] INFO [ReplicaFetcherManager on broker 1] shutdown completed (kafka.server.ReplicaFetcherManager)
benchi-kafka     | [2025-03-18 16:07:57,685] INFO [ReplicaAlterLogDirsManager on broker 1] shutting down (kafka.server.ReplicaAlterLogDirsManager)
benchi-kafka     | [2025-03-18 16:07:57,686] INFO [ReplicaAlterLogDirsManager on broker 1] shutdown completed (kafka.server.ReplicaAlterLogDirsManager)
benchi-kafka     | [2025-03-18 16:07:57,686] INFO [ExpirationReaper-1-Fetch]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:07:57,686] INFO [ExpirationReaper-1-Fetch]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:07:57,686] INFO [ExpirationReaper-1-Fetch]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:07:57,686] INFO [ExpirationReaper-1-Produce]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:07:57,686] INFO [ExpirationReaper-1-Produce]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:07:57,686] INFO [ExpirationReaper-1-Produce]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:07:57,686] INFO [ExpirationReaper-1-DeleteRecords]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:07:57,686] INFO [ExpirationReaper-1-DeleteRecords]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:07:57,686] INFO [ExpirationReaper-1-DeleteRecords]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:07:57,686] INFO [ExpirationReaper-1-ElectLeader]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:07:57,687] INFO [ExpirationReaper-1-ElectLeader]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:07:57,687] INFO [ExpirationReaper-1-ElectLeader]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:07:57,687] INFO [ExpirationReaper-1-ListOffsets]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:07:57,687] INFO [ExpirationReaper-1-ListOffsets]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:07:57,687] INFO [ExpirationReaper-1-ListOffsets]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:07:57,690] INFO [AddPartitionsToTxnSenderThread-1]: Shutting down (kafka.server.AddPartitionsToTxnManager)
benchi-kafka     | [2025-03-18 16:07:57,690] INFO [AddPartitionsToTxnSenderThread-1]: Stopped (kafka.server.AddPartitionsToTxnManager)
benchi-kafka     | [2025-03-18 16:07:57,690] INFO [AddPartitionsToTxnSenderThread-1]: Shutdown completed (kafka.server.AddPartitionsToTxnManager)
benchi-kafka     | [2025-03-18 16:07:57,691] INFO [ReplicaManager broker=1] Shut down completely (kafka.server.ReplicaManager)
benchi-kafka     | [2025-03-18 16:07:57,691] INFO [ClusterLinkManager-broker-1] Shutting down (kafka.server.link.ClusterLinkManager)
benchi-kafka     | [2025-03-18 16:07:57,693] INFO App info kafka.admin.client for cluster-link--local-admin-1 unregistered (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:07:57,693] INFO [AdminClient clientId=cluster-link--local-admin-1] Metadata update failed (org.apache.kafka.clients.admin.internals.AdminMetadataManager)
benchi-kafka     | org.apache.kafka.common.errors.TimeoutException: The AdminClient thread has exited. Call: fetchMetadata
benchi-kafka     | [2025-03-18 16:07:57,693] INFO [AdminClient clientId=cluster-link--local-admin-1] Timed out 1 remaining operation(s) during close. (org.apache.kafka.clients.admin.KafkaAdminClient)
benchi-kafka     | [2025-03-18 16:07:57,693] INFO [ExpirationReaper-1-ClusterLink]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:07:57,694] INFO [ExpirationReaper-1-ClusterLink]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:07:57,694] INFO [ExpirationReaper-1-ClusterLink]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:07:57,695] INFO [ClusterLinkManager-broker-1] Shutdown completed (kafka.server.link.ClusterLinkManager)
benchi-kafka     | [2025-03-18 16:07:57,696] INFO [broker-1-to-controller-alter-partition-channel-manager]: Shutting down (kafka.server.NodeToControllerRequestThread)
benchi-kafka     | [2025-03-18 16:07:57,696] INFO [broker-1-to-controller-alter-partition-channel-manager]: Stopped (kafka.server.NodeToControllerRequestThread)
benchi-kafka     | [2025-03-18 16:07:57,696] INFO [broker-1-to-controller-alter-partition-channel-manager]: Shutdown completed (kafka.server.NodeToControllerRequestThread)
benchi-kafka     | [2025-03-18 16:07:57,696] INFO Node to controller channel manager for alter-partition shutdown (kafka.server.NodeToControllerChannelManagerImpl)
benchi-kafka     | [2025-03-18 16:07:57,696] INFO [broker-1-to-controller-forwarding-channel-manager]: Shutting down (kafka.server.NodeToControllerRequestThread)
benchi-kafka     | [2025-03-18 16:07:57,696] INFO [broker-1-to-controller-forwarding-channel-manager]: Stopped (kafka.server.NodeToControllerRequestThread)
benchi-kafka     | [2025-03-18 16:07:57,696] INFO [broker-1-to-controller-forwarding-channel-manager]: Shutdown completed (kafka.server.NodeToControllerRequestThread)
benchi-kafka     | [2025-03-18 16:07:57,697] INFO Node to controller channel manager for forwarding shutdown (kafka.server.NodeToControllerChannelManagerImpl)
benchi-kafka     | [2025-03-18 16:07:57,697] INFO Shutting down. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,697] INFO Shutting down the log cleaner. (kafka.log.LogCleaner)
benchi-kafka     | [2025-03-18 16:07:57,697] INFO [kafka-log-cleaner-thread-0]: Shutting down (kafka.log.LogCleaner$CleanerThread)
benchi-kafka     | [2025-03-18 16:07:57,697] INFO [kafka-log-cleaner-thread-0]: Stopped (kafka.log.LogCleaner$CleanerThread)
benchi-kafka     | [2025-03-18 16:07:57,697] INFO [kafka-log-cleaner-thread-0]: Shutdown completed (kafka.log.LogCleaner$CleanerThread)
benchi-kafka     | [2025-03-18 16:07:57,702] INFO Tier partition state for TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-43 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,703] INFO Closing log for __consumer_offsets-43 took 4 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,704] INFO Tier partition state for FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-0 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,704] INFO Closing log for _confluent-link-metadata-0 took 2 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,705] INFO Tier partition state for FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-10 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,706] INFO Closing log for _confluent-link-metadata-10 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,708] INFO Tier partition state for TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-30 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,708] INFO Closing log for __consumer_offsets-30 took 2 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,711] INFO Tier partition state for TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-32 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,711] INFO Closing log for __consumer_offsets-32 took 3 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,714] INFO Tier partition state for FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-15 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,714] INFO Closing log for _confluent-link-metadata-15 took 2 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,716] INFO Tier partition state for TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-25 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,716] INFO Closing log for __consumer_offsets-25 took 2 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,717] INFO Tier partition state for FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-7 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,717] INFO Closing log for _confluent-link-metadata-7 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,719] INFO Tier partition state for TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-41 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,719] INFO Closing log for __consumer_offsets-41 took 2 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,720] INFO Tier partition state for ED-u4HVARJua1x3-BA6aMQ:_confluent-telemetry-metrics-6 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,721] INFO [ProducerStateManager partition=_confluent-telemetry-metrics-6] Wrote producer snapshot at offset 121 with 0 producer ids in 0 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
benchi-kafka     | [2025-03-18 16:07:57,721] INFO Closing log for _confluent-telemetry-metrics-6 took 3 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,724] INFO Tier partition state for ED-u4HVARJua1x3-BA6aMQ:_confluent-telemetry-metrics-4 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,724] INFO Closing log for _confluent-telemetry-metrics-4 took 2 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,726] INFO Tier partition state for _WH1I7T7TFuBhdihyIdWew:_confluent-command-0 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,726] INFO Closing log for _confluent-command-0 took 2 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,727] INFO Tier partition state for FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-28 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,728] INFO Closing log for _confluent-link-metadata-28 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,729] INFO Tier partition state for TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-49 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,729] INFO Closing log for __consumer_offsets-49 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,730] INFO Tier partition state for ED-u4HVARJua1x3-BA6aMQ:_confluent-telemetry-metrics-2 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,731] INFO Closing log for _confluent-telemetry-metrics-2 took 2 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,734] INFO Tier partition state for FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-47 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,734] INFO Closing log for _confluent-link-metadata-47 took 3 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,735] INFO Tier partition state for TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-40 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,735] INFO Closing log for __consumer_offsets-40 took 2 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,736] INFO Tier partition state for FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-30 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,737] INFO Closing log for _confluent-link-metadata-30 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,738] INFO Tier partition state for FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-27 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,738] INFO Closing log for _confluent-link-metadata-27 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,739] INFO Tier partition state for TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-27 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,739] INFO Closing log for __consumer_offsets-27 took 2 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,742] INFO Tier partition state for FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-5 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,742] INFO Closing log for _confluent-link-metadata-5 took 2 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,743] INFO Tier partition state for FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-24 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,743] INFO Closing log for _confluent-link-metadata-24 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,744] INFO Tier partition state for TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-12 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,744] INFO Closing log for __consumer_offsets-12 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,746] INFO Tier partition state for TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-36 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,746] INFO Closing log for __consumer_offsets-36 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,747] INFO Tier partition state for TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-45 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,747] INFO Closing log for __consumer_offsets-45 took 2 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,749] INFO Tier partition state for TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-10 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,749] INFO Closing log for __consumer_offsets-10 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,751] INFO Tier partition state for FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-36 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,751] INFO Closing log for _confluent-link-metadata-36 took 3 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,753] INFO Tier partition state for TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-11 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,753] INFO Closing log for __consumer_offsets-11 took 2 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,754] INFO Tier partition state for FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-4 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,754] INFO Closing log for _confluent-link-metadata-4 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,755] INFO Tier partition state for FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-40 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,756] INFO Closing log for _confluent-link-metadata-40 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,757] INFO Tier partition state for TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-20 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,757] INFO Closing log for __consumer_offsets-20 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,758] INFO Tier partition state for TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-18 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,758] INFO Closing log for __consumer_offsets-18 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,761] INFO Tier partition state for FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-42 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,761] INFO Closing log for _confluent-link-metadata-42 took 3 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,762] INFO Tier partition state for FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-31 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,762] INFO Closing log for _confluent-link-metadata-31 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,763] INFO Tier partition state for TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-48 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,763] INFO Closing log for __consumer_offsets-48 took 2 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,765] INFO Tier partition state for FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-44 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,765] INFO Closing log for _confluent-link-metadata-44 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,766] INFO Tier partition state for FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-43 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,766] INFO Closing log for _confluent-link-metadata-43 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,767] INFO [Producer clientId=_confluent_balancer_api_state-producer-1] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:07:57,767] WARN [Producer clientId=_confluent_balancer_api_state-producer-1] Connection to node 1 (broker/172.20.0.2:29092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:07:57,768] INFO Tier partition state for FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-49 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,769] INFO Closing log for _confluent-link-metadata-49 took 3 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,769] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:07:57,770] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:07:57,770] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Node 2147483646 disconnected. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:07:57,770] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Group coordinator broker:29092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: true. Rediscovery will be attempted. (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
benchi-kafka     | [2025-03-18 16:07:57,770] INFO Tier partition state for FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-21 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,770] INFO Closing log for _confluent-link-metadata-21 took 2 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,771] INFO Tier partition state for FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-41 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,772] INFO Closing log for _confluent-link-metadata-41 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,773] INFO Tier partition state for 312Wt30jSxCsJ4PMdsfwvw:_confluent_balancer_api_state-0 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,773] INFO [ProducerStateManager partition=_confluent_balancer_api_state-0] Wrote producer snapshot at offset 1 with 0 producer ids in 0 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
benchi-kafka     | [2025-03-18 16:07:57,774] INFO Closing log for _confluent_balancer_api_state-0 took 3 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,775] INFO [Consumer clientId=_confluent_balancer_api_state-consumer-1, groupId=null] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:07:57,775] WARN [Consumer clientId=_confluent_balancer_api_state-consumer-1, groupId=null] Connection to node 1 (broker/172.20.0.2:29092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:07:57,775] INFO [Consumer clientId=_confluent_balancer_api_state-consumer-1, groupId=null] Error sending fetch request (sessionId=392188395, epoch=INITIAL) to node 1: (org.apache.kafka.clients.FetchSessionHandler)
benchi-kafka     | org.apache.kafka.common.errors.DisconnectException
benchi-kafka     | [2025-03-18 16:07:57,776] INFO Tier partition state for TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-23 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,776] INFO [ProducerStateManager partition=__consumer_offsets-23] Wrote producer snapshot at offset 1 with 0 producer ids in 0 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
benchi-kafka     | [2025-03-18 16:07:57,776] INFO Closing log for __consumer_offsets-23 took 2 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,778] INFO Tier partition state for FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-1 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,778] INFO Closing log for _confluent-link-metadata-1 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,779] INFO Tier partition state for FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-11 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,779] INFO Closing log for _confluent-link-metadata-11 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,781] INFO Tier partition state for TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-42 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,781] INFO Closing log for __consumer_offsets-42 took 3 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,783] INFO Tier partition state for TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-28 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,783] INFO Closing log for __consumer_offsets-28 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,784] INFO Tier partition state for FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-17 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,784] INFO Closing log for _confluent-link-metadata-17 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,785] INFO Tier partition state for TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-4 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,785] INFO Closing log for __consumer_offsets-4 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,787] INFO Tier partition state for FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-18 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,787] INFO Closing log for _confluent-link-metadata-18 took 3 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,788] INFO Tier partition state for TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-1 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,788] INFO Closing log for __consumer_offsets-1 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,790] INFO Tier partition state for TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-38 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,790] INFO Closing log for __consumer_offsets-38 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,791] INFO Tier partition state for ED-u4HVARJua1x3-BA6aMQ:_confluent-telemetry-metrics-9 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,791] INFO Closing log for _confluent-telemetry-metrics-9 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,792] INFO Tier partition state for TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-14 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,792] INFO Closing log for __consumer_offsets-14 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,794] INFO Tier partition state for FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-16 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,794] INFO Closing log for _confluent-link-metadata-16 took 3 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,796] INFO Tier partition state for FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-8 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,796] INFO Closing log for _confluent-link-metadata-8 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,797] INFO Tier partition state for FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-46 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,797] INFO Closing log for _confluent-link-metadata-46 took 2 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,798] INFO Tier partition state for TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-29 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,799] INFO Closing log for __consumer_offsets-29 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,801] INFO Tier partition state for TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-6 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,801] INFO Closing log for __consumer_offsets-6 took 2 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,802] INFO Tier partition state for TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-0 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,802] INFO Closing log for __consumer_offsets-0 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,803] INFO Tier partition state for TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-35 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,803] INFO Closing log for __consumer_offsets-35 took 2 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,805] INFO Tier partition state for TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-13 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,805] INFO Closing log for __consumer_offsets-13 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,807] INFO Tier partition state for TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-26 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,807] INFO Closing log for __consumer_offsets-26 took 2 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,808] INFO Tier partition state for TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-21 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,808] INFO Closing log for __consumer_offsets-21 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,809] INFO Tier partition state for TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-19 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,809] INFO Closing log for __consumer_offsets-19 took 2 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,810] INFO Tier partition state for FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-6 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,810] INFO Closing log for _confluent-link-metadata-6 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,812] INFO Tier partition state for TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-33 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,812] INFO Closing log for __consumer_offsets-33 took 2 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,814] INFO Tier partition state for FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-25 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,814] INFO Closing log for _confluent-link-metadata-25 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,816] INFO Tier partition state for FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-2 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,816] INFO Closing log for _confluent-link-metadata-2 took 2 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,817] INFO Tier partition state for TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-37 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,817] INFO Closing log for __consumer_offsets-37 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,818] INFO Tier partition state for TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-8 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,818] INFO Closing log for __consumer_offsets-8 took 2 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,820] INFO Tier partition state for TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-24 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,820] INFO Closing log for __consumer_offsets-24 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,821] INFO Tier partition state for ED-u4HVARJua1x3-BA6aMQ:_confluent-telemetry-metrics-11 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,821] INFO Closing log for _confluent-telemetry-metrics-11 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,823] INFO Tier partition state for FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-48 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,823] INFO Closing log for _confluent-link-metadata-48 took 3 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,824] INFO Tier partition state for FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-45 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,824] INFO Closing log for _confluent-link-metadata-45 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,826] INFO Tier partition state for ED-u4HVARJua1x3-BA6aMQ:_confluent-telemetry-metrics-7 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,826] INFO Closing log for _confluent-telemetry-metrics-7 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,827] INFO Tier partition state for FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-29 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,827] INFO Closing log for _confluent-link-metadata-29 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,829] INFO Tier partition state for TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-3 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,829] INFO Closing log for __consumer_offsets-3 took 3 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,831] INFO Tier partition state for FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-20 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,831] INFO Closing log for _confluent-link-metadata-20 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,832] INFO Tier partition state for TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-17 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,832] INFO Closing log for __consumer_offsets-17 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,834] INFO Tier partition state for TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-39 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,834] INFO Closing log for __consumer_offsets-39 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,836] INFO Tier partition state for TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-2 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,836] INFO Closing log for __consumer_offsets-2 took 3 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,837] INFO Tier partition state for FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-37 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,837] INFO Closing log for _confluent-link-metadata-37 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,839] INFO Tier partition state for TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-44 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,839] INFO Closing log for __consumer_offsets-44 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,840] INFO Tier partition state for FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-12 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,840] INFO Closing log for _confluent-link-metadata-12 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,842] INFO Tier partition state for FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-13 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,842] INFO Closing log for _confluent-link-metadata-13 took 3 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,843] INFO Tier partition state for TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-16 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,844] INFO Closing log for __consumer_offsets-16 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,845] INFO Tier partition state for FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-14 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,845] INFO Closing log for _confluent-link-metadata-14 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,846] INFO Tier partition state for ED-u4HVARJua1x3-BA6aMQ:_confluent-telemetry-metrics-3 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,846] INFO Closing log for _confluent-telemetry-metrics-3 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,847] INFO Tier partition state for ED-u4HVARJua1x3-BA6aMQ:_confluent-telemetry-metrics-10 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,847] INFO Closing log for _confluent-telemetry-metrics-10 took 2 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,848] INFO Tier partition state for ED-u4HVARJua1x3-BA6aMQ:_confluent-telemetry-metrics-1 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,848] INFO Closing log for _confluent-telemetry-metrics-1 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,851] INFO Tier partition state for FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-22 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,851] INFO Closing log for _confluent-link-metadata-22 took 2 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,852] INFO Tier partition state for TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-47 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,852] INFO Closing log for __consumer_offsets-47 took 2 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,853] INFO Tier partition state for FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-3 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,853] INFO Closing log for _confluent-link-metadata-3 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,854] INFO Tier partition state for FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-26 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,854] INFO Closing log for _confluent-link-metadata-26 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,857] INFO Tier partition state for TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-7 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,857] INFO Closing log for __consumer_offsets-7 took 2 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,859] INFO Tier partition state for FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-35 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,859] INFO Closing log for _confluent-link-metadata-35 took 2 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,860] INFO Tier partition state for FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-38 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,860] INFO Closing log for _confluent-link-metadata-38 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,862] INFO Tier partition state for FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-39 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,862] INFO Closing log for _confluent-link-metadata-39 took 3 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,864] INFO Tier partition state for FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-34 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,864] INFO Closing log for _confluent-link-metadata-34 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,865] INFO Tier partition state for TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-22 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,865] INFO Closing log for __consumer_offsets-22 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,866] INFO Tier partition state for FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-32 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,866] INFO Closing log for _confluent-link-metadata-32 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,867] INFO Tier partition state for FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-19 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,867] INFO Closing log for _confluent-link-metadata-19 took 2 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,869] INFO Tier partition state for ED-u4HVARJua1x3-BA6aMQ:_confluent-telemetry-metrics-8 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,869] INFO Closing log for _confluent-telemetry-metrics-8 took 2 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,870] INFO Tier partition state for TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-46 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,871] INFO Closing log for __consumer_offsets-46 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,872] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:07:57,872] WARN [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Connection to node 1 (broker/172.20.0.2:29092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:07:57,872] INFO Tier partition state for ED-u4HVARJua1x3-BA6aMQ:_confluent-telemetry-metrics-0 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,872] INFO Closing log for _confluent-telemetry-metrics-0 took 2 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,875] INFO Tier partition state for FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-23 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,875] INFO Closing log for _confluent-link-metadata-23 took 2 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,876] INFO [Consumer clientId=_confluent_balancer_api_state-consumer-1, groupId=null] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:07:57,876] WARN [Consumer clientId=_confluent_balancer_api_state-consumer-1, groupId=null] Connection to node 1 (broker/172.20.0.2:29092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:07:57,876] INFO [Consumer clientId=_confluent_balancer_api_state-consumer-1, groupId=null] Error sending fetch request (sessionId=392188395, epoch=INITIAL) to node 1: (org.apache.kafka.clients.FetchSessionHandler)
benchi-kafka     | org.apache.kafka.common.errors.DisconnectException
benchi-kafka     | [2025-03-18 16:07:57,876] INFO Tier partition state for FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-9 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,876] INFO Closing log for _confluent-link-metadata-9 took 2 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,878] INFO Tier partition state for TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-31 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,878] INFO Closing log for __consumer_offsets-31 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,879] INFO Tier partition state for FgiypwCwTt68SpYw3T9ahw:_confluent-link-metadata-33 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,879] INFO Closing log for _confluent-link-metadata-33 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,882] INFO Tier partition state for TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-5 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,883] INFO Closing log for __consumer_offsets-5 took 4 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,884] INFO Tier partition state for ED-u4HVARJua1x3-BA6aMQ:_confluent-telemetry-metrics-5 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,884] INFO [ProducerStateManager partition=_confluent-telemetry-metrics-5] Wrote producer snapshot at offset 132 with 0 producer ids in 0 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
benchi-kafka     | [2025-03-18 16:07:57,885] INFO Closing log for _confluent-telemetry-metrics-5 took 2 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,886] INFO Tier partition state for TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-15 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,886] INFO Closing log for __consumer_offsets-15 took 2 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,887] INFO Tier partition state for TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-34 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,888] INFO Closing log for __consumer_offsets-34 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,889] INFO Tier partition state for TDrV9QLDQhGxmWrWM4O8uw:__consumer_offsets-9 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,889] INFO Closing log for __consumer_offsets-9 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,902] INFO Shutdown complete. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:07:57,902] INFO [BrokerHealthManager]: Shutting down (kafka.availability.BrokerHealthManager)
benchi-kafka     | [2025-03-18 16:07:57,902] INFO [BrokerHealthManager]: Stopped (kafka.availability.BrokerHealthManager)
benchi-kafka     | [2025-03-18 16:07:57,902] INFO [BrokerHealthManager]: Shutdown completed (kafka.availability.BrokerHealthManager)
benchi-kafka     | [2025-03-18 16:07:57,902] INFO [broker-1-ThrottledChannelReaper-Fetch]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:07:57,903] INFO [broker-1-ThrottledChannelReaper-Fetch]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:07:57,903] INFO [broker-1-ThrottledChannelReaper-Fetch]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:07:57,903] INFO [broker-1-ThrottledChannelReaper-Produce]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:07:57,903] INFO [broker-1-ThrottledChannelReaper-Produce]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:07:57,903] INFO [broker-1-ThrottledChannelReaper-Produce]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:07:57,903] INFO [broker-1-ThrottledChannelReaper-Request]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:07:57,903] INFO [broker-1-ThrottledChannelReaper-Request]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:07:57,903] INFO [broker-1-ThrottledChannelReaper-Request]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:07:57,904] INFO [broker-1-ThrottledChannelReaper-ClusterLinkRequest]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:07:57,904] INFO [broker-1-ThrottledChannelReaper-ClusterLinkRequest]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:07:57,904] INFO [broker-1-ThrottledChannelReaper-ClusterLinkRequest]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:07:57,904] INFO [broker-1-ThrottledChannelReaper-ControllerMutation]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:07:57,904] INFO [broker-1-ThrottledChannelReaper-ControllerMutation]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:07:57,904] INFO [broker-1-ThrottledChannelReaper-ControllerMutation]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:07:57,904] INFO [SocketServer listenerType=BROKER, nodeId=1] Shutting down socket server (kafka.network.SocketServer)
benchi-kafka     | [2025-03-18 16:07:57,910] INFO [SocketServer listenerType=BROKER, nodeId=1] Shutdown completed (kafka.network.SocketServer)
benchi-kafka     | [2025-03-18 16:07:57,910] INFO Broker and topic stats closed (kafka.server.BrokerTopicStats)
benchi-kafka     | [2025-03-18 16:07:57,911] INFO [BrokerLifecycleManager id=1] closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
benchi-kafka     | [2025-03-18 16:07:57,911] INFO [client-metrics-reaper]: Shutting down (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
benchi-kafka     | [2025-03-18 16:07:57,911] INFO [client-metrics-reaper]: Stopped (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
benchi-kafka     | [2025-03-18 16:07:57,911] INFO [client-metrics-reaper]: Shutdown completed (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
benchi-kafka     | [2025-03-18 16:07:57,912] INFO [BrokerServer id=1] shut down completed (kafka.server.BrokerServer)
benchi-kafka     | [2025-03-18 16:07:57,912] INFO [BrokerServer id=1] Transition from SHUTTING_DOWN to SHUTDOWN (kafka.server.BrokerServer)
benchi-kafka     | [2025-03-18 16:07:57,912] INFO [ControllerServer id=1] shutting down (kafka.server.ControllerServer)
benchi-kafka     | [2025-03-18 16:07:57,914] INFO [raft-expiration-reaper]: Shutting down (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:07:57,920] INFO [Producer clientId=_confluent_balancer_api_state-producer-1] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:07:57,920] WARN [Producer clientId=_confluent_balancer_api_state-producer-1] Connection to node 1 (broker/172.20.0.2:29092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:07:57,930] INFO [raft-expiration-reaper]: Stopped (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:07:57,930] INFO [raft-expiration-reaper]: Shutdown completed (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:07:57,931] INFO [kafka-1-raft-io-thread]: Shutting down (org.apache.kafka.raft.KafkaRaftClientDriver)
benchi-kafka     | [2025-03-18 16:07:57,931] INFO [RaftManager id=1] Beginning graceful shutdown (org.apache.kafka.raft.KafkaRaftClient)
benchi-kafka     | [2025-03-18 16:07:57,931] INFO [RaftManager id=1] Graceful shutdown completed (org.apache.kafka.raft.KafkaRaftClient)
benchi-kafka     | [2025-03-18 16:07:57,931] INFO [RaftManager id=1] Completed graceful shutdown of RaftClient (org.apache.kafka.raft.KafkaRaftClientDriver)
benchi-kafka     | [2025-03-18 16:07:57,931] INFO [kafka-1-raft-io-thread]: Stopped (org.apache.kafka.raft.KafkaRaftClientDriver)
benchi-kafka     | [2025-03-18 16:07:57,931] INFO [kafka-1-raft-io-thread]: Shutdown completed (org.apache.kafka.raft.KafkaRaftClientDriver)
benchi-kafka     | [2025-03-18 16:07:57,932] INFO [kafka-1-raft-outbound-request-thread]: Shutting down (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
benchi-kafka     | [2025-03-18 16:07:57,932] INFO [kafka-1-raft-outbound-request-thread]: Stopped (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
benchi-kafka     | [2025-03-18 16:07:57,932] INFO [kafka-1-raft-outbound-request-thread]: Shutdown completed (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
benchi-kafka     | [2025-03-18 16:07:57,932] INFO Tier partition state for __cluster_metadata-0 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:07:57,933] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 524 with 0 producer ids in 0 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
benchi-kafka     | [2025-03-18 16:07:57,934] INFO [ControllerRegistrationManager id=1 incarnation=g1LiT4BhQhCyUixJaN6eeg] beginShutdown: shutting down event queue. (org.apache.kafka.queue.KafkaEventQueue)
benchi-kafka     | [2025-03-18 16:07:57,934] INFO [ControllerRegistrationManager id=1 incarnation=g1LiT4BhQhCyUixJaN6eeg] shutting down. (kafka.server.ControllerRegistrationManager)
benchi-kafka     | [2025-03-18 16:07:57,935] INFO [controller-1-to-controller-registration-channel-manager]: Shutting down (kafka.server.NodeToControllerRequestThread)
benchi-kafka     | [2025-03-18 16:07:57,935] INFO [controller-1-to-controller-registration-channel-manager]: Stopped (kafka.server.NodeToControllerRequestThread)
benchi-kafka     | [2025-03-18 16:07:57,935] INFO [controller-1-to-controller-registration-channel-manager]: Shutdown completed (kafka.server.NodeToControllerRequestThread)
benchi-kafka     | [2025-03-18 16:07:57,935] INFO Node to controller channel manager for registration shutdown (kafka.server.NodeToControllerChannelManagerImpl)
benchi-kafka     | [2025-03-18 16:07:57,935] INFO [ControllerRegistrationManager id=1 incarnation=g1LiT4BhQhCyUixJaN6eeg] closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
benchi-kafka     | [2025-03-18 16:07:57,935] INFO [controller-1-to-controller-registration-channel-manager]: Shutdown completed (kafka.server.NodeToControllerRequestThread)
benchi-kafka     | [2025-03-18 16:07:57,935] WARN [NodeToControllerChannelManager id=1 name=registration] Attempting to close NetworkClient that has already been closed. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:07:57,935] INFO Node to controller channel manager for registration shutdown (kafka.server.NodeToControllerChannelManagerImpl)
benchi-kafka     | [2025-03-18 16:07:57,936] INFO [ControllerRegistrationManager id=1 incarnation=g1LiT4BhQhCyUixJaN6eeg] closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
benchi-kafka     | [2025-03-18 16:07:57,936] INFO [CelltControllerMetricsPublisher id=1] KafkaEventQueue#close: shutting down event queue. (org.apache.kafka.queue.KafkaEventQueue)
benchi-kafka     | [2025-03-18 16:07:57,936] INFO [CelltControllerMetricsPublisher id=1] closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
benchi-kafka     | [2025-03-18 16:07:57,937] INFO SbcEventQueueKafkaEventQueue#close: shutting down event queue. (org.apache.kafka.queue.KafkaEventQueue)
benchi-kafka     | [2025-03-18 16:07:57,937] INFO Handling event SbcShutdownEvent-284 (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:07:57,937] INFO SBC shutdown initiated. (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:07:57,937] INFO SbcEventQueueclosed event queue. (org.apache.kafka.queue.KafkaEventQueue)
benchi-kafka     | [2025-03-18 16:07:57,937] INFO [SocketServer listenerType=CONTROLLER, nodeId=1] Stopping socket server request processors (kafka.network.SocketServer)
benchi-kafka     | [2025-03-18 16:07:57,939] INFO [SocketServer listenerType=CONTROLLER, nodeId=1] Stopped socket server request processors (kafka.network.SocketServer)
benchi-kafka     | [2025-03-18 16:07:57,939] INFO [ControllerServer id=1] QuorumController#beginShutdown: shutting down event queue. (org.apache.kafka.queue.KafkaEventQueue)
benchi-kafka     | [2025-03-18 16:07:57,940] INFO [SocketServer listenerType=CONTROLLER, nodeId=1] Shutting down socket server (kafka.network.SocketServer)
benchi-kafka     | [2025-03-18 16:07:57,940] INFO [ControllerServer id=1] writeNoOpRecord: event unable to start processing because of RejectedExecutionException (treated as TimeoutException). Exception message: The event queue is shutting down (org.apache.kafka.controller.QuorumController)
benchi-kafka     | [2025-03-18 16:07:57,940] INFO [ControllerServer id=1] maybeBalancePartitionLeaders: event unable to start processing because of RejectedExecutionException (treated as TimeoutException). Exception message: The event queue is shutting down (org.apache.kafka.controller.QuorumController)
benchi-kafka     | [2025-03-18 16:07:57,941] INFO [SocketServer listenerType=CONTROLLER, nodeId=1] Shutdown completed (kafka.network.SocketServer)
benchi-kafka     | [2025-03-18 16:07:57,942] INFO [data-plane Kafka Request Handler on Broker 1], shutting down (kafka.server.KafkaRequestHandlerPool)
benchi-kafka     | [2025-03-18 16:07:57,942] INFO [data-plane Kafka Request Handler on Broker 1], shut down completely (kafka.server.KafkaRequestHandlerPool)
benchi-kafka     | [2025-03-18 16:07:57,942] INFO [ExpirationReaper-1-AlterAcls]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:07:57,942] INFO [ExpirationReaper-1-AlterAcls]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:07:57,942] INFO [ExpirationReaper-1-AlterAcls]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:07:57,942] INFO [controller-1-ThrottledChannelReaper-Fetch]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:07:57,942] INFO [controller-1-ThrottledChannelReaper-Fetch]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:07:57,942] INFO [controller-1-ThrottledChannelReaper-Fetch]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:07:57,943] INFO [controller-1-ThrottledChannelReaper-Produce]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:07:57,943] INFO [controller-1-ThrottledChannelReaper-Produce]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:07:57,943] INFO [controller-1-ThrottledChannelReaper-Produce]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:07:57,943] INFO [controller-1-ThrottledChannelReaper-Request]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:07:57,943] INFO [controller-1-ThrottledChannelReaper-Request]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:07:57,943] INFO [controller-1-ThrottledChannelReaper-Request]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:07:57,943] INFO [controller-1-ThrottledChannelReaper-ClusterLinkRequest]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:07:57,943] INFO [controller-1-ThrottledChannelReaper-ClusterLinkRequest]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:07:57,943] INFO [controller-1-ThrottledChannelReaper-ClusterLinkRequest]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:07:57,943] INFO [controller-1-ThrottledChannelReaper-ControllerMutation]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:07:57,943] INFO [controller-1-ThrottledChannelReaper-ControllerMutation]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:07:57,943] INFO [controller-1-ThrottledChannelReaper-ControllerMutation]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:07:57,943] INFO [ControllerServer id=1] closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
benchi-kafka     | [2025-03-18 16:07:57,944] INFO [SharedServer id=1] Stopping SharedServer (kafka.server.SharedServer)
benchi-kafka     | [2025-03-18 16:07:57,944] INFO [MetadataLoader id=1] beginShutdown: shutting down event queue. (org.apache.kafka.queue.KafkaEventQueue)
benchi-kafka     | [2025-03-18 16:07:57,944] INFO [SnapshotGenerator id=1] beginShutdown: shutting down event queue. (org.apache.kafka.queue.KafkaEventQueue)
benchi-kafka     | [2025-03-18 16:07:57,945] INFO [SnapshotGenerator id=1] closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
benchi-kafka     | [2025-03-18 16:07:57,945] INFO [MetadataLoader id=1] closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
benchi-kafka     | [2025-03-18 16:07:57,945] INFO [SnapshotGenerator id=1] closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
benchi-kafka     | [2025-03-18 16:07:57,946] INFO Stopping Confluent metrics reporter (io.confluent.metrics.reporter.ConfluentMetricsReporter)
benchi-kafka     | [2025-03-18 16:07:57,946] INFO [Producer clientId=confluent-metrics-reporter] Closing the Kafka producer with timeoutMillis = 0 ms. (org.apache.kafka.clients.producer.KafkaProducer)
benchi-kafka     | [2025-03-18 16:07:57,946] INFO [Producer clientId=confluent-metrics-reporter] Proceeding to force close the producer since pending requests could not be completed within timeout 0 ms. (org.apache.kafka.clients.producer.KafkaProducer)
benchi-kafka     | [2025-03-18 16:07:57,946] INFO App info kafka.producer for confluent-metrics-reporter unregistered (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:07:57,946] INFO Stopping TelemetryReporter collectorTask (io.confluent.telemetry.reporter.TelemetryReporter)
benchi-kafka     | [2025-03-18 16:07:57,946] INFO Closing the event logger (io.confluent.telemetry.reporter.TelemetryReporter)
benchi-kafka     | [2025-03-18 16:07:57,949] INFO [Producer clientId=confluent-telemetry-reporter-local-producer] Closing the Kafka producer with timeoutMillis = 0 ms. (org.apache.kafka.clients.producer.KafkaProducer)
benchi-kafka     | [2025-03-18 16:07:57,949] INFO [Producer clientId=confluent-telemetry-reporter-local-producer] Proceeding to force close the producer since pending requests could not be completed within timeout 0 ms. (org.apache.kafka.clients.producer.KafkaProducer)
benchi-kafka     | [2025-03-18 16:07:57,950] INFO App info kafka.producer for confluent-telemetry-reporter-local-producer unregistered (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:07:57,950] INFO Stopping TelemetryReporter remoteConfigTask (io.confluent.telemetry.reporter.TelemetryReporter)
benchi-kafka     | [2025-03-18 16:07:57,952] INFO App info kafka.server for 1 unregistered (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:07:57,953] INFO App info kafka.server for 1 unregistered (org.apache.kafka.common.utils.AppInfoParser)
[Kbenchi-kafka exited with code 143

 Network infra_default  Creating
 Network infra_default  Created
 Container benchi-kafka  Creating
 Container benchi-postgres  Creating
 Container benchi-kafka  Created
 Container benchi-postgres  Created
Attaching to benchi-kafka, benchi-postgres
benchi-kafka     | ===> User
benchi-kafka     | uid=1000(appuser) gid=1000(appuser) groups=1000(appuser)
benchi-kafka     | ===> Configuring ...
benchi-kafka     | Running in KRaft mode...
benchi-postgres  | The files belonging to this database system will be owned by user "postgres".
benchi-postgres  | This user must also own the server process.
benchi-postgres  | 
benchi-postgres  | The database cluster will be initialized with locale "en_US.utf8".
benchi-postgres  | The default database encoding has accordingly been set to "UTF8".
benchi-postgres  | The default text search configuration will be set to "english".
benchi-postgres  | 
benchi-postgres  | Data page checksums are disabled.
benchi-postgres  | 
benchi-postgres  | fixing permissions on existing directory /var/lib/postgresql/data ... ok
benchi-postgres  | creating subdirectories ... ok
benchi-postgres  | selecting dynamic shared memory implementation ... posix
benchi-postgres  | selecting default max_connections ... 100
benchi-postgres  | selecting default shared_buffers ... 128MB
benchi-postgres  | selecting default time zone ... Etc/UTC
benchi-postgres  | creating configuration files ... ok
benchi-postgres  | running bootstrap script ... ok
benchi-postgres  | performing post-bootstrap initialization ... ok
benchi-postgres  | syncing data to disk ... ok
benchi-postgres  | 
benchi-postgres  | 
benchi-postgres  | Success. You can now start the database server using:
benchi-postgres  | 
benchi-postgres  |     pg_ctl -D /var/lib/postgresql/data -l logfile start
benchi-postgres  | 
benchi-postgres  | initdb: warning: enabling "trust" authentication for local connections
benchi-postgres  | initdb: hint: You can change this by editing pg_hba.conf or using the option -A, or --auth-local and --auth-host, the next time you run initdb.
benchi-postgres  | waiting for server to start....2025-03-18 16:03:15.680 UTC [48] LOG:  starting PostgreSQL 16.4 (Debian 16.4-1.pgdg110+2) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 10.2.1-6) 10.2.1 20210110, 64-bit
benchi-postgres  | 2025-03-18 16:03:15.681 UTC [48] LOG:  listening on Unix socket "/var/run/postgresql/.s.PGSQL.5432"
benchi-postgres  | 2025-03-18 16:03:15.682 UTC [51] LOG:  database system was shut down at 2025-03-18 16:03:15 UTC
benchi-postgres  | 2025-03-18 16:03:15.684 UTC [48] LOG:  database system is ready to accept connections
benchi-postgres  |  done
benchi-postgres  | server started
benchi-postgres  | CREATE DATABASE
benchi-postgres  | 
benchi-postgres  | 
benchi-postgres  | /usr/local/bin/docker-entrypoint.sh: sourcing /docker-entrypoint-initdb.d/init-permissions.sh
benchi-postgres  | 
benchi-postgres  | /usr/local/bin/docker-entrypoint.sh: running /docker-entrypoint-initdb.d/init.sql
benchi-postgres  | DROP TABLE
benchi-postgres  | DROP SEQUENCE
benchi-postgres  | psql:/docker-entrypoint-initdb.d/init.sql:1: NOTICE:  table "employees" does not exist, skipping
benchi-postgres  | psql:/docker-entrypoint-initdb.d/init.sql:2: NOTICE:  sequence "employees_id_seq" does not exist, skipping
benchi-postgres  | CREATE TABLE
benchi-postgres  | CREATE SEQUENCE
benchi-postgres  | ALTER TABLE
benchi-postgres  | ALTER TABLE
benchi-postgres  | 
benchi-postgres  | 
benchi-postgres  | waiting for server to shut down....2025-03-18 16:03:15.858 UTC [48] LOG:  received fast shutdown request
benchi-postgres  | 2025-03-18 16:03:15.859 UTC [48] LOG:  aborting any active transactions
benchi-postgres  | 2025-03-18 16:03:15.859 UTC [48] LOG:  background worker "logical replication launcher" (PID 54) exited with exit code 1
benchi-postgres  | 2025-03-18 16:03:15.860 UTC [49] LOG:  shutting down
benchi-postgres  | 2025-03-18 16:03:15.860 UTC [49] LOG:  checkpoint starting: shutdown immediate
benchi-postgres  | 2025-03-18 16:03:15.887 UTC [49] LOG:  checkpoint complete: wrote 928 buffers (5.7%); 0 WAL file(s) added, 0 removed, 0 recycled; write=0.006 s, sync=0.020 s, total=0.028 s; sync files=304, longest=0.006 s, average=0.001 s; distance=4267 kB, estimate=4267 kB; lsn=0/19D91E8, redo lsn=0/19D91E8
benchi-postgres  | 2025-03-18 16:03:15.888 UTC [48] LOG:  database system is shut down
benchi-postgres  |  done
benchi-postgres  | server stopped
benchi-postgres  | 
benchi-postgres  | PostgreSQL init process complete; ready for start up.
benchi-postgres  | 
benchi-postgres  | 2025-03-18 16:03:15.970 UTC [1] LOG:  starting PostgreSQL 16.4 (Debian 16.4-1.pgdg110+2) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 10.2.1-6) 10.2.1 20210110, 64-bit
benchi-postgres  | 2025-03-18 16:03:15.971 UTC [1] LOG:  listening on IPv4 address "0.0.0.0", port 5432
benchi-postgres  | 2025-03-18 16:03:15.971 UTC [1] LOG:  listening on IPv6 address "::", port 5432
benchi-postgres  | 2025-03-18 16:03:15.972 UTC [1] LOG:  listening on Unix socket "/var/run/postgresql/.s.PGSQL.5432"
benchi-postgres  | 2025-03-18 16:03:15.974 UTC [66] LOG:  database system was shut down at 2025-03-18 16:03:15 UTC
benchi-postgres  | 2025-03-18 16:03:15.975 UTC [1] LOG:  database system is ready to accept connections
benchi-kafka     | ===> Running preflight checks ... 
benchi-kafka     | ===> Check if /var/lib/kafka/data is writable ...
benchi-kafka     | ===> Running in KRaft mode, skipping Zookeeper health check...
benchi-kafka     | ===> Using provided cluster id MkU3OEVBNTcwNTJENDM2Qk ...
benchi-kafka     | ===> Launching ... 
benchi-kafka     | ===> Launching kafka ... 
benchi-kafka     | [2025-03-18 16:03:18,677] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
benchi-kafka     | [2025-03-18 16:03:18,955] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
benchi-kafka     | [2025-03-18 16:03:18,955] INFO RemoteLogManagerConfig values: 
benchi-kafka     | 	log.local.retention.bytes = -2
benchi-kafka     | 	log.local.retention.ms = -2
benchi-kafka     | 	remote.fetch.max.wait.ms = 500
benchi-kafka     | 	remote.log.index.file.cache.total.size.bytes = 1073741824
benchi-kafka     | 	remote.log.manager.copier.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.copy.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.copy.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.expiration.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.fetch.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.fetch.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.task.interval.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.max.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.ms = 500
benchi-kafka     | 	remote.log.manager.task.retry.jitter = 0.2
benchi-kafka     | 	remote.log.manager.thread.pool.size = 10
benchi-kafka     | 	remote.log.metadata.custom.metadata.max.bytes = 128
benchi-kafka     | 	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
benchi-kafka     | 	remote.log.metadata.manager.class.path = null
benchi-kafka     | 	remote.log.metadata.manager.impl.prefix = rlmm.config.
benchi-kafka     | 	remote.log.metadata.manager.listener.name = null
benchi-kafka     | 	remote.log.reader.max.pending.tasks = 100
benchi-kafka     | 	remote.log.reader.threads = 10
benchi-kafka     | 	remote.log.storage.manager.class.name = null
benchi-kafka     | 	remote.log.storage.manager.class.path = null
benchi-kafka     | 	remote.log.storage.manager.impl.prefix = rsm.config.
benchi-kafka     | 	remote.log.storage.system.enable = false
benchi-kafka     |  (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
benchi-kafka     | [2025-03-18 16:03:19,053] INFO RemoteLogManagerConfig values: 
benchi-kafka     | 	log.local.retention.bytes = -2
benchi-kafka     | 	log.local.retention.ms = -2
benchi-kafka     | 	remote.fetch.max.wait.ms = 500
benchi-kafka     | 	remote.log.index.file.cache.total.size.bytes = 1073741824
benchi-kafka     | 	remote.log.manager.copier.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.copy.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.copy.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.expiration.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.fetch.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.fetch.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.task.interval.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.max.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.ms = 500
benchi-kafka     | 	remote.log.manager.task.retry.jitter = 0.2
benchi-kafka     | 	remote.log.manager.thread.pool.size = 10
benchi-kafka     | 	remote.log.metadata.custom.metadata.max.bytes = 128
benchi-kafka     | 	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
benchi-kafka     | 	remote.log.metadata.manager.class.path = null
benchi-kafka     | 	remote.log.metadata.manager.impl.prefix = rlmm.config.
benchi-kafka     | 	remote.log.metadata.manager.listener.name = null
benchi-kafka     | 	remote.log.reader.max.pending.tasks = 100
benchi-kafka     | 	remote.log.reader.threads = 10
benchi-kafka     | 	remote.log.storage.manager.class.name = null
benchi-kafka     | 	remote.log.storage.manager.class.path = null
benchi-kafka     | 	remote.log.storage.manager.impl.prefix = rsm.config.
benchi-kafka     | 	remote.log.storage.system.enable = false
benchi-kafka     |  (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
benchi-kafka     | [2025-03-18 16:03:19,057] INFO RemoteLogManagerConfig values: 
benchi-kafka     | 	log.local.retention.bytes = -2
benchi-kafka     | 	log.local.retention.ms = -2
benchi-kafka     | 	remote.fetch.max.wait.ms = 500
benchi-kafka     | 	remote.log.index.file.cache.total.size.bytes = 1073741824
benchi-kafka     | 	remote.log.manager.copier.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.copy.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.copy.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.expiration.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.fetch.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.fetch.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.task.interval.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.max.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.ms = 500
benchi-kafka     | 	remote.log.manager.task.retry.jitter = 0.2
benchi-kafka     | 	remote.log.manager.thread.pool.size = 10
benchi-kafka     | 	remote.log.metadata.custom.metadata.max.bytes = 128
benchi-kafka     | 	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
benchi-kafka     | 	remote.log.metadata.manager.class.path = null
benchi-kafka     | 	remote.log.metadata.manager.impl.prefix = rlmm.config.
benchi-kafka     | 	remote.log.metadata.manager.listener.name = null
benchi-kafka     | 	remote.log.reader.max.pending.tasks = 100
benchi-kafka     | 	remote.log.reader.threads = 10
benchi-kafka     | 	remote.log.storage.manager.class.name = null
benchi-kafka     | 	remote.log.storage.manager.class.path = null
benchi-kafka     | 	remote.log.storage.manager.impl.prefix = rsm.config.
benchi-kafka     | 	remote.log.storage.system.enable = false
benchi-kafka     |  (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
benchi-kafka     | [2025-03-18 16:03:19,078] INFO RemoteLogManagerConfig values: 
benchi-kafka     | 	log.local.retention.bytes = -2
benchi-kafka     | 	log.local.retention.ms = -2
benchi-kafka     | 	remote.fetch.max.wait.ms = 500
benchi-kafka     | 	remote.log.index.file.cache.total.size.bytes = 1073741824
benchi-kafka     | 	remote.log.manager.copier.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.copy.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.copy.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.expiration.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.fetch.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.fetch.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.task.interval.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.max.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.ms = 500
benchi-kafka     | 	remote.log.manager.task.retry.jitter = 0.2
benchi-kafka     | 	remote.log.manager.thread.pool.size = 10
benchi-kafka     | 	remote.log.metadata.custom.metadata.max.bytes = 128
benchi-kafka     | 	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
benchi-kafka     | 	remote.log.metadata.manager.class.path = null
benchi-kafka     | 	remote.log.metadata.manager.impl.prefix = rlmm.config.
benchi-kafka     | 	remote.log.metadata.manager.listener.name = null
benchi-kafka     | 	remote.log.reader.max.pending.tasks = 100
benchi-kafka     | 	remote.log.reader.threads = 10
benchi-kafka     | 	remote.log.storage.manager.class.name = null
benchi-kafka     | 	remote.log.storage.manager.class.path = null
benchi-kafka     | 	remote.log.storage.manager.impl.prefix = rsm.config.
benchi-kafka     | 	remote.log.storage.system.enable = false
benchi-kafka     |  (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
benchi-kafka     | [2025-03-18 16:03:19,086] INFO KafkaConfig values: 
benchi-kafka     | 	advertised.listeners = PLAINTEXT://broker:29092,PLAINTEXT_HOST://localhost:9092
benchi-kafka     | 	alter.config.policy.class.name = null
benchi-kafka     | 	alter.log.dirs.replication.quota.window.num = 11
benchi-kafka     | 	alter.log.dirs.replication.quota.window.size.seconds = 1
benchi-kafka     | 	authorizer.class.name = 
benchi-kafka     | 	auto.create.topics.enable = true
benchi-kafka     | 	auto.include.jmx.reporter = true
benchi-kafka     | 	auto.leader.rebalance.enable = true
benchi-kafka     | 	background.threads = 10
benchi-kafka     | 	broker.heartbeat.interval.ms = 2000
benchi-kafka     | 	broker.id = 1
benchi-kafka     | 	broker.id.generation.enable = true
benchi-kafka     | 	broker.interceptor.class = class org.apache.kafka.server.interceptor.DefaultBrokerInterceptor
benchi-kafka     | 	broker.rack = null
benchi-kafka     | 	broker.session.timeout.ms = 9000
benchi-kafka     | 	broker.session.uuid = 22eDK4SjR7GWVEmuEgKyNQ
benchi-kafka     | 	client.quota.callback.class = null
benchi-kafka     | 	client.quota.max.throttle.time.ms = 5000
benchi-kafka     | 	compression.gzip.level = -1
benchi-kafka     | 	compression.lz4.level = 9
benchi-kafka     | 	compression.type = producer
benchi-kafka     | 	compression.zstd.level = 3
benchi-kafka     | 	confluent.alter.broker.health.max.demoted.brokers = 2147483647
benchi-kafka     | 	confluent.alter.broker.health.max.demoted.brokers.percentage = 0
benchi-kafka     | 	confluent.ansible.managed = false
benchi-kafka     | 	confluent.api.visibility = DEFAULT
benchi-kafka     | 	confluent.append.record.interceptor.classes = []
benchi-kafka     | 	confluent.apply.create.topic.policy.to.create.partitions = false
benchi-kafka     | 	confluent.authorizer.authority.name = 
benchi-kafka     | 	confluent.automatic.alter.broker.health.retry.backoff.ms = 2000
benchi-kafka     | 	confluent.backpressure.disk.enable = false
benchi-kafka     | 	confluent.backpressure.disk.free.threshold.bytes = 21474836480
benchi-kafka     | 	confluent.backpressure.disk.produce.bytes.per.second = 131072
benchi-kafka     | 	confluent.backpressure.disk.threshold.recovery.factor = 1.5
benchi-kafka     | 	confluent.backpressure.request.min.broker.limit = 200
benchi-kafka     | 	confluent.backpressure.request.queue.size.percentile = p95
benchi-kafka     | 	confluent.backpressure.types = null
benchi-kafka     | 	confluent.balancer.api.state.topic = _confluent_balancer_api_state
benchi-kafka     | 	confluent.balancer.broker.addition.elapsed.time.ms.completion.threshold = 57600000
benchi-kafka     | 	confluent.balancer.broker.addition.mean.cpu.percent.completion.threshold = 0.5
benchi-kafka     | 	confluent.balancer.capacity.threshold.upper.limit = 0.95
benchi-kafka     | 	confluent.balancer.cell.load.upper.bound = 0.7
benchi-kafka     | 	confluent.balancer.cell.overload.detection.interval.ms = 3600000
benchi-kafka     | 	confluent.balancer.cell.overload.duration.ms = 86400000
benchi-kafka     | 	confluent.balancer.class = io.confluent.databalancer.SbcDataBalanceManager
benchi-kafka     | 	confluent.balancer.consumer.out.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.balancer.cpu.balance.threshold = 1.1
benchi-kafka     | 	confluent.balancer.cpu.low.utilization.threshold = 0.2
benchi-kafka     | 	confluent.balancer.cpu.utilization.detector.duration.ms = 600000
benchi-kafka     | 	confluent.balancer.cpu.utilization.detector.enabled = false
benchi-kafka     | 	confluent.balancer.cpu.utilization.detector.overutilization.threshold = 80.0
benchi-kafka     | 	confluent.balancer.cpu.utilization.detector.underutilization.threshold = 50.0
benchi-kafka     | 	confluent.balancer.disk.max.load = 0.85
benchi-kafka     | 	confluent.balancer.disk.min.free.space.gb = 0
benchi-kafka     | 	confluent.balancer.disk.min.free.space.lower.limit.gb = 0
benchi-kafka     | 	confluent.balancer.disk.utilization.detector.duration.ms = 600000
benchi-kafka     | 	confluent.balancer.disk.utilization.detector.enabled = false
benchi-kafka     | 	confluent.balancer.disk.utilization.detector.overutilization.threshold = 80.0
benchi-kafka     | 	confluent.balancer.disk.utilization.detector.reserved.capacity = 150000.0
benchi-kafka     | 	confluent.balancer.disk.utilization.detector.underutilization.threshold = 35.0
benchi-kafka     | 	confluent.balancer.enable = true
benchi-kafka     | 	confluent.balancer.exclude.topic.names = []
benchi-kafka     | 	confluent.balancer.exclude.topic.prefixes = []
benchi-kafka     | 	confluent.balancer.goal.violation.delay.on.new.brokers.ms = 1800000
benchi-kafka     | 	confluent.balancer.goal.violation.distribution.threshold.multiplier = 1.1
benchi-kafka     | 	confluent.balancer.heal.broker.failure.threshold.ms = 3600000
benchi-kafka     | 	confluent.balancer.heal.uneven.load.trigger = EMPTY_BROKER
benchi-kafka     | 	confluent.balancer.incremental.balancing.cpu.top.proposal.tracking.enabled = true
benchi-kafka     | 	confluent.balancer.incremental.balancing.cpu.top.proposal.tracking.num.proposals = 15
benchi-kafka     | 	confluent.balancer.incremental.balancing.enabled = false
benchi-kafka     | 	confluent.balancer.incremental.balancing.goals = []
benchi-kafka     | 	confluent.balancer.incremental.balancing.lower.bound = 0.02
benchi-kafka     | 	confluent.balancer.incremental.balancing.min.valid.windows = 5
benchi-kafka     | 	confluent.balancer.incremental.balancing.step.ratio = 0.2
benchi-kafka     | 	confluent.balancer.inter.cell.balancing.enabled = false
benchi-kafka     | 	confluent.balancer.max.capacity.balancing.delta.percentage = 0.0
benchi-kafka     | 	confluent.balancer.max.replicas = 2147483647
benchi-kafka     | 	confluent.balancer.network.in.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.balancer.network.out.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.balancer.plan.computation.retry.timeout.ms = 3600000
benchi-kafka     | 	confluent.balancer.producer.in.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.balancer.rebalancing.goals = []
benchi-kafka     | 	confluent.balancer.replication.in.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.balancer.resource.utilization.detector.interval.ms = 60000
benchi-kafka     | 	confluent.balancer.sbc.metrics.parser.enabled = false
benchi-kafka     | 	confluent.balancer.self.healing.maximum.rounds = 1
benchi-kafka     | 	confluent.balancer.task.history.retention.days = 30
benchi-kafka     | 	confluent.balancer.tenant.maximum.movements = 0
benchi-kafka     | 	confluent.balancer.tenant.suspension.ms = 86400000
benchi-kafka     | 	confluent.balancer.throttle.bytes.per.second = 10485760
benchi-kafka     | 	confluent.balancer.topic.partition.maximum.movements = 5
benchi-kafka     | 	confluent.balancer.topic.partition.movement.expiration.ms = 10800000
benchi-kafka     | 	confluent.balancer.topic.partition.suspension.ms = 18000000
benchi-kafka     | 	confluent.balancer.topic.replication.factor = 1
benchi-kafka     | 	confluent.balancer.triggering.goals = []
benchi-kafka     | 	confluent.balancer.v2.addition.enabled = false
benchi-kafka     | 	confluent.basic.auth.credentials.source = null
benchi-kafka     | 	confluent.basic.auth.user.info = null
benchi-kafka     | 	confluent.bearer.auth.cache.expiry.buffer.seconds = 300
benchi-kafka     | 	confluent.bearer.auth.client.id = null
benchi-kafka     | 	confluent.bearer.auth.client.secret = null
benchi-kafka     | 	confluent.bearer.auth.credentials.source = null
benchi-kafka     | 	confluent.bearer.auth.identity.pool.id = null
benchi-kafka     | 	confluent.bearer.auth.issuer.endpoint.url = null
benchi-kafka     | 	confluent.bearer.auth.logical.cluster = null
benchi-kafka     | 	confluent.bearer.auth.scope = null
benchi-kafka     | 	confluent.bearer.auth.scope.claim.name = scope
benchi-kafka     | 	confluent.bearer.auth.sub.claim.name = sub
benchi-kafka     | 	confluent.bearer.auth.token = null
benchi-kafka     | 	confluent.broker.health.manager.enabled = true
benchi-kafka     | 	confluent.broker.health.manager.engine.request.handler.threads.stuck.criteria = AllThreadsStuck
benchi-kafka     | 	confluent.broker.health.manager.hard.kill.duration.ms = 60000
benchi-kafka     | 	confluent.broker.health.manager.mitigation.enabled = false
benchi-kafka     | 	confluent.broker.health.manager.num.samples.before.broker.suspect = 30
benchi-kafka     | 	confluent.broker.health.manager.num.samples.before.broker.unhealthy = 180
benchi-kafka     | 	confluent.broker.health.manager.percentage.unhealthy.samples.before.broker.suspect = 90
benchi-kafka     | 	confluent.broker.health.manager.percentage.unhealthy.samples.before.broker.unhealthy = 70
benchi-kafka     | 	confluent.broker.health.manager.sample.duration.ms = 1000
benchi-kafka     | 	confluent.broker.health.manager.storage.background.threads.stuck.criteria = AnyThreadStuck
benchi-kafka     | 	confluent.broker.health.manager.storage.network.threads.stuck.criteria = AnyThreadStuck
benchi-kafka     | 	confluent.broker.health.manager.storage.request.handler.threads.stuck.criteria = AnyThreadStuck
benchi-kafka     | 	confluent.broker.limit.consumer.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.broker.limit.producer.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.broker.load.advertised.limit.load = 0.8
benchi-kafka     | 	confluent.broker.load.average.service.request.time.ms = 0.1
benchi-kafka     | 	confluent.broker.load.delay.metric.start.ms = 180000
benchi-kafka     | 	confluent.broker.load.enabled = false
benchi-kafka     | 	confluent.broker.load.num.samples = 60
benchi-kafka     | 	confluent.broker.load.tenant.metric.enable = false
benchi-kafka     | 	confluent.broker.load.update.metric.tags.interval.ms = 60000
benchi-kafka     | 	confluent.broker.load.window.size.ms = 60000
benchi-kafka     | 	confluent.broker.load.workload.coefficient = 20.0
benchi-kafka     | 	confluent.broker.registration.delay.ms = 0
benchi-kafka     | 	confluent.catalog.collector.destination.topic = telemetry.events.data_catalog_source
benchi-kafka     | 	confluent.catalog.collector.enable = false
benchi-kafka     | 	confluent.catalog.collector.full.configs.enable = false
benchi-kafka     | 	confluent.catalog.collector.max.bytes.per.snapshot = 850000
benchi-kafka     | 	confluent.catalog.collector.max.topics.process = 500
benchi-kafka     | 	confluent.catalog.collector.max.zookeeper.request.per.sec = 100
benchi-kafka     | 	confluent.catalog.collector.snapshot.init.delay.sec = 60
benchi-kafka     | 	confluent.catalog.collector.snapshot.interval.sec = 300
benchi-kafka     | 	confluent.ccloud.host.suffixes = .confluent.cloud,.cpdev.cloud,.confluentgov.com,.confluentgov-internal.com
benchi-kafka     | 	confluent.ccloud.intranet.host.suffixes = .intranet.stag.cpdev.cloud,.intranet.stag.cpdev-untrusted.cloud,.intranet.devel.cpdev.cloud,.intranet.devel.cpdev-untrusted.cloud,.intranet.confluent.cloud,.intranet.confluent-untrusted.cloud
benchi-kafka     | 	confluent.cdc.api.keys.topic = 
benchi-kafka     | 	confluent.cdc.api.keys.topic.load.timeout.ms = 600000
benchi-kafka     | 	confluent.cdc.client.quotas.enable = false
benchi-kafka     | 	confluent.cdc.client.quotas.topic.name = 
benchi-kafka     | 	confluent.cdc.lkc.metadata.topic = 
benchi-kafka     | 	confluent.cdc.user.metadata.enable = false
benchi-kafka     | 	confluent.cdc.user.metadata.topic = _confluent-user_metadata
benchi-kafka     | 	confluent.cell.metrics.refresh.period.ms = 60000
benchi-kafka     | 	confluent.cells.default.size = 15
benchi-kafka     | 	confluent.cells.enable = false
benchi-kafka     | 	confluent.cells.implicit.creation.enable = false
benchi-kafka     | 	confluent.cells.load.refresher.enable = true
benchi-kafka     | 	confluent.cells.max.size = 15
benchi-kafka     | 	confluent.cells.min.size = 6
benchi-kafka     | 	confluent.checksum.enabled.files = [none]
benchi-kafka     | 	confluent.clm.enabled = false
benchi-kafka     | 	confluent.clm.frequency.in.hours = 6
benchi-kafka     | 	confluent.clm.list.object.thread_pool.size = 1
benchi-kafka     | 	confluent.clm.max.backup.days = 3
benchi-kafka     | 	confluent.clm.min.delay.in.minutes = 30
benchi-kafka     | 	confluent.clm.thread.pool.size = 2
benchi-kafka     | 	confluent.clm.topic.retention.days.to.backup.days = 0:0,3:3
benchi-kafka     | 	confluent.close.connections.on.credential.delete = false
benchi-kafka     | 	confluent.cluster.link.admin.max.in.flight.requests = 1000
benchi-kafka     | 	confluent.cluster.link.admin.request.batch.size = 1
benchi-kafka     | 	confluent.cluster.link.allow.config.providers = true
benchi-kafka     | 	confluent.cluster.link.allow.legacy.message.format = false
benchi-kafka     | 	confluent.cluster.link.allow.truncation.below.hwm = true
benchi-kafka     | 	confluent.cluster.link.availability.check.mode = ALL
benchi-kafka     | 	confluent.cluster.link.background.thread.affinity = LINK
benchi-kafka     | 	confluent.cluster.link.clients.max.idle.ms = 3153600000000
benchi-kafka     | 	confluent.cluster.link.enable = true
benchi-kafka     | 	confluent.cluster.link.enable.local.admin = false
benchi-kafka     | 	confluent.cluster.link.enable.metrics.reduction = false
benchi-kafka     | 	confluent.cluster.link.fetch.response.min.bytes = 1
benchi-kafka     | 	confluent.cluster.link.fetch.response.total.bytes = 2147483647
benchi-kafka     | 	confluent.cluster.link.fetcher.auto.tune.enable = false
benchi-kafka     | 	confluent.cluster.link.fetcher.thread.pool.mode = ENDPOINT
benchi-kafka     | 	confluent.cluster.link.insync.fetch.response.min.bytes = 1
benchi-kafka     | 	confluent.cluster.link.insync.fetch.response.total.bytes = 2147483647
benchi-kafka     | 	confluent.cluster.link.intranet.connectivity.enable = false
benchi-kafka     | 	confluent.cluster.link.io.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.cluster.link.local.reverse.connection.listener.map = null
benchi-kafka     | 	confluent.cluster.link.max.client.connections = 2147483647
benchi-kafka     | 	confluent.cluster.link.metadata.topic.create.retry.delay.ms = 1000
benchi-kafka     | 	confluent.cluster.link.metadata.topic.enable = false
benchi-kafka     | 	confluent.cluster.link.metadata.topic.min.isr = 2
benchi-kafka     | 	confluent.cluster.link.metadata.topic.partitions = 50
benchi-kafka     | 	confluent.cluster.link.metadata.topic.replication.factor = 1
benchi-kafka     | 	confluent.cluster.link.mirror.transition.batch.size = 10
benchi-kafka     | 	confluent.cluster.link.num.background.threads = 1
benchi-kafka     | 	confluent.cluster.link.periodic.task.batch.size = 2147483647
benchi-kafka     | 	confluent.cluster.link.periodic.task.min.interval.ms = 1000
benchi-kafka     | 	confluent.cluster.link.replica.fetch.connections.mode = combined
benchi-kafka     | 	confluent.cluster.link.replication.quota.mode = CLUSTER_LINK_ONLY
benchi-kafka     | 	confluent.cluster.link.replication.quota.mode.per.tenant.overrides = 
benchi-kafka     | 	confluent.cluster.link.replication.quota.window.num = 11
benchi-kafka     | 	confluent.cluster.link.replication.quota.window.size.seconds = 2
benchi-kafka     | 	confluent.cluster.link.request.quota.capacity = 400
benchi-kafka     | 	confluent.cluster.link.request.quota.request.percentage.multiplier = 1.0
benchi-kafka     | 	confluent.cluster.link.tenant.replication.quota.enable = false
benchi-kafka     | 	confluent.cluster.link.tenant.request.quota.enable = false
benchi-kafka     | 	confluent.cluster.metadata.snapshot.tier.delete.enable = false
benchi-kafka     | 	confluent.cluster.metadata.snapshot.tier.delete.maintain.min.snapshots = 3
benchi-kafka     | 	confluent.cluster.metadata.snapshot.tier.delete.retention.ms = 604800000
benchi-kafka     | 	confluent.cluster.metadata.snapshot.tier.upload.enable = false
benchi-kafka     | 	confluent.compacted.topic.prefer.tier.fetch.ms = -1
benchi-kafka     | 	confluent.connection.invalid.request.delay.enable = false
benchi-kafka     | 	confluent.connections.idle.expiry.manager.ignore.idleness.requests = []
benchi-kafka     | 	confluent.consumer.fetch.partition.pruning.enable = true
benchi-kafka     | 	confluent.consumer.lag.emitter.enabled = false
benchi-kafka     | 	confluent.consumer.lag.emitter.interval.ms = 60000
benchi-kafka     | 	confluent.dataflow.policy.watch.monitor.ms = 300000
benchi-kafka     | 	confluent.defer.isr.shrink.enable = false
benchi-kafka     | 	confluent.describe.topic.partitions.enabled = false
benchi-kafka     | 	confluent.disk.io.manager.enable = false
benchi-kafka     | 	confluent.disk.throughput.headroom = 10485760
benchi-kafka     | 	confluent.disk.throughput.limit = 10485760000
benchi-kafka     | 	confluent.disk.throughput.quota.tier.archive = 1048576000
benchi-kafka     | 	confluent.disk.throughput.quota.tier.archive.throttled = 104857600
benchi-kafka     | 	confluent.durability.audit.batch.flush.frequency.ms = 900000
benchi-kafka     | 	confluent.durability.audit.checks = PeriodicalAudit,ChecksumAudit
benchi-kafka     | 	confluent.durability.audit.enable = false
benchi-kafka     | 	confluent.durability.audit.idempotent.producer = false
benchi-kafka     | 	confluent.durability.audit.initial.job.delay.ms = 900000
benchi-kafka     | 	confluent.durability.audit.io.bytes.per.sec = 10485760
benchi-kafka     | 	confluent.durability.audit.log.ignored.event.types = 
benchi-kafka     | 	confluent.durability.audit.reporting.batch.ms = 1800000
benchi-kafka     | 	confluent.durability.audit.tier.compaction.audit.duration.ms = 14400000
benchi-kafka     | 	confluent.durability.events.allowed = OffsetChangeType,EpochChangeType,IsrExpandType,DeleteRecordsType,RetentionChangeType,StartOffsetChangeType,DeletePartitionType,HealthCheckType
benchi-kafka     | 	confluent.durability.topic.partition.count = 50
benchi-kafka     | 	confluent.durability.topic.replication.factor = 1
benchi-kafka     | 	confluent.e2e_checksum.protection.enabled = false
benchi-kafka     | 	confluent.e2e_checksum.protection.files = [none]
benchi-kafka     | 	confluent.e2e_checksum.protection.store.entry.ttl.ms = 2592000000
benchi-kafka     | 	confluent.elastic.cku.enabled = false
benchi-kafka     | 	confluent.elastic.cku.scaletozero.enabled = false
benchi-kafka     | 	confluent.eligible.controllers = []
benchi-kafka     | 	confluent.enable.stray.partition.deletion = false
benchi-kafka     | 	confluent.encryption.key.manager.rotation.interval.ms = 31536000000
benchi-kafka     | 	confluent.fetch.from.follower.require.leader.epoch.enable = false
benchi-kafka     | 	confluent.fetch.partition.pruning.enable = true
benchi-kafka     | 	confluent.floor.connection.rate.per.ip = -1.0
benchi-kafka     | 	confluent.floor.connection.rate.per.tenant = -1.0
benchi-kafka     | 	confluent.group.coordinator.offsets.batching.enable = false
benchi-kafka     | 	confluent.group.coordinator.offsets.writer.threads = 2
benchi-kafka     | 	confluent.group.metadata.load.threads = 32
benchi-kafka     | 	confluent.heap.tenured.notify.bytes = 0
benchi-kafka     | 	confluent.heap.tenured.notify.enabled = false
benchi-kafka     | 	confluent.hot.partition.ratio = 0.8
benchi-kafka     | 	confluent.http.server.start.timeout.ms = 60000
benchi-kafka     | 	confluent.http.server.stop.timeout.ms = 30000
benchi-kafka     | 	confluent.internal.metrics.enable = false
benchi-kafka     | 	confluent.internal.rest.server.bind.port = null
benchi-kafka     | 	confluent.internal.tenant.scoped.listener.name = INTERNAL_TENANT_SCOPED
benchi-kafka     | 	confluent.leader.epoch.checkpoint.checksum.enabled = false
benchi-kafka     | 	confluent.log.cleaner.timestamp.validation.enable = true
benchi-kafka     | 	confluent.log.placement.constraints = 
benchi-kafka     | 	confluent.max.connection.creation.rate.per.ip = 1.7976931348623157E308
benchi-kafka     | 	confluent.max.connection.creation.rate.per.tenant = 1.7976931348623157E308
benchi-kafka     | 	confluent.max.connection.rate.per.ip = -1.0
benchi-kafka     | 	confluent.max.connection.rate.per.tenant = -1.0
benchi-kafka     | 	confluent.max.connection.throttle.ms = null
benchi-kafka     | 	confluent.max.segment.ms = 9223372036854775807
benchi-kafka     | 	confluent.metadata.active.encryptor = null
benchi-kafka     | 	confluent.metadata.controlled.shutdown.partition.slice.delay.ms = 100
benchi-kafka     | 	confluent.metadata.encryptor.classes = null
benchi-kafka     | 	confluent.metadata.encryptor.required = false
benchi-kafka     | 	confluent.metadata.encryptor.secret.file = null
benchi-kafka     | 	confluent.metadata.encryptor.secrets = null
benchi-kafka     | 	confluent.metadata.jvm.warmup.ms = 60000
benchi-kafka     | 	confluent.metadata.leader.balance.slice.delay.ms = 100
benchi-kafka     | 	confluent.metadata.max.controlled.shutdown.partition.changes.per.slice = 1000
benchi-kafka     | 	confluent.metadata.max.leader.balance.changes.per.slice = 1000
benchi-kafka     | 	confluent.metadata.server.cluster.registry.clusters = []
benchi-kafka     | 	confluent.metadata.throttle.pre.check.for.non.wildcard.requests.enable = false
benchi-kafka     | 	confluent.metadata.throttle.pre.check.for.wildcard.requests.enable = false
benchi-kafka     | 	confluent.metrics.reporter.bootstrap.servers = localhost:9092
benchi-kafka     | 	confluent.min.acks = 0
benchi-kafka     | 	confluent.min.connection.throttle.ms = 0
benchi-kafka     | 	confluent.min.segment.ms = 1
benchi-kafka     | 	confluent.missing.id.cache.ttl.sec = 60
benchi-kafka     | 	confluent.missing.id.query.range = 20000
benchi-kafka     | 	confluent.missing.schema.cache.ttl.sec = 60
benchi-kafka     | 	confluent.mtls.enable = false
benchi-kafka     | 	confluent.mtls.listener.name = EXTERNAL
benchi-kafka     | 	confluent.mtls.sasl.authenticator.request.max.bytes = 104857600
benchi-kafka     | 	confluent.mtls.truststore.manager.class.name = null
benchi-kafka     | 	confluent.multitenant.authorizer.enable.acl.state = false
benchi-kafka     | 	confluent.multitenant.interceptor.balancer.apis.enabled = false
benchi-kafka     | 	confluent.multitenant.interceptor.collect.client.apiversions.max.per.tenant = 1000
benchi-kafka     | 	confluent.multitenant.interceptor.collect.client.apiversions.metric = false
benchi-kafka     | 	confluent.multitenant.listener.hostname.cluster.prefix.enable = false
benchi-kafka     | 	confluent.multitenant.listener.hostname.subdomain.suffix.enable = false
benchi-kafka     | 	confluent.multitenant.listener.names = null
benchi-kafka     | 	confluent.multitenant.parse.lkc.id.enable = false
benchi-kafka     | 	confluent.multitenant.parse.sni.host.name.enable = false
benchi-kafka     | 	confluent.network.health.manager.enabled = false
benchi-kafka     | 	confluent.network.health.manager.external.listener.name = EXTERNAL
benchi-kafka     | 	confluent.network.health.manager.externalconnectivitystartup.enabled = false
benchi-kafka     | 	confluent.network.health.manager.min.healthy.network.samples = 3
benchi-kafka     | 	confluent.network.health.manager.min.percentage.healthy.network.samples = 3
benchi-kafka     | 	confluent.network.health.manager.mitigation.enabled = false
benchi-kafka     | 	confluent.network.health.manager.network.sample.window.size = 120
benchi-kafka     | 	confluent.network.health.manager.sample.duration.ms = 1000
benchi-kafka     | 	confluent.oauth.flat.networking.verification.enable = false
benchi-kafka     | 	confluent.offsets.log.cleaner.delete.retention.ms = 86400000
benchi-kafka     | 	confluent.offsets.log.cleaner.max.compaction.lag.ms = 9223372036854775807
benchi-kafka     | 	confluent.offsets.log.cleaner.min.cleanable.dirty.ratio = 0.5
benchi-kafka     | 	confluent.offsets.topic.placement.constraints = 
benchi-kafka     | 	confluent.omit.network.processor.metric.tag = false
benchi-kafka     | 	confluent.operator.managed = false
benchi-kafka     | 	confluent.password.encoder.old.secret.ttl.ms = 9223372036854775807
benchi-kafka     | 	confluent.plugins.cluster.link.policy.max.destination.links.per.tenant = 10
benchi-kafka     | 	confluent.plugins.cluster.link.policy.max.source.links.per.tenant = 10
benchi-kafka     | 	confluent.plugins.topic.policy.max.partitions.per.cluster = 2147483647
benchi-kafka     | 	confluent.plugins.topic.policy.max.partitions.per.tenant = 512
benchi-kafka     | 	confluent.plugins.topic.policy.max.replicas.per.broker = 2147483647
benchi-kafka     | 	confluent.plugins.topic.policy.max.topics.per.cluster = 2147483647
benchi-kafka     | 	confluent.prefer.tier.fetch.ms = -1
benchi-kafka     | 	confluent.produce.throttle.pre.check.enable = false
benchi-kafka     | 	confluent.producer.id.cache.broker.hard.limit = -1
benchi-kafka     | 	confluent.producer.id.cache.eviction.minimal.expiration.ms = 900000
benchi-kafka     | 	confluent.producer.id.cache.extra.eviction.percentage = 0
benchi-kafka     | 	confluent.producer.id.cache.limit = 2147483647
benchi-kafka     | 	confluent.producer.id.cache.partition.hard.limit = -1
benchi-kafka     | 	confluent.producer.id.cache.tenant.hard.limit = -1
benchi-kafka     | 	confluent.producer.id.quota.manager.enable = false
benchi-kafka     | 	confluent.producer.id.quota.window.num = 11
benchi-kafka     | 	confluent.producer.id.quota.window.size.seconds = 1
benchi-kafka     | 	confluent.producer.id.throttle.enable = false
benchi-kafka     | 	confluent.producer.id.throttle.enable.threshold.percentage = 100
benchi-kafka     | 	confluent.proxy.mode.local.default = false
benchi-kafka     | 	confluent.proxy.protocol.fallback.enabled = false
benchi-kafka     | 	confluent.proxy.protocol.version = NONE
benchi-kafka     | 	confluent.quota.computing.usage.adjustment = 0.5
benchi-kafka     | 	confluent.quota.dynamic.enable = false
benchi-kafka     | 	confluent.quota.dynamic.publishing.interval.ms = 60000
benchi-kafka     | 	confluent.quota.dynamic.reporting.interval.ms = 30000
benchi-kafka     | 	confluent.quota.dynamic.reporting.min.usage = 102400
benchi-kafka     | 	confluent.quota.tenant.broker.max.consumer.rate = 13107200
benchi-kafka     | 	confluent.quota.tenant.broker.max.producer.rate = 13107200
benchi-kafka     | 	confluent.quota.tenant.default.controller.mutation.rate = 2.147483647E9
benchi-kafka     | 	confluent.quota.tenant.default.producer.id.rate = 2.147483647E9
benchi-kafka     | 	confluent.quota.tenant.fetch.multiplier = 1.0
benchi-kafka     | 	confluent.quota.tenant.follower.broker.min.consumer.rate = 10485760
benchi-kafka     | 	confluent.quota.tenant.follower.broker.min.producer.rate = 10485760
benchi-kafka     | 	confluent.quota.tenant.produce.multiplier = 1.0
benchi-kafka     | 	confluent.quota.tenant.user.quotas.enable = false
benchi-kafka     | 	confluent.regional.metadata.client.class = null
benchi-kafka     | 	confluent.regional.resource.manager.client.scheduler.threads = 2
benchi-kafka     | 	confluent.regional.resource.manager.endpoint = null
benchi-kafka     | 	confluent.regional.resource.manager.watch.endpoint = null
benchi-kafka     | 	confluent.replica.fetch.backoff.max.ms = 1000
benchi-kafka     | 	confluent.replica.fetch.connections.mode = combined
benchi-kafka     | 	confluent.replication.mode = PULL
benchi-kafka     | 	confluent.replication.push.feature.enable = false
benchi-kafka     | 	confluent.reporters.telemetry.auto.enable = true
benchi-kafka     | 	confluent.request.log.filter.class = class org.apache.kafka.common.requests.SamplingRequestLogFilter
benchi-kafka     | 	confluent.request.pipelining.enable = true
benchi-kafka     | 	confluent.request.pipelining.max.in.flight.requests.per.connection = 5
benchi-kafka     | 	confluent.require.calling.resource.identity = false
benchi-kafka     | 	confluent.require.compatible.keystore.updates = true
benchi-kafka     | 	confluent.require.confluent.issuer = false
benchi-kafka     | 	confluent.roll.check.interval.ms = 300000
benchi-kafka     | 	confluent.schema.registry.max.cache.size = 10000
benchi-kafka     | 	confluent.schema.registry.max.retries = 1
benchi-kafka     | 	confluent.schema.registry.retries.wait.ms = 0
benchi-kafka     | 	confluent.schema.registry.url = null
benchi-kafka     | 	confluent.schema.validator.interceptor.class = io.confluent.kafka.schemaregistry.validator.RecordSchemaValidator
benchi-kafka     | 	confluent.schema.validator.multitenant.enable = false
benchi-kafka     | 	confluent.schema.validator.samples.per.min = 0
benchi-kafka     | 	confluent.security.bc.approved.mode.enable = false
benchi-kafka     | 	confluent.security.event.logger.authentication.enable = false
benchi-kafka     | 	confluent.security.event.logger.authentication.event.rate.limit = -1
benchi-kafka     | 	confluent.security.event.logger.authorization.event.rate.limit = -1
benchi-kafka     | 	confluent.security.event.logger.detailed.audit.logs.filter.class = class org.apache.kafka.common.requests.DetailedRequestAuditLogFilter
benchi-kafka     | 	confluent.security.event.logger.enable = true
benchi-kafka     | 	confluent.security.event.logger.kafka.request.rate.limit = -1
benchi-kafka     | 	confluent.security.event.logger.physical.cluster.id = 
benchi-kafka     | 	confluent.security.event.router.config = 
benchi-kafka     | 	confluent.security.revoked.certificate.ids = 
benchi-kafka     | 	confluent.segment.eager.roll.enable = false
benchi-kafka     | 	confluent.segment.speculative.prefetch.enable = false
benchi-kafka     | 	confluent.spiffe.id.principal.extraction.rules = 
benchi-kafka     | 	confluent.ssl.key.password = null
benchi-kafka     | 	confluent.ssl.keystore.location = null
benchi-kafka     | 	confluent.ssl.keystore.password = null
benchi-kafka     | 	confluent.ssl.keystore.type = null
benchi-kafka     | 	confluent.ssl.protocol = null
benchi-kafka     | 	confluent.ssl.truststore.location = null
benchi-kafka     | 	confluent.ssl.truststore.password = null
benchi-kafka     | 	confluent.ssl.truststore.type = null
benchi-kafka     | 	confluent.step.connection.rate.per.ip = -1.0
benchi-kafka     | 	confluent.step.connection.rate.per.tenant = -1.0
benchi-kafka     | 	confluent.storage.probe.period.ms = -1
benchi-kafka     | 	confluent.storage.probe.slow.write.threshold.ms = 5000
benchi-kafka     | 	confluent.stray.log.delete.delay.ms = 604800000
benchi-kafka     | 	confluent.stray.log.max.deletions.per.run = 72
benchi-kafka     | 	confluent.subdomain.prefix = null
benchi-kafka     | 	confluent.subdomain.separator.map = null
benchi-kafka     | 	confluent.subdomain.separator.variable = %sep
benchi-kafka     | 	confluent.system.time.roll.enable = false
benchi-kafka     | 	confluent.telemetry.enabled = false
benchi-kafka     | 	confluent.telemetry.external.client.metrics.push.enabled = false
benchi-kafka     | 	confluent.tenant.latency.metric.enabled = false
benchi-kafka     | 	confluent.tier.archiver.num.threads = 2
benchi-kafka     | 	confluent.tier.azure.block.blob.auto.abort.threshold.bytes = 500000
benchi-kafka     | 	confluent.tier.azure.block.blob.container = null
benchi-kafka     | 	confluent.tier.azure.block.blob.cred.file.path = null
benchi-kafka     | 	confluent.tier.azure.block.blob.endpoint = null
benchi-kafka     | 	confluent.tier.azure.block.blob.prefix = 
benchi-kafka     | 	confluent.tier.backend = 
benchi-kafka     | 	confluent.tier.bucket.probe.period.ms = -1
benchi-kafka     | 	confluent.tier.cleaner.compact.min.efficiency = 0.5
benchi-kafka     | 	confluent.tier.cleaner.compact.segment.min.bytes = 20971520
benchi-kafka     | 	confluent.tier.cleaner.dedupe.buffer.size = 134217728
benchi-kafka     | 	confluent.tier.cleaner.dual.compaction = false
benchi-kafka     | 	confluent.tier.cleaner.dual.compaction.validation.max.bytes = 1073741824
benchi-kafka     | 	confluent.tier.cleaner.dual.compaction.validation.percent = 0
benchi-kafka     | 	confluent.tier.cleaner.enable = false
benchi-kafka     | 	confluent.tier.cleaner.excluded.topics = [^_confluent.*]
benchi-kafka     | 	confluent.tier.cleaner.feature.enable = false
benchi-kafka     | 	confluent.tier.cleaner.io.buffer.load.factor = 0.9
benchi-kafka     | 	confluent.tier.cleaner.io.buffer.size = 10485760
benchi-kafka     | 	confluent.tier.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
benchi-kafka     | 	confluent.tier.cleaner.min.cleanable.ratio = 0.75
benchi-kafka     | 	confluent.tier.cleaner.num.threads = 2
benchi-kafka     | 	confluent.tier.enable = false
benchi-kafka     | 	confluent.tier.feature = false
benchi-kafka     | 	confluent.tier.fenced.segment.delete.delay.ms = 600000
benchi-kafka     | 	confluent.tier.fetcher.async.enable = false
benchi-kafka     | 	confluent.tier.fetcher.async.timestamp.offset.parallelism = 1
benchi-kafka     | 	confluent.tier.fetcher.fetch.based.on.segment_and_metadata_layout.field = false
benchi-kafka     | 	confluent.tier.fetcher.memorypool.bytes = 0
benchi-kafka     | 	confluent.tier.fetcher.num.threads = 4
benchi-kafka     | 	confluent.tier.fetcher.offset.cache.expiration.ms = 1800000
benchi-kafka     | 	confluent.tier.fetcher.offset.cache.period.ms = 60000
benchi-kafka     | 	confluent.tier.fetcher.offset.cache.size = 200000
benchi-kafka     | 	confluent.tier.gcs.bucket = null
benchi-kafka     | 	confluent.tier.gcs.cred.file.path = null
benchi-kafka     | 	confluent.tier.gcs.prefix = 
benchi-kafka     | 	confluent.tier.gcs.region = null
benchi-kafka     | 	confluent.tier.gcs.sse.customer.encryption.key = null
benchi-kafka     | 	confluent.tier.gcs.write.chunk.size = 0
benchi-kafka     | 	confluent.tier.local.hotset.bytes = -1
benchi-kafka     | 	confluent.tier.local.hotset.ms = 86400000
benchi-kafka     | 	confluent.tier.max.partition.fetch.bytes.override = 0
benchi-kafka     | 	confluent.tier.metadata.bootstrap.servers = null
benchi-kafka     | 	confluent.tier.metadata.max.poll.ms = 100
benchi-kafka     | 	confluent.tier.metadata.namespace = null
benchi-kafka     | 	confluent.tier.metadata.num.partitions = 50
benchi-kafka     | 	confluent.tier.metadata.replication.factor = 1
benchi-kafka     | 	confluent.tier.metadata.request.timeout.ms = 30000
benchi-kafka     | 	confluent.tier.metadata.snapshots.enable = false
benchi-kafka     | 	confluent.tier.metadata.snapshots.interval.ms = 86400000
benchi-kafka     | 	confluent.tier.metadata.snapshots.retention.days = 7
benchi-kafka     | 	confluent.tier.metadata.snapshots.threads = 2
benchi-kafka     | 	confluent.tier.object.fetcher.num.threads = 1
benchi-kafka     | 	confluent.tier.partition.state.cleanup.delay.ms = 2592000000
benchi-kafka     | 	confluent.tier.partition.state.cleanup.enable = false
benchi-kafka     | 	confluent.tier.partition.state.cleanup.interval.ms = 86400000
benchi-kafka     | 	confluent.tier.partition.state.commit.interval.ms = 15000
benchi-kafka     | 	confluent.tier.s3.assumerole.arn = null
benchi-kafka     | 	confluent.tier.s3.auto.abort.threshold.bytes = 500000
benchi-kafka     | 	confluent.tier.s3.aws.endpoint.override = null
benchi-kafka     | 	confluent.tier.s3.aws.signer.override = null
benchi-kafka     | 	confluent.tier.s3.bucket = null
benchi-kafka     | 	confluent.tier.s3.cred.file.path = null
benchi-kafka     | 	confluent.tier.s3.force.path.style.access = false
benchi-kafka     | 	confluent.tier.s3.prefix = 
benchi-kafka     | 	confluent.tier.s3.region = null
benchi-kafka     | 	confluent.tier.s3.security.providers = null
benchi-kafka     | 	confluent.tier.s3.sse.algorithm = AES256
benchi-kafka     | 	confluent.tier.s3.sse.customer.encryption.key = null
benchi-kafka     | 	confluent.tier.s3.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	confluent.tier.s3.ssl.key.password = null
benchi-kafka     | 	confluent.tier.s3.ssl.keystore.location = null
benchi-kafka     | 	confluent.tier.s3.ssl.keystore.password = null
benchi-kafka     | 	confluent.tier.s3.ssl.keystore.type = null
benchi-kafka     | 	confluent.tier.s3.ssl.protocol = TLSv1.3
benchi-kafka     | 	confluent.tier.s3.ssl.provider = null
benchi-kafka     | 	confluent.tier.s3.ssl.truststore.location = null
benchi-kafka     | 	confluent.tier.s3.ssl.truststore.password = null
benchi-kafka     | 	confluent.tier.s3.ssl.truststore.type = null
benchi-kafka     | 	confluent.tier.s3.user.agent.prefix = APN/1.0 Confluent/1.0 TieredStorageS3/1.0
benchi-kafka     | 	confluent.tier.segment.hotset.roll.min.bytes = 104857600
benchi-kafka     | 	confluent.tier.segment.metadata.layout.put.mode = LegacyMultiObject
benchi-kafka     | 	confluent.tier.topic.data.loss.validation.fencing.enable = false
benchi-kafka     | 	confluent.tier.topic.delete.backoff.ms = 21600000
benchi-kafka     | 	confluent.tier.topic.delete.check.interval.ms = 300000
benchi-kafka     | 	confluent.tier.topic.delete.max.inprogress.partitions = 100
benchi-kafka     | 	confluent.tier.topic.head.data.loss.validation.enable = true
benchi-kafka     | 	confluent.tier.topic.head.data.loss.validation.max.timeout.ms = 900000
benchi-kafka     | 	confluent.tier.topic.materialization.from.snapshot.enable = false
benchi-kafka     | 	confluent.tier.topic.producer.enable.idempotence = true
benchi-kafka     | 	confluent.tier.topic.snapshots.enable = false
benchi-kafka     | 	confluent.tier.topic.snapshots.interval.ms = 300000
benchi-kafka     | 	confluent.tier.topic.snapshots.max.records = 100000
benchi-kafka     | 	confluent.tier.topic.snapshots.retention.hours = 168
benchi-kafka     | 	confluent.topic.partition.default.placement = 2
benchi-kafka     | 	confluent.topic.policy.use.computed.assignments = false
benchi-kafka     | 	confluent.topic.replica.assignor.builder.class = 
benchi-kafka     | 	confluent.track.api.key.per.ip = false
benchi-kafka     | 	confluent.track.per.ip.max.size = 100000
benchi-kafka     | 	confluent.track.tenant.id.per.ip = false
benchi-kafka     | 	confluent.traffic.cdc.network.id.routes.enable = false
benchi-kafka     | 	confluent.traffic.cdc.network.id.routes.listener.names = EXTERNAL_BACKCHANNEL
benchi-kafka     | 	confluent.traffic.cdc.network.id.routes.periodic.start.task.ms = 300000
benchi-kafka     | 	confluent.traffic.cdc.network.id.routes.topic.name = _confluent-network_id_routes
benchi-kafka     | 	confluent.traffic.network.id = 
benchi-kafka     | 	confluent.transaction.2pc.timeout.ms = -1
benchi-kafka     | 	confluent.transaction.logging.verbosity = 0
benchi-kafka     | 	confluent.transaction.state.log.placement.constraints = 
benchi-kafka     | 	confluent.valid.broker.rack.set = null
benchi-kafka     | 	confluent.verify.group.subscription.prefix = false
benchi-kafka     | 	confluent.virtual.topic.creation.enabled = false
benchi-kafka     | 	confluent.zookeeper.metadata.migration.controller.check.disable = false
benchi-kafka     | 	confluent.zookeeper.metadata.migration.trigger.file.path = null
benchi-kafka     | 	connection.failed.authentication.delay.ms = 100
benchi-kafka     | 	connection.min.expire.interval.ms = 250
benchi-kafka     | 	connections.max.age.ms = 3153600000000
benchi-kafka     | 	connections.max.idle.ms = 600000
benchi-kafka     | 	connections.max.reauth.ms = 0
benchi-kafka     | 	control.plane.listener.name = null
benchi-kafka     | 	controlled.shutdown.enable = true
benchi-kafka     | 	controlled.shutdown.max.retries = 3
benchi-kafka     | 	controlled.shutdown.retry.backoff.ms = 5000
benchi-kafka     | 	controller.listener.names = CONTROLLER
benchi-kafka     | 	controller.quorum.append.linger.ms = 25
benchi-kafka     | 	controller.quorum.bootstrap.servers = []
benchi-kafka     | 	controller.quorum.election.backoff.max.ms = 1000
benchi-kafka     | 	controller.quorum.election.timeout.ms = 1000
benchi-kafka     | 	controller.quorum.fetch.timeout.ms = 2000
benchi-kafka     | 	controller.quorum.request.timeout.ms = 2000
benchi-kafka     | 	controller.quorum.retry.backoff.ms = 20
benchi-kafka     | 	controller.quorum.voters = [1@broker:29093]
benchi-kafka     | 	controller.quota.window.num = 11
benchi-kafka     | 	controller.quota.window.size.seconds = 1
benchi-kafka     | 	controller.socket.timeout.ms = 30000
benchi-kafka     | 	create.cluster.link.policy.class.name = null
benchi-kafka     | 	create.topic.policy.class.name = null
benchi-kafka     | 	default.replication.factor = 1
benchi-kafka     | 	delegation.token.expiry.check.interval.ms = 3600000
benchi-kafka     | 	delegation.token.expiry.time.ms = 86400000
benchi-kafka     | 	delegation.token.master.key = null
benchi-kafka     | 	delegation.token.max.lifetime.ms = 604800000
benchi-kafka     | 	delegation.token.secret.key = null
benchi-kafka     | 	delete.records.purgatory.purge.interval.requests = 1
benchi-kafka     | 	delete.topic.enable = true
benchi-kafka     | 	early.start.listeners = null
benchi-kafka     | 	eligible.leader.replicas.enable = false
benchi-kafka     | 	enable.fips = false
benchi-kafka     | 	fetch.max.bytes = 57671680
benchi-kafka     | 	fetch.purgatory.purge.interval.requests = 1000
benchi-kafka     | 	floor.max.connection.creation.rate = null
benchi-kafka     | 	follower.replication.throttled.rate = 9223372036854775807
benchi-kafka     | 	follower.replication.throttled.replicas = none
benchi-kafka     | 	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
benchi-kafka     | 	group.consumer.heartbeat.interval.ms = 5000
benchi-kafka     | 	group.consumer.max.heartbeat.interval.ms = 15000
benchi-kafka     | 	group.consumer.max.session.timeout.ms = 60000
benchi-kafka     | 	group.consumer.max.size = 2147483647
benchi-kafka     | 	group.consumer.migration.policy = disabled
benchi-kafka     | 	group.consumer.min.heartbeat.interval.ms = 5000
benchi-kafka     | 	group.consumer.min.session.timeout.ms = 45000
benchi-kafka     | 	group.consumer.session.timeout.ms = 45000
benchi-kafka     | 	group.coordinator.append.linger.ms = 10
benchi-kafka     | 	group.coordinator.new.enable = false
benchi-kafka     | 	group.coordinator.rebalance.protocols = [classic]
benchi-kafka     | 	group.coordinator.threads = 1
benchi-kafka     | 	group.initial.rebalance.delay.ms = 0
benchi-kafka     | 	group.max.session.timeout.ms = 1800000
benchi-kafka     | 	group.max.size = 2147483647
benchi-kafka     | 	group.min.session.timeout.ms = 6000
benchi-kafka     | 	initial.broker.registration.timeout.ms = 60000
benchi-kafka     | 	inter.broker.listener.name = PLAINTEXT
benchi-kafka     | 	inter.broker.protocol.version = 3.8-IV0A
benchi-kafka     | 	k2.stack.builder.class.name = null
benchi-kafka     | 	k2.startup.timeout.ms = 60000
benchi-kafka     | 	k2.topic.metadata.max.total.partition.count = 20000
benchi-kafka     | 	k2.topic.metadata.refresh.ms = 10000
benchi-kafka     | 	kafka.metrics.polling.interval.secs = 10
benchi-kafka     | 	kafka.metrics.reporters = []
benchi-kafka     | 	leader.imbalance.check.interval.seconds = 300
benchi-kafka     | 	leader.imbalance.per.broker.percentage = 10
benchi-kafka     | 	leader.replication.throttled.rate = 9223372036854775807
benchi-kafka     | 	leader.replication.throttled.replicas = none
benchi-kafka     | 	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
benchi-kafka     | 	listeners = PLAINTEXT://broker:29092,CONTROLLER://broker:29093,PLAINTEXT_HOST://0.0.0.0:9092
benchi-kafka     | 	log.cleaner.backoff.ms = 15000
benchi-kafka     | 	log.cleaner.dedupe.buffer.size = 134217728
benchi-kafka     | 	log.cleaner.delete.retention.ms = 86400000
benchi-kafka     | 	log.cleaner.enable = true
benchi-kafka     | 	log.cleaner.hash.algorithm = MD5
benchi-kafka     | 	log.cleaner.io.buffer.load.factor = 0.9
benchi-kafka     | 	log.cleaner.io.buffer.size = 524288
benchi-kafka     | 	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
benchi-kafka     | 	log.cleaner.max.compaction.lag.ms = 9223372036854775807
benchi-kafka     | 	log.cleaner.min.cleanable.ratio = 0.5
benchi-kafka     | 	log.cleaner.min.compaction.lag.ms = 0
benchi-kafka     | 	log.cleaner.threads = 1
benchi-kafka     | 	log.cleanup.policy = [delete]
benchi-kafka     | 	log.deletion.max.segments.per.run = 2147483647
benchi-kafka     | 	log.deletion.throttler.disk.free.headroom.bytes = 21474836480
benchi-kafka     | 	log.dir = /tmp/kafka-logs
benchi-kafka     | 	log.dir.failure.timeout.ms = 30000
benchi-kafka     | 	log.dirs = /tmp/kraft-combined-logs
benchi-kafka     | 	log.flush.interval.messages = 9223372036854775807
benchi-kafka     | 	log.flush.interval.ms = null
benchi-kafka     | 	log.flush.offset.checkpoint.interval.ms = 60000
benchi-kafka     | 	log.flush.scheduler.interval.ms = 9223372036854775807
benchi-kafka     | 	log.flush.start.offset.checkpoint.interval.ms = 60000
benchi-kafka     | 	log.index.interval.bytes = 4096
benchi-kafka     | 	log.index.size.max.bytes = 10485760
benchi-kafka     | 	log.initial.task.delay.ms = 30000
benchi-kafka     | 	log.local.retention.bytes = -2
benchi-kafka     | 	log.local.retention.ms = -2
benchi-kafka     | 	log.message.downconversion.enable = true
benchi-kafka     | 	log.message.format.version = 3.0-IV1
benchi-kafka     | 	log.message.timestamp.after.max.ms = 9223372036854775807
benchi-kafka     | 	log.message.timestamp.before.max.ms = 9223372036854775807
benchi-kafka     | 	log.message.timestamp.difference.max.ms = 9223372036854775807
benchi-kafka     | 	log.message.timestamp.type = CreateTime
benchi-kafka     | 	log.preallocate = false
benchi-kafka     | 	log.retention.bytes = -1
benchi-kafka     | 	log.retention.check.interval.ms = 300000
benchi-kafka     | 	log.retention.hours = 168
benchi-kafka     | 	log.retention.minutes = null
benchi-kafka     | 	log.retention.ms = null
benchi-kafka     | 	log.roll.hours = 168
benchi-kafka     | 	log.roll.jitter.hours = 0
benchi-kafka     | 	log.roll.jitter.ms = null
benchi-kafka     | 	log.roll.ms = null
benchi-kafka     | 	log.segment.bytes = 1073741824
benchi-kafka     | 	log.segment.delete.delay.ms = 60000
benchi-kafka     | 	max.connection.creation.rate = 1.7976931348623157E308
benchi-kafka     | 	max.connection.creation.rate.per.ip.enable.threshold = 0.0
benchi-kafka     | 	max.connection.creation.rate.per.tenant.enable.threshold = 0.0
benchi-kafka     | 	max.connections = 2147483647
benchi-kafka     | 	max.connections.per.ip = 2147483647
benchi-kafka     | 	max.connections.per.ip.overrides = 
benchi-kafka     | 	max.connections.per.tenant = 0
benchi-kafka     | 	max.connections.protected.listeners = []
benchi-kafka     | 	max.connections.reap.amount = 0
benchi-kafka     | 	max.incremental.fetch.session.cache.slots = 1000
benchi-kafka     | 	max.request.partition.size.limit = 2000
benchi-kafka     | 	message.max.bytes = 1048588
benchi-kafka     | 	metadata.log.dir = null
benchi-kafka     | 	metadata.log.max.record.bytes.between.snapshots = 20971520
benchi-kafka     | 	metadata.log.max.snapshot.interval.ms = 3600000
benchi-kafka     | 	metadata.log.segment.bytes = 1073741824
benchi-kafka     | 	metadata.log.segment.min.bytes = 8388608
benchi-kafka     | 	metadata.log.segment.ms = 604800000
benchi-kafka     | 	metadata.max.idle.interval.ms = 500
benchi-kafka     | 	metadata.max.retention.bytes = 104857600
benchi-kafka     | 	metadata.max.retention.ms = 604800000
benchi-kafka     | 	metric.reporters = [io.confluent.metrics.reporter.ConfluentMetricsReporter]
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.recording.level = INFO
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	min.insync.replicas = 1
benchi-kafka     | 	multitenant.authorizer.support.resource.ids = false
benchi-kafka     | 	multitenant.metadata.class = null
benchi-kafka     | 	multitenant.metadata.dir = null
benchi-kafka     | 	multitenant.metadata.reload.delay.ms = 120000
benchi-kafka     | 	multitenant.metadata.ssl.certs.path = null
benchi-kafka     | 	multitenant.tenant.delete.batch.size = 10
benchi-kafka     | 	multitenant.tenant.delete.check.ms = 120000
benchi-kafka     | 	multitenant.tenant.delete.delay = 604800000
benchi-kafka     | 	node.id = 1
benchi-kafka     | 	num.io.threads = 8
benchi-kafka     | 	num.network.threads = 3
benchi-kafka     | 	num.partitions = 1
benchi-kafka     | 	num.recovery.threads.per.data.dir = 1
benchi-kafka     | 	num.replica.alter.log.dirs.threads = null
benchi-kafka     | 	num.replica.fetchers = 1
benchi-kafka     | 	offset.metadata.max.bytes = 4096
benchi-kafka     | 	offsets.commit.required.acks = -1
benchi-kafka     | 	offsets.commit.timeout.ms = 5000
benchi-kafka     | 	offsets.load.buffer.size = 5242880
benchi-kafka     | 	offsets.retention.check.interval.ms = 600000
benchi-kafka     | 	offsets.retention.minutes = 10080
benchi-kafka     | 	offsets.topic.compression.codec = 0
benchi-kafka     | 	offsets.topic.num.partitions = 50
benchi-kafka     | 	offsets.topic.replication.factor = 1
benchi-kafka     | 	offsets.topic.segment.bytes = 104857600
benchi-kafka     | 	otel.exporter.otlp.custom.endpoint = default
benchi-kafka     | 	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
benchi-kafka     | 	password.encoder.iterations = 4096
benchi-kafka     | 	password.encoder.key.length = 128
benchi-kafka     | 	password.encoder.keyfactory.algorithm = null
benchi-kafka     | 	password.encoder.old.secret = null
benchi-kafka     | 	password.encoder.secret = null
benchi-kafka     | 	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
benchi-kafka     | 	process.roles = [broker, controller]
benchi-kafka     | 	producer.id.expiration.check.interval.ms = 600000
benchi-kafka     | 	producer.id.expiration.ms = 86400000
benchi-kafka     | 	producer.purgatory.purge.interval.requests = 1000
benchi-kafka     | 	queued.max.request.bytes = -1
benchi-kafka     | 	queued.max.requests = 500
benchi-kafka     | 	quota.window.num = 11
benchi-kafka     | 	quota.window.size.seconds = 1
benchi-kafka     | 	quotas.consumption.expiration.time.ms = 600000
benchi-kafka     | 	quotas.expiration.interval.ms = 3600000
benchi-kafka     | 	quotas.expiration.time.ms = 604800000
benchi-kafka     | 	quotas.lazy.evaluation.threshold = 0.5
benchi-kafka     | 	quotas.topic.append.timeout.ms = 5000
benchi-kafka     | 	quotas.topic.compression.codec = 3
benchi-kafka     | 	quotas.topic.load.buffer.size = 5242880
benchi-kafka     | 	quotas.topic.num.partitions = 50
benchi-kafka     | 	quotas.topic.placement.constraints = 
benchi-kafka     | 	quotas.topic.replication.factor = 3
benchi-kafka     | 	quotas.topic.segment.bytes = 104857600
benchi-kafka     | 	remote.fetch.max.wait.ms = 500
benchi-kafka     | 	remote.log.index.file.cache.total.size.bytes = 1073741824
benchi-kafka     | 	remote.log.manager.copier.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.copy.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.copy.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.expiration.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.fetch.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.fetch.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.task.interval.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.max.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.ms = 500
benchi-kafka     | 	remote.log.manager.task.retry.jitter = 0.2
benchi-kafka     | 	remote.log.manager.thread.pool.size = 10
benchi-kafka     | 	remote.log.metadata.custom.metadata.max.bytes = 128
benchi-kafka     | 	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
benchi-kafka     | 	remote.log.metadata.manager.class.path = null
benchi-kafka     | 	remote.log.metadata.manager.impl.prefix = rlmm.config.
benchi-kafka     | 	remote.log.metadata.manager.listener.name = null
benchi-kafka     | 	remote.log.reader.max.pending.tasks = 100
benchi-kafka     | 	remote.log.reader.threads = 10
benchi-kafka     | 	remote.log.storage.manager.class.name = null
benchi-kafka     | 	remote.log.storage.manager.class.path = null
benchi-kafka     | 	remote.log.storage.manager.impl.prefix = rsm.config.
benchi-kafka     | 	remote.log.storage.system.enable = false
benchi-kafka     | 	replica.fetch.backoff.ms = 1000
benchi-kafka     | 	replica.fetch.max.bytes = 1048576
benchi-kafka     | 	replica.fetch.min.bytes = 1
benchi-kafka     | 	replica.fetch.response.max.bytes = 10485760
benchi-kafka     | 	replica.fetch.wait.max.ms = 500
benchi-kafka     | 	replica.high.watermark.checkpoint.interval.ms = 5000
benchi-kafka     | 	replica.lag.time.max.ms = 30000
benchi-kafka     | 	replica.selector.class = null
benchi-kafka     | 	replica.socket.receive.buffer.bytes = 65536
benchi-kafka     | 	replica.socket.timeout.ms = 30000
benchi-kafka     | 	replication.quota.window.num = 11
benchi-kafka     | 	replication.quota.window.size.seconds = 1
benchi-kafka     | 	request.timeout.ms = 30000
benchi-kafka     | 	reserved.broker.max.id = 1000
benchi-kafka     | 	sasl.client.callback.handler.class = null
benchi-kafka     | 	sasl.enabled.mechanisms = [GSSAPI]
benchi-kafka     | 	sasl.jaas.config = null
benchi-kafka     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
benchi-kafka     | 	sasl.kerberos.min.time.before.relogin = 60000
benchi-kafka     | 	sasl.kerberos.principal.to.local.rules = [DEFAULT]
benchi-kafka     | 	sasl.kerberos.service.name = null
benchi-kafka     | 	sasl.kerberos.ticket.renew.jitter = 0.05
benchi-kafka     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
benchi-kafka     | 	sasl.login.callback.handler.class = null
benchi-kafka     | 	sasl.login.class = null
benchi-kafka     | 	sasl.login.connect.timeout.ms = null
benchi-kafka     | 	sasl.login.read.timeout.ms = null
benchi-kafka     | 	sasl.login.refresh.buffer.seconds = 300
benchi-kafka     | 	sasl.login.refresh.min.period.seconds = 60
benchi-kafka     | 	sasl.login.refresh.window.factor = 0.8
benchi-kafka     | 	sasl.login.refresh.window.jitter = 0.05
benchi-kafka     | 	sasl.login.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.login.retry.backoff.ms = 100
benchi-kafka     | 	sasl.mechanism.controller.protocol = GSSAPI
benchi-kafka     | 	sasl.mechanism.inter.broker.protocol = GSSAPI
benchi-kafka     | 	sasl.oauthbearer.clientassertion.audience = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.expiration = 5
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.jti.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.nbf.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.issuer = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.location = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key.passphrase = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.subject = null
benchi-kafka     | 	sasl.oauthbearer.clock.skew.seconds = 30
benchi-kafka     | 	sasl.oauthbearer.expected.audience = null
benchi-kafka     | 	sasl.oauthbearer.expected.issuer = null
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.url = null
benchi-kafka     | 	sasl.oauthbearer.scope.claim.name = scope
benchi-kafka     | 	sasl.oauthbearer.sub.claim.name = sub
benchi-kafka     | 	sasl.oauthbearer.token.endpoint.url = null
benchi-kafka     | 	sasl.server.authn.async.enable = false
benchi-kafka     | 	sasl.server.authn.async.max.threads = 1
benchi-kafka     | 	sasl.server.authn.async.timeout.ms = 30000
benchi-kafka     | 	sasl.server.callback.handler.class = null
benchi-kafka     | 	sasl.server.max.receive.size = 524288
benchi-kafka     | 	security.inter.broker.protocol = PLAINTEXT
benchi-kafka     | 	security.providers = null
benchi-kafka     | 	server.max.startup.time.ms = 9223372036854775807
benchi-kafka     | 	socket.connection.setup.timeout.max.ms = 30000
benchi-kafka     | 	socket.connection.setup.timeout.ms = 10000
benchi-kafka     | 	socket.listen.backlog.size = 50
benchi-kafka     | 	socket.receive.buffer.bytes = 102400
benchi-kafka     | 	socket.request.max.bytes = 104857600
benchi-kafka     | 	socket.send.buffer.bytes = 102400
benchi-kafka     | 	ssl.allow.dn.changes = false
benchi-kafka     | 	ssl.allow.san.changes = false
benchi-kafka     | 	ssl.cipher.suites = []
benchi-kafka     | 	ssl.client.auth = none
benchi-kafka     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	ssl.engine.factory.class = null
benchi-kafka     | 	ssl.key.password = null
benchi-kafka     | 	ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	ssl.keystore.certificate.chain = null
benchi-kafka     | 	ssl.keystore.key = null
benchi-kafka     | 	ssl.keystore.location = null
benchi-kafka     | 	ssl.keystore.password = null
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.principal.mapping.rules = DEFAULT
benchi-kafka     | 	ssl.protocol = TLSv1.3
benchi-kafka     | 	ssl.provider = null
benchi-kafka     | 	ssl.secure.random.implementation = null
benchi-kafka     | 	ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	ssl.truststore.certificates = null
benchi-kafka     | 	ssl.truststore.location = null
benchi-kafka     | 	ssl.truststore.password = null
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     | 	telemetry.max.bytes = 1048576
benchi-kafka     | 	throughput.quota.window.num = 11
benchi-kafka     | 	token.impersonation.validation = true
benchi-kafka     | 	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
benchi-kafka     | 	transaction.max.timeout.ms = 900000
benchi-kafka     | 	transaction.metadata.load.threads = 32
benchi-kafka     | 	transaction.partition.verification.enable = true
benchi-kafka     | 	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
benchi-kafka     | 	transaction.state.log.load.buffer.size = 5242880
benchi-kafka     | 	transaction.state.log.min.isr = 1
benchi-kafka     | 	transaction.state.log.num.partitions = 50
benchi-kafka     | 	transaction.state.log.replication.factor = 1
benchi-kafka     | 	transaction.state.log.segment.bytes = 104857600
benchi-kafka     | 	transactional.id.expiration.ms = 604800000
benchi-kafka     | 	unclean.leader.election.enable = false
benchi-kafka     | 	unstable.api.versions.enable = false
benchi-kafka     | 	unstable.feature.versions.enable = false
benchi-kafka     | 	zookeeper.acl.change.notification.expiration.ms = 900000
benchi-kafka     | 	zookeeper.clientCnxnSocket = null
benchi-kafka     | 	zookeeper.connect = 
benchi-kafka     | 	zookeeper.connection.timeout.ms = null
benchi-kafka     | 	zookeeper.max.in.flight.requests = 10
benchi-kafka     | 	zookeeper.metadata.migration.enable = false
benchi-kafka     | 	zookeeper.metadata.migration.min.batch.size = 200
benchi-kafka     | 	zookeeper.session.timeout.ms = 18000
benchi-kafka     | 	zookeeper.set.acl = false
benchi-kafka     | 	zookeeper.ssl.cipher.suites = null
benchi-kafka     | 	zookeeper.ssl.client.enable = false
benchi-kafka     | 	zookeeper.ssl.crl.enable = false
benchi-kafka     | 	zookeeper.ssl.enabled.protocols = null
benchi-kafka     | 	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
benchi-kafka     | 	zookeeper.ssl.keystore.location = null
benchi-kafka     | 	zookeeper.ssl.keystore.password = null
benchi-kafka     | 	zookeeper.ssl.keystore.type = null
benchi-kafka     | 	zookeeper.ssl.ocsp.enable = false
benchi-kafka     | 	zookeeper.ssl.protocol = TLSv1.2
benchi-kafka     | 	zookeeper.ssl.truststore.location = null
benchi-kafka     | 	zookeeper.ssl.truststore.password = null
benchi-kafka     | 	zookeeper.ssl.truststore.type = null
benchi-kafka     |  (kafka.server.KafkaConfig)
benchi-kafka     | [2025-03-18 16:03:19,087] INFO RemoteLogManagerConfig values: 
benchi-kafka     | 	log.local.retention.bytes = -2
benchi-kafka     | 	log.local.retention.ms = -2
benchi-kafka     | 	remote.fetch.max.wait.ms = 500
benchi-kafka     | 	remote.log.index.file.cache.total.size.bytes = 1073741824
benchi-kafka     | 	remote.log.manager.copier.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.copy.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.copy.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.expiration.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.fetch.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.fetch.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.task.interval.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.max.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.ms = 500
benchi-kafka     | 	remote.log.manager.task.retry.jitter = 0.2
benchi-kafka     | 	remote.log.manager.thread.pool.size = 10
benchi-kafka     | 	remote.log.metadata.custom.metadata.max.bytes = 128
benchi-kafka     | 	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
benchi-kafka     | 	remote.log.metadata.manager.class.path = null
benchi-kafka     | 	remote.log.metadata.manager.impl.prefix = rlmm.config.
benchi-kafka     | 	remote.log.metadata.manager.listener.name = null
benchi-kafka     | 	remote.log.reader.max.pending.tasks = 100
benchi-kafka     | 	remote.log.reader.threads = 10
benchi-kafka     | 	remote.log.storage.manager.class.name = null
benchi-kafka     | 	remote.log.storage.manager.class.path = null
benchi-kafka     | 	remote.log.storage.manager.impl.prefix = rsm.config.
benchi-kafka     | 	remote.log.storage.system.enable = false
benchi-kafka     |  (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
benchi-kafka     | [2025-03-18 16:03:19,100] INFO RemoteLogManagerConfig values: 
benchi-kafka     | 	log.local.retention.bytes = -2
benchi-kafka     | 	log.local.retention.ms = -2
benchi-kafka     | 	remote.fetch.max.wait.ms = 500
benchi-kafka     | 	remote.log.index.file.cache.total.size.bytes = 1073741824
benchi-kafka     | 	remote.log.manager.copier.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.copy.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.copy.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.expiration.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.fetch.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.fetch.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.task.interval.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.max.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.ms = 500
benchi-kafka     | 	remote.log.manager.task.retry.jitter = 0.2
benchi-kafka     | 	remote.log.manager.thread.pool.size = 10
benchi-kafka     | 	remote.log.metadata.custom.metadata.max.bytes = 128
benchi-kafka     | 	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
benchi-kafka     | 	remote.log.metadata.manager.class.path = null
benchi-kafka     | 	remote.log.metadata.manager.impl.prefix = rlmm.config.
benchi-kafka     | 	remote.log.metadata.manager.listener.name = null
benchi-kafka     | 	remote.log.reader.max.pending.tasks = 100
benchi-kafka     | 	remote.log.reader.threads = 10
benchi-kafka     | 	remote.log.storage.manager.class.name = null
benchi-kafka     | 	remote.log.storage.manager.class.path = null
benchi-kafka     | 	remote.log.storage.manager.impl.prefix = rsm.config.
benchi-kafka     | 	remote.log.storage.system.enable = false
benchi-kafka     |  (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
benchi-kafka     | [2025-03-18 16:03:19,103] INFO KafkaConfig values: 
benchi-kafka     | 	advertised.listeners = PLAINTEXT://broker:29092,PLAINTEXT_HOST://localhost:9092
benchi-kafka     | 	alter.config.policy.class.name = null
benchi-kafka     | 	alter.log.dirs.replication.quota.window.num = 11
benchi-kafka     | 	alter.log.dirs.replication.quota.window.size.seconds = 1
benchi-kafka     | 	authorizer.class.name = 
benchi-kafka     | 	auto.create.topics.enable = true
benchi-kafka     | 	auto.include.jmx.reporter = true
benchi-kafka     | 	auto.leader.rebalance.enable = true
benchi-kafka     | 	background.threads = 10
benchi-kafka     | 	broker.heartbeat.interval.ms = 2000
benchi-kafka     | 	broker.id = 1
benchi-kafka     | 	broker.id.generation.enable = true
benchi-kafka     | 	broker.interceptor.class = class org.apache.kafka.server.interceptor.DefaultBrokerInterceptor
benchi-kafka     | 	broker.rack = null
benchi-kafka     | 	broker.session.timeout.ms = 9000
benchi-kafka     | 	broker.session.uuid = 22eDK4SjR7GWVEmuEgKyNQ
benchi-kafka     | 	client.quota.callback.class = null
benchi-kafka     | 	client.quota.max.throttle.time.ms = 5000
benchi-kafka     | 	compression.gzip.level = -1
benchi-kafka     | 	compression.lz4.level = 9
benchi-kafka     | 	compression.type = producer
benchi-kafka     | 	compression.zstd.level = 3
benchi-kafka     | 	confluent.alter.broker.health.max.demoted.brokers = 2147483647
benchi-kafka     | 	confluent.alter.broker.health.max.demoted.brokers.percentage = 0
benchi-kafka     | 	confluent.ansible.managed = false
benchi-kafka     | 	confluent.api.visibility = DEFAULT
benchi-kafka     | 	confluent.append.record.interceptor.classes = []
benchi-kafka     | 	confluent.apply.create.topic.policy.to.create.partitions = false
benchi-kafka     | 	confluent.authorizer.authority.name = 
benchi-kafka     | 	confluent.automatic.alter.broker.health.retry.backoff.ms = 2000
benchi-kafka     | 	confluent.backpressure.disk.enable = false
benchi-kafka     | 	confluent.backpressure.disk.free.threshold.bytes = 21474836480
benchi-kafka     | 	confluent.backpressure.disk.produce.bytes.per.second = 131072
benchi-kafka     | 	confluent.backpressure.disk.threshold.recovery.factor = 1.5
benchi-kafka     | 	confluent.backpressure.request.min.broker.limit = 200
benchi-kafka     | 	confluent.backpressure.request.queue.size.percentile = p95
benchi-kafka     | 	confluent.backpressure.types = null
benchi-kafka     | 	confluent.balancer.api.state.topic = _confluent_balancer_api_state
benchi-kafka     | 	confluent.balancer.broker.addition.elapsed.time.ms.completion.threshold = 57600000
benchi-kafka     | 	confluent.balancer.broker.addition.mean.cpu.percent.completion.threshold = 0.5
benchi-kafka     | 	confluent.balancer.capacity.threshold.upper.limit = 0.95
benchi-kafka     | 	confluent.balancer.cell.load.upper.bound = 0.7
benchi-kafka     | 	confluent.balancer.cell.overload.detection.interval.ms = 3600000
benchi-kafka     | 	confluent.balancer.cell.overload.duration.ms = 86400000
benchi-kafka     | 	confluent.balancer.class = io.confluent.databalancer.SbcDataBalanceManager
benchi-kafka     | 	confluent.balancer.consumer.out.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.balancer.cpu.balance.threshold = 1.1
benchi-kafka     | 	confluent.balancer.cpu.low.utilization.threshold = 0.2
benchi-kafka     | 	confluent.balancer.cpu.utilization.detector.duration.ms = 600000
benchi-kafka     | 	confluent.balancer.cpu.utilization.detector.enabled = false
benchi-kafka     | 	confluent.balancer.cpu.utilization.detector.overutilization.threshold = 80.0
benchi-kafka     | 	confluent.balancer.cpu.utilization.detector.underutilization.threshold = 50.0
benchi-kafka     | 	confluent.balancer.disk.max.load = 0.85
benchi-kafka     | 	confluent.balancer.disk.min.free.space.gb = 0
benchi-kafka     | 	confluent.balancer.disk.min.free.space.lower.limit.gb = 0
benchi-kafka     | 	confluent.balancer.disk.utilization.detector.duration.ms = 600000
benchi-kafka     | 	confluent.balancer.disk.utilization.detector.enabled = false
benchi-kafka     | 	confluent.balancer.disk.utilization.detector.overutilization.threshold = 80.0
benchi-kafka     | 	confluent.balancer.disk.utilization.detector.reserved.capacity = 150000.0
benchi-kafka     | 	confluent.balancer.disk.utilization.detector.underutilization.threshold = 35.0
benchi-kafka     | 	confluent.balancer.enable = true
benchi-kafka     | 	confluent.balancer.exclude.topic.names = []
benchi-kafka     | 	confluent.balancer.exclude.topic.prefixes = []
benchi-kafka     | 	confluent.balancer.goal.violation.delay.on.new.brokers.ms = 1800000
benchi-kafka     | 	confluent.balancer.goal.violation.distribution.threshold.multiplier = 1.1
benchi-kafka     | 	confluent.balancer.heal.broker.failure.threshold.ms = 3600000
benchi-kafka     | 	confluent.balancer.heal.uneven.load.trigger = EMPTY_BROKER
benchi-kafka     | 	confluent.balancer.incremental.balancing.cpu.top.proposal.tracking.enabled = true
benchi-kafka     | 	confluent.balancer.incremental.balancing.cpu.top.proposal.tracking.num.proposals = 15
benchi-kafka     | 	confluent.balancer.incremental.balancing.enabled = false
benchi-kafka     | 	confluent.balancer.incremental.balancing.goals = []
benchi-kafka     | 	confluent.balancer.incremental.balancing.lower.bound = 0.02
benchi-kafka     | 	confluent.balancer.incremental.balancing.min.valid.windows = 5
benchi-kafka     | 	confluent.balancer.incremental.balancing.step.ratio = 0.2
benchi-kafka     | 	confluent.balancer.inter.cell.balancing.enabled = false
benchi-kafka     | 	confluent.balancer.max.capacity.balancing.delta.percentage = 0.0
benchi-kafka     | 	confluent.balancer.max.replicas = 2147483647
benchi-kafka     | 	confluent.balancer.network.in.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.balancer.network.out.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.balancer.plan.computation.retry.timeout.ms = 3600000
benchi-kafka     | 	confluent.balancer.producer.in.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.balancer.rebalancing.goals = []
benchi-kafka     | 	confluent.balancer.replication.in.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.balancer.resource.utilization.detector.interval.ms = 60000
benchi-kafka     | 	confluent.balancer.sbc.metrics.parser.enabled = false
benchi-kafka     | 	confluent.balancer.self.healing.maximum.rounds = 1
benchi-kafka     | 	confluent.balancer.task.history.retention.days = 30
benchi-kafka     | 	confluent.balancer.tenant.maximum.movements = 0
benchi-kafka     | 	confluent.balancer.tenant.suspension.ms = 86400000
benchi-kafka     | 	confluent.balancer.throttle.bytes.per.second = 10485760
benchi-kafka     | 	confluent.balancer.topic.partition.maximum.movements = 5
benchi-kafka     | 	confluent.balancer.topic.partition.movement.expiration.ms = 10800000
benchi-kafka     | 	confluent.balancer.topic.partition.suspension.ms = 18000000
benchi-kafka     | 	confluent.balancer.topic.replication.factor = 1
benchi-kafka     | 	confluent.balancer.triggering.goals = []
benchi-kafka     | 	confluent.balancer.v2.addition.enabled = false
benchi-kafka     | 	confluent.basic.auth.credentials.source = null
benchi-kafka     | 	confluent.basic.auth.user.info = null
benchi-kafka     | 	confluent.bearer.auth.cache.expiry.buffer.seconds = 300
benchi-kafka     | 	confluent.bearer.auth.client.id = null
benchi-kafka     | 	confluent.bearer.auth.client.secret = null
benchi-kafka     | 	confluent.bearer.auth.credentials.source = null
benchi-kafka     | 	confluent.bearer.auth.identity.pool.id = null
benchi-kafka     | 	confluent.bearer.auth.issuer.endpoint.url = null
benchi-kafka     | 	confluent.bearer.auth.logical.cluster = null
benchi-kafka     | 	confluent.bearer.auth.scope = null
benchi-kafka     | 	confluent.bearer.auth.scope.claim.name = scope
benchi-kafka     | 	confluent.bearer.auth.sub.claim.name = sub
benchi-kafka     | 	confluent.bearer.auth.token = null
benchi-kafka     | 	confluent.broker.health.manager.enabled = true
benchi-kafka     | 	confluent.broker.health.manager.engine.request.handler.threads.stuck.criteria = AllThreadsStuck
benchi-kafka     | 	confluent.broker.health.manager.hard.kill.duration.ms = 60000
benchi-kafka     | 	confluent.broker.health.manager.mitigation.enabled = false
benchi-kafka     | 	confluent.broker.health.manager.num.samples.before.broker.suspect = 30
benchi-kafka     | 	confluent.broker.health.manager.num.samples.before.broker.unhealthy = 180
benchi-kafka     | 	confluent.broker.health.manager.percentage.unhealthy.samples.before.broker.suspect = 90
benchi-kafka     | 	confluent.broker.health.manager.percentage.unhealthy.samples.before.broker.unhealthy = 70
benchi-kafka     | 	confluent.broker.health.manager.sample.duration.ms = 1000
benchi-kafka     | 	confluent.broker.health.manager.storage.background.threads.stuck.criteria = AnyThreadStuck
benchi-kafka     | 	confluent.broker.health.manager.storage.network.threads.stuck.criteria = AnyThreadStuck
benchi-kafka     | 	confluent.broker.health.manager.storage.request.handler.threads.stuck.criteria = AnyThreadStuck
benchi-kafka     | 	confluent.broker.limit.consumer.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.broker.limit.producer.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.broker.load.advertised.limit.load = 0.8
benchi-kafka     | 	confluent.broker.load.average.service.request.time.ms = 0.1
benchi-kafka     | 	confluent.broker.load.delay.metric.start.ms = 180000
benchi-kafka     | 	confluent.broker.load.enabled = false
benchi-kafka     | 	confluent.broker.load.num.samples = 60
benchi-kafka     | 	confluent.broker.load.tenant.metric.enable = false
benchi-kafka     | 	confluent.broker.load.update.metric.tags.interval.ms = 60000
benchi-kafka     | 	confluent.broker.load.window.size.ms = 60000
benchi-kafka     | 	confluent.broker.load.workload.coefficient = 20.0
benchi-kafka     | 	confluent.broker.registration.delay.ms = 0
benchi-kafka     | 	confluent.catalog.collector.destination.topic = telemetry.events.data_catalog_source
benchi-kafka     | 	confluent.catalog.collector.enable = false
benchi-kafka     | 	confluent.catalog.collector.full.configs.enable = false
benchi-kafka     | 	confluent.catalog.collector.max.bytes.per.snapshot = 850000
benchi-kafka     | 	confluent.catalog.collector.max.topics.process = 500
benchi-kafka     | 	confluent.catalog.collector.max.zookeeper.request.per.sec = 100
benchi-kafka     | 	confluent.catalog.collector.snapshot.init.delay.sec = 60
benchi-kafka     | 	confluent.catalog.collector.snapshot.interval.sec = 300
benchi-kafka     | 	confluent.ccloud.host.suffixes = .confluent.cloud,.cpdev.cloud,.confluentgov.com,.confluentgov-internal.com
benchi-kafka     | 	confluent.ccloud.intranet.host.suffixes = .intranet.stag.cpdev.cloud,.intranet.stag.cpdev-untrusted.cloud,.intranet.devel.cpdev.cloud,.intranet.devel.cpdev-untrusted.cloud,.intranet.confluent.cloud,.intranet.confluent-untrusted.cloud
benchi-kafka     | 	confluent.cdc.api.keys.topic = 
benchi-kafka     | 	confluent.cdc.api.keys.topic.load.timeout.ms = 600000
benchi-kafka     | 	confluent.cdc.client.quotas.enable = false
benchi-kafka     | 	confluent.cdc.client.quotas.topic.name = 
benchi-kafka     | 	confluent.cdc.lkc.metadata.topic = 
benchi-kafka     | 	confluent.cdc.user.metadata.enable = false
benchi-kafka     | 	confluent.cdc.user.metadata.topic = _confluent-user_metadata
benchi-kafka     | 	confluent.cell.metrics.refresh.period.ms = 60000
benchi-kafka     | 	confluent.cells.default.size = 15
benchi-kafka     | 	confluent.cells.enable = false
benchi-kafka     | 	confluent.cells.implicit.creation.enable = false
benchi-kafka     | 	confluent.cells.load.refresher.enable = true
benchi-kafka     | 	confluent.cells.max.size = 15
benchi-kafka     | 	confluent.cells.min.size = 6
benchi-kafka     | 	confluent.checksum.enabled.files = [none]
benchi-kafka     | 	confluent.clm.enabled = false
benchi-kafka     | 	confluent.clm.frequency.in.hours = 6
benchi-kafka     | 	confluent.clm.list.object.thread_pool.size = 1
benchi-kafka     | 	confluent.clm.max.backup.days = 3
benchi-kafka     | 	confluent.clm.min.delay.in.minutes = 30
benchi-kafka     | 	confluent.clm.thread.pool.size = 2
benchi-kafka     | 	confluent.clm.topic.retention.days.to.backup.days = 0:0,3:3
benchi-kafka     | 	confluent.close.connections.on.credential.delete = false
benchi-kafka     | 	confluent.cluster.link.admin.max.in.flight.requests = 1000
benchi-kafka     | 	confluent.cluster.link.admin.request.batch.size = 1
benchi-kafka     | 	confluent.cluster.link.allow.config.providers = true
benchi-kafka     | 	confluent.cluster.link.allow.legacy.message.format = false
benchi-kafka     | 	confluent.cluster.link.allow.truncation.below.hwm = true
benchi-kafka     | 	confluent.cluster.link.availability.check.mode = ALL
benchi-kafka     | 	confluent.cluster.link.background.thread.affinity = LINK
benchi-kafka     | 	confluent.cluster.link.clients.max.idle.ms = 3153600000000
benchi-kafka     | 	confluent.cluster.link.enable = true
benchi-kafka     | 	confluent.cluster.link.enable.local.admin = false
benchi-kafka     | 	confluent.cluster.link.enable.metrics.reduction = false
benchi-kafka     | 	confluent.cluster.link.fetch.response.min.bytes = 1
benchi-kafka     | 	confluent.cluster.link.fetch.response.total.bytes = 2147483647
benchi-kafka     | 	confluent.cluster.link.fetcher.auto.tune.enable = false
benchi-kafka     | 	confluent.cluster.link.fetcher.thread.pool.mode = ENDPOINT
benchi-kafka     | 	confluent.cluster.link.insync.fetch.response.min.bytes = 1
benchi-kafka     | 	confluent.cluster.link.insync.fetch.response.total.bytes = 2147483647
benchi-kafka     | 	confluent.cluster.link.intranet.connectivity.enable = false
benchi-kafka     | 	confluent.cluster.link.io.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.cluster.link.local.reverse.connection.listener.map = null
benchi-kafka     | 	confluent.cluster.link.max.client.connections = 2147483647
benchi-kafka     | 	confluent.cluster.link.metadata.topic.create.retry.delay.ms = 1000
benchi-kafka     | 	confluent.cluster.link.metadata.topic.enable = false
benchi-kafka     | 	confluent.cluster.link.metadata.topic.min.isr = 2
benchi-kafka     | 	confluent.cluster.link.metadata.topic.partitions = 50
benchi-kafka     | 	confluent.cluster.link.metadata.topic.replication.factor = 1
benchi-kafka     | 	confluent.cluster.link.mirror.transition.batch.size = 10
benchi-kafka     | 	confluent.cluster.link.num.background.threads = 1
benchi-kafka     | 	confluent.cluster.link.periodic.task.batch.size = 2147483647
benchi-kafka     | 	confluent.cluster.link.periodic.task.min.interval.ms = 1000
benchi-kafka     | 	confluent.cluster.link.replica.fetch.connections.mode = combined
benchi-kafka     | 	confluent.cluster.link.replication.quota.mode = CLUSTER_LINK_ONLY
benchi-kafka     | 	confluent.cluster.link.replication.quota.mode.per.tenant.overrides = 
benchi-kafka     | 	confluent.cluster.link.replication.quota.window.num = 11
benchi-kafka     | 	confluent.cluster.link.replication.quota.window.size.seconds = 2
benchi-kafka     | 	confluent.cluster.link.request.quota.capacity = 400
benchi-kafka     | 	confluent.cluster.link.request.quota.request.percentage.multiplier = 1.0
benchi-kafka     | 	confluent.cluster.link.tenant.replication.quota.enable = false
benchi-kafka     | 	confluent.cluster.link.tenant.request.quota.enable = false
benchi-kafka     | 	confluent.cluster.metadata.snapshot.tier.delete.enable = false
benchi-kafka     | 	confluent.cluster.metadata.snapshot.tier.delete.maintain.min.snapshots = 3
benchi-kafka     | 	confluent.cluster.metadata.snapshot.tier.delete.retention.ms = 604800000
benchi-kafka     | 	confluent.cluster.metadata.snapshot.tier.upload.enable = false
benchi-kafka     | 	confluent.compacted.topic.prefer.tier.fetch.ms = -1
benchi-kafka     | 	confluent.connection.invalid.request.delay.enable = false
benchi-kafka     | 	confluent.connections.idle.expiry.manager.ignore.idleness.requests = []
benchi-kafka     | 	confluent.consumer.fetch.partition.pruning.enable = true
benchi-kafka     | 	confluent.consumer.lag.emitter.enabled = false
benchi-kafka     | 	confluent.consumer.lag.emitter.interval.ms = 60000
benchi-kafka     | 	confluent.dataflow.policy.watch.monitor.ms = 300000
benchi-kafka     | 	confluent.defer.isr.shrink.enable = false
benchi-kafka     | 	confluent.describe.topic.partitions.enabled = false
benchi-kafka     | 	confluent.disk.io.manager.enable = false
benchi-kafka     | 	confluent.disk.throughput.headroom = 10485760
benchi-kafka     | 	confluent.disk.throughput.limit = 10485760000
benchi-kafka     | 	confluent.disk.throughput.quota.tier.archive = 1048576000
benchi-kafka     | 	confluent.disk.throughput.quota.tier.archive.throttled = 104857600
benchi-kafka     | 	confluent.durability.audit.batch.flush.frequency.ms = 900000
benchi-kafka     | 	confluent.durability.audit.checks = PeriodicalAudit,ChecksumAudit
benchi-kafka     | 	confluent.durability.audit.enable = false
benchi-kafka     | 	confluent.durability.audit.idempotent.producer = false
benchi-kafka     | 	confluent.durability.audit.initial.job.delay.ms = 900000
benchi-kafka     | 	confluent.durability.audit.io.bytes.per.sec = 10485760
benchi-kafka     | 	confluent.durability.audit.log.ignored.event.types = 
benchi-kafka     | 	confluent.durability.audit.reporting.batch.ms = 1800000
benchi-kafka     | 	confluent.durability.audit.tier.compaction.audit.duration.ms = 14400000
benchi-kafka     | 	confluent.durability.events.allowed = OffsetChangeType,EpochChangeType,IsrExpandType,DeleteRecordsType,RetentionChangeType,StartOffsetChangeType,DeletePartitionType,HealthCheckType
benchi-kafka     | 	confluent.durability.topic.partition.count = 50
benchi-kafka     | 	confluent.durability.topic.replication.factor = 1
benchi-kafka     | 	confluent.e2e_checksum.protection.enabled = false
benchi-kafka     | 	confluent.e2e_checksum.protection.files = [none]
benchi-kafka     | 	confluent.e2e_checksum.protection.store.entry.ttl.ms = 2592000000
benchi-kafka     | 	confluent.elastic.cku.enabled = false
benchi-kafka     | 	confluent.elastic.cku.scaletozero.enabled = false
benchi-kafka     | 	confluent.eligible.controllers = []
benchi-kafka     | 	confluent.enable.stray.partition.deletion = false
benchi-kafka     | 	confluent.encryption.key.manager.rotation.interval.ms = 31536000000
benchi-kafka     | 	confluent.fetch.from.follower.require.leader.epoch.enable = false
benchi-kafka     | 	confluent.fetch.partition.pruning.enable = true
benchi-kafka     | 	confluent.floor.connection.rate.per.ip = -1.0
benchi-kafka     | 	confluent.floor.connection.rate.per.tenant = -1.0
benchi-kafka     | 	confluent.group.coordinator.offsets.batching.enable = false
benchi-kafka     | 	confluent.group.coordinator.offsets.writer.threads = 2
benchi-kafka     | 	confluent.group.metadata.load.threads = 32
benchi-kafka     | 	confluent.heap.tenured.notify.bytes = 0
benchi-kafka     | 	confluent.heap.tenured.notify.enabled = false
benchi-kafka     | 	confluent.hot.partition.ratio = 0.8
benchi-kafka     | 	confluent.http.server.start.timeout.ms = 60000
benchi-kafka     | 	confluent.http.server.stop.timeout.ms = 30000
benchi-kafka     | 	confluent.internal.metrics.enable = false
benchi-kafka     | 	confluent.internal.rest.server.bind.port = null
benchi-kafka     | 	confluent.internal.tenant.scoped.listener.name = INTERNAL_TENANT_SCOPED
benchi-kafka     | 	confluent.leader.epoch.checkpoint.checksum.enabled = false
benchi-kafka     | 	confluent.log.cleaner.timestamp.validation.enable = true
benchi-kafka     | 	confluent.log.placement.constraints = 
benchi-kafka     | 	confluent.max.connection.creation.rate.per.ip = 1.7976931348623157E308
benchi-kafka     | 	confluent.max.connection.creation.rate.per.tenant = 1.7976931348623157E308
benchi-kafka     | 	confluent.max.connection.rate.per.ip = -1.0
benchi-kafka     | 	confluent.max.connection.rate.per.tenant = -1.0
benchi-kafka     | 	confluent.max.connection.throttle.ms = null
benchi-kafka     | 	confluent.max.segment.ms = 9223372036854775807
benchi-kafka     | 	confluent.metadata.active.encryptor = null
benchi-kafka     | 	confluent.metadata.controlled.shutdown.partition.slice.delay.ms = 100
benchi-kafka     | 	confluent.metadata.encryptor.classes = null
benchi-kafka     | 	confluent.metadata.encryptor.required = false
benchi-kafka     | 	confluent.metadata.encryptor.secret.file = null
benchi-kafka     | 	confluent.metadata.encryptor.secrets = null
benchi-kafka     | 	confluent.metadata.jvm.warmup.ms = 60000
benchi-kafka     | 	confluent.metadata.leader.balance.slice.delay.ms = 100
benchi-kafka     | 	confluent.metadata.max.controlled.shutdown.partition.changes.per.slice = 1000
benchi-kafka     | 	confluent.metadata.max.leader.balance.changes.per.slice = 1000
benchi-kafka     | 	confluent.metadata.server.cluster.registry.clusters = []
benchi-kafka     | 	confluent.metadata.throttle.pre.check.for.non.wildcard.requests.enable = false
benchi-kafka     | 	confluent.metadata.throttle.pre.check.for.wildcard.requests.enable = false
benchi-kafka     | 	confluent.metrics.reporter.bootstrap.servers = localhost:9092
benchi-kafka     | 	confluent.min.acks = 0
benchi-kafka     | 	confluent.min.connection.throttle.ms = 0
benchi-kafka     | 	confluent.min.segment.ms = 1
benchi-kafka     | 	confluent.missing.id.cache.ttl.sec = 60
benchi-kafka     | 	confluent.missing.id.query.range = 20000
benchi-kafka     | 	confluent.missing.schema.cache.ttl.sec = 60
benchi-kafka     | 	confluent.mtls.enable = false
benchi-kafka     | 	confluent.mtls.listener.name = EXTERNAL
benchi-kafka     | 	confluent.mtls.sasl.authenticator.request.max.bytes = 104857600
benchi-kafka     | 	confluent.mtls.truststore.manager.class.name = null
benchi-kafka     | 	confluent.multitenant.authorizer.enable.acl.state = false
benchi-kafka     | 	confluent.multitenant.interceptor.balancer.apis.enabled = false
benchi-kafka     | 	confluent.multitenant.interceptor.collect.client.apiversions.max.per.tenant = 1000
benchi-kafka     | 	confluent.multitenant.interceptor.collect.client.apiversions.metric = false
benchi-kafka     | 	confluent.multitenant.listener.hostname.cluster.prefix.enable = false
benchi-kafka     | 	confluent.multitenant.listener.hostname.subdomain.suffix.enable = false
benchi-kafka     | 	confluent.multitenant.listener.names = null
benchi-kafka     | 	confluent.multitenant.parse.lkc.id.enable = false
benchi-kafka     | 	confluent.multitenant.parse.sni.host.name.enable = false
benchi-kafka     | 	confluent.network.health.manager.enabled = false
benchi-kafka     | 	confluent.network.health.manager.external.listener.name = EXTERNAL
benchi-kafka     | 	confluent.network.health.manager.externalconnectivitystartup.enabled = false
benchi-kafka     | 	confluent.network.health.manager.min.healthy.network.samples = 3
benchi-kafka     | 	confluent.network.health.manager.min.percentage.healthy.network.samples = 3
benchi-kafka     | 	confluent.network.health.manager.mitigation.enabled = false
benchi-kafka     | 	confluent.network.health.manager.network.sample.window.size = 120
benchi-kafka     | 	confluent.network.health.manager.sample.duration.ms = 1000
benchi-kafka     | 	confluent.oauth.flat.networking.verification.enable = false
benchi-kafka     | 	confluent.offsets.log.cleaner.delete.retention.ms = 86400000
benchi-kafka     | 	confluent.offsets.log.cleaner.max.compaction.lag.ms = 9223372036854775807
benchi-kafka     | 	confluent.offsets.log.cleaner.min.cleanable.dirty.ratio = 0.5
benchi-kafka     | 	confluent.offsets.topic.placement.constraints = 
benchi-kafka     | 	confluent.omit.network.processor.metric.tag = false
benchi-kafka     | 	confluent.operator.managed = false
benchi-kafka     | 	confluent.password.encoder.old.secret.ttl.ms = 9223372036854775807
benchi-kafka     | 	confluent.plugins.cluster.link.policy.max.destination.links.per.tenant = 10
benchi-kafka     | 	confluent.plugins.cluster.link.policy.max.source.links.per.tenant = 10
benchi-kafka     | 	confluent.plugins.topic.policy.max.partitions.per.cluster = 2147483647
benchi-kafka     | 	confluent.plugins.topic.policy.max.partitions.per.tenant = 512
benchi-kafka     | 	confluent.plugins.topic.policy.max.replicas.per.broker = 2147483647
benchi-kafka     | 	confluent.plugins.topic.policy.max.topics.per.cluster = 2147483647
benchi-kafka     | 	confluent.prefer.tier.fetch.ms = -1
benchi-kafka     | 	confluent.produce.throttle.pre.check.enable = false
benchi-kafka     | 	confluent.producer.id.cache.broker.hard.limit = -1
benchi-kafka     | 	confluent.producer.id.cache.eviction.minimal.expiration.ms = 900000
benchi-kafka     | 	confluent.producer.id.cache.extra.eviction.percentage = 0
benchi-kafka     | 	confluent.producer.id.cache.limit = 2147483647
benchi-kafka     | 	confluent.producer.id.cache.partition.hard.limit = -1
benchi-kafka     | 	confluent.producer.id.cache.tenant.hard.limit = -1
benchi-kafka     | 	confluent.producer.id.quota.manager.enable = false
benchi-kafka     | 	confluent.producer.id.quota.window.num = 11
benchi-kafka     | 	confluent.producer.id.quota.window.size.seconds = 1
benchi-kafka     | 	confluent.producer.id.throttle.enable = false
benchi-kafka     | 	confluent.producer.id.throttle.enable.threshold.percentage = 100
benchi-kafka     | 	confluent.proxy.mode.local.default = false
benchi-kafka     | 	confluent.proxy.protocol.fallback.enabled = false
benchi-kafka     | 	confluent.proxy.protocol.version = NONE
benchi-kafka     | 	confluent.quota.computing.usage.adjustment = 0.5
benchi-kafka     | 	confluent.quota.dynamic.enable = false
benchi-kafka     | 	confluent.quota.dynamic.publishing.interval.ms = 60000
benchi-kafka     | 	confluent.quota.dynamic.reporting.interval.ms = 30000
benchi-kafka     | 	confluent.quota.dynamic.reporting.min.usage = 102400
benchi-kafka     | 	confluent.quota.tenant.broker.max.consumer.rate = 13107200
benchi-kafka     | 	confluent.quota.tenant.broker.max.producer.rate = 13107200
benchi-kafka     | 	confluent.quota.tenant.default.controller.mutation.rate = 2.147483647E9
benchi-kafka     | 	confluent.quota.tenant.default.producer.id.rate = 2.147483647E9
benchi-kafka     | 	confluent.quota.tenant.fetch.multiplier = 1.0
benchi-kafka     | 	confluent.quota.tenant.follower.broker.min.consumer.rate = 10485760
benchi-kafka     | 	confluent.quota.tenant.follower.broker.min.producer.rate = 10485760
benchi-kafka     | 	confluent.quota.tenant.produce.multiplier = 1.0
benchi-kafka     | 	confluent.quota.tenant.user.quotas.enable = false
benchi-kafka     | 	confluent.regional.metadata.client.class = null
benchi-kafka     | 	confluent.regional.resource.manager.client.scheduler.threads = 2
benchi-kafka     | 	confluent.regional.resource.manager.endpoint = null
benchi-kafka     | 	confluent.regional.resource.manager.watch.endpoint = null
benchi-kafka     | 	confluent.replica.fetch.backoff.max.ms = 1000
benchi-kafka     | 	confluent.replica.fetch.connections.mode = combined
benchi-kafka     | 	confluent.replication.mode = PULL
benchi-kafka     | 	confluent.replication.push.feature.enable = false
benchi-kafka     | 	confluent.reporters.telemetry.auto.enable = true
benchi-kafka     | 	confluent.request.log.filter.class = class org.apache.kafka.common.requests.SamplingRequestLogFilter
benchi-kafka     | 	confluent.request.pipelining.enable = true
benchi-kafka     | 	confluent.request.pipelining.max.in.flight.requests.per.connection = 5
benchi-kafka     | 	confluent.require.calling.resource.identity = false
benchi-kafka     | 	confluent.require.compatible.keystore.updates = true
benchi-kafka     | 	confluent.require.confluent.issuer = false
benchi-kafka     | 	confluent.roll.check.interval.ms = 300000
benchi-kafka     | 	confluent.schema.registry.max.cache.size = 10000
benchi-kafka     | 	confluent.schema.registry.max.retries = 1
benchi-kafka     | 	confluent.schema.registry.retries.wait.ms = 0
benchi-kafka     | 	confluent.schema.registry.url = null
benchi-kafka     | 	confluent.schema.validator.interceptor.class = io.confluent.kafka.schemaregistry.validator.RecordSchemaValidator
benchi-kafka     | 	confluent.schema.validator.multitenant.enable = false
benchi-kafka     | 	confluent.schema.validator.samples.per.min = 0
benchi-kafka     | 	confluent.security.bc.approved.mode.enable = false
benchi-kafka     | 	confluent.security.event.logger.authentication.enable = false
benchi-kafka     | 	confluent.security.event.logger.authentication.event.rate.limit = -1
benchi-kafka     | 	confluent.security.event.logger.authorization.event.rate.limit = -1
benchi-kafka     | 	confluent.security.event.logger.detailed.audit.logs.filter.class = class org.apache.kafka.common.requests.DetailedRequestAuditLogFilter
benchi-kafka     | 	confluent.security.event.logger.enable = true
benchi-kafka     | 	confluent.security.event.logger.kafka.request.rate.limit = -1
benchi-kafka     | 	confluent.security.event.logger.physical.cluster.id = 
benchi-kafka     | 	confluent.security.event.router.config = 
benchi-kafka     | 	confluent.security.revoked.certificate.ids = 
benchi-kafka     | 	confluent.segment.eager.roll.enable = false
benchi-kafka     | 	confluent.segment.speculative.prefetch.enable = false
benchi-kafka     | 	confluent.spiffe.id.principal.extraction.rules = 
benchi-kafka     | 	confluent.ssl.key.password = null
benchi-kafka     | 	confluent.ssl.keystore.location = null
benchi-kafka     | 	confluent.ssl.keystore.password = null
benchi-kafka     | 	confluent.ssl.keystore.type = null
benchi-kafka     | 	confluent.ssl.protocol = null
benchi-kafka     | 	confluent.ssl.truststore.location = null
benchi-kafka     | 	confluent.ssl.truststore.password = null
benchi-kafka     | 	confluent.ssl.truststore.type = null
benchi-kafka     | 	confluent.step.connection.rate.per.ip = -1.0
benchi-kafka     | 	confluent.step.connection.rate.per.tenant = -1.0
benchi-kafka     | 	confluent.storage.probe.period.ms = -1
benchi-kafka     | 	confluent.storage.probe.slow.write.threshold.ms = 5000
benchi-kafka     | 	confluent.stray.log.delete.delay.ms = 604800000
benchi-kafka     | 	confluent.stray.log.max.deletions.per.run = 72
benchi-kafka     | 	confluent.subdomain.prefix = null
benchi-kafka     | 	confluent.subdomain.separator.map = null
benchi-kafka     | 	confluent.subdomain.separator.variable = %sep
benchi-kafka     | 	confluent.system.time.roll.enable = false
benchi-kafka     | 	confluent.telemetry.enabled = false
benchi-kafka     | 	confluent.telemetry.external.client.metrics.push.enabled = false
benchi-kafka     | 	confluent.tenant.latency.metric.enabled = false
benchi-kafka     | 	confluent.tier.archiver.num.threads = 2
benchi-kafka     | 	confluent.tier.azure.block.blob.auto.abort.threshold.bytes = 500000
benchi-kafka     | 	confluent.tier.azure.block.blob.container = null
benchi-kafka     | 	confluent.tier.azure.block.blob.cred.file.path = null
benchi-kafka     | 	confluent.tier.azure.block.blob.endpoint = null
benchi-kafka     | 	confluent.tier.azure.block.blob.prefix = 
benchi-kafka     | 	confluent.tier.backend = 
benchi-kafka     | 	confluent.tier.bucket.probe.period.ms = -1
benchi-kafka     | 	confluent.tier.cleaner.compact.min.efficiency = 0.5
benchi-kafka     | 	confluent.tier.cleaner.compact.segment.min.bytes = 20971520
benchi-kafka     | 	confluent.tier.cleaner.dedupe.buffer.size = 134217728
benchi-kafka     | 	confluent.tier.cleaner.dual.compaction = false
benchi-kafka     | 	confluent.tier.cleaner.dual.compaction.validation.max.bytes = 1073741824
benchi-kafka     | 	confluent.tier.cleaner.dual.compaction.validation.percent = 0
benchi-kafka     | 	confluent.tier.cleaner.enable = false
benchi-kafka     | 	confluent.tier.cleaner.excluded.topics = [^_confluent.*]
benchi-kafka     | 	confluent.tier.cleaner.feature.enable = false
benchi-kafka     | 	confluent.tier.cleaner.io.buffer.load.factor = 0.9
benchi-kafka     | 	confluent.tier.cleaner.io.buffer.size = 10485760
benchi-kafka     | 	confluent.tier.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
benchi-kafka     | 	confluent.tier.cleaner.min.cleanable.ratio = 0.75
benchi-kafka     | 	confluent.tier.cleaner.num.threads = 2
benchi-kafka     | 	confluent.tier.enable = false
benchi-kafka     | 	confluent.tier.feature = false
benchi-kafka     | 	confluent.tier.fenced.segment.delete.delay.ms = 600000
benchi-kafka     | 	confluent.tier.fetcher.async.enable = false
benchi-kafka     | 	confluent.tier.fetcher.async.timestamp.offset.parallelism = 1
benchi-kafka     | 	confluent.tier.fetcher.fetch.based.on.segment_and_metadata_layout.field = false
benchi-kafka     | 	confluent.tier.fetcher.memorypool.bytes = 0
benchi-kafka     | 	confluent.tier.fetcher.num.threads = 4
benchi-kafka     | 	confluent.tier.fetcher.offset.cache.expiration.ms = 1800000
benchi-kafka     | 	confluent.tier.fetcher.offset.cache.period.ms = 60000
benchi-kafka     | 	confluent.tier.fetcher.offset.cache.size = 200000
benchi-kafka     | 	confluent.tier.gcs.bucket = null
benchi-kafka     | 	confluent.tier.gcs.cred.file.path = null
benchi-kafka     | 	confluent.tier.gcs.prefix = 
benchi-kafka     | 	confluent.tier.gcs.region = null
benchi-kafka     | 	confluent.tier.gcs.sse.customer.encryption.key = null
benchi-kafka     | 	confluent.tier.gcs.write.chunk.size = 0
benchi-kafka     | 	confluent.tier.local.hotset.bytes = -1
benchi-kafka     | 	confluent.tier.local.hotset.ms = 86400000
benchi-kafka     | 	confluent.tier.max.partition.fetch.bytes.override = 0
benchi-kafka     | 	confluent.tier.metadata.bootstrap.servers = null
benchi-kafka     | 	confluent.tier.metadata.max.poll.ms = 100
benchi-kafka     | 	confluent.tier.metadata.namespace = null
benchi-kafka     | 	confluent.tier.metadata.num.partitions = 50
benchi-kafka     | 	confluent.tier.metadata.replication.factor = 1
benchi-kafka     | 	confluent.tier.metadata.request.timeout.ms = 30000
benchi-kafka     | 	confluent.tier.metadata.snapshots.enable = false
benchi-kafka     | 	confluent.tier.metadata.snapshots.interval.ms = 86400000
benchi-kafka     | 	confluent.tier.metadata.snapshots.retention.days = 7
benchi-kafka     | 	confluent.tier.metadata.snapshots.threads = 2
benchi-kafka     | 	confluent.tier.object.fetcher.num.threads = 1
benchi-kafka     | 	confluent.tier.partition.state.cleanup.delay.ms = 2592000000
benchi-kafka     | 	confluent.tier.partition.state.cleanup.enable = false
benchi-kafka     | 	confluent.tier.partition.state.cleanup.interval.ms = 86400000
benchi-kafka     | 	confluent.tier.partition.state.commit.interval.ms = 15000
benchi-kafka     | 	confluent.tier.s3.assumerole.arn = null
benchi-kafka     | 	confluent.tier.s3.auto.abort.threshold.bytes = 500000
benchi-kafka     | 	confluent.tier.s3.aws.endpoint.override = null
benchi-kafka     | 	confluent.tier.s3.aws.signer.override = null
benchi-kafka     | 	confluent.tier.s3.bucket = null
benchi-kafka     | 	confluent.tier.s3.cred.file.path = null
benchi-kafka     | 	confluent.tier.s3.force.path.style.access = false
benchi-kafka     | 	confluent.tier.s3.prefix = 
benchi-kafka     | 	confluent.tier.s3.region = null
benchi-kafka     | 	confluent.tier.s3.security.providers = null
benchi-kafka     | 	confluent.tier.s3.sse.algorithm = AES256
benchi-kafka     | 	confluent.tier.s3.sse.customer.encryption.key = null
benchi-kafka     | 	confluent.tier.s3.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	confluent.tier.s3.ssl.key.password = null
benchi-kafka     | 	confluent.tier.s3.ssl.keystore.location = null
benchi-kafka     | 	confluent.tier.s3.ssl.keystore.password = null
benchi-kafka     | 	confluent.tier.s3.ssl.keystore.type = null
benchi-kafka     | 	confluent.tier.s3.ssl.protocol = TLSv1.3
benchi-kafka     | 	confluent.tier.s3.ssl.provider = null
benchi-kafka     | 	confluent.tier.s3.ssl.truststore.location = null
benchi-kafka     | 	confluent.tier.s3.ssl.truststore.password = null
benchi-kafka     | 	confluent.tier.s3.ssl.truststore.type = null
benchi-kafka     | 	confluent.tier.s3.user.agent.prefix = APN/1.0 Confluent/1.0 TieredStorageS3/1.0
benchi-kafka     | 	confluent.tier.segment.hotset.roll.min.bytes = 104857600
benchi-kafka     | 	confluent.tier.segment.metadata.layout.put.mode = LegacyMultiObject
benchi-kafka     | 	confluent.tier.topic.data.loss.validation.fencing.enable = false
benchi-kafka     | 	confluent.tier.topic.delete.backoff.ms = 21600000
benchi-kafka     | 	confluent.tier.topic.delete.check.interval.ms = 300000
benchi-kafka     | 	confluent.tier.topic.delete.max.inprogress.partitions = 100
benchi-kafka     | 	confluent.tier.topic.head.data.loss.validation.enable = true
benchi-kafka     | 	confluent.tier.topic.head.data.loss.validation.max.timeout.ms = 900000
benchi-kafka     | 	confluent.tier.topic.materialization.from.snapshot.enable = false
benchi-kafka     | 	confluent.tier.topic.producer.enable.idempotence = true
benchi-kafka     | 	confluent.tier.topic.snapshots.enable = false
benchi-kafka     | 	confluent.tier.topic.snapshots.interval.ms = 300000
benchi-kafka     | 	confluent.tier.topic.snapshots.max.records = 100000
benchi-kafka     | 	confluent.tier.topic.snapshots.retention.hours = 168
benchi-kafka     | 	confluent.topic.partition.default.placement = 2
benchi-kafka     | 	confluent.topic.policy.use.computed.assignments = false
benchi-kafka     | 	confluent.topic.replica.assignor.builder.class = 
benchi-kafka     | 	confluent.track.api.key.per.ip = false
benchi-kafka     | 	confluent.track.per.ip.max.size = 100000
benchi-kafka     | 	confluent.track.tenant.id.per.ip = false
benchi-kafka     | 	confluent.traffic.cdc.network.id.routes.enable = false
benchi-kafka     | 	confluent.traffic.cdc.network.id.routes.listener.names = EXTERNAL_BACKCHANNEL
benchi-kafka     | 	confluent.traffic.cdc.network.id.routes.periodic.start.task.ms = 300000
benchi-kafka     | 	confluent.traffic.cdc.network.id.routes.topic.name = _confluent-network_id_routes
benchi-kafka     | 	confluent.traffic.network.id = 
benchi-kafka     | 	confluent.transaction.2pc.timeout.ms = -1
benchi-kafka     | 	confluent.transaction.logging.verbosity = 0
benchi-kafka     | 	confluent.transaction.state.log.placement.constraints = 
benchi-kafka     | 	confluent.valid.broker.rack.set = null
benchi-kafka     | 	confluent.verify.group.subscription.prefix = false
benchi-kafka     | 	confluent.virtual.topic.creation.enabled = false
benchi-kafka     | 	confluent.zookeeper.metadata.migration.controller.check.disable = false
benchi-kafka     | 	confluent.zookeeper.metadata.migration.trigger.file.path = null
benchi-kafka     | 	connection.failed.authentication.delay.ms = 100
benchi-kafka     | 	connection.min.expire.interval.ms = 250
benchi-kafka     | 	connections.max.age.ms = 3153600000000
benchi-kafka     | 	connections.max.idle.ms = 600000
benchi-kafka     | 	connections.max.reauth.ms = 0
benchi-kafka     | 	control.plane.listener.name = null
benchi-kafka     | 	controlled.shutdown.enable = true
benchi-kafka     | 	controlled.shutdown.max.retries = 3
benchi-kafka     | 	controlled.shutdown.retry.backoff.ms = 5000
benchi-kafka     | 	controller.listener.names = CONTROLLER
benchi-kafka     | 	controller.quorum.append.linger.ms = 25
benchi-kafka     | 	controller.quorum.bootstrap.servers = []
benchi-kafka     | 	controller.quorum.election.backoff.max.ms = 1000
benchi-kafka     | 	controller.quorum.election.timeout.ms = 1000
benchi-kafka     | 	controller.quorum.fetch.timeout.ms = 2000
benchi-kafka     | 	controller.quorum.request.timeout.ms = 2000
benchi-kafka     | 	controller.quorum.retry.backoff.ms = 20
benchi-kafka     | 	controller.quorum.voters = [1@broker:29093]
benchi-kafka     | 	controller.quota.window.num = 11
benchi-kafka     | 	controller.quota.window.size.seconds = 1
benchi-kafka     | 	controller.socket.timeout.ms = 30000
benchi-kafka     | 	create.cluster.link.policy.class.name = null
benchi-kafka     | 	create.topic.policy.class.name = null
benchi-kafka     | 	default.replication.factor = 1
benchi-kafka     | 	delegation.token.expiry.check.interval.ms = 3600000
benchi-kafka     | 	delegation.token.expiry.time.ms = 86400000
benchi-kafka     | 	delegation.token.master.key = null
benchi-kafka     | 	delegation.token.max.lifetime.ms = 604800000
benchi-kafka     | 	delegation.token.secret.key = null
benchi-kafka     | 	delete.records.purgatory.purge.interval.requests = 1
benchi-kafka     | 	delete.topic.enable = true
benchi-kafka     | 	early.start.listeners = null
benchi-kafka     | 	eligible.leader.replicas.enable = false
benchi-kafka     | 	enable.fips = false
benchi-kafka     | 	fetch.max.bytes = 57671680
benchi-kafka     | 	fetch.purgatory.purge.interval.requests = 1000
benchi-kafka     | 	floor.max.connection.creation.rate = null
benchi-kafka     | 	follower.replication.throttled.rate = 9223372036854775807
benchi-kafka     | 	follower.replication.throttled.replicas = none
benchi-kafka     | 	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
benchi-kafka     | 	group.consumer.heartbeat.interval.ms = 5000
benchi-kafka     | 	group.consumer.max.heartbeat.interval.ms = 15000
benchi-kafka     | 	group.consumer.max.session.timeout.ms = 60000
benchi-kafka     | 	group.consumer.max.size = 2147483647
benchi-kafka     | 	group.consumer.migration.policy = disabled
benchi-kafka     | 	group.consumer.min.heartbeat.interval.ms = 5000
benchi-kafka     | 	group.consumer.min.session.timeout.ms = 45000
benchi-kafka     | 	group.consumer.session.timeout.ms = 45000
benchi-kafka     | 	group.coordinator.append.linger.ms = 10
benchi-kafka     | 	group.coordinator.new.enable = false
benchi-kafka     | 	group.coordinator.rebalance.protocols = [classic]
benchi-kafka     | 	group.coordinator.threads = 1
benchi-kafka     | 	group.initial.rebalance.delay.ms = 0
benchi-kafka     | 	group.max.session.timeout.ms = 1800000
benchi-kafka     | 	group.max.size = 2147483647
benchi-kafka     | 	group.min.session.timeout.ms = 6000
benchi-kafka     | 	initial.broker.registration.timeout.ms = 60000
benchi-kafka     | 	inter.broker.listener.name = PLAINTEXT
benchi-kafka     | 	inter.broker.protocol.version = 3.8-IV0A
benchi-kafka     | 	k2.stack.builder.class.name = null
benchi-kafka     | 	k2.startup.timeout.ms = 60000
benchi-kafka     | 	k2.topic.metadata.max.total.partition.count = 20000
benchi-kafka     | 	k2.topic.metadata.refresh.ms = 10000
benchi-kafka     | 	kafka.metrics.polling.interval.secs = 10
benchi-kafka     | 	kafka.metrics.reporters = []
benchi-kafka     | 	leader.imbalance.check.interval.seconds = 300
benchi-kafka     | 	leader.imbalance.per.broker.percentage = 10
benchi-kafka     | 	leader.replication.throttled.rate = 9223372036854775807
benchi-kafka     | 	leader.replication.throttled.replicas = none
benchi-kafka     | 	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
benchi-kafka     | 	listeners = PLAINTEXT://broker:29092,CONTROLLER://broker:29093,PLAINTEXT_HOST://0.0.0.0:9092
benchi-kafka     | 	log.cleaner.backoff.ms = 15000
benchi-kafka     | 	log.cleaner.dedupe.buffer.size = 134217728
benchi-kafka     | 	log.cleaner.delete.retention.ms = 86400000
benchi-kafka     | 	log.cleaner.enable = true
benchi-kafka     | 	log.cleaner.hash.algorithm = MD5
benchi-kafka     | 	log.cleaner.io.buffer.load.factor = 0.9
benchi-kafka     | 	log.cleaner.io.buffer.size = 524288
benchi-kafka     | 	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
benchi-kafka     | 	log.cleaner.max.compaction.lag.ms = 9223372036854775807
benchi-kafka     | 	log.cleaner.min.cleanable.ratio = 0.5
benchi-kafka     | 	log.cleaner.min.compaction.lag.ms = 0
benchi-kafka     | 	log.cleaner.threads = 1
benchi-kafka     | 	log.cleanup.policy = [delete]
benchi-kafka     | 	log.deletion.max.segments.per.run = 2147483647
benchi-kafka     | 	log.deletion.throttler.disk.free.headroom.bytes = 21474836480
benchi-kafka     | 	log.dir = /tmp/kafka-logs
benchi-kafka     | 	log.dir.failure.timeout.ms = 30000
benchi-kafka     | 	log.dirs = /tmp/kraft-combined-logs
benchi-kafka     | 	log.flush.interval.messages = 9223372036854775807
benchi-kafka     | 	log.flush.interval.ms = null
benchi-kafka     | 	log.flush.offset.checkpoint.interval.ms = 60000
benchi-kafka     | 	log.flush.scheduler.interval.ms = 9223372036854775807
benchi-kafka     | 	log.flush.start.offset.checkpoint.interval.ms = 60000
benchi-kafka     | 	log.index.interval.bytes = 4096
benchi-kafka     | 	log.index.size.max.bytes = 10485760
benchi-kafka     | 	log.initial.task.delay.ms = 30000
benchi-kafka     | 	log.local.retention.bytes = -2
benchi-kafka     | 	log.local.retention.ms = -2
benchi-kafka     | 	log.message.downconversion.enable = true
benchi-kafka     | 	log.message.format.version = 3.0-IV1
benchi-kafka     | 	log.message.timestamp.after.max.ms = 9223372036854775807
benchi-kafka     | 	log.message.timestamp.before.max.ms = 9223372036854775807
benchi-kafka     | 	log.message.timestamp.difference.max.ms = 9223372036854775807
benchi-kafka     | 	log.message.timestamp.type = CreateTime
benchi-kafka     | 	log.preallocate = false
benchi-kafka     | 	log.retention.bytes = -1
benchi-kafka     | 	log.retention.check.interval.ms = 300000
benchi-kafka     | 	log.retention.hours = 168
benchi-kafka     | 	log.retention.minutes = null
benchi-kafka     | 	log.retention.ms = null
benchi-kafka     | 	log.roll.hours = 168
benchi-kafka     | 	log.roll.jitter.hours = 0
benchi-kafka     | 	log.roll.jitter.ms = null
benchi-kafka     | 	log.roll.ms = null
benchi-kafka     | 	log.segment.bytes = 1073741824
benchi-kafka     | 	log.segment.delete.delay.ms = 60000
benchi-kafka     | 	max.connection.creation.rate = 1.7976931348623157E308
benchi-kafka     | 	max.connection.creation.rate.per.ip.enable.threshold = 0.0
benchi-kafka     | 	max.connection.creation.rate.per.tenant.enable.threshold = 0.0
benchi-kafka     | 	max.connections = 2147483647
benchi-kafka     | 	max.connections.per.ip = 2147483647
benchi-kafka     | 	max.connections.per.ip.overrides = 
benchi-kafka     | 	max.connections.per.tenant = 0
benchi-kafka     | 	max.connections.protected.listeners = []
benchi-kafka     | 	max.connections.reap.amount = 0
benchi-kafka     | 	max.incremental.fetch.session.cache.slots = 1000
benchi-kafka     | 	max.request.partition.size.limit = 2000
benchi-kafka     | 	message.max.bytes = 1048588
benchi-kafka     | 	metadata.log.dir = null
benchi-kafka     | 	metadata.log.max.record.bytes.between.snapshots = 20971520
benchi-kafka     | 	metadata.log.max.snapshot.interval.ms = 3600000
benchi-kafka     | 	metadata.log.segment.bytes = 1073741824
benchi-kafka     | 	metadata.log.segment.min.bytes = 8388608
benchi-kafka     | 	metadata.log.segment.ms = 604800000
benchi-kafka     | 	metadata.max.idle.interval.ms = 500
benchi-kafka     | 	metadata.max.retention.bytes = 104857600
benchi-kafka     | 	metadata.max.retention.ms = 604800000
benchi-kafka     | 	metric.reporters = [io.confluent.metrics.reporter.ConfluentMetricsReporter]
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.recording.level = INFO
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	min.insync.replicas = 1
benchi-kafka     | 	multitenant.authorizer.support.resource.ids = false
benchi-kafka     | 	multitenant.metadata.class = null
benchi-kafka     | 	multitenant.metadata.dir = null
benchi-kafka     | 	multitenant.metadata.reload.delay.ms = 120000
benchi-kafka     | 	multitenant.metadata.ssl.certs.path = null
benchi-kafka     | 	multitenant.tenant.delete.batch.size = 10
benchi-kafka     | 	multitenant.tenant.delete.check.ms = 120000
benchi-kafka     | 	multitenant.tenant.delete.delay = 604800000
benchi-kafka     | 	node.id = 1
benchi-kafka     | 	num.io.threads = 8
benchi-kafka     | 	num.network.threads = 3
benchi-kafka     | 	num.partitions = 1
benchi-kafka     | 	num.recovery.threads.per.data.dir = 1
benchi-kafka     | 	num.replica.alter.log.dirs.threads = null
benchi-kafka     | 	num.replica.fetchers = 1
benchi-kafka     | 	offset.metadata.max.bytes = 4096
benchi-kafka     | 	offsets.commit.required.acks = -1
benchi-kafka     | 	offsets.commit.timeout.ms = 5000
benchi-kafka     | 	offsets.load.buffer.size = 5242880
benchi-kafka     | 	offsets.retention.check.interval.ms = 600000
benchi-kafka     | 	offsets.retention.minutes = 10080
benchi-kafka     | 	offsets.topic.compression.codec = 0
benchi-kafka     | 	offsets.topic.num.partitions = 50
benchi-kafka     | 	offsets.topic.replication.factor = 1
benchi-kafka     | 	offsets.topic.segment.bytes = 104857600
benchi-kafka     | 	otel.exporter.otlp.custom.endpoint = default
benchi-kafka     | 	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
benchi-kafka     | 	password.encoder.iterations = 4096
benchi-kafka     | 	password.encoder.key.length = 128
benchi-kafka     | 	password.encoder.keyfactory.algorithm = null
benchi-kafka     | 	password.encoder.old.secret = null
benchi-kafka     | 	password.encoder.secret = null
benchi-kafka     | 	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
benchi-kafka     | 	process.roles = [broker, controller]
benchi-kafka     | 	producer.id.expiration.check.interval.ms = 600000
benchi-kafka     | 	producer.id.expiration.ms = 86400000
benchi-kafka     | 	producer.purgatory.purge.interval.requests = 1000
benchi-kafka     | 	queued.max.request.bytes = -1
benchi-kafka     | 	queued.max.requests = 500
benchi-kafka     | 	quota.window.num = 11
benchi-kafka     | 	quota.window.size.seconds = 1
benchi-kafka     | 	quotas.consumption.expiration.time.ms = 600000
benchi-kafka     | 	quotas.expiration.interval.ms = 3600000
benchi-kafka     | 	quotas.expiration.time.ms = 604800000
benchi-kafka     | 	quotas.lazy.evaluation.threshold = 0.5
benchi-kafka     | 	quotas.topic.append.timeout.ms = 5000
benchi-kafka     | 	quotas.topic.compression.codec = 3
benchi-kafka     | 	quotas.topic.load.buffer.size = 5242880
benchi-kafka     | 	quotas.topic.num.partitions = 50
benchi-kafka     | 	quotas.topic.placement.constraints = 
benchi-kafka     | 	quotas.topic.replication.factor = 3
benchi-kafka     | 	quotas.topic.segment.bytes = 104857600
benchi-kafka     | 	remote.fetch.max.wait.ms = 500
benchi-kafka     | 	remote.log.index.file.cache.total.size.bytes = 1073741824
benchi-kafka     | 	remote.log.manager.copier.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.copy.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.copy.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.expiration.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.fetch.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.fetch.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.task.interval.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.max.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.ms = 500
benchi-kafka     | 	remote.log.manager.task.retry.jitter = 0.2
benchi-kafka     | 	remote.log.manager.thread.pool.size = 10
benchi-kafka     | 	remote.log.metadata.custom.metadata.max.bytes = 128
benchi-kafka     | 	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
benchi-kafka     | 	remote.log.metadata.manager.class.path = null
benchi-kafka     | 	remote.log.metadata.manager.impl.prefix = rlmm.config.
benchi-kafka     | 	remote.log.metadata.manager.listener.name = null
benchi-kafka     | 	remote.log.reader.max.pending.tasks = 100
benchi-kafka     | 	remote.log.reader.threads = 10
benchi-kafka     | 	remote.log.storage.manager.class.name = null
benchi-kafka     | 	remote.log.storage.manager.class.path = null
benchi-kafka     | 	remote.log.storage.manager.impl.prefix = rsm.config.
benchi-kafka     | 	remote.log.storage.system.enable = false
benchi-kafka     | 	replica.fetch.backoff.ms = 1000
benchi-kafka     | 	replica.fetch.max.bytes = 1048576
benchi-kafka     | 	replica.fetch.min.bytes = 1
benchi-kafka     | 	replica.fetch.response.max.bytes = 10485760
benchi-kafka     | 	replica.fetch.wait.max.ms = 500
benchi-kafka     | 	replica.high.watermark.checkpoint.interval.ms = 5000
benchi-kafka     | 	replica.lag.time.max.ms = 30000
benchi-kafka     | 	replica.selector.class = null
benchi-kafka     | 	replica.socket.receive.buffer.bytes = 65536
benchi-kafka     | 	replica.socket.timeout.ms = 30000
benchi-kafka     | 	replication.quota.window.num = 11
benchi-kafka     | 	replication.quota.window.size.seconds = 1
benchi-kafka     | 	request.timeout.ms = 30000
benchi-kafka     | 	reserved.broker.max.id = 1000
benchi-kafka     | 	sasl.client.callback.handler.class = null
benchi-kafka     | 	sasl.enabled.mechanisms = [GSSAPI]
benchi-kafka     | 	sasl.jaas.config = null
benchi-kafka     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
benchi-kafka     | 	sasl.kerberos.min.time.before.relogin = 60000
benchi-kafka     | 	sasl.kerberos.principal.to.local.rules = [DEFAULT]
benchi-kafka     | 	sasl.kerberos.service.name = null
benchi-kafka     | 	sasl.kerberos.ticket.renew.jitter = 0.05
benchi-kafka     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
benchi-kafka     | 	sasl.login.callback.handler.class = null
benchi-kafka     | 	sasl.login.class = null
benchi-kafka     | 	sasl.login.connect.timeout.ms = null
benchi-kafka     | 	sasl.login.read.timeout.ms = null
benchi-kafka     | 	sasl.login.refresh.buffer.seconds = 300
benchi-kafka     | 	sasl.login.refresh.min.period.seconds = 60
benchi-kafka     | 	sasl.login.refresh.window.factor = 0.8
benchi-kafka     | 	sasl.login.refresh.window.jitter = 0.05
benchi-kafka     | 	sasl.login.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.login.retry.backoff.ms = 100
benchi-kafka     | 	sasl.mechanism.controller.protocol = GSSAPI
benchi-kafka     | 	sasl.mechanism.inter.broker.protocol = GSSAPI
benchi-kafka     | 	sasl.oauthbearer.clientassertion.audience = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.expiration = 5
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.jti.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.nbf.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.issuer = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.location = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key.passphrase = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.subject = null
benchi-kafka     | 	sasl.oauthbearer.clock.skew.seconds = 30
benchi-kafka     | 	sasl.oauthbearer.expected.audience = null
benchi-kafka     | 	sasl.oauthbearer.expected.issuer = null
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.url = null
benchi-kafka     | 	sasl.oauthbearer.scope.claim.name = scope
benchi-kafka     | 	sasl.oauthbearer.sub.claim.name = sub
benchi-kafka     | 	sasl.oauthbearer.token.endpoint.url = null
benchi-kafka     | 	sasl.server.authn.async.enable = false
benchi-kafka     | 	sasl.server.authn.async.max.threads = 1
benchi-kafka     | 	sasl.server.authn.async.timeout.ms = 30000
benchi-kafka     | 	sasl.server.callback.handler.class = null
benchi-kafka     | 	sasl.server.max.receive.size = 524288
benchi-kafka     | 	security.inter.broker.protocol = PLAINTEXT
benchi-kafka     | 	security.providers = null
benchi-kafka     | 	server.max.startup.time.ms = 9223372036854775807
benchi-kafka     | 	socket.connection.setup.timeout.max.ms = 30000
benchi-kafka     | 	socket.connection.setup.timeout.ms = 10000
benchi-kafka     | 	socket.listen.backlog.size = 50
benchi-kafka     | 	socket.receive.buffer.bytes = 102400
benchi-kafka     | 	socket.request.max.bytes = 104857600
benchi-kafka     | 	socket.send.buffer.bytes = 102400
benchi-kafka     | 	ssl.allow.dn.changes = false
benchi-kafka     | 	ssl.allow.san.changes = false
benchi-kafka     | 	ssl.cipher.suites = []
benchi-kafka     | 	ssl.client.auth = none
benchi-kafka     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	ssl.engine.factory.class = null
benchi-kafka     | 	ssl.key.password = null
benchi-kafka     | 	ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	ssl.keystore.certificate.chain = null
benchi-kafka     | 	ssl.keystore.key = null
benchi-kafka     | 	ssl.keystore.location = null
benchi-kafka     | 	ssl.keystore.password = null
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.principal.mapping.rules = DEFAULT
benchi-kafka     | 	ssl.protocol = TLSv1.3
benchi-kafka     | 	ssl.provider = null
benchi-kafka     | 	ssl.secure.random.implementation = null
benchi-kafka     | 	ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	ssl.truststore.certificates = null
benchi-kafka     | 	ssl.truststore.location = null
benchi-kafka     | 	ssl.truststore.password = null
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     | 	telemetry.max.bytes = 1048576
benchi-kafka     | 	throughput.quota.window.num = 11
benchi-kafka     | 	token.impersonation.validation = true
benchi-kafka     | 	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
benchi-kafka     | 	transaction.max.timeout.ms = 900000
benchi-kafka     | 	transaction.metadata.load.threads = 32
benchi-kafka     | 	transaction.partition.verification.enable = true
benchi-kafka     | 	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
benchi-kafka     | 	transaction.state.log.load.buffer.size = 5242880
benchi-kafka     | 	transaction.state.log.min.isr = 1
benchi-kafka     | 	transaction.state.log.num.partitions = 50
benchi-kafka     | 	transaction.state.log.replication.factor = 1
benchi-kafka     | 	transaction.state.log.segment.bytes = 104857600
benchi-kafka     | 	transactional.id.expiration.ms = 604800000
benchi-kafka     | 	unclean.leader.election.enable = false
benchi-kafka     | 	unstable.api.versions.enable = false
benchi-kafka     | 	unstable.feature.versions.enable = false
benchi-kafka     | 	zookeeper.acl.change.notification.expiration.ms = 900000
benchi-kafka     | 	zookeeper.clientCnxnSocket = null
benchi-kafka     | 	zookeeper.connect = 
benchi-kafka     | 	zookeeper.connection.timeout.ms = null
benchi-kafka     | 	zookeeper.max.in.flight.requests = 10
benchi-kafka     | 	zookeeper.metadata.migration.enable = false
benchi-kafka     | 	zookeeper.metadata.migration.min.batch.size = 200
benchi-kafka     | 	zookeeper.session.timeout.ms = 18000
benchi-kafka     | 	zookeeper.set.acl = false
benchi-kafka     | 	zookeeper.ssl.cipher.suites = null
benchi-kafka     | 	zookeeper.ssl.client.enable = false
benchi-kafka     | 	zookeeper.ssl.crl.enable = false
benchi-kafka     | 	zookeeper.ssl.enabled.protocols = null
benchi-kafka     | 	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
benchi-kafka     | 	zookeeper.ssl.keystore.location = null
benchi-kafka     | 	zookeeper.ssl.keystore.password = null
benchi-kafka     | 	zookeeper.ssl.keystore.type = null
benchi-kafka     | 	zookeeper.ssl.ocsp.enable = false
benchi-kafka     | 	zookeeper.ssl.protocol = TLSv1.2
benchi-kafka     | 	zookeeper.ssl.truststore.location = null
benchi-kafka     | 	zookeeper.ssl.truststore.password = null
benchi-kafka     | 	zookeeper.ssl.truststore.type = null
benchi-kafka     |  (kafka.server.KafkaConfig)
benchi-kafka     | [2025-03-18 16:03:19,104] INFO RemoteLogManagerConfig values: 
benchi-kafka     | 	log.local.retention.bytes = -2
benchi-kafka     | 	log.local.retention.ms = -2
benchi-kafka     | 	remote.fetch.max.wait.ms = 500
benchi-kafka     | 	remote.log.index.file.cache.total.size.bytes = 1073741824
benchi-kafka     | 	remote.log.manager.copier.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.copy.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.copy.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.expiration.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.fetch.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.fetch.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.task.interval.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.max.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.ms = 500
benchi-kafka     | 	remote.log.manager.task.retry.jitter = 0.2
benchi-kafka     | 	remote.log.manager.thread.pool.size = 10
benchi-kafka     | 	remote.log.metadata.custom.metadata.max.bytes = 128
benchi-kafka     | 	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
benchi-kafka     | 	remote.log.metadata.manager.class.path = null
benchi-kafka     | 	remote.log.metadata.manager.impl.prefix = rlmm.config.
benchi-kafka     | 	remote.log.metadata.manager.listener.name = null
benchi-kafka     | 	remote.log.reader.max.pending.tasks = 100
benchi-kafka     | 	remote.log.reader.threads = 10
benchi-kafka     | 	remote.log.storage.manager.class.name = null
benchi-kafka     | 	remote.log.storage.manager.class.path = null
benchi-kafka     | 	remote.log.storage.manager.impl.prefix = rsm.config.
benchi-kafka     | 	remote.log.storage.system.enable = false
benchi-kafka     |  (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
benchi-kafka     | [2025-03-18 16:03:19,119] INFO Registering metric ActiveBalancerCount (io.confluent.databalancer.KafkaDataBalanceManager)
benchi-kafka     | [2025-03-18 16:03:19,137] INFO Instantiating ClusterBalanceManager with an instance of io.confluent.databalancer.SbcDataBalanceManager (ClusterBalanceManager)
benchi-kafka     | [2025-03-18 16:03:19,138] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)
benchi-kafka     | [2025-03-18 16:03:19,140] INFO [ControllerServer id=1] Starting controller (kafka.server.ControllerServer)
benchi-kafka     | [2025-03-18 16:03:19,142] INFO RemoteLogManagerConfig values: 
benchi-kafka     | 	log.local.retention.bytes = -2
benchi-kafka     | 	log.local.retention.ms = -2
benchi-kafka     | 	remote.fetch.max.wait.ms = 500
benchi-kafka     | 	remote.log.index.file.cache.total.size.bytes = 1073741824
benchi-kafka     | 	remote.log.manager.copier.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.copy.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.copy.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.expiration.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.fetch.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.fetch.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.task.interval.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.max.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.ms = 500
benchi-kafka     | 	remote.log.manager.task.retry.jitter = 0.2
benchi-kafka     | 	remote.log.manager.thread.pool.size = 10
benchi-kafka     | 	remote.log.metadata.custom.metadata.max.bytes = 128
benchi-kafka     | 	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
benchi-kafka     | 	remote.log.metadata.manager.class.path = null
benchi-kafka     | 	remote.log.metadata.manager.impl.prefix = rlmm.config.
benchi-kafka     | 	remote.log.metadata.manager.listener.name = null
benchi-kafka     | 	remote.log.reader.max.pending.tasks = 100
benchi-kafka     | 	remote.log.reader.threads = 10
benchi-kafka     | 	remote.log.storage.manager.class.name = null
benchi-kafka     | 	remote.log.storage.manager.class.path = null
benchi-kafka     | 	remote.log.storage.manager.impl.prefix = rsm.config.
benchi-kafka     | 	remote.log.storage.system.enable = false
benchi-kafka     |  (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
benchi-kafka     | [2025-03-18 16:03:19,143] INFO [ControllerServer id=1] FIPS mode enabled: false (kafka.server.ControllerServer)
benchi-kafka     | [2025-03-18 16:03:19,149] INFO AuditLogConfig values: 
benchi-kafka     | 	confluent.security.event.logger.authentication.enable = false
benchi-kafka     | 	confluent.security.event.logger.authentication.event.rate.limit = -1
benchi-kafka     | 	confluent.security.event.logger.authorization.event.rate.limit = -1
benchi-kafka     | 	confluent.security.event.logger.cloudevent.codec = structured
benchi-kafka     | 	confluent.security.event.logger.enable = true
benchi-kafka     | 	confluent.security.event.logger.exporter.class = class io.confluent.security.audit.telemetry.exporter.NonBlockingKafkaExporter
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.create = true
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.partitions = 12
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.replicas = 3
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.retention.bytes = -1
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.retention.ms = 2592000000
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.roll.ms = 14400000
benchi-kafka     | 	confluent.security.event.logger.kafka.request.rate.limit = -1
benchi-kafka     | 	confluent.security.event.logger.physical.cluster.id = 
benchi-kafka     | 	confluent.security.event.router.cache.entries = 10000
benchi-kafka     | 	confluent.security.event.router.config = 
benchi-kafka     | 	confluent.security.event.router.named.config.enabled = false
benchi-kafka     |  (io.confluent.security.audit.AuditLogConfig)
benchi-kafka     | [2025-03-18 16:03:19,149] INFO CrnAuthorityConfig values: 
benchi-kafka     | 	confluent.authorizer.authority.cache.entries = 10000
benchi-kafka     | 	confluent.authorizer.authority.name = 
benchi-kafka     | 	confluent.metadata.server.api.flavor = CP
benchi-kafka     |  (io.confluent.crn.CrnAuthorityConfig)
benchi-kafka     | [2025-03-18 16:03:19,150] INFO NamedConfigEnabled: false (io.confluent.security.audit.provider.ConfluentAuditLogProvider)
benchi-kafka     | [2025-03-18 16:03:19,150] INFO AuditLogConfig values: 
benchi-kafka     | 	confluent.security.event.logger.authentication.enable = false
benchi-kafka     | 	confluent.security.event.logger.authentication.event.rate.limit = -1
benchi-kafka     | 	confluent.security.event.logger.authorization.event.rate.limit = -1
benchi-kafka     | 	confluent.security.event.logger.cloudevent.codec = structured
benchi-kafka     | 	confluent.security.event.logger.enable = true
benchi-kafka     | 	confluent.security.event.logger.exporter.class = class io.confluent.security.audit.telemetry.exporter.NonBlockingKafkaExporter
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.create = true
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.partitions = 12
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.replicas = 3
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.retention.bytes = -1
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.retention.ms = 2592000000
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.roll.ms = 14400000
benchi-kafka     | 	confluent.security.event.logger.kafka.request.rate.limit = -1
benchi-kafka     | 	confluent.security.event.logger.physical.cluster.id = 
benchi-kafka     | 	confluent.security.event.router.cache.entries = 10000
benchi-kafka     | 	confluent.security.event.router.config = 
benchi-kafka     | 	confluent.security.event.router.named.config.enabled = false
benchi-kafka     |  (io.confluent.security.audit.AuditLogConfig)
benchi-kafka     | [2025-03-18 16:03:19,162] INFO CrnAuthorityConfig values: 
benchi-kafka     | 	confluent.authorizer.authority.cache.entries = 10000
benchi-kafka     | 	confluent.authorizer.authority.name = 
benchi-kafka     | 	confluent.metadata.server.api.flavor = CP
benchi-kafka     |  (io.confluent.crn.CrnAuthorityConfig)
benchi-kafka     | [2025-03-18 16:03:19,163] INFO MultiTenantAuditLogConfig values: 
benchi-kafka     | 	confluent.security.event.logger.client.ip.enable = false
benchi-kafka     | 	confluent.security.event.logger.multitenant.enable = false
benchi-kafka     |  (io.confluent.kafka.multitenant.authorizer.MultiTenantAuditLogConfig)
benchi-kafka     | [2025-03-18 16:03:19,163] INFO AuditLogConfig values: 
benchi-kafka     | 	confluent.security.event.logger.authentication.enable = false
benchi-kafka     | 	confluent.security.event.logger.authentication.event.rate.limit = -1
benchi-kafka     | 	confluent.security.event.logger.authorization.event.rate.limit = -1
benchi-kafka     | 	confluent.security.event.logger.cloudevent.codec = structured
benchi-kafka     | 	confluent.security.event.logger.enable = true
benchi-kafka     | 	confluent.security.event.logger.exporter.class = class io.confluent.security.audit.telemetry.exporter.NonBlockingKafkaExporter
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.create = true
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.partitions = 12
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.replicas = 3
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.retention.bytes = -1
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.retention.ms = 2592000000
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.roll.ms = 14400000
benchi-kafka     | 	confluent.security.event.logger.kafka.request.rate.limit = -1
benchi-kafka     | 	confluent.security.event.logger.physical.cluster.id = 
benchi-kafka     | 	confluent.security.event.router.cache.entries = 10000
benchi-kafka     | 	confluent.security.event.router.config = 
benchi-kafka     | 	confluent.security.event.router.named.config.enabled = false
benchi-kafka     |  (io.confluent.security.audit.AuditLogConfig)
benchi-kafka     | [2025-03-18 16:03:19,163] INFO Audit Log rate limiter reconfigured: Authn: -1, Authz: -1, Kafka request: -1 (io.confluent.security.audit.provider.AuditLogRateLimiter)
benchi-kafka     | [2025-03-18 16:03:19,256] INFO [SocketServer listenerType=CONTROLLER, nodeId=1] Creating data-plane acceptor and processors for endpoint : ListenerName(CONTROLLER) (kafka.network.SocketServer)
benchi-kafka     | [2025-03-18 16:03:19,263] INFO Quota CONTROLLER-per-ip-connection-rate configured - (max: -1.0, floor: -1.0, adjustment: -1.0) (kafka.network.ListenerIpAutoTuningQuota)
benchi-kafka     | [2025-03-18 16:03:19,264] INFO Quota CONTROLLER-per-tenant-connection-rate configured - (max: -1.0, floor: -1.0, adjustment: -1.0) (kafka.network.ListenerTenantAutoTuningQuota)
benchi-kafka     | [2025-03-18 16:03:19,264] INFO Quota CONTROLLER-connection-rate configured - (max: 1.7976931348623157E308, floor: 1.7976931348623157E308, adjustment: 5.0) (kafka.network.ListenerAutoTuningQuota)
benchi-kafka     | [2025-03-18 16:03:19,265] INFO Updated connection-tokens max connection creation rate to 1.7976931348623157E308 (kafka.network.ConnectionQuotas)
benchi-kafka     | [2025-03-18 16:03:19,276] INFO Config values: 
benchi-kafka     | 	confluent.security.event.logger.detailed.audit.logs.disabled.apis = 
benchi-kafka     | 	confluent.security.event.logger.enable.detailed.audit.logs = false
benchi-kafka     | 	confluent.security.event.logger.enable.produce.consume.audit.logs = false
benchi-kafka     |  (org.apache.kafka.common.requests.DetailedRequestAuditLogFilter$Config)
benchi-kafka     | [2025-03-18 16:03:19,309] INFO Config values: 
benchi-kafka     | 	confluent.security.event.logger.detailed.audit.logs.disabled.apis = 
benchi-kafka     | 	confluent.security.event.logger.enable.detailed.audit.logs = false
benchi-kafka     | 	confluent.security.event.logger.enable.produce.consume.audit.logs = false
benchi-kafka     |  (org.apache.kafka.common.requests.DetailedRequestAuditLogFilter$Config)
benchi-kafka     | [2025-03-18 16:03:19,310] INFO Config values: 
benchi-kafka     | 	confluent.security.event.logger.detailed.audit.logs.disabled.apis = 
benchi-kafka     | 	confluent.security.event.logger.enable.detailed.audit.logs = false
benchi-kafka     | 	confluent.security.event.logger.enable.produce.consume.audit.logs = false
benchi-kafka     |  (org.apache.kafka.common.requests.DetailedRequestAuditLogFilter$Config)
benchi-kafka     | [2025-03-18 16:03:19,311] INFO [SocketServer listenerType=CONTROLLER, nodeId=1] Created data-plane acceptor and processors for endpoint : ListenerName(CONTROLLER) (kafka.network.SocketServer)
benchi-kafka     | [2025-03-18 16:03:19,315] INFO [SharedServer id=1] Using broker:29092 as bootstrap.servers for inter broker client config. (kafka.server.SharedServer)
benchi-kafka     | [2025-03-18 16:03:19,322] INFO [SharedServer id=1] Starting SharedServer (kafka.server.SharedServer)
benchi-kafka     | [2025-03-18 16:03:19,323] INFO RemoteLogManagerConfig values: 
benchi-kafka     | 	log.local.retention.bytes = -2
benchi-kafka     | 	log.local.retention.ms = -2
benchi-kafka     | 	remote.fetch.max.wait.ms = 500
benchi-kafka     | 	remote.log.index.file.cache.total.size.bytes = 1073741824
benchi-kafka     | 	remote.log.manager.copier.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.copy.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.copy.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.expiration.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.fetch.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.fetch.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.task.interval.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.max.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.ms = 500
benchi-kafka     | 	remote.log.manager.task.retry.jitter = 0.2
benchi-kafka     | 	remote.log.manager.thread.pool.size = 10
benchi-kafka     | 	remote.log.metadata.custom.metadata.max.bytes = 128
benchi-kafka     | 	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
benchi-kafka     | 	remote.log.metadata.manager.class.path = null
benchi-kafka     | 	remote.log.metadata.manager.impl.prefix = rlmm.config.
benchi-kafka     | 	remote.log.metadata.manager.listener.name = null
benchi-kafka     | 	remote.log.reader.max.pending.tasks = 100
benchi-kafka     | 	remote.log.reader.threads = 10
benchi-kafka     | 	remote.log.storage.manager.class.name = null
benchi-kafka     | 	remote.log.storage.manager.class.path = null
benchi-kafka     | 	remote.log.storage.manager.impl.prefix = rsm.config.
benchi-kafka     | 	remote.log.storage.system.enable = false
benchi-kafka     |  (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
benchi-kafka     | [2025-03-18 16:03:19,330] INFO RemoteLogManagerConfig values: 
benchi-kafka     | 	log.local.retention.bytes = -2
benchi-kafka     | 	log.local.retention.ms = -2
benchi-kafka     | 	remote.fetch.max.wait.ms = 500
benchi-kafka     | 	remote.log.index.file.cache.total.size.bytes = 1073741824
benchi-kafka     | 	remote.log.manager.copier.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.copy.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.copy.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.expiration.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.fetch.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.fetch.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.task.interval.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.max.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.ms = 500
benchi-kafka     | 	remote.log.manager.task.retry.jitter = 0.2
benchi-kafka     | 	remote.log.manager.thread.pool.size = 10
benchi-kafka     | 	remote.log.metadata.custom.metadata.max.bytes = 128
benchi-kafka     | 	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
benchi-kafka     | 	remote.log.metadata.manager.class.path = null
benchi-kafka     | 	remote.log.metadata.manager.impl.prefix = rlmm.config.
benchi-kafka     | 	remote.log.metadata.manager.listener.name = null
benchi-kafka     | 	remote.log.reader.max.pending.tasks = 100
benchi-kafka     | 	remote.log.reader.threads = 10
benchi-kafka     | 	remote.log.storage.manager.class.name = null
benchi-kafka     | 	remote.log.storage.manager.class.path = null
benchi-kafka     | 	remote.log.storage.manager.impl.prefix = rsm.config.
benchi-kafka     | 	remote.log.storage.system.enable = false
benchi-kafka     |  (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
benchi-kafka     | [2025-03-18 16:03:19,388] INFO [MergedLog partition=__cluster_metadata-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:03:19,388] INFO [MergedLog partition=__cluster_metadata-0, dir=/tmp/kraft-combined-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:03:19,388] INFO [MergedLog partition=__cluster_metadata-0, dir=/tmp/kraft-combined-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:03:19,393] INFO Initialized snapshots with IDs SortedSet() from /tmp/kraft-combined-logs/__cluster_metadata-0 (kafka.raft.KafkaMetadataLog$)
benchi-kafka     | [2025-03-18 16:03:19,399] INFO [raft-expiration-reaper]: Starting (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:03:19,404] INFO [RaftManager id=1] Reading KRaft snapshot and log as part of the initialization (org.apache.kafka.raft.KafkaRaftClient)
benchi-kafka     | [2025-03-18 16:03:19,405] INFO [RaftManager id=1] Starting request manager with static voters: [broker:29093 (id: 1 rack: null)] (org.apache.kafka.raft.KafkaRaftClient)
benchi-kafka     | [2025-03-18 16:03:19,417] INFO [RaftManager id=1] Completed transition to Unattached(epoch=0, voters=[1], electionTimeoutMs=1051) from null (org.apache.kafka.raft.QuorumState)
benchi-kafka     | [2025-03-18 16:03:19,421] INFO [RaftManager id=1] Completed transition to CandidateState(localId=1, localDirectoryId=xbl_6JROMTfs6aQ8gB3fjw,epoch=1, retries=1, voteStates={1=GRANTED}, highWatermark=Optional.empty, electionTimeoutMs=1068) from Unattached(epoch=0, voters=[1], electionTimeoutMs=1051) (org.apache.kafka.raft.QuorumState)
benchi-kafka     | [2025-03-18 16:03:19,424] INFO [RaftManager id=1] Completed transition to Leader(localId=1, epoch=1, epochStartOffset=0, highWatermark=Optional.empty, voterStates={1=ReplicaState(nodeId=1, endOffset=Optional.empty, lastFetchTimestamp=-1, lastCaughtUpTimestamp=-1, hasAcknowledgedLeader=true)}) from CandidateState(localId=1, localDirectoryId=xbl_6JROMTfs6aQ8gB3fjw,epoch=1, retries=1, voteStates={1=GRANTED}, highWatermark=Optional.empty, electionTimeoutMs=1068) (org.apache.kafka.raft.QuorumState)
benchi-kafka     | [2025-03-18 16:03:19,429] INFO [kafka-1-raft-outbound-request-thread]: Starting (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
benchi-kafka     | [2025-03-18 16:03:19,429] INFO [kafka-1-raft-io-thread]: Starting (org.apache.kafka.raft.KafkaRaftClientDriver)
benchi-kafka     | [2025-03-18 16:03:19,439] INFO [RaftManager id=1] High watermark set to LogOffsetMetadata(offset=1, metadata=Optional[(segmentBaseOffset=0,relativePositionInSegment=91)]) for the first time for epoch 1 based on indexOfHw 0 and voters [ReplicaState(nodeId=1, endOffset=Optional[LogOffsetMetadata(offset=1, metadata=Optional[(segmentBaseOffset=0,relativePositionInSegment=91)])], lastFetchTimestamp=-1, lastCaughtUpTimestamp=-1, hasAcknowledgedLeader=true)] (org.apache.kafka.raft.LeaderState)
benchi-kafka     | [2025-03-18 16:03:19,443] INFO [MetadataLoader id=1] initializeNewPublishers: The loader is still catching up because we have loaded up to offset -1, but the high water mark is 1 (org.apache.kafka.image.loader.MetadataLoader)
benchi-kafka     | [2025-03-18 16:03:19,444] INFO [ControllerServer id=1] Waiting for controller quorum voters future (kafka.server.ControllerServer)
benchi-kafka     | [2025-03-18 16:03:19,444] INFO [ControllerServer id=1] Finished waiting for controller quorum voters future (kafka.server.ControllerServer)
benchi-kafka     | [2025-03-18 16:03:19,447] INFO [RaftManager id=1] Registered the listener org.apache.kafka.image.loader.MetadataLoader@1057686833 (org.apache.kafka.raft.KafkaRaftClient)
benchi-kafka     | [2025-03-18 16:03:19,448] INFO [MetadataLoader id=1] maybePublishMetadata(LOG_DELTA): The loader is still catching up because we have not loaded a controller record as of offset 0 and high water mark is 1 (org.apache.kafka.image.loader.MetadataLoader)
benchi-kafka     | [2025-03-18 16:03:19,467] INFO [ControllerServer id=1] Creating new QuorumController with clusterId MkU3OEVBNTcwNTJENDM2Qk. (org.apache.kafka.controller.QuorumController)
benchi-kafka     | [2025-03-18 16:03:19,468] INFO [RaftManager id=1] Registered the listener org.apache.kafka.controller.QuorumController$QuorumMetaLogListener@446694225 (org.apache.kafka.raft.KafkaRaftClient)
benchi-kafka     | [2025-03-18 16:03:19,469] INFO [ControllerServer id=1] Becoming the active controller at epoch 1, next write offset 1. (org.apache.kafka.controller.QuorumController)
benchi-kafka     | [2025-03-18 16:03:19,472] WARN [ControllerServer id=1] Performing controller activation. The metadata log appears to be empty. Appending 1 bootstrap record(s) in metadata transaction at metadata.version 3.8-IV0A from bootstrap source 'the binary bootstrap metadata file: /tmp/kraft-combined-logs/bootstrap.checkpoint'. Setting the ZK migration state to NONE since this is a de-novo KRaft cluster. (org.apache.kafka.controller.QuorumController)
benchi-kafka     | [2025-03-18 16:03:19,473] INFO [ControllerServer id=1] Replayed BeginTransactionRecord(name='Bootstrap records') at offset 1. (org.apache.kafka.controller.OffsetControlManager)
benchi-kafka     | [2025-03-18 16:03:19,473] INFO [ControllerServer id=1] Replayed a Confluent FeatureLevelRecord setting metadata version to 3.8-IV0A (org.apache.kafka.controller.FeatureControlManager)
benchi-kafka     | [2025-03-18 16:03:19,473] INFO [ControllerServer id=1] Replayed EndTransactionRecord() at offset 4. (org.apache.kafka.controller.OffsetControlManager)
benchi-kafka     | [2025-03-18 16:03:19,475] INFO Empty logDirs received! Disk based throttling won't be activated! (org.apache.kafka.server.config.DiskUsageBasedThrottlingConfig)
benchi-kafka     | [2025-03-18 16:03:19,475] INFO Empty logDirs received! Disk based throttling won't be activated! (org.apache.kafka.server.config.DiskUsageBasedThrottlingConfig)
benchi-kafka     | [2025-03-18 16:03:19,479] INFO [controller-1-ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:03:19,481] INFO [controller-1-ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:03:19,481] INFO Empty logDirs received! Disk based throttling won't be activated! (org.apache.kafka.server.config.DiskUsageBasedThrottlingConfig)
benchi-kafka     | [2025-03-18 16:03:19,482] INFO [controller-1-ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:03:19,483] INFO Client Quota Max Throttle Time is updated from 5000 to 1000 (kafka.server.ClientRequestQuotaManager)
benchi-kafka     | [2025-03-18 16:03:19,487] INFO Empty logDirs received! Disk based throttling won't be activated! (org.apache.kafka.server.config.DiskUsageBasedThrottlingConfig)
benchi-kafka     | [2025-03-18 16:03:19,488] INFO Client Quota Max Throttle Time is updated from 5000 to 1000 (kafka.server.ClusterLinkRequestQuotaManager)
benchi-kafka     | [2025-03-18 16:03:19,488] INFO [controller-1-ThrottledChannelReaper-ClusterLinkRequest]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:03:19,490] INFO [controller-1-ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:03:19,500] INFO [ExpirationReaper-1-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:03:19,503] INFO Config values: 
benchi-kafka     | 	confluent.request.log.api.samples.per.min = 
benchi-kafka     | 	confluent.request.log.enable.admin.apis = true
benchi-kafka     | 	confluent.request.log.enable.slowlog = false
benchi-kafka     | 	confluent.request.log.samples.per.min = 0
benchi-kafka     | 	confluent.request.slowlog.threshold.override = -1.0
benchi-kafka     | 	confluent.request.slowlog.threshold.p99.min = -1.0
benchi-kafka     |  (org.apache.kafka.common.requests.SamplingRequestLogFilter$Config)
benchi-kafka     | [2025-03-18 16:03:19,504] INFO Config values: 
benchi-kafka     | 	confluent.request.log.api.samples.per.min = 
benchi-kafka     | 	confluent.request.log.enable.admin.apis = true
benchi-kafka     | 	confluent.request.log.enable.slowlog = false
benchi-kafka     | 	confluent.request.log.samples.per.min = 0
benchi-kafka     | 	confluent.request.slowlog.threshold.override = -1.0
benchi-kafka     | 	confluent.request.slowlog.threshold.p99.min = -1.0
benchi-kafka     |  (org.apache.kafka.common.requests.SamplingRequestLogFilter$Config)
benchi-kafka     | [2025-03-18 16:03:19,504] INFO [MetadataLoader id=1] maybePublishMetadata(LOG_DELTA): The loader finished catching up to the current high water mark of 5 (org.apache.kafka.image.loader.MetadataLoader)
benchi-kafka     | [2025-03-18 16:03:19,505] INFO Config values: 
benchi-kafka     | 	confluent.request.log.api.samples.per.min = 
benchi-kafka     | 	confluent.request.log.enable.admin.apis = true
benchi-kafka     | 	confluent.request.log.enable.slowlog = false
benchi-kafka     | 	confluent.request.log.samples.per.min = 0
benchi-kafka     | 	confluent.request.slowlog.threshold.override = -1.0
benchi-kafka     | 	confluent.request.slowlog.threshold.p99.min = -1.0
benchi-kafka     |  (org.apache.kafka.common.requests.SamplingRequestLogFilter$Config)
benchi-kafka     | [2025-03-18 16:03:19,505] INFO Config values: 
benchi-kafka     | 	confluent.request.log.api.samples.per.min = 
benchi-kafka     | 	confluent.request.log.enable.admin.apis = true
benchi-kafka     | 	confluent.request.log.enable.slowlog = false
benchi-kafka     | 	confluent.request.log.samples.per.min = 0
benchi-kafka     | 	confluent.request.slowlog.threshold.override = -1.0
benchi-kafka     | 	confluent.request.slowlog.threshold.p99.min = -1.0
benchi-kafka     |  (org.apache.kafka.common.requests.SamplingRequestLogFilter$Config)
benchi-kafka     | [2025-03-18 16:03:19,505] INFO Config values: 
benchi-kafka     | 	confluent.request.log.api.samples.per.min = 
benchi-kafka     | 	confluent.request.log.enable.admin.apis = true
benchi-kafka     | 	confluent.request.log.enable.slowlog = false
benchi-kafka     | 	confluent.request.log.samples.per.min = 0
benchi-kafka     | 	confluent.request.slowlog.threshold.override = -1.0
benchi-kafka     | 	confluent.request.slowlog.threshold.p99.min = -1.0
benchi-kafka     |  (org.apache.kafka.common.requests.SamplingRequestLogFilter$Config)
benchi-kafka     | [2025-03-18 16:03:19,505] INFO Config values: 
benchi-kafka     | 	confluent.request.log.api.samples.per.min = 
benchi-kafka     | 	confluent.request.log.enable.admin.apis = true
benchi-kafka     | 	confluent.request.log.enable.slowlog = false
benchi-kafka     | 	confluent.request.log.samples.per.min = 0
benchi-kafka     | 	confluent.request.slowlog.threshold.override = -1.0
benchi-kafka     | 	confluent.request.slowlog.threshold.p99.min = -1.0
benchi-kafka     |  (org.apache.kafka.common.requests.SamplingRequestLogFilter$Config)
benchi-kafka     | [2025-03-18 16:03:19,506] INFO Config values: 
benchi-kafka     | 	confluent.request.log.api.samples.per.min = 
benchi-kafka     | 	confluent.request.log.enable.admin.apis = true
benchi-kafka     | 	confluent.request.log.enable.slowlog = false
benchi-kafka     | 	confluent.request.log.samples.per.min = 0
benchi-kafka     | 	confluent.request.slowlog.threshold.override = -1.0
benchi-kafka     | 	confluent.request.slowlog.threshold.p99.min = -1.0
benchi-kafka     |  (org.apache.kafka.common.requests.SamplingRequestLogFilter$Config)
benchi-kafka     | [2025-03-18 16:03:19,506] INFO Config values: 
benchi-kafka     | 	confluent.request.log.api.samples.per.min = 
benchi-kafka     | 	confluent.request.log.enable.admin.apis = true
benchi-kafka     | 	confluent.request.log.enable.slowlog = false
benchi-kafka     | 	confluent.request.log.samples.per.min = 0
benchi-kafka     | 	confluent.request.slowlog.threshold.override = -1.0
benchi-kafka     | 	confluent.request.slowlog.threshold.p99.min = -1.0
benchi-kafka     |  (org.apache.kafka.common.requests.SamplingRequestLogFilter$Config)
benchi-kafka     | [2025-03-18 16:03:19,508] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing SnapshotGenerator with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)
benchi-kafka     | [2025-03-18 16:03:19,511] INFO [ControllerServer id=1] Self-Balancing Kafka is enabled and will be installed as a metadata publisher. (kafka.server.ControllerServer)
benchi-kafka     | [2025-03-18 16:03:19,512] INFO [ControllerServer id=1] Waiting for the controller metadata publishers to be installed (kafka.server.ControllerServer)
benchi-kafka     | [2025-03-18 16:03:19,512] INFO [ControllerServer id=1] Finished waiting for the controller metadata publishers to be installed (kafka.server.ControllerServer)
benchi-kafka     | [2025-03-18 16:03:19,512] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing KRaftMetadataCachePublisher with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)
benchi-kafka     | [2025-03-18 16:03:19,512] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing FeaturesPublisher with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)
benchi-kafka     | [2025-03-18 16:03:19,512] INFO [SocketServer listenerType=CONTROLLER, nodeId=1] Enabling request processing. (kafka.network.SocketServer)
benchi-kafka     | [2025-03-18 16:03:19,512] INFO [ControllerServer id=1] Loaded new metadata Features(metadataVersion=3.8-IV0A, finalizedFeatures={confluent.metadata.version=120}, finalizedFeaturesEpoch=4). (org.apache.kafka.metadata.publisher.FeaturesPublisher)
benchi-kafka     | [2025-03-18 16:03:19,512] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing ControllerRegistrationsPublisher with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)
benchi-kafka     | [2025-03-18 16:03:19,512] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing ControllerRegistrationManager with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)
benchi-kafka     | [2025-03-18 16:03:19,513] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing DynamicConfigPublisher controller id=1 with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)
benchi-kafka     | [2025-03-18 16:03:19,513] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing DynamicClientQuotaPublisher controller id=1 with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)
benchi-kafka     | [2025-03-18 16:03:19,513] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing ScramPublisher controller id=1 with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)
benchi-kafka     | [2025-03-18 16:03:19,514] INFO Awaiting socket connections on broker:29093. (kafka.network.DataPlaneAcceptor)
benchi-kafka     | [2025-03-18 16:03:19,514] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing DelegationTokenPublisher controller id=1 with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)
benchi-kafka     | [2025-03-18 16:03:19,514] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing ControllerMetadataMetricsPublisher with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)
benchi-kafka     | [2025-03-18 16:03:19,515] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing ConfluentControllerMetricsPublisher with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)
benchi-kafka     | [2025-03-18 16:03:19,515] INFO [ConfluentControllerMetricsChanges id=1] Finished reloading all Confluent controller metrics in 0 ms. (org.apache.kafka.controller.metrics.ConfluentControllerMetricsChanges)
benchi-kafka     | [2025-03-18 16:03:19,515] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing CellControllerMetadataMetricsPublisher with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)
benchi-kafka     | [2025-03-18 16:03:19,515] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing SbcDataBalanceManager with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)
benchi-kafka     | [2025-03-18 16:03:19,515] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing AclPublisher controller id=1 with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)
benchi-kafka     | [2025-03-18 16:03:19,518] INFO Balancer received a new Replica Exclusions Image (image: , delta: BrokerReplicaExclusionsDelta{newExclusions=[], removedExclusions=[]}) (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:03:19,518] INFO Handling event SbcAlteredExclusionsEvent-4 (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:03:19,519] INFO SBC Event SbcMetadataUpdateEvent-1 generated 1 more events to enqueue in the following order - [SbcConfigUpdateEvent-3]. Enqueuing... (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:03:19,520] INFO Handling event SbcLeaderUpdateEvent-2 (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:03:19,520] INFO This balancer node is now the metadata quorum leader. Activating kafkadatabalance manager without alive broker snapshot. (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:03:19,520] INFO Handling event SbcConfigUpdateEvent-3 (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:03:19,520] INFO Cluster metadata containing at least one unfenced broker not yet available, SBC config processing delayed. (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:03:19,520] INFO Handling event SbcKraftStartupEvent-5 (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:03:19,520] INFO Cluster metadata containing at least one unfenced broker not yet available, SBC startup delayed. (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:03:19,521] INFO [controller-1-to-controller-registration-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
benchi-kafka     | [2025-03-18 16:03:19,521] INFO [ControllerServer id=1] Waiting for all of the authorizer futures to be completed (kafka.server.ControllerServer)
benchi-kafka     | [2025-03-18 16:03:19,521] INFO [ControllerServer id=1] Finished waiting for all of the authorizer futures to be completed (kafka.server.ControllerServer)
benchi-kafka     | [2025-03-18 16:03:19,521] INFO [ControllerServer id=1] Waiting for all of the SocketServer Acceptors to be started (kafka.server.ControllerServer)
benchi-kafka     | [2025-03-18 16:03:19,521] INFO [ControllerServer id=1] Finished waiting for all of the SocketServer Acceptors to be started (kafka.server.ControllerServer)
benchi-kafka     | [2025-03-18 16:03:19,521] INFO [ControllerServer id=1] Waiting for userDeletionHandler futures to be completed (kafka.server.ControllerServer)
benchi-kafka     | [2025-03-18 16:03:19,521] INFO [ControllerServer id=1] Finished waiting for userDeletionHandler futures to be completed (kafka.server.ControllerServer)
benchi-kafka     | [2025-03-18 16:03:19,521] INFO [ControllerRegistrationManager id=1 incarnation=IPkszY1BS0qIQqG4IWtfkw] initialized channel manager. (kafka.server.ControllerRegistrationManager)
benchi-kafka     | [2025-03-18 16:03:19,521] INFO [controller-1-to-controller-registration-channel-manager]: Recorded new KRaft controller, from now on will use node broker:29093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
benchi-kafka     | [2025-03-18 16:03:19,522] INFO [BrokerServer id=1] Transition from SHUTDOWN to STARTING (kafka.server.BrokerServer)
benchi-kafka     | [2025-03-18 16:03:19,522] INFO [ControllerRegistrationManager id=1 incarnation=IPkszY1BS0qIQqG4IWtfkw] sendControllerRegistration: attempting to send ControllerRegistrationRequestData(controllerId=1, incarnationId=IPkszY1BS0qIQqG4IWtfkw, zkMigrationReady=false, listeners=[Listener(name='CONTROLLER', host='broker', port=29093, securityProtocol=0)], features=[Feature(name='confluent.metadata.version', minSupportedVersion=1, maxSupportedVersion=120), Feature(name='metadata.version', minSupportedVersion=1, maxSupportedVersion=20)], metadataEncryptors=[]) (kafka.server.ControllerRegistrationManager)
benchi-kafka     | [2025-03-18 16:03:19,522] INFO [BrokerServer id=1] Starting broker (kafka.server.BrokerServer)
benchi-kafka     | [2025-03-18 16:03:19,524] INFO RemoteLogManagerConfig values: 
benchi-kafka     | 	log.local.retention.bytes = -2
benchi-kafka     | 	log.local.retention.ms = -2
benchi-kafka     | 	remote.fetch.max.wait.ms = 500
benchi-kafka     | 	remote.log.index.file.cache.total.size.bytes = 1073741824
benchi-kafka     | 	remote.log.manager.copier.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.copy.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.copy.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.expiration.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.fetch.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.fetch.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.task.interval.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.max.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.ms = 500
benchi-kafka     | 	remote.log.manager.task.retry.jitter = 0.2
benchi-kafka     | 	remote.log.manager.thread.pool.size = 10
benchi-kafka     | 	remote.log.metadata.custom.metadata.max.bytes = 128
benchi-kafka     | 	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
benchi-kafka     | 	remote.log.metadata.manager.class.path = null
benchi-kafka     | 	remote.log.metadata.manager.impl.prefix = rlmm.config.
benchi-kafka     | 	remote.log.metadata.manager.listener.name = null
benchi-kafka     | 	remote.log.reader.max.pending.tasks = 100
benchi-kafka     | 	remote.log.reader.threads = 10
benchi-kafka     | 	remote.log.storage.manager.class.name = null
benchi-kafka     | 	remote.log.storage.manager.class.path = null
benchi-kafka     | 	remote.log.storage.manager.impl.prefix = rsm.config.
benchi-kafka     | 	remote.log.storage.system.enable = false
benchi-kafka     |  (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
benchi-kafka     | [2025-03-18 16:03:19,529] INFO [BrokerServer id=1] FIPS mode enabled: false (kafka.server.BrokerServer)
benchi-kafka     | [2025-03-18 16:03:19,533] INFO Empty logDirs received! Disk based throttling won't be activated! (org.apache.kafka.server.config.DiskUsageBasedThrottlingConfig)
benchi-kafka     | [2025-03-18 16:03:19,533] INFO [broker-1-ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:03:19,534] INFO Empty logDirs received! Disk based throttling won't be activated! (org.apache.kafka.server.config.DiskUsageBasedThrottlingConfig)
benchi-kafka     | [2025-03-18 16:03:19,534] INFO [broker-1-ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:03:19,535] INFO Client Quota Max Throttle Time is updated from 5000 to 1000 (kafka.server.ClientRequestQuotaManager)
benchi-kafka     | [2025-03-18 16:03:19,535] INFO [broker-1-ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:03:19,535] INFO Empty logDirs received! Disk based throttling won't be activated! (org.apache.kafka.server.config.DiskUsageBasedThrottlingConfig)
benchi-kafka     | [2025-03-18 16:03:19,536] INFO Client Quota Max Throttle Time is updated from 5000 to 1000 (kafka.server.ClusterLinkRequestQuotaManager)
benchi-kafka     | [2025-03-18 16:03:19,536] INFO [broker-1-ThrottledChannelReaper-ClusterLinkRequest]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:03:19,536] INFO [broker-1-ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:03:19,547] INFO Skip DiskIOManager init: confluent.disk.io.manager.enable = false (kafka.server.resource.DiskIOManager)
benchi-kafka     | [2025-03-18 16:03:19,548] INFO AuditLogConfig values: 
benchi-kafka     | 	confluent.security.event.logger.authentication.enable = false
benchi-kafka     | 	confluent.security.event.logger.authentication.event.rate.limit = -1
benchi-kafka     | 	confluent.security.event.logger.authorization.event.rate.limit = -1
benchi-kafka     | 	confluent.security.event.logger.cloudevent.codec = structured
benchi-kafka     | 	confluent.security.event.logger.enable = true
benchi-kafka     | 	confluent.security.event.logger.exporter.class = class io.confluent.security.audit.telemetry.exporter.NonBlockingKafkaExporter
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.create = true
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.partitions = 12
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.replicas = 3
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.retention.bytes = -1
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.retention.ms = 2592000000
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.roll.ms = 14400000
benchi-kafka     | 	confluent.security.event.logger.kafka.request.rate.limit = -1
benchi-kafka     | 	confluent.security.event.logger.physical.cluster.id = 
benchi-kafka     | 	confluent.security.event.router.cache.entries = 10000
benchi-kafka     | 	confluent.security.event.router.config = 
benchi-kafka     | 	confluent.security.event.router.named.config.enabled = false
benchi-kafka     |  (io.confluent.security.audit.AuditLogConfig)
benchi-kafka     | [2025-03-18 16:03:19,548] INFO CrnAuthorityConfig values: 
benchi-kafka     | 	confluent.authorizer.authority.cache.entries = 10000
benchi-kafka     | 	confluent.authorizer.authority.name = 
benchi-kafka     | 	confluent.metadata.server.api.flavor = CP
benchi-kafka     |  (io.confluent.crn.CrnAuthorityConfig)
benchi-kafka     | [2025-03-18 16:03:19,548] INFO NamedConfigEnabled: false (io.confluent.security.audit.provider.ConfluentAuditLogProvider)
benchi-kafka     | [2025-03-18 16:03:19,548] INFO AuditLogConfig values: 
benchi-kafka     | 	confluent.security.event.logger.authentication.enable = false
benchi-kafka     | 	confluent.security.event.logger.authentication.event.rate.limit = -1
benchi-kafka     | 	confluent.security.event.logger.authorization.event.rate.limit = -1
benchi-kafka     | 	confluent.security.event.logger.cloudevent.codec = structured
benchi-kafka     | 	confluent.security.event.logger.enable = true
benchi-kafka     | 	confluent.security.event.logger.exporter.class = class io.confluent.security.audit.telemetry.exporter.NonBlockingKafkaExporter
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.create = true
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.partitions = 12
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.replicas = 3
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.retention.bytes = -1
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.retention.ms = 2592000000
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.roll.ms = 14400000
benchi-kafka     | 	confluent.security.event.logger.kafka.request.rate.limit = -1
benchi-kafka     | 	confluent.security.event.logger.physical.cluster.id = 
benchi-kafka     | 	confluent.security.event.router.cache.entries = 10000
benchi-kafka     | 	confluent.security.event.router.config = 
benchi-kafka     | 	confluent.security.event.router.named.config.enabled = false
benchi-kafka     |  (io.confluent.security.audit.AuditLogConfig)
benchi-kafka     | [2025-03-18 16:03:19,548] INFO CrnAuthorityConfig values: 
benchi-kafka     | 	confluent.authorizer.authority.cache.entries = 10000
benchi-kafka     | 	confluent.authorizer.authority.name = 
benchi-kafka     | 	confluent.metadata.server.api.flavor = CP
benchi-kafka     |  (io.confluent.crn.CrnAuthorityConfig)
benchi-kafka     | [2025-03-18 16:03:19,548] INFO MultiTenantAuditLogConfig values: 
benchi-kafka     | 	confluent.security.event.logger.client.ip.enable = false
benchi-kafka     | 	confluent.security.event.logger.multitenant.enable = false
benchi-kafka     |  (io.confluent.kafka.multitenant.authorizer.MultiTenantAuditLogConfig)
benchi-kafka     | [2025-03-18 16:03:19,548] INFO AuditLogConfig values: 
benchi-kafka     | 	confluent.security.event.logger.authentication.enable = false
benchi-kafka     | 	confluent.security.event.logger.authentication.event.rate.limit = -1
benchi-kafka     | 	confluent.security.event.logger.authorization.event.rate.limit = -1
benchi-kafka     | 	confluent.security.event.logger.cloudevent.codec = structured
benchi-kafka     | 	confluent.security.event.logger.enable = true
benchi-kafka     | 	confluent.security.event.logger.exporter.class = class io.confluent.security.audit.telemetry.exporter.NonBlockingKafkaExporter
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.create = true
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.partitions = 12
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.replicas = 3
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.retention.bytes = -1
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.retention.ms = 2592000000
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.roll.ms = 14400000
benchi-kafka     | 	confluent.security.event.logger.kafka.request.rate.limit = -1
benchi-kafka     | 	confluent.security.event.logger.physical.cluster.id = 
benchi-kafka     | 	confluent.security.event.router.cache.entries = 10000
benchi-kafka     | 	confluent.security.event.router.config = 
benchi-kafka     | 	confluent.security.event.router.named.config.enabled = false
benchi-kafka     |  (io.confluent.security.audit.AuditLogConfig)
benchi-kafka     | [2025-03-18 16:03:19,549] INFO Audit Log rate limiter reconfigured: Authn: -1, Authz: -1, Kafka request: -1 (io.confluent.security.audit.provider.AuditLogRateLimiter)
benchi-kafka     | [2025-03-18 16:03:19,551] INFO [BrokerServer id=1] Waiting for controller quorum voters future (kafka.server.BrokerServer)
benchi-kafka     | [2025-03-18 16:03:19,551] INFO [BrokerServer id=1] Finished waiting for controller quorum voters future (kafka.server.BrokerServer)
benchi-kafka     | [2025-03-18 16:03:19,557] INFO [broker-1-to-controller-forwarding-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
benchi-kafka     | [2025-03-18 16:03:19,557] INFO [broker-1-to-controller-forwarding-channel-manager]: Recorded new KRaft controller, from now on will use node broker:29093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
benchi-kafka     | [2025-03-18 16:03:19,559] INFO [client-metrics-reaper]: Starting (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
benchi-kafka     | [2025-03-18 16:03:19,567] INFO [ControllerServer id=1] Node 1 identified 0 potential metadata log encryptor rotation candidates: [] (org.apache.kafka.controller.ClusterControlManager)
benchi-kafka     | [2025-03-18 16:03:19,567] INFO [ControllerServer id=1] Potential metadata log encryptor rotation candidates that are existing in all controllers: [] (org.apache.kafka.controller.ClusterControlManager)
benchi-kafka     | [2025-03-18 16:03:19,568] INFO [ControllerServer id=1] Potential metadata log encryptor rotation candidates that are existing in all brokers: [] (org.apache.kafka.controller.ClusterControlManager)
benchi-kafka     | [2025-03-18 16:03:19,570] INFO [ControllerServer id=1] Replayed RegisterControllerRecord contaning ControllerRegistration(id=1, incarnationId=IPkszY1BS0qIQqG4IWtfkw, zkMigrationReady=false, listeners=[Endpoint(listenerName='CONTROLLER', securityProtocol=PLAINTEXT, host='broker', port=29093)], supportedFeatures={confluent.metadata.version: 1-120, metadata.version: 1-20}, metadataEncryptors=[]). (org.apache.kafka.controller.ClusterControlManager)
benchi-kafka     | [2025-03-18 16:03:19,598] INFO [ControllerRegistrationManager id=1 incarnation=IPkszY1BS0qIQqG4IWtfkw] Our registration has been persisted to the metadata log. (kafka.server.ControllerRegistrationManager)
benchi-kafka     | [2025-03-18 16:03:19,598] INFO [ExpirationReaper-1-ClusterLink]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:03:19,599] INFO [ControllerRegistrationManager id=1 incarnation=IPkszY1BS0qIQqG4IWtfkw] RegistrationResponseHandler: controller acknowledged ControllerRegistrationRequest. (kafka.server.ControllerRegistrationManager)
benchi-kafka     | [2025-03-18 16:03:19,611] INFO [SocketServer listenerType=BROKER, nodeId=1] Creating data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)
benchi-kafka     | [2025-03-18 16:03:19,611] INFO Quota PLAINTEXT-per-ip-connection-rate configured - (max: -1.0, floor: -1.0, adjustment: -1.0) (kafka.network.ListenerIpAutoTuningQuota)
benchi-kafka     | [2025-03-18 16:03:19,611] INFO Quota PLAINTEXT-per-tenant-connection-rate configured - (max: -1.0, floor: -1.0, adjustment: -1.0) (kafka.network.ListenerTenantAutoTuningQuota)
benchi-kafka     | [2025-03-18 16:03:19,611] INFO Quota PLAINTEXT-connection-rate configured - (max: 1.7976931348623157E308, floor: 1.7976931348623157E308, adjustment: 5.0) (kafka.network.ListenerAutoTuningQuota)
benchi-kafka     | [2025-03-18 16:03:19,611] INFO Updated connection-tokens max connection creation rate to 1.7976931348623157E308 (kafka.network.ConnectionQuotas)
benchi-kafka     | [2025-03-18 16:03:19,612] INFO Config values: 
benchi-kafka     | 	confluent.security.event.logger.detailed.audit.logs.disabled.apis = 
benchi-kafka     | 	confluent.security.event.logger.enable.detailed.audit.logs = false
benchi-kafka     | 	confluent.security.event.logger.enable.produce.consume.audit.logs = false
benchi-kafka     |  (org.apache.kafka.common.requests.DetailedRequestAuditLogFilter$Config)
benchi-kafka     | [2025-03-18 16:03:19,613] INFO Config values: 
benchi-kafka     | 	confluent.security.event.logger.detailed.audit.logs.disabled.apis = 
benchi-kafka     | 	confluent.security.event.logger.enable.detailed.audit.logs = false
benchi-kafka     | 	confluent.security.event.logger.enable.produce.consume.audit.logs = false
benchi-kafka     |  (org.apache.kafka.common.requests.DetailedRequestAuditLogFilter$Config)
benchi-kafka     | [2025-03-18 16:03:19,614] INFO Config values: 
benchi-kafka     | 	confluent.security.event.logger.detailed.audit.logs.disabled.apis = 
benchi-kafka     | 	confluent.security.event.logger.enable.detailed.audit.logs = false
benchi-kafka     | 	confluent.security.event.logger.enable.produce.consume.audit.logs = false
benchi-kafka     |  (org.apache.kafka.common.requests.DetailedRequestAuditLogFilter$Config)
benchi-kafka     | [2025-03-18 16:03:19,615] INFO [SocketServer listenerType=BROKER, nodeId=1] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)
benchi-kafka     | [2025-03-18 16:03:19,615] INFO [SocketServer listenerType=BROKER, nodeId=1] Creating data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT_HOST) (kafka.network.SocketServer)
benchi-kafka     | [2025-03-18 16:03:19,615] INFO Quota PLAINTEXT_HOST-per-ip-connection-rate configured - (max: -1.0, floor: -1.0, adjustment: -1.0) (kafka.network.ListenerIpAutoTuningQuota)
benchi-kafka     | [2025-03-18 16:03:19,615] INFO Quota PLAINTEXT_HOST-per-tenant-connection-rate configured - (max: -1.0, floor: -1.0, adjustment: -1.0) (kafka.network.ListenerTenantAutoTuningQuota)
benchi-kafka     | [2025-03-18 16:03:19,616] INFO Quota PLAINTEXT_HOST-connection-rate configured - (max: 1.7976931348623157E308, floor: 1.7976931348623157E308, adjustment: 5.0) (kafka.network.ListenerAutoTuningQuota)
benchi-kafka     | [2025-03-18 16:03:19,616] INFO Updated connection-tokens max connection creation rate to 1.7976931348623157E308 (kafka.network.ConnectionQuotas)
benchi-kafka     | [2025-03-18 16:03:19,616] INFO Config values: 
benchi-kafka     | 	confluent.security.event.logger.detailed.audit.logs.disabled.apis = 
benchi-kafka     | 	confluent.security.event.logger.enable.detailed.audit.logs = false
benchi-kafka     | 	confluent.security.event.logger.enable.produce.consume.audit.logs = false
benchi-kafka     |  (org.apache.kafka.common.requests.DetailedRequestAuditLogFilter$Config)
benchi-kafka     | [2025-03-18 16:03:19,617] INFO Config values: 
benchi-kafka     | 	confluent.security.event.logger.detailed.audit.logs.disabled.apis = 
benchi-kafka     | 	confluent.security.event.logger.enable.detailed.audit.logs = false
benchi-kafka     | 	confluent.security.event.logger.enable.produce.consume.audit.logs = false
benchi-kafka     |  (org.apache.kafka.common.requests.DetailedRequestAuditLogFilter$Config)
benchi-kafka     | [2025-03-18 16:03:19,618] INFO Config values: 
benchi-kafka     | 	confluent.security.event.logger.detailed.audit.logs.disabled.apis = 
benchi-kafka     | 	confluent.security.event.logger.enable.detailed.audit.logs = false
benchi-kafka     | 	confluent.security.event.logger.enable.produce.consume.audit.logs = false
benchi-kafka     |  (org.apache.kafka.common.requests.DetailedRequestAuditLogFilter$Config)
benchi-kafka     | [2025-03-18 16:03:19,618] INFO [SocketServer listenerType=BROKER, nodeId=1] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT_HOST) (kafka.network.SocketServer)
benchi-kafka     | [2025-03-18 16:03:19,622] INFO [broker-1-to-controller-alter-partition-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
benchi-kafka     | [2025-03-18 16:03:19,622] INFO [broker-1-to-controller-alter-partition-channel-manager]: Recorded new KRaft controller, from now on will use node broker:29093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
benchi-kafka     | [2025-03-18 16:03:19,625] INFO [broker-1-to-controller-directory-assignments-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
benchi-kafka     | [2025-03-18 16:03:19,625] INFO [broker-1-to-controller-directory-assignments-channel-manager]: Recorded new KRaft controller, from now on will use node broker:29093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
benchi-kafka     | [2025-03-18 16:03:19,634] INFO [ExpirationReaper-1-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:03:19,634] INFO [ExpirationReaper-1-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:03:19,635] INFO [ExpirationReaper-1-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:03:19,635] INFO [ExpirationReaper-1-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:03:19,635] INFO [ExpirationReaper-1-ListOffsets]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:03:19,640] INFO ReplicationConfig values: 
benchi-kafka     | 	confluent.replication.linger.ms = 0
benchi-kafka     | 	confluent.replication.max.in.flight.requests = 1
benchi-kafka     | 	confluent.replication.max.memory.buffer.bytes = 209715200
benchi-kafka     | 	confluent.replication.max.replica.pushers = 4
benchi-kafka     | 	confluent.replication.max.wait.ms = 500
benchi-kafka     | 	confluent.replication.mode = PULL
benchi-kafka     | 	confluent.replication.num.pushers.per.broker = 1
benchi-kafka     | 	confluent.replication.push.internal.topics.enable = false
benchi-kafka     | 	confluent.replication.request.max.bytes = 52428800
benchi-kafka     | 	confluent.replication.request.max.partition.bytes = 52428800
benchi-kafka     | 	confluent.replication.request.timeout.ms = 5000
benchi-kafka     | 	confluent.replication.retry.timeout.ms = 10000
benchi-kafka     | 	confluent.replication.socket.send.buffer.bytes = 1048576
benchi-kafka     |  (io.confluent.kafka.replication.push.ReplicationConfig)
benchi-kafka     | [2025-03-18 16:03:19,645] INFO [BrokerHealthManager]: Starting (kafka.availability.BrokerHealthManager)
benchi-kafka     | [2025-03-18 16:03:19,649] INFO [ExpirationReaper-1-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:03:19,649] INFO [ExpirationReaper-1-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:03:19,666] INFO Unable to read the broker epoch in /tmp/kraft-combined-logs. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:03:19,667] INFO [broker-1-to-controller-heartbeat-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
benchi-kafka     | [2025-03-18 16:03:19,667] INFO [broker-1-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node broker:29093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
benchi-kafka     | [2025-03-18 16:03:19,667] INFO [SharedServer id=1] Using broker:29092 as bootstrap.servers for inter broker client config. (kafka.server.SharedServer)
benchi-kafka     | [2025-03-18 16:03:19,667] INFO [SharedServer id=1] Using broker:29092 as bootstrap.servers for inter broker client config. (kafka.server.SharedServer)
benchi-kafka     | [2025-03-18 16:03:19,668] INFO [BrokerLifecycleManager id=1] Incarnation vvST2hrfSyyBAahkwPNWmA of broker 1 in cluster MkU3OEVBNTcwNTJENDM2Qk is now STARTING. (kafka.server.BrokerLifecycleManager)
benchi-kafka     | [2025-03-18 16:03:19,673] INFO [ControllerServer id=1] No previous registration found for broker 1. New incarnation ID is vvST2hrfSyyBAahkwPNWmA.  Generated 0 record(s) to clean up previous incarnations. New broker epoch is 6. (org.apache.kafka.controller.ClusterControlManager)
benchi-kafka     | [2025-03-18 16:03:19,673] INFO [ControllerServer id=1] Node 1 identified 0 potential metadata log encryptor rotation candidates: [] (org.apache.kafka.controller.ClusterControlManager)
benchi-kafka     | [2025-03-18 16:03:19,673] INFO [ControllerServer id=1] Potential metadata log encryptor rotation candidates that are existing in all controllers: [] (org.apache.kafka.controller.ClusterControlManager)
benchi-kafka     | [2025-03-18 16:03:19,673] INFO [ControllerServer id=1] Potential metadata log encryptor rotation candidates that are existing in all brokers: [] (org.apache.kafka.controller.ClusterControlManager)
benchi-kafka     | [2025-03-18 16:03:19,674] INFO [ControllerServer id=1] Replayed initial RegisterBrokerRecord for broker 1: RegisterBrokerRecord(brokerId=1, isMigratingZkBroker=false, incarnationId=vvST2hrfSyyBAahkwPNWmA, brokerEpoch=6, endPoints=[BrokerEndpoint(name='PLAINTEXT', host='broker', port=29092, securityProtocol=0), BrokerEndpoint(name='PLAINTEXT_HOST', host='localhost', port=9092, securityProtocol=0)], features=[BrokerFeature(name='confluent.metadata.version', minSupportedVersion=1, maxSupportedVersion=120), BrokerFeature(name='metadata.version', minSupportedVersion=1, maxSupportedVersion=20)], rack=null, fenced=true, inControlledShutdown=false, degradedComponents=[], metadataEncryptors=[], logDirs=[xbl_6JROMTfs6aQ8gB3fjw]) (org.apache.kafka.controller.ClusterControlManager)
benchi-kafka     | [2025-03-18 16:03:19,683] INFO [ExpirationReaper-1-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:03:19,685] INFO Config values: 
benchi-kafka     | 	confluent.request.log.api.samples.per.min = 
benchi-kafka     | 	confluent.request.log.enable.admin.apis = true
benchi-kafka     | 	confluent.request.log.enable.slowlog = false
benchi-kafka     | 	confluent.request.log.samples.per.min = 0
benchi-kafka     | 	confluent.request.slowlog.threshold.override = -1.0
benchi-kafka     | 	confluent.request.slowlog.threshold.p99.min = -1.0
benchi-kafka     |  (org.apache.kafka.common.requests.SamplingRequestLogFilter$Config)
benchi-kafka     | [2025-03-18 16:03:19,685] INFO Config values: 
benchi-kafka     | 	confluent.request.log.api.samples.per.min = 
benchi-kafka     | 	confluent.request.log.enable.admin.apis = true
benchi-kafka     | 	confluent.request.log.enable.slowlog = false
benchi-kafka     | 	confluent.request.log.samples.per.min = 0
benchi-kafka     | 	confluent.request.slowlog.threshold.override = -1.0
benchi-kafka     | 	confluent.request.slowlog.threshold.p99.min = -1.0
benchi-kafka     |  (org.apache.kafka.common.requests.SamplingRequestLogFilter$Config)
benchi-kafka     | [2025-03-18 16:03:19,686] INFO Config values: 
benchi-kafka     | 	confluent.request.log.api.samples.per.min = 
benchi-kafka     | 	confluent.request.log.enable.admin.apis = true
benchi-kafka     | 	confluent.request.log.enable.slowlog = false
benchi-kafka     | 	confluent.request.log.samples.per.min = 0
benchi-kafka     | 	confluent.request.slowlog.threshold.override = -1.0
benchi-kafka     | 	confluent.request.slowlog.threshold.p99.min = -1.0
benchi-kafka     |  (org.apache.kafka.common.requests.SamplingRequestLogFilter$Config)
benchi-kafka     | [2025-03-18 16:03:19,686] INFO Config values: 
benchi-kafka     | 	confluent.request.log.api.samples.per.min = 
benchi-kafka     | 	confluent.request.log.enable.admin.apis = true
benchi-kafka     | 	confluent.request.log.enable.slowlog = false
benchi-kafka     | 	confluent.request.log.samples.per.min = 0
benchi-kafka     | 	confluent.request.slowlog.threshold.override = -1.0
benchi-kafka     | 	confluent.request.slowlog.threshold.p99.min = -1.0
benchi-kafka     |  (org.apache.kafka.common.requests.SamplingRequestLogFilter$Config)
benchi-kafka     | [2025-03-18 16:03:19,686] INFO Config values: 
benchi-kafka     | 	confluent.request.log.api.samples.per.min = 
benchi-kafka     | 	confluent.request.log.enable.admin.apis = true
benchi-kafka     | 	confluent.request.log.enable.slowlog = false
benchi-kafka     | 	confluent.request.log.samples.per.min = 0
benchi-kafka     | 	confluent.request.slowlog.threshold.override = -1.0
benchi-kafka     | 	confluent.request.slowlog.threshold.p99.min = -1.0
benchi-kafka     |  (org.apache.kafka.common.requests.SamplingRequestLogFilter$Config)
benchi-kafka     | [2025-03-18 16:03:19,686] INFO Config values: 
benchi-kafka     | 	confluent.request.log.api.samples.per.min = 
benchi-kafka     | 	confluent.request.log.enable.admin.apis = true
benchi-kafka     | 	confluent.request.log.enable.slowlog = false
benchi-kafka     | 	confluent.request.log.samples.per.min = 0
benchi-kafka     | 	confluent.request.slowlog.threshold.override = -1.0
benchi-kafka     | 	confluent.request.slowlog.threshold.p99.min = -1.0
benchi-kafka     |  (org.apache.kafka.common.requests.SamplingRequestLogFilter$Config)
benchi-kafka     | [2025-03-18 16:03:19,687] INFO Config values: 
benchi-kafka     | 	confluent.request.log.api.samples.per.min = 
benchi-kafka     | 	confluent.request.log.enable.admin.apis = true
benchi-kafka     | 	confluent.request.log.enable.slowlog = false
benchi-kafka     | 	confluent.request.log.samples.per.min = 0
benchi-kafka     | 	confluent.request.slowlog.threshold.override = -1.0
benchi-kafka     | 	confluent.request.slowlog.threshold.p99.min = -1.0
benchi-kafka     |  (org.apache.kafka.common.requests.SamplingRequestLogFilter$Config)
benchi-kafka     | [2025-03-18 16:03:19,687] INFO Config values: 
benchi-kafka     | 	confluent.request.log.api.samples.per.min = 
benchi-kafka     | 	confluent.request.log.enable.admin.apis = true
benchi-kafka     | 	confluent.request.log.enable.slowlog = false
benchi-kafka     | 	confluent.request.log.samples.per.min = 0
benchi-kafka     | 	confluent.request.slowlog.threshold.override = -1.0
benchi-kafka     | 	confluent.request.slowlog.threshold.p99.min = -1.0
benchi-kafka     |  (org.apache.kafka.common.requests.SamplingRequestLogFilter$Config)
benchi-kafka     | [2025-03-18 16:03:19,687] INFO [BrokerServer id=1] Waiting for broker metadata to catch up (kafka.server.BrokerServer)
benchi-kafka     | [2025-03-18 16:03:19,702] INFO Handling event SbcConfigUpdateEvent-3 (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:03:19,702] INFO Cluster metadata containing at least one unfenced broker not yet available, SBC config processing delayed. (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:03:19,702] INFO Handling event SbcKraftStartupEvent-5 (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:03:19,702] INFO Cluster metadata containing at least one unfenced broker not yet available, SBC startup delayed. (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:03:19,702] INFO [BrokerLifecycleManager id=1] Successfully registered broker 1 with broker epoch 6 (kafka.server.BrokerLifecycleManager)
benchi-kafka     | [2025-03-18 16:03:19,705] INFO [BrokerLifecycleManager id=1] The broker has caught up. Transitioning from STARTING to RECOVERY. (kafka.server.BrokerLifecycleManager)
benchi-kafka     | [2025-03-18 16:03:19,705] INFO [BrokerServer id=1] Finished waiting for broker metadata to catch up (kafka.server.BrokerServer)
benchi-kafka     | [2025-03-18 16:03:19,718] INFO [BrokerLifecycleManager id=1] The broker is in RECOVERY. (kafka.server.BrokerLifecycleManager)
benchi-kafka     | [2025-03-18 16:03:19,738] INFO ConfluentMetricsReporterConfig values: 
benchi-kafka     | 	confluent.metrics.reporter.bootstrap.servers = localhost:9092
benchi-kafka     | 	confluent.metrics.reporter.include = .*MaxLag.*|kafka.log:type=Log,name=Size.*|.*name=(ActiveControllerCount|BytesInPerSec|BytesOutPerSec|CaughtUpReplicasCount|FailedFetchRequestsPerSec|FailedProduceRequestsPerSec|InSyncReplicasCount|LeaderCount|LeaderElectionRateAndTimeMs|LocalTimeMs|LogEndOffset|LogStartOffset|NetworkProcessorAvgIdlePercent|NumLogSegments|NumPartitionsInError|OfflinePartitionsCount|ObserverReplicasCount|PartitionCount|RemoteTimeMs|ReplicasCount|RequestHandlerAvgIdlePercent|RequestQueueSize|RequestQueueTimeMs|RequestsPerSec|ResponseQueueSize|ResponseQueueTimeMs|ResponseSendTimeMs|Size|TierSize|TotalFetchRequestsPerSec|TotalLag|TotalProduceRequestsPerSec|TotalSize|TotalTimeMs|UncleanLeaderElectionsPerSec|UnderReplicated|UnderReplicatedPartitions|UnderMinIsrPartitionCount|ZooKeeperDisconnectsPerSec|ZooKeeperExpiresPerSec|TieredPartitionsUndergoingUncleanLeaderRecoveryCount|NonTieredPartitionsUndergoingUncleanLeaderRecoveryCount|TierTopicPartitionsUndergoingUncleanLeaderRecoveryCount|ContiguousUnhealthySamples|ContiguousSamplesEngineThreadGroupsStuck|ContiguousSamplesStorageThreadGroupsStuck|ContiguousSamplesNoStorageThreadMakingProgress).*|.*(BytesFetchedRate).*
benchi-kafka     | 	confluent.metrics.reporter.publish.ms = 15000
benchi-kafka     | 	confluent.metrics.reporter.topic = _confluent-metrics
benchi-kafka     | 	confluent.metrics.reporter.topic.create = true
benchi-kafka     | 	confluent.metrics.reporter.topic.max.message.bytes = 10485760
benchi-kafka     | 	confluent.metrics.reporter.topic.partitions = 12
benchi-kafka     | 	confluent.metrics.reporter.topic.replicas = 3
benchi-kafka     | 	confluent.metrics.reporter.topic.retention.bytes = -1
benchi-kafka     | 	confluent.metrics.reporter.topic.retention.ms = 259200000
benchi-kafka     | 	confluent.metrics.reporter.topic.roll.ms = 14400000
benchi-kafka     | 	confluent.metrics.reporter.volume.metrics.refresh.ms = 15000
benchi-kafka     | 	confluent.metrics.reporter.whitelist = null
benchi-kafka     |  (io.confluent.metrics.reporter.ConfluentMetricsReporterConfig)
benchi-kafka     | [2025-03-18 16:03:19,755] INFO ProducerConfig values: 
benchi-kafka     | 	acks = -1
benchi-kafka     | 	auto.include.jmx.reporter = true
benchi-kafka     | 	batch.size = 16384
benchi-kafka     | 	bootstrap.servers = [localhost:9092]
benchi-kafka     | 	buffer.memory = 33554432
benchi-kafka     | 	client.dns.lookup = use_all_dns_ips
benchi-kafka     | 	client.id = confluent-metrics-reporter
benchi-kafka     | 	compression.gzip.level = -1
benchi-kafka     | 	compression.lz4.level = 9
benchi-kafka     | 	compression.type = zstd
benchi-kafka     | 	compression.zstd.level = 3
benchi-kafka     | 	confluent.lkc.id = null
benchi-kafka     | 	confluent.proxy.protocol.client.address = null
benchi-kafka     | 	confluent.proxy.protocol.client.mode = PROXY
benchi-kafka     | 	confluent.proxy.protocol.client.port = null
benchi-kafka     | 	confluent.proxy.protocol.client.version = NONE
benchi-kafka     | 	connections.max.idle.ms = 540000
benchi-kafka     | 	delivery.timeout.ms = 120000
benchi-kafka     | 	enable.idempotence = false
benchi-kafka     | 	enable.metrics.push = true
benchi-kafka     | 	interceptor.classes = []
benchi-kafka     | 	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
benchi-kafka     | 	linger.ms = 500
benchi-kafka     | 	max.block.ms = 60000
benchi-kafka     | 	max.in.flight.requests.per.connection = 1
benchi-kafka     | 	max.request.size = 10485760
benchi-kafka     | 	metadata.max.age.ms = 300000
benchi-kafka     | 	metadata.max.idle.ms = 300000
benchi-kafka     | 	metadata.recovery.strategy = none
benchi-kafka     | 	metric.reporters = []
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.recording.level = INFO
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	partitioner.adaptive.partitioning.enable = true
benchi-kafka     | 	partitioner.availability.timeout.ms = 0
benchi-kafka     | 	partitioner.class = null
benchi-kafka     | 	partitioner.ignore.keys = false
benchi-kafka     | 	receive.buffer.bytes = 32768
benchi-kafka     | 	reconnect.backoff.max.ms = 1000
benchi-kafka     | 	reconnect.backoff.ms = 50
benchi-kafka     | 	request.timeout.ms = 30000
benchi-kafka     | 	retries = 2147483647
benchi-kafka     | 	retry.backoff.max.ms = 1000
benchi-kafka     | 	retry.backoff.ms = 500
benchi-kafka     | 	sasl.client.callback.handler.class = null
benchi-kafka     | 	sasl.jaas.config = null
benchi-kafka     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
benchi-kafka     | 	sasl.kerberos.min.time.before.relogin = 60000
benchi-kafka     | 	sasl.kerberos.service.name = null
benchi-kafka     | 	sasl.kerberos.ticket.renew.jitter = 0.05
benchi-kafka     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
benchi-kafka     | 	sasl.login.callback.handler.class = null
benchi-kafka     | 	sasl.login.class = null
benchi-kafka     | 	sasl.login.connect.timeout.ms = null
benchi-kafka     | 	sasl.login.read.timeout.ms = null
benchi-kafka     | 	sasl.login.refresh.buffer.seconds = 300
benchi-kafka     | 	sasl.login.refresh.min.period.seconds = 60
benchi-kafka     | 	sasl.login.refresh.window.factor = 0.8
benchi-kafka     | 	sasl.login.refresh.window.jitter = 0.05
benchi-kafka     | 	sasl.login.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.login.retry.backoff.ms = 100
benchi-kafka     | 	sasl.mechanism = GSSAPI
benchi-kafka     | 	sasl.oauthbearer.clientassertion.audience = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.expiration = 5
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.jti.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.nbf.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.issuer = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.location = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key.passphrase = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.subject = null
benchi-kafka     | 	sasl.oauthbearer.clock.skew.seconds = 30
benchi-kafka     | 	sasl.oauthbearer.expected.audience = null
benchi-kafka     | 	sasl.oauthbearer.expected.issuer = null
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.url = null
benchi-kafka     | 	sasl.oauthbearer.scope.claim.name = scope
benchi-kafka     | 	sasl.oauthbearer.sub.claim.name = sub
benchi-kafka     | 	sasl.oauthbearer.token.endpoint.url = null
benchi-kafka     | 	security.protocol = PLAINTEXT
benchi-kafka     | 	security.providers = null
benchi-kafka     | 	send.buffer.bytes = 131072
benchi-kafka     | 	socket.connection.setup.timeout.max.ms = 30000
benchi-kafka     | 	socket.connection.setup.timeout.ms = 10000
benchi-kafka     | 	ssl.cipher.suites = null
benchi-kafka     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	ssl.engine.factory.class = null
benchi-kafka     | 	ssl.key.password = null
benchi-kafka     | 	ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	ssl.keystore.certificate.chain = null
benchi-kafka     | 	ssl.keystore.key = null
benchi-kafka     | 	ssl.keystore.location = null
benchi-kafka     | 	ssl.keystore.password = null
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.protocol = TLSv1.3
benchi-kafka     | 	ssl.provider = null
benchi-kafka     | 	ssl.secure.random.implementation = null
benchi-kafka     | 	ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	ssl.truststore.certificates = null
benchi-kafka     | 	ssl.truststore.location = null
benchi-kafka     | 	ssl.truststore.password = null
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     | 	transaction.timeout.ms = 60000
benchi-kafka     | 	transactional.id = null
benchi-kafka     | 	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
benchi-kafka     |  (org.apache.kafka.clients.producer.ProducerConfig)
benchi-kafka     | [2025-03-18 16:03:19,768] INFO initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector)
benchi-kafka     | [2025-03-18 16:03:19,780] INFO Kafka version: 7.8.1-ce (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:03:19,780] INFO Kafka commitId: eefb51c6f2f1b888b8b03296353338afbac8e213 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:03:19,780] INFO Kafka startTimeMs: 1742313799779 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:03:19,782] INFO [Producer clientId=confluent-metrics-reporter] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:03:19,782] WARN [Producer clientId=confluent-metrics-reporter] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:03:19,782] WARN [Producer clientId=confluent-metrics-reporter] Bootstrap broker localhost:9092 (id: -1 rack: null) disconnected (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:03:19,784] INFO EventEmitterConfig values: 
benchi-kafka     |  (io.confluent.shaded.io.confluent.telemetry.events.EventEmitterConfig)
benchi-kafka     | [2025-03-18 16:03:19,792] INFO Linux CPU collector enabled: true (io.confluent.telemetry.ConfluentTelemetryConfig)
benchi-kafka     | [2025-03-18 16:03:19,792] INFO Using cpu metric: io\.confluent\.kafka\.server/server/linux_system_cpu_utilization_1m (io.confluent.telemetry.ConfluentTelemetryConfig)
benchi-kafka     | [2025-03-18 16:03:19,799] INFO Configuring named client _confluentClient for exporter _confluent (io.confluent.telemetry.exporter.http.HttpExporterConfig)
benchi-kafka     | [2025-03-18 16:03:19,800] WARN no telemetry exporters are enabled (io.confluent.telemetry.ConfluentTelemetryConfig)
benchi-kafka     | [2025-03-18 16:03:19,808] WARN Ignoring redefinition of existing telemetry label kafka.version (io.confluent.shaded.io.confluent.telemetry.ResourceBuilderFacade)
benchi-kafka     | [2025-03-18 16:03:19,815] INFO ConfluentTelemetryConfig values: 
benchi-kafka     | 	confluent.telemetry.api.key = null
benchi-kafka     | 	confluent.telemetry.api.secret = null
benchi-kafka     | 	confluent.telemetry.cluster.id = null
benchi-kafka     | 	confluent.telemetry.debug.enabled = false
benchi-kafka     | 	confluent.telemetry.enabled = false
benchi-kafka     | 	confluent.telemetry.events.collector.include = .*transaction.remove.expired.transaction.cleanup.interval.ms.*|.*transaction.state.log.load.buffer.size.*|.*transaction.state.log.min.isr.*|.*transaction.state.log.num.partitions.*|.*transaction.state.log.replication.factor.*|.*transaction.state.log.segment.bytes.*|.*transactional.id.expiration.ms.*|.*controlled.shutdown.enable.*|.*controlled.shutdown.max.retries.*|.*controlled.shutdown.retry.backoff.ms.*|.*fetch.max.bytes.*|.*fetch.purgatory.purge.interval.requests.*|.*group.initial.rebalance.delay.ms.*|.*group.max.session.timeout.ms.*|.*group.max.size.*|.*group.min.session.timeout.ms.*|.*log.cleaner.backoff.ms.*|.*log.cleaner.dedupe.buffer.size.*|.*log.cleaner.delete.retention.ms.*|.*log.cleaner.enable.*|.*log.cleaner.io.buffer.load.factor.*|.*log.cleaner.io.buffer.size.*|.*log.cleaner.io.max.bytes.per.second.*|.*log.cleaner.max.compaction.lag.ms.*|.*log.cleaner.min.cleanable.ratio.*|.*log.cleaner.min.compaction.lag.ms.*|.*log.cleaner.threads.*|.*log.cleanup.policy.*|.*log.index.interval.bytes.*|.*log.index.size.max.bytes.*|.*log.message.downconversion.enable.*|.*log.message.format.version.*|.*log.message.timestamp.difference.max.ms.*|.*log.message.timestamp.type.*|.*max.connection.creation.rate.*|.*max.connections.*|.*max.connections.per.ip.*|.*max.incremental.fetch.session.cache.slots.*|.*replica.fetch.backoff.ms.*|.*replica.fetch.max.bytes.*|.*replica.fetch.min.bytes.*|.*replica.fetch.response.max.bytes.*|.*replica.fetch.wait.max.ms.*|.*replica.high.watermark.checkpoint.interval.ms.*|.*replica.lag.time.max.ms.*|.*replica.selector.class.*|.*replica.socket.receive.buffer.bytes.*|.*replica.socket.timeout.ms.*|.*alter.config.policy.class.name.*|.*alter.log.dirs.replication.quota.window.num.*|.*alter.log.dirs.replication.quota.window.size.seconds.*|.*metrics.num.samples.*|.*metrics.recording.level.*|.*metrics.sample.window.ms.*|.*quota.window.num.*|.*quota.window.size.seconds.*|.*replication.quota.window.num.*|.*replication.quota.window.size.seconds.*|.*confluent.balancer.enable.*|.*confluent.balancer.throttle.bytes.per.second.*|.*confluent.balancer.heal.uneven.load.trigger.*|.*confluent.balancer.heal.broker.failure.threshold.ms.*|.*confluent.balancer.disk.max.load.*|.*confluent.balancer.exclude.topic.prefixes.*|.*confluent.balancer.exclude.topic.names.*|.*confluent.tier.local.hotset.bytes.*|.*confluent.tier.local.hotset.ms.*|.*confluent.tier.archiver.num.threads.*|.*confluent.tier.backend.*|.*confluent.tier.enable.*|.*confluent.tier.feature.*|.*confluent.tier.fetcher.num.threads.*|.*confluent.tier.max.partition.fetch.bytes.override.*|.*confluent.tier.metadata.replication.factor.*|.*confluent.operator.managed.*|.*confluent.ansible.managed.*|.*confluent.license.*
benchi-kafka     | 	confluent.telemetry.events.enable = true
benchi-kafka     | 	confluent.telemetry.metrics.collector.include = .*io.confluent.telemetry/.*.*|.*io.confluent.system/.*(process_cpu_load|max_file_descriptor_count|open_file_descriptor_count|system_cpu_load|system_load_average|free_physical_memory_size|total_physical_memory_size|disk_total_bytes|disk_usable_bytes|jvm/mem|jvm/gc).*|.*io.confluent.kafka.server/.*(confluent_audit/audit_log_fallback_rate_per_minute|confluent_audit/audit_log_rate_per_minute|confluent_authorizer/authorization_request_rate_per_minute|confluent_authorizer/authorization_allowed_rate_per_minute|confluent_authorizer/authorization_denied_rate_per_minute|confluent_auth_store/rbac_role_bindings_count|confluent_auth_store/rbac_access_rules_count|confluent_auth_store/acl_access_rules_count).*|.*io.confluent.kafka.server/.*(acl_authorizer/zookeeper_disconnects/total/delta|acl_authorizer/zookeeper_expires/total/delta|broker_failure/zookeeper_disconnects/total/delta|broker_failure/zookeeper_expires/total/delta|broker_topic/bytes_in/total/delta|broker_topic/bytes_out/total/delta|broker_topic/failed_produce_requests/total/delta|broker_topic/failed_fetch_requests/total/delta|broker_topic/produce_message_conversions/total/delta|broker_topic/fetch_message_conversions/total/delta|cluster_link/active_link_count|cluster_link/consumer_offset_committed_rate|cluster_link/consumer_offset_committed_total|cluster_link/fetch_throttle_time_avg|cluster_link/fetch_throttle_time_max|cluster_link/link_count|cluster_link/linked_leader_epoch_change_rate|cluster_link/linked_leader_epoch_change_total|cluster_link/linked_topic_partition_addition_rate|cluster_link/linked_topic_partition_addition_total|cluster_link/mirror_partition_count|cluster_link/mirror_topic_byte_total|cluster_link/mirror_topic_count|cluster_link/mirror_topic_lag|cluster_link/topic_config_update_rate|cluster_link/topic_config_update_total|cluster_link_fetcher/connection_count|cluster_link_fetcher/failed_reauthentication_rate|cluster_link_fetcher/failed_reauthentication_total|cluster_link_fetcher/incoming_byte_rate|cluster_link_fetcher/incoming_byte_total|cluster_link_fetcher/outgoing_byte_rate|cluster_link_fetcher/outgoing_byte_total|cluster_link_fetcher/reauthentication_latency_avg|cluster_link_fetcher_manager/max_lag|controller/active_controller_count|controller/leader_election_rate_and_time_ms|controller/offline_partitions_count|controller/partition_availability|controller/preferred_replica_imbalance_count|controller/tenant_partition_availability|controller/global_under_min_isr_partition_count|controller/unclean_leader_elections/total|controller_channel/connection_close_rate|controller_channel/connection_close_total|controller_channel/connection_count|controller_channel/connection_creation_rate|controller_channel/connection_creation_total|controller_channel/request_size_avg|controller_channel/request_size_max|controller_channel_manager/queue_size|controller_channel_manager/total_queue_size|controller_event_manager/event_queue_size|delayed_operation_purgatory/purgatory_size|executor/zookeeper_disconnects/total/delta|executor/zookeeper_expires/total/delta|fetch/queue_size|fetcher/bytes_per_sec|fetcher_lag/consumer_lag|group_coordinator/partition_load_time_max|log_cleaner_manager/achieved_cleaning_ratio/time/delta|log_cleaner_manager/achieved_cleaning_ratio/total/delta|log_cleaner_manager/compacted_partition_bytes|log_cleaner_manager/max_dirty_percent|log_cleaner_manager/time_since_last_run_ms|log_cleaner_manager/uncleanable_bytes|log_cleaner_manager/uncleanable_partitions_count|replica_alter_log_dirs_manager/max_lag|replica_fetcher/request_size_avg|replica_fetcher/request_size_max|replica_fetcher_manager/max_lag|replica_manager/blocked_on_mirror_source_partition_count|replica_manager/isr_shrinks|replica_manager/leader_count|replica_manager/partition_count|replica_manager/under_min_isr_mirror_partition_count|replica_manager/under_min_isr_partition_count|replica_manager/under_replicated_mirror_partitions|replica_manager/under_replicated_partitions|request/errors/total/delta|request/local_time_ms/time/delta|request/local_time_ms/total/delta|request/queue_size|request/remote_time_ms/time/delta|request/remote_time_ms/total/delta|request/request_queue_time_ms/time/delta|request/request_queue_time_ms/total/delta|request/requests|request/response_queue_time_ms/time/delta|request/response_queue_time_ms/total/delta|request/response_send_time_ms/time/delta|request/response_send_time_ms/total/delta|request/total_time_ms/time/delta|request/total_time_ms/total/delta|request_channel/request_queue_size|request_channel/response_queue_size|request_handler_pool/request_handler_avg_idle_percent|session_expire_listener/zookeeper_disconnects/total/delta|session_expire_listener/zookeeper_expires/total/delta|socket_server/connections|socket_server/successful_authentication_total/delta|socket_server/failed_authentication_total/delta|socket_server/network_processor_avg_idle_percent|socket_server/request_size_avg|socket_server/request_size_max|tenant/consumer_lag_offsets).*|.*org\.apache\.kafka\.(producer\.connection\.creation\.rate|producer\.node\.request\.latency\.avg|producer\.node\.request\.latency\.max|producer\.produce\.throttle\.time\.avg|producer\.produce\.throttle\.time\.max|producer\.record\.queue\.time\.avg|producer\.record\.queue\.time\.max|producer\.connection\.creation\.total|consumer\.connection\.creation\.rate|consumer\.connection\.creation\.total|consumer\.node\.request\.latency\.avg|consumer\.node\.request\.latency\.max|consumer\.poll\.idle\.ratio\.avg|consumer\.coordinator\.commit\.latency\.avg|consumer\.coordinator\.commit\.latency\.max|consumer\.coordinator\.assigned\.partitions|consumer\.coordinator\.rebalance\.latency\.avg|consumer\.coordinator\.rebalance\.latency\.max|consumer\.coordinator\.rebalance\.latency\.total|consumer\.fetch\.manager\.fetch\.latency\.avg|consumer\.fetch\.manager\.fetch\.latency\.max).*
benchi-kafka     | 	confluent.telemetry.metrics.collector.interval.ms = 60000
benchi-kafka     | 	confluent.telemetry.metrics.collector.slo.enabled = false
benchi-kafka     | 	confluent.telemetry.proxy.password = null
benchi-kafka     | 	confluent.telemetry.proxy.url = null
benchi-kafka     | 	confluent.telemetry.proxy.username = null
benchi-kafka     |  (io.confluent.telemetry.ConfluentTelemetryConfig)
benchi-kafka     | [2025-03-18 16:03:19,815] INFO VolumeMetricsCollectorConfig values: 
benchi-kafka     | 	confluent.telemetry.metrics.collector.volume.update.ms = 15000
benchi-kafka     |  (io.confluent.telemetry.collector.VolumeMetricsCollector$VolumeMetricsCollectorConfig)
benchi-kafka     | [2025-03-18 16:03:19,815] INFO HttpClientConfig values: 
benchi-kafka     | 	api.key = null
benchi-kafka     | 	api.secret = null
benchi-kafka     | 	buffer.batch.duration.max.ms = null
benchi-kafka     | 	buffer.batch.items.max = null
benchi-kafka     | 	buffer.inflight.submissions.max = null
benchi-kafka     | 	buffer.pending.batches.max = null
benchi-kafka     | 	client.attempts.max = null
benchi-kafka     | 	client.base.url = https://collector.telemetry.confluent.cloud
benchi-kafka     | 	client.compression = null
benchi-kafka     | 	client.connect.timeout.ms = null
benchi-kafka     | 	client.metrics.path.override = /v1/metrics
benchi-kafka     | 	client.request.timeout.ms = null
benchi-kafka     | 	client.retry.delay.seconds = null
benchi-kafka     | 	proxy.password = null
benchi-kafka     | 	proxy.url = null
benchi-kafka     | 	proxy.username = null
benchi-kafka     | 	type = httpTelemetryClient
benchi-kafka     |  (io.confluent.telemetry.exporter.http.HttpClientConfig)
benchi-kafka     | [2025-03-18 16:03:19,815] INFO HttpExporterConfig values: 
benchi-kafka     | 	api.key = null
benchi-kafka     | 	api.secret = null
benchi-kafka     | 	buffer.batch.duration.max.ms = null
benchi-kafka     | 	buffer.batch.items.max = null
benchi-kafka     | 	buffer.inflight.submissions.max = null
benchi-kafka     | 	buffer.pending.batches.max = null
benchi-kafka     | 	client = _confluentClient
benchi-kafka     | 	client.attempts.max = null
benchi-kafka     | 	client.base.url = null
benchi-kafka     | 	client.compression = null
benchi-kafka     | 	client.connect.timeout.ms = null
benchi-kafka     | 	client.metrics.path.override = /v1/metrics
benchi-kafka     | 	client.request.timeout.ms = null
benchi-kafka     | 	client.retry.delay.seconds = null
benchi-kafka     | 	enabled = false
benchi-kafka     | 	events.enabled = true
benchi-kafka     | 	metrics.enabled = true
benchi-kafka     | 	metrics.include = null
benchi-kafka     | 	proxy.password = null
benchi-kafka     | 	proxy.url = null
benchi-kafka     | 	proxy.username = null
benchi-kafka     | 	remote.configurable = false
benchi-kafka     | 	type = http
benchi-kafka     |  (io.confluent.telemetry.exporter.http.HttpExporterConfig)
benchi-kafka     | [2025-03-18 16:03:19,815] INFO Configuring named client _confluentClient for exporter _confluent (io.confluent.telemetry.exporter.http.HttpExporterConfig)
benchi-kafka     | [2025-03-18 16:03:19,817] INFO KafkaExporterConfig values: 
benchi-kafka     | 	client = 
benchi-kafka     | 	enabled = true
benchi-kafka     | 	events.enabled = true
benchi-kafka     | 	metrics.enabled = true
benchi-kafka     | 	metrics.include = (io\.confluent\.kafka\.server/broker_load/broker_load_percent|io\.confluent\.kafka\.server/broker_topic/bytes_in/rate/1_min|io\.confluent\.kafka\.server/broker_topic/bytes_out/rate/1_min|io\.confluent\.kafka\.server/broker_topic/fetch_from_follower_bytes_out/rate/1_min|io\.confluent\.kafka\.server/broker_topic/messages_in/rate/1_min|io\.confluent\.kafka\.server/broker_topic/replication_bytes_in/rate/1_min|io\.confluent\.kafka\.server/broker_topic/replication_bytes_out/rate/1_min|io\.confluent\.kafka\.server/broker_topic/total_fetch_requests/rate/1_min|io\.confluent\.kafka\.server/broker_topic/total_follower_fetch_requests/rate/1_min|io\.confluent\.kafka\.server/broker_topic/mirror_bytes_in/rate/1_min|io\.confluent\.kafka\.server/broker_topic/total_fetch_from_follower_requests/rate/1_min|io\.confluent\.kafka\.server/broker_topic/total_produce_requests/rate/1_min|io\.confluent\.kafka\.server/log/size|io\.confluent\.kafka\.server/request/requests/rate/1_min|io\.confluent\.kafka\.server/request_handler_pool/request_handler_avg_idle_percent/rate/1_min|io\.confluent\.kafka\.server/server/linux_system_cpu_utilization_1m|io\.confluent\.system/volume/disk_total_bytes)
benchi-kafka     | 	producer.bootstrap.servers = broker:29092
benchi-kafka     | 	remote.configurable = false
benchi-kafka     | 	topic.create = true
benchi-kafka     | 	topic.max.message.bytes = 10485760
benchi-kafka     | 	topic.name = _confluent-telemetry-metrics
benchi-kafka     | 	topic.partitions = 12
benchi-kafka     | 	topic.replicas = 1
benchi-kafka     | 	topic.retention.bytes = -1
benchi-kafka     | 	topic.retention.ms = 259200000
benchi-kafka     | 	topic.roll.ms = 14400000
benchi-kafka     | 	type = kafka
benchi-kafka     |  (io.confluent.telemetry.exporter.kafka.KafkaExporterConfig)
benchi-kafka     | [2025-03-18 16:03:19,817] INFO PollingRemoteConfigurationConfig values: 
benchi-kafka     | 	enabled = true
benchi-kafka     | 	refresh.interval.ms = 60000
benchi-kafka     |  (io.confluent.shaded.io.confluent.telemetry.config.remote.polling.PollingRemoteConfigurationConfig)
benchi-kafka     | [2025-03-18 16:03:19,817] INFO Initializing the event logger (io.confluent.telemetry.reporter.TelemetryReporter)
benchi-kafka     | [2025-03-18 16:03:19,819] INFO EventLoggerConfig values: 
benchi-kafka     | 	event.logger.cloudevent.codec = structured
benchi-kafka     | 	event.logger.exporter.class = class io.confluent.shaded.io.confluent.telemetry.events.exporter.http.EventHttpExporter
benchi-kafka     |  (io.confluent.shaded.io.confluent.telemetry.events.EventLoggerConfig)
benchi-kafka     | [2025-03-18 16:03:19,821] INFO HttpExporterConfig values: 
benchi-kafka     | 	api.key = null
benchi-kafka     | 	api.secret = null
benchi-kafka     | 	buffer.batch.duration.max.ms = null
benchi-kafka     | 	buffer.batch.items.max = null
benchi-kafka     | 	buffer.inflight.submissions.max = null
benchi-kafka     | 	buffer.pending.batches.max = null
benchi-kafka     | 	client.attempts.max = null
benchi-kafka     | 	client.base.url = https://collector.telemetry.confluent.cloud
benchi-kafka     | 	client.compression = null
benchi-kafka     | 	client.connect.timeout.ms = null
benchi-kafka     | 	client.request.timeout.ms = null
benchi-kafka     | 	client.retry.delay.seconds = null
benchi-kafka     | 	enabled = true
benchi-kafka     | 	events.enabled = true
benchi-kafka     | 	metrics.enabled = true
benchi-kafka     | 	proxy.password = null
benchi-kafka     | 	proxy.url = null
benchi-kafka     | 	proxy.username = null
benchi-kafka     | 	type = http
benchi-kafka     |  (io.confluent.shaded.io.confluent.telemetry.events.exporter.http.HttpExporterConfig)
benchi-kafka     | [2025-03-18 16:03:19,836] INFO [Producer clientId=confluent-metrics-reporter] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:03:19,836] WARN [Producer clientId=confluent-metrics-reporter] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:03:19,836] WARN [Producer clientId=confluent-metrics-reporter] Bootstrap broker localhost:9092 (id: -1 rack: null) disconnected (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:03:19,990] INFO [Producer clientId=confluent-metrics-reporter] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:03:19,990] WARN [Producer clientId=confluent-metrics-reporter] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:03:19,990] WARN [Producer clientId=confluent-metrics-reporter] Bootstrap broker localhost:9092 (id: -1 rack: null) disconnected (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:03:20,001] INFO Handling event SbcConfigUpdateEvent-3 (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:03:20,002] INFO Cluster metadata containing at least one unfenced broker not yet available, SBC config processing delayed. (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:03:20,002] INFO Handling event SbcKraftStartupEvent-5 (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:03:20,002] INFO Cluster metadata containing at least one unfenced broker not yet available, SBC startup delayed. (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:03:20,089] INFO Creating kafka exporter named '_local' (io.confluent.telemetry.reporter.TelemetryReporter)
benchi-kafka     | [2025-03-18 16:03:20,092] INFO Kafka Exporter _local getting producer client  (io.confluent.telemetry.exporter.kafka.KafkaExporter)
benchi-kafka     | [2025-03-18 16:03:20,092] INFO Creating new non-static producer client (io.confluent.telemetry.exporter.kafka.KafkaClientFactory)
benchi-kafka     | [2025-03-18 16:03:20,093] INFO ProducerConfig values: 
benchi-kafka     | 	acks = -1
benchi-kafka     | 	auto.include.jmx.reporter = true
benchi-kafka     | 	batch.size = 16384
benchi-kafka     | 	bootstrap.servers = [broker:29092]
benchi-kafka     | 	buffer.memory = 33554432
benchi-kafka     | 	client.dns.lookup = use_all_dns_ips
benchi-kafka     | 	client.id = confluent-telemetry-reporter-local-producer
benchi-kafka     | 	compression.gzip.level = -1
benchi-kafka     | 	compression.lz4.level = 9
benchi-kafka     | 	compression.type = zstd
benchi-kafka     | 	compression.zstd.level = 3
benchi-kafka     | 	confluent.lkc.id = null
benchi-kafka     | 	confluent.proxy.protocol.client.address = null
benchi-kafka     | 	confluent.proxy.protocol.client.mode = PROXY
benchi-kafka     | 	confluent.proxy.protocol.client.port = null
benchi-kafka     | 	confluent.proxy.protocol.client.version = NONE
benchi-kafka     | 	connections.max.idle.ms = 540000
benchi-kafka     | 	delivery.timeout.ms = 120000
benchi-kafka     | 	enable.idempotence = false
benchi-kafka     | 	enable.metrics.push = true
benchi-kafka     | 	interceptor.classes = []
benchi-kafka     | 	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
benchi-kafka     | 	linger.ms = 500
benchi-kafka     | 	max.block.ms = 60000
benchi-kafka     | 	max.in.flight.requests.per.connection = 1
benchi-kafka     | 	max.request.size = 10485760
benchi-kafka     | 	metadata.max.age.ms = 300000
benchi-kafka     | 	metadata.max.idle.ms = 300000
benchi-kafka     | 	metadata.recovery.strategy = none
benchi-kafka     | 	metric.reporters = []
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.recording.level = INFO
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	partitioner.adaptive.partitioning.enable = true
benchi-kafka     | 	partitioner.availability.timeout.ms = 0
benchi-kafka     | 	partitioner.class = class io.confluent.shaded.io.confluent.telemetry.events.exporter.kafka.RandomBrokerPartitionSubsetPartitioner
benchi-kafka     | 	partitioner.ignore.keys = false
benchi-kafka     | 	receive.buffer.bytes = 32768
benchi-kafka     | 	reconnect.backoff.max.ms = 1000
benchi-kafka     | 	reconnect.backoff.ms = 50
benchi-kafka     | 	request.timeout.ms = 30000
benchi-kafka     | 	retries = 2147483647
benchi-kafka     | 	retry.backoff.max.ms = 1000
benchi-kafka     | 	retry.backoff.ms = 500
benchi-kafka     | 	sasl.client.callback.handler.class = null
benchi-kafka     | 	sasl.jaas.config = null
benchi-kafka     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
benchi-kafka     | 	sasl.kerberos.min.time.before.relogin = 60000
benchi-kafka     | 	sasl.kerberos.service.name = null
benchi-kafka     | 	sasl.kerberos.ticket.renew.jitter = 0.05
benchi-kafka     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
benchi-kafka     | 	sasl.login.callback.handler.class = null
benchi-kafka     | 	sasl.login.class = null
benchi-kafka     | 	sasl.login.connect.timeout.ms = null
benchi-kafka     | 	sasl.login.read.timeout.ms = null
benchi-kafka     | 	sasl.login.refresh.buffer.seconds = 300
benchi-kafka     | 	sasl.login.refresh.min.period.seconds = 60
benchi-kafka     | 	sasl.login.refresh.window.factor = 0.8
benchi-kafka     | 	sasl.login.refresh.window.jitter = 0.05
benchi-kafka     | 	sasl.login.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.login.retry.backoff.ms = 100
benchi-kafka     | 	sasl.mechanism = GSSAPI
benchi-kafka     | 	sasl.oauthbearer.clientassertion.audience = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.expiration = 5
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.jti.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.nbf.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.issuer = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.location = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key.passphrase = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.subject = null
benchi-kafka     | 	sasl.oauthbearer.clock.skew.seconds = 30
benchi-kafka     | 	sasl.oauthbearer.expected.audience = null
benchi-kafka     | 	sasl.oauthbearer.expected.issuer = null
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.url = null
benchi-kafka     | 	sasl.oauthbearer.scope.claim.name = scope
benchi-kafka     | 	sasl.oauthbearer.sub.claim.name = sub
benchi-kafka     | 	sasl.oauthbearer.token.endpoint.url = null
benchi-kafka     | 	security.protocol = PLAINTEXT
benchi-kafka     | 	security.providers = null
benchi-kafka     | 	send.buffer.bytes = 131072
benchi-kafka     | 	socket.connection.setup.timeout.max.ms = 30000
benchi-kafka     | 	socket.connection.setup.timeout.ms = 10000
benchi-kafka     | 	ssl.cipher.suites = null
benchi-kafka     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	ssl.engine.factory.class = null
benchi-kafka     | 	ssl.key.password = null
benchi-kafka     | 	ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	ssl.keystore.certificate.chain = null
benchi-kafka     | 	ssl.keystore.key = null
benchi-kafka     | 	ssl.keystore.location = null
benchi-kafka     | 	ssl.keystore.password = null
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.protocol = TLSv1.3
benchi-kafka     | 	ssl.provider = null
benchi-kafka     | 	ssl.secure.random.implementation = null
benchi-kafka     | 	ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	ssl.truststore.certificates = null
benchi-kafka     | 	ssl.truststore.location = null
benchi-kafka     | 	ssl.truststore.password = null
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     | 	transaction.timeout.ms = 60000
benchi-kafka     | 	transactional.id = null
benchi-kafka     | 	value.serializer = class io.confluent.telemetry.serde.OpenTelemetryMetricsSerde
benchi-kafka     |  (org.apache.kafka.clients.producer.ProducerConfig)
benchi-kafka     | [2025-03-18 16:03:20,093] INFO initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector)
benchi-kafka     | [2025-03-18 16:03:20,095] INFO These configurations '[confluent.metrics.reporter.bootstrap.servers]' were supplied but are not used yet. (org.apache.kafka.clients.producer.ProducerConfig)
benchi-kafka     | [2025-03-18 16:03:20,095] INFO Kafka version: 7.8.1-ce (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:03:20,095] INFO Kafka commitId: eefb51c6f2f1b888b8b03296353338afbac8e213 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:03:20,095] INFO Kafka startTimeMs: 1742313800095 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:03:20,096] INFO [Producer clientId=confluent-telemetry-reporter-local-producer] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:03:20,096] WARN [Producer clientId=confluent-telemetry-reporter-local-producer] Connection to node -1 (broker/172.20.0.2:29092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:03:20,096] WARN [Producer clientId=confluent-telemetry-reporter-local-producer] Bootstrap broker broker:29092 (id: -1 rack: null) disconnected (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:03:20,147] INFO [Producer clientId=confluent-telemetry-reporter-local-producer] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:03:20,147] WARN [Producer clientId=confluent-telemetry-reporter-local-producer] Connection to node -1 (broker/172.20.0.2:29092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:03:20,147] WARN [Producer clientId=confluent-telemetry-reporter-local-producer] Bootstrap broker broker:29092 (id: -1 rack: null) disconnected (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:03:20,238] INFO [Producer clientId=confluent-metrics-reporter] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:03:20,238] WARN [Producer clientId=confluent-metrics-reporter] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:03:20,238] WARN [Producer clientId=confluent-metrics-reporter] Bootstrap broker localhost:9092 (id: -1 rack: null) disconnected (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:03:20,262] INFO Starting Confluent telemetry reporter with an interval of 60000 ms) (io.confluent.telemetry.reporter.TelemetryReporter)
benchi-kafka     | [2025-03-18 16:03:20,292] INFO Starting Confluent metrics reporter for cluster id MkU3OEVBNTcwNTJENDM2Qk with an interval of 15000 ms (io.confluent.metrics.reporter.ConfluentMetricsReporter)
benchi-kafka     | [2025-03-18 16:03:20,299] INFO [BrokerServer id=1] Waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
benchi-kafka     | [2025-03-18 16:03:20,299] INFO [BrokerServer id=1] Finished waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
benchi-kafka     | [2025-03-18 16:03:20,299] INFO [BrokerServer id=1] Waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
benchi-kafka     | [2025-03-18 16:03:20,299] INFO [BrokerServer id=1] Finished waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
benchi-kafka     | [2025-03-18 16:03:20,299] INFO [BrokerServer id=1] Waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
benchi-kafka     | [2025-03-18 16:03:20,300] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing MetadataVersionPublisher(id=1) with a snapshot at offset 7 (org.apache.kafka.image.loader.MetadataLoader)
benchi-kafka     | [2025-03-18 16:03:20,300] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing BrokerMetadataPublisher with a snapshot at offset 7 (org.apache.kafka.image.loader.MetadataLoader)
benchi-kafka     | [2025-03-18 16:03:20,300] INFO [BrokerMetadataPublisher id=1] Publishing initial metadata at offset OffsetAndEpoch(offset=7, epoch=1) with metadata.version 3.8-IV0A. (kafka.server.metadata.BrokerMetadataPublisher)
benchi-kafka     | [2025-03-18 16:03:20,300] INFO Loading logs from log dirs ArrayBuffer(/tmp/kraft-combined-logs) (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:03:20,300] INFO [Producer clientId=confluent-telemetry-reporter-local-producer] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:03:20,300] WARN [Producer clientId=confluent-telemetry-reporter-local-producer] Connection to node -1 (broker/172.20.0.2:29092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:03:20,300] WARN [Producer clientId=confluent-telemetry-reporter-local-producer] Bootstrap broker broker:29092 (id: -1 rack: null) disconnected (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:03:20,302] INFO No logs found to be loaded in /tmp/kraft-combined-logs (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:03:20,303] INFO Loaded 0 logs in 3ms (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:03:20,303] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:03:20,304] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:03:20,304] INFO Starting log roller with a period of 300000 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:03:20,311] INFO Starting the log cleaner (kafka.log.LogCleaner)
benchi-kafka     | [2025-03-18 16:03:20,330] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner$CleanerThread)
benchi-kafka     | [2025-03-18 16:03:20,332] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
benchi-kafka     | [2025-03-18 16:03:20,333] INFO [AddPartitionsToTxnSenderThread-1]: Starting (kafka.server.AddPartitionsToTxnManager)
benchi-kafka     | [2025-03-18 16:03:20,340] INFO AdminClientConfig values: 
benchi-kafka     | 	auto.include.jmx.reporter = true
benchi-kafka     | 	bootstrap.controllers = []
benchi-kafka     | 	bootstrap.servers = [broker:29092]
benchi-kafka     | 	client.dns.lookup = use_all_dns_ips
benchi-kafka     | 	client.id = cluster-link--local-admin-1
benchi-kafka     | 	confluent.lkc.id = null
benchi-kafka     | 	confluent.metrics.reporter.bootstrap.servers = localhost:9092
benchi-kafka     | 	confluent.proxy.protocol.client.address = null
benchi-kafka     | 	confluent.proxy.protocol.client.mode = PROXY
benchi-kafka     | 	confluent.proxy.protocol.client.port = null
benchi-kafka     | 	confluent.proxy.protocol.client.version = NONE
benchi-kafka     | 	connections.max.idle.ms = 300000
benchi-kafka     | 	default.api.timeout.ms = 60000
benchi-kafka     | 	enable.metrics.push = true
benchi-kafka     | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
benchi-kafka     | 	metadata.max.age.ms = 300000
benchi-kafka     | 	metadata.recovery.strategy = none
benchi-kafka     | 	metric.reporters = []
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.recording.level = INFO
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	receive.buffer.bytes = 65536
benchi-kafka     | 	reconnect.backoff.max.ms = 1000
benchi-kafka     | 	reconnect.backoff.ms = 50
benchi-kafka     | 	request.timeout.ms = 30000
benchi-kafka     | 	retries = 2147483647
benchi-kafka     | 	retry.backoff.max.ms = 1000
benchi-kafka     | 	retry.backoff.ms = 100
benchi-kafka     | 	sasl.client.callback.handler.class = null
benchi-kafka     | 	sasl.jaas.config = null
benchi-kafka     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
benchi-kafka     | 	sasl.kerberos.min.time.before.relogin = 60000
benchi-kafka     | 	sasl.kerberos.service.name = null
benchi-kafka     | 	sasl.kerberos.ticket.renew.jitter = 0.05
benchi-kafka     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
benchi-kafka     | 	sasl.login.callback.handler.class = null
benchi-kafka     | 	sasl.login.class = null
benchi-kafka     | 	sasl.login.connect.timeout.ms = null
benchi-kafka     | 	sasl.login.read.timeout.ms = null
benchi-kafka     | 	sasl.login.refresh.buffer.seconds = 300
benchi-kafka     | 	sasl.login.refresh.min.period.seconds = 60
benchi-kafka     | 	sasl.login.refresh.window.factor = 0.8
benchi-kafka     | 	sasl.login.refresh.window.jitter = 0.05
benchi-kafka     | 	sasl.login.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.login.retry.backoff.ms = 100
benchi-kafka     | 	sasl.mechanism = GSSAPI
benchi-kafka     | 	sasl.oauthbearer.clientassertion.audience = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.expiration = 5
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.jti.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.nbf.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.issuer = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.location = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key.passphrase = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.subject = null
benchi-kafka     | 	sasl.oauthbearer.clock.skew.seconds = 30
benchi-kafka     | 	sasl.oauthbearer.expected.audience = null
benchi-kafka     | 	sasl.oauthbearer.expected.issuer = null
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.url = null
benchi-kafka     | 	sasl.oauthbearer.scope.claim.name = scope
benchi-kafka     | 	sasl.oauthbearer.sub.claim.name = sub
benchi-kafka     | 	sasl.oauthbearer.token.endpoint.url = null
benchi-kafka     | 	security.protocol = PLAINTEXT
benchi-kafka     | 	security.providers = null
benchi-kafka     | 	send.buffer.bytes = 131072
benchi-kafka     | 	socket.connection.setup.timeout.max.ms = 30000
benchi-kafka     | 	socket.connection.setup.timeout.ms = 10000
benchi-kafka     | 	ssl.cipher.suites = null
benchi-kafka     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	ssl.engine.factory.class = null
benchi-kafka     | 	ssl.key.password = null
benchi-kafka     | 	ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	ssl.keystore.certificate.chain = null
benchi-kafka     | 	ssl.keystore.key = null
benchi-kafka     | 	ssl.keystore.location = null
benchi-kafka     | 	ssl.keystore.password = null
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.protocol = TLSv1.3
benchi-kafka     | 	ssl.provider = null
benchi-kafka     | 	ssl.secure.random.implementation = null
benchi-kafka     | 	ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	ssl.truststore.certificates = null
benchi-kafka     | 	ssl.truststore.location = null
benchi-kafka     | 	ssl.truststore.password = null
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     |  (org.apache.kafka.clients.admin.AdminClientConfig)
benchi-kafka     | [2025-03-18 16:03:20,356] INFO These configurations '[confluent.metrics.topic.replicas, confluent.metrics.reporter.bootstrap.servers, jmx.hostname, cluster.link.metadata.topic.replication.factor, jmx.port, confluent.license.topic.replication.factor]' were supplied but are not used yet. (org.apache.kafka.clients.admin.AdminClientConfig)
benchi-kafka     | [2025-03-18 16:03:20,356] INFO Kafka version: 7.8.1-ce (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:03:20,356] INFO Kafka commitId: eefb51c6f2f1b888b8b03296353338afbac8e213 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:03:20,356] INFO Kafka startTimeMs: 1742313800356 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:03:20,358] INFO [ClusterLinkManager-broker-1] ClusterLinkManager has started up. (kafka.server.link.ClusterLinkManager)
benchi-kafka     | [2025-03-18 16:03:20,358] INFO [GroupCoordinator 1]: Starting up. (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:03:20,359] INFO [AdminClient clientId=cluster-link--local-admin-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:03:20,359] WARN [AdminClient clientId=cluster-link--local-admin-1] Connection to node -1 (broker/172.20.0.2:29092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:03:20,359] INFO [GroupCoordinator 1]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:03:20,359] INFO [TransactionCoordinator id=1] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
benchi-kafka     | [2025-03-18 16:03:20,360] INFO [TxnMarkerSenderThread-1]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
benchi-kafka     | [2025-03-18 16:03:20,360] INFO [TransactionCoordinator id=1] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
benchi-kafka     | [2025-03-18 16:03:20,365] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing BrokerRegistrationTracker(id=1) with a snapshot at offset 7 (org.apache.kafka.image.loader.MetadataLoader)
benchi-kafka     | [2025-03-18 16:03:20,365] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing ClusterLinkCoordinatorListener with a snapshot at offset 7 (org.apache.kafka.image.loader.MetadataLoader)
benchi-kafka     | [2025-03-18 16:03:20,365] INFO [BrokerServer id=1] Finished waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
benchi-kafka     | [2025-03-18 16:03:20,367] INFO KafkaConfig values: 
benchi-kafka     | 	advertised.listeners = PLAINTEXT://broker:29092,PLAINTEXT_HOST://localhost:9092
benchi-kafka     | 	alter.config.policy.class.name = null
benchi-kafka     | 	alter.log.dirs.replication.quota.window.num = 11
benchi-kafka     | 	alter.log.dirs.replication.quota.window.size.seconds = 1
benchi-kafka     | 	authorizer.class.name = 
benchi-kafka     | 	auto.create.topics.enable = true
benchi-kafka     | 	auto.include.jmx.reporter = true
benchi-kafka     | 	auto.leader.rebalance.enable = true
benchi-kafka     | 	background.threads = 10
benchi-kafka     | 	broker.heartbeat.interval.ms = 2000
benchi-kafka     | 	broker.id = 1
benchi-kafka     | 	broker.id.generation.enable = true
benchi-kafka     | 	broker.interceptor.class = class org.apache.kafka.server.interceptor.DefaultBrokerInterceptor
benchi-kafka     | 	broker.rack = null
benchi-kafka     | 	broker.session.timeout.ms = 9000
benchi-kafka     | 	broker.session.uuid = 22eDK4SjR7GWVEmuEgKyNQ
benchi-kafka     | 	client.quota.callback.class = null
benchi-kafka     | 	client.quota.max.throttle.time.ms = 5000
benchi-kafka     | 	compression.gzip.level = -1
benchi-kafka     | 	compression.lz4.level = 9
benchi-kafka     | 	compression.type = producer
benchi-kafka     | 	compression.zstd.level = 3
benchi-kafka     | 	confluent.alter.broker.health.max.demoted.brokers = 2147483647
benchi-kafka     | 	confluent.alter.broker.health.max.demoted.brokers.percentage = 0
benchi-kafka     | 	confluent.ansible.managed = false
benchi-kafka     | 	confluent.api.visibility = DEFAULT
benchi-kafka     | 	confluent.append.record.interceptor.classes = []
benchi-kafka     | 	confluent.apply.create.topic.policy.to.create.partitions = false
benchi-kafka     | 	confluent.authorizer.authority.name = 
benchi-kafka     | 	confluent.automatic.alter.broker.health.retry.backoff.ms = 2000
benchi-kafka     | 	confluent.backpressure.disk.enable = false
benchi-kafka     | 	confluent.backpressure.disk.free.threshold.bytes = 21474836480
benchi-kafka     | 	confluent.backpressure.disk.produce.bytes.per.second = 131072
benchi-kafka     | 	confluent.backpressure.disk.threshold.recovery.factor = 1.5
benchi-kafka     | 	confluent.backpressure.request.min.broker.limit = 200
benchi-kafka     | 	confluent.backpressure.request.queue.size.percentile = p95
benchi-kafka     | 	confluent.backpressure.types = null
benchi-kafka     | 	confluent.balancer.api.state.topic = _confluent_balancer_api_state
benchi-kafka     | 	confluent.balancer.broker.addition.elapsed.time.ms.completion.threshold = 57600000
benchi-kafka     | 	confluent.balancer.broker.addition.mean.cpu.percent.completion.threshold = 0.5
benchi-kafka     | 	confluent.balancer.capacity.threshold.upper.limit = 0.95
benchi-kafka     | 	confluent.balancer.cell.load.upper.bound = 0.7
benchi-kafka     | 	confluent.balancer.cell.overload.detection.interval.ms = 3600000
benchi-kafka     | 	confluent.balancer.cell.overload.duration.ms = 86400000
benchi-kafka     | 	confluent.balancer.class = io.confluent.databalancer.SbcDataBalanceManager
benchi-kafka     | 	confluent.balancer.consumer.out.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.balancer.cpu.balance.threshold = 1.1
benchi-kafka     | 	confluent.balancer.cpu.low.utilization.threshold = 0.2
benchi-kafka     | 	confluent.balancer.cpu.utilization.detector.duration.ms = 600000
benchi-kafka     | 	confluent.balancer.cpu.utilization.detector.enabled = false
benchi-kafka     | 	confluent.balancer.cpu.utilization.detector.overutilization.threshold = 80.0
benchi-kafka     | 	confluent.balancer.cpu.utilization.detector.underutilization.threshold = 50.0
benchi-kafka     | 	confluent.balancer.disk.max.load = 0.85
benchi-kafka     | 	confluent.balancer.disk.min.free.space.gb = 0
benchi-kafka     | 	confluent.balancer.disk.min.free.space.lower.limit.gb = 0
benchi-kafka     | 	confluent.balancer.disk.utilization.detector.duration.ms = 600000
benchi-kafka     | 	confluent.balancer.disk.utilization.detector.enabled = false
benchi-kafka     | 	confluent.balancer.disk.utilization.detector.overutilization.threshold = 80.0
benchi-kafka     | 	confluent.balancer.disk.utilization.detector.reserved.capacity = 150000.0
benchi-kafka     | 	confluent.balancer.disk.utilization.detector.underutilization.threshold = 35.0
benchi-kafka     | 	confluent.balancer.enable = true
benchi-kafka     | 	confluent.balancer.exclude.topic.names = []
benchi-kafka     | 	confluent.balancer.exclude.topic.prefixes = []
benchi-kafka     | 	confluent.balancer.goal.violation.delay.on.new.brokers.ms = 1800000
benchi-kafka     | 	confluent.balancer.goal.violation.distribution.threshold.multiplier = 1.1
benchi-kafka     | 	confluent.balancer.heal.broker.failure.threshold.ms = 3600000
benchi-kafka     | 	confluent.balancer.heal.uneven.load.trigger = EMPTY_BROKER
benchi-kafka     | 	confluent.balancer.incremental.balancing.cpu.top.proposal.tracking.enabled = true
benchi-kafka     | 	confluent.balancer.incremental.balancing.cpu.top.proposal.tracking.num.proposals = 15
benchi-kafka     | 	confluent.balancer.incremental.balancing.enabled = false
benchi-kafka     | 	confluent.balancer.incremental.balancing.goals = []
benchi-kafka     | 	confluent.balancer.incremental.balancing.lower.bound = 0.02
benchi-kafka     | 	confluent.balancer.incremental.balancing.min.valid.windows = 5
benchi-kafka     | 	confluent.balancer.incremental.balancing.step.ratio = 0.2
benchi-kafka     | 	confluent.balancer.inter.cell.balancing.enabled = false
benchi-kafka     | 	confluent.balancer.max.capacity.balancing.delta.percentage = 0.0
benchi-kafka     | 	confluent.balancer.max.replicas = 2147483647
benchi-kafka     | 	confluent.balancer.network.in.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.balancer.network.out.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.balancer.plan.computation.retry.timeout.ms = 3600000
benchi-kafka     | 	confluent.balancer.producer.in.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.balancer.rebalancing.goals = []
benchi-kafka     | 	confluent.balancer.replication.in.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.balancer.resource.utilization.detector.interval.ms = 60000
benchi-kafka     | 	confluent.balancer.sbc.metrics.parser.enabled = false
benchi-kafka     | 	confluent.balancer.self.healing.maximum.rounds = 1
benchi-kafka     | 	confluent.balancer.task.history.retention.days = 30
benchi-kafka     | 	confluent.balancer.tenant.maximum.movements = 0
benchi-kafka     | 	confluent.balancer.tenant.suspension.ms = 86400000
benchi-kafka     | 	confluent.balancer.throttle.bytes.per.second = 10485760
benchi-kafka     | 	confluent.balancer.topic.partition.maximum.movements = 5
benchi-kafka     | 	confluent.balancer.topic.partition.movement.expiration.ms = 10800000
benchi-kafka     | 	confluent.balancer.topic.partition.suspension.ms = 18000000
benchi-kafka     | 	confluent.balancer.topic.replication.factor = 1
benchi-kafka     | 	confluent.balancer.triggering.goals = []
benchi-kafka     | 	confluent.balancer.v2.addition.enabled = false
benchi-kafka     | 	confluent.basic.auth.credentials.source = null
benchi-kafka     | 	confluent.basic.auth.user.info = null
benchi-kafka     | 	confluent.bearer.auth.cache.expiry.buffer.seconds = 300
benchi-kafka     | 	confluent.bearer.auth.client.id = null
benchi-kafka     | 	confluent.bearer.auth.client.secret = null
benchi-kafka     | 	confluent.bearer.auth.credentials.source = null
benchi-kafka     | 	confluent.bearer.auth.identity.pool.id = null
benchi-kafka     | 	confluent.bearer.auth.issuer.endpoint.url = null
benchi-kafka     | 	confluent.bearer.auth.logical.cluster = null
benchi-kafka     | 	confluent.bearer.auth.scope = null
benchi-kafka     | 	confluent.bearer.auth.scope.claim.name = scope
benchi-kafka     | 	confluent.bearer.auth.sub.claim.name = sub
benchi-kafka     | 	confluent.bearer.auth.token = null
benchi-kafka     | 	confluent.broker.health.manager.enabled = true
benchi-kafka     | 	confluent.broker.health.manager.engine.request.handler.threads.stuck.criteria = AllThreadsStuck
benchi-kafka     | 	confluent.broker.health.manager.hard.kill.duration.ms = 60000
benchi-kafka     | 	confluent.broker.health.manager.mitigation.enabled = false
benchi-kafka     | 	confluent.broker.health.manager.num.samples.before.broker.suspect = 30
benchi-kafka     | 	confluent.broker.health.manager.num.samples.before.broker.unhealthy = 180
benchi-kafka     | 	confluent.broker.health.manager.percentage.unhealthy.samples.before.broker.suspect = 90
benchi-kafka     | 	confluent.broker.health.manager.percentage.unhealthy.samples.before.broker.unhealthy = 70
benchi-kafka     | 	confluent.broker.health.manager.sample.duration.ms = 1000
benchi-kafka     | 	confluent.broker.health.manager.storage.background.threads.stuck.criteria = AnyThreadStuck
benchi-kafka     | 	confluent.broker.health.manager.storage.network.threads.stuck.criteria = AnyThreadStuck
benchi-kafka     | 	confluent.broker.health.manager.storage.request.handler.threads.stuck.criteria = AnyThreadStuck
benchi-kafka     | 	confluent.broker.limit.consumer.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.broker.limit.producer.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.broker.load.advertised.limit.load = 0.8
benchi-kafka     | 	confluent.broker.load.average.service.request.time.ms = 0.1
benchi-kafka     | 	confluent.broker.load.delay.metric.start.ms = 180000
benchi-kafka     | 	confluent.broker.load.enabled = false
benchi-kafka     | 	confluent.broker.load.num.samples = 60
benchi-kafka     | 	confluent.broker.load.tenant.metric.enable = false
benchi-kafka     | 	confluent.broker.load.update.metric.tags.interval.ms = 60000
benchi-kafka     | 	confluent.broker.load.window.size.ms = 60000
benchi-kafka     | 	confluent.broker.load.workload.coefficient = 20.0
benchi-kafka     | 	confluent.broker.registration.delay.ms = 0
benchi-kafka     | 	confluent.catalog.collector.destination.topic = telemetry.events.data_catalog_source
benchi-kafka     | 	confluent.catalog.collector.enable = false
benchi-kafka     | 	confluent.catalog.collector.full.configs.enable = false
benchi-kafka     | 	confluent.catalog.collector.max.bytes.per.snapshot = 850000
benchi-kafka     | 	confluent.catalog.collector.max.topics.process = 500
benchi-kafka     | 	confluent.catalog.collector.max.zookeeper.request.per.sec = 100
benchi-kafka     | 	confluent.catalog.collector.snapshot.init.delay.sec = 60
benchi-kafka     | 	confluent.catalog.collector.snapshot.interval.sec = 300
benchi-kafka     | 	confluent.ccloud.host.suffixes = .confluent.cloud,.cpdev.cloud,.confluentgov.com,.confluentgov-internal.com
benchi-kafka     | 	confluent.ccloud.intranet.host.suffixes = .intranet.stag.cpdev.cloud,.intranet.stag.cpdev-untrusted.cloud,.intranet.devel.cpdev.cloud,.intranet.devel.cpdev-untrusted.cloud,.intranet.confluent.cloud,.intranet.confluent-untrusted.cloud
benchi-kafka     | 	confluent.cdc.api.keys.topic = 
benchi-kafka     | 	confluent.cdc.api.keys.topic.load.timeout.ms = 600000
benchi-kafka     | 	confluent.cdc.client.quotas.enable = false
benchi-kafka     | 	confluent.cdc.client.quotas.topic.name = 
benchi-kafka     | 	confluent.cdc.lkc.metadata.topic = 
benchi-kafka     | 	confluent.cdc.user.metadata.enable = false
benchi-kafka     | 	confluent.cdc.user.metadata.topic = _confluent-user_metadata
benchi-kafka     | 	confluent.cell.metrics.refresh.period.ms = 60000
benchi-kafka     | 	confluent.cells.default.size = 15
benchi-kafka     | 	confluent.cells.enable = false
benchi-kafka     | 	confluent.cells.implicit.creation.enable = false
benchi-kafka     | 	confluent.cells.load.refresher.enable = true
benchi-kafka     | 	confluent.cells.max.size = 15
benchi-kafka     | 	confluent.cells.min.size = 6
benchi-kafka     | 	confluent.checksum.enabled.files = [none]
benchi-kafka     | 	confluent.clm.enabled = false
benchi-kafka     | 	confluent.clm.frequency.in.hours = 6
benchi-kafka     | 	confluent.clm.list.object.thread_pool.size = 1
benchi-kafka     | 	confluent.clm.max.backup.days = 3
benchi-kafka     | 	confluent.clm.min.delay.in.minutes = 30
benchi-kafka     | 	confluent.clm.thread.pool.size = 2
benchi-kafka     | 	confluent.clm.topic.retention.days.to.backup.days = 0:0,3:3
benchi-kafka     | 	confluent.close.connections.on.credential.delete = false
benchi-kafka     | 	confluent.cluster.link.admin.max.in.flight.requests = 1000
benchi-kafka     | 	confluent.cluster.link.admin.request.batch.size = 1
benchi-kafka     | 	confluent.cluster.link.allow.config.providers = true
benchi-kafka     | 	confluent.cluster.link.allow.legacy.message.format = false
benchi-kafka     | 	confluent.cluster.link.allow.truncation.below.hwm = true
benchi-kafka     | 	confluent.cluster.link.availability.check.mode = ALL
benchi-kafka     | 	confluent.cluster.link.background.thread.affinity = LINK
benchi-kafka     | 	confluent.cluster.link.clients.max.idle.ms = 3153600000000
benchi-kafka     | 	confluent.cluster.link.enable = true
benchi-kafka     | 	confluent.cluster.link.enable.local.admin = false
benchi-kafka     | 	confluent.cluster.link.enable.metrics.reduction = false
benchi-kafka     | 	confluent.cluster.link.fetch.response.min.bytes = 1
benchi-kafka     | 	confluent.cluster.link.fetch.response.total.bytes = 2147483647
benchi-kafka     | 	confluent.cluster.link.fetcher.auto.tune.enable = false
benchi-kafka     | 	confluent.cluster.link.fetcher.thread.pool.mode = ENDPOINT
benchi-kafka     | 	confluent.cluster.link.insync.fetch.response.min.bytes = 1
benchi-kafka     | 	confluent.cluster.link.insync.fetch.response.total.bytes = 2147483647
benchi-kafka     | 	confluent.cluster.link.intranet.connectivity.enable = false
benchi-kafka     | 	confluent.cluster.link.io.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.cluster.link.local.reverse.connection.listener.map = null
benchi-kafka     | 	confluent.cluster.link.max.client.connections = 2147483647
benchi-kafka     | 	confluent.cluster.link.metadata.topic.create.retry.delay.ms = 1000
benchi-kafka     | 	confluent.cluster.link.metadata.topic.enable = false
benchi-kafka     | 	confluent.cluster.link.metadata.topic.min.isr = 2
benchi-kafka     | 	confluent.cluster.link.metadata.topic.partitions = 50
benchi-kafka     | 	confluent.cluster.link.metadata.topic.replication.factor = 1
benchi-kafka     | 	confluent.cluster.link.mirror.transition.batch.size = 10
benchi-kafka     | 	confluent.cluster.link.num.background.threads = 1
benchi-kafka     | 	confluent.cluster.link.periodic.task.batch.size = 2147483647
benchi-kafka     | 	confluent.cluster.link.periodic.task.min.interval.ms = 1000
benchi-kafka     | 	confluent.cluster.link.replica.fetch.connections.mode = combined
benchi-kafka     | 	confluent.cluster.link.replication.quota.mode = CLUSTER_LINK_ONLY
benchi-kafka     | 	confluent.cluster.link.replication.quota.mode.per.tenant.overrides = 
benchi-kafka     | 	confluent.cluster.link.replication.quota.window.num = 11
benchi-kafka     | 	confluent.cluster.link.replication.quota.window.size.seconds = 2
benchi-kafka     | 	confluent.cluster.link.request.quota.capacity = 400
benchi-kafka     | 	confluent.cluster.link.request.quota.request.percentage.multiplier = 1.0
benchi-kafka     | 	confluent.cluster.link.tenant.replication.quota.enable = false
benchi-kafka     | 	confluent.cluster.link.tenant.request.quota.enable = false
benchi-kafka     | 	confluent.cluster.metadata.snapshot.tier.delete.enable = false
benchi-kafka     | 	confluent.cluster.metadata.snapshot.tier.delete.maintain.min.snapshots = 3
benchi-kafka     | 	confluent.cluster.metadata.snapshot.tier.delete.retention.ms = 604800000
benchi-kafka     | 	confluent.cluster.metadata.snapshot.tier.upload.enable = false
benchi-kafka     | 	confluent.compacted.topic.prefer.tier.fetch.ms = -1
benchi-kafka     | 	confluent.connection.invalid.request.delay.enable = false
benchi-kafka     | 	confluent.connections.idle.expiry.manager.ignore.idleness.requests = []
benchi-kafka     | 	confluent.consumer.fetch.partition.pruning.enable = true
benchi-kafka     | 	confluent.consumer.lag.emitter.enabled = false
benchi-kafka     | 	confluent.consumer.lag.emitter.interval.ms = 60000
benchi-kafka     | 	confluent.dataflow.policy.watch.monitor.ms = 300000
benchi-kafka     | 	confluent.defer.isr.shrink.enable = false
benchi-kafka     | 	confluent.describe.topic.partitions.enabled = false
benchi-kafka     | 	confluent.disk.io.manager.enable = false
benchi-kafka     | 	confluent.disk.throughput.headroom = 10485760
benchi-kafka     | 	confluent.disk.throughput.limit = 10485760000
benchi-kafka     | 	confluent.disk.throughput.quota.tier.archive = 1048576000
benchi-kafka     | 	confluent.disk.throughput.quota.tier.archive.throttled = 104857600
benchi-kafka     | 	confluent.durability.audit.batch.flush.frequency.ms = 900000
benchi-kafka     | 	confluent.durability.audit.checks = PeriodicalAudit,ChecksumAudit
benchi-kafka     | 	confluent.durability.audit.enable = false
benchi-kafka     | 	confluent.durability.audit.idempotent.producer = false
benchi-kafka     | 	confluent.durability.audit.initial.job.delay.ms = 900000
benchi-kafka     | 	confluent.durability.audit.io.bytes.per.sec = 10485760
benchi-kafka     | 	confluent.durability.audit.log.ignored.event.types = 
benchi-kafka     | 	confluent.durability.audit.reporting.batch.ms = 1800000
benchi-kafka     | 	confluent.durability.audit.tier.compaction.audit.duration.ms = 14400000
benchi-kafka     | 	confluent.durability.events.allowed = OffsetChangeType,EpochChangeType,IsrExpandType,DeleteRecordsType,RetentionChangeType,StartOffsetChangeType,DeletePartitionType,HealthCheckType
benchi-kafka     | 	confluent.durability.topic.partition.count = 50
benchi-kafka     | 	confluent.durability.topic.replication.factor = 1
benchi-kafka     | 	confluent.e2e_checksum.protection.enabled = false
benchi-kafka     | 	confluent.e2e_checksum.protection.files = [none]
benchi-kafka     | 	confluent.e2e_checksum.protection.store.entry.ttl.ms = 2592000000
benchi-kafka     | 	confluent.elastic.cku.enabled = false
benchi-kafka     | 	confluent.elastic.cku.scaletozero.enabled = false
benchi-kafka     | 	confluent.eligible.controllers = []
benchi-kafka     | 	confluent.enable.stray.partition.deletion = false
benchi-kafka     | 	confluent.encryption.key.manager.rotation.interval.ms = 31536000000
benchi-kafka     | 	confluent.fetch.from.follower.require.leader.epoch.enable = false
benchi-kafka     | 	confluent.fetch.partition.pruning.enable = true
benchi-kafka     | 	confluent.floor.connection.rate.per.ip = -1.0
benchi-kafka     | 	confluent.floor.connection.rate.per.tenant = -1.0
benchi-kafka     | 	confluent.group.coordinator.offsets.batching.enable = false
benchi-kafka     | 	confluent.group.coordinator.offsets.writer.threads = 2
benchi-kafka     | 	confluent.group.metadata.load.threads = 32
benchi-kafka     | 	confluent.heap.tenured.notify.bytes = 0
benchi-kafka     | 	confluent.heap.tenured.notify.enabled = false
benchi-kafka     | 	confluent.hot.partition.ratio = 0.8
benchi-kafka     | 	confluent.http.server.start.timeout.ms = 60000
benchi-kafka     | 	confluent.http.server.stop.timeout.ms = 30000
benchi-kafka     | 	confluent.internal.metrics.enable = false
benchi-kafka     | 	confluent.internal.rest.server.bind.port = null
benchi-kafka     | 	confluent.internal.tenant.scoped.listener.name = INTERNAL_TENANT_SCOPED
benchi-kafka     | 	confluent.leader.epoch.checkpoint.checksum.enabled = false
benchi-kafka     | 	confluent.log.cleaner.timestamp.validation.enable = true
benchi-kafka     | 	confluent.log.placement.constraints = 
benchi-kafka     | 	confluent.max.connection.creation.rate.per.ip = 1.7976931348623157E308
benchi-kafka     | 	confluent.max.connection.creation.rate.per.tenant = 1.7976931348623157E308
benchi-kafka     | 	confluent.max.connection.rate.per.ip = -1.0
benchi-kafka     | 	confluent.max.connection.rate.per.tenant = -1.0
benchi-kafka     | 	confluent.max.connection.throttle.ms = null
benchi-kafka     | 	confluent.max.segment.ms = 9223372036854775807
benchi-kafka     | 	confluent.metadata.active.encryptor = null
benchi-kafka     | 	confluent.metadata.controlled.shutdown.partition.slice.delay.ms = 100
benchi-kafka     | 	confluent.metadata.encryptor.classes = null
benchi-kafka     | 	confluent.metadata.encryptor.required = false
benchi-kafka     | 	confluent.metadata.encryptor.secret.file = null
benchi-kafka     | 	confluent.metadata.encryptor.secrets = null
benchi-kafka     | 	confluent.metadata.jvm.warmup.ms = 60000
benchi-kafka     | 	confluent.metadata.leader.balance.slice.delay.ms = 100
benchi-kafka     | 	confluent.metadata.max.controlled.shutdown.partition.changes.per.slice = 1000
benchi-kafka     | 	confluent.metadata.max.leader.balance.changes.per.slice = 1000
benchi-kafka     | 	confluent.metadata.server.cluster.registry.clusters = []
benchi-kafka     | 	confluent.metadata.throttle.pre.check.for.non.wildcard.requests.enable = false
benchi-kafka     | 	confluent.metadata.throttle.pre.check.for.wildcard.requests.enable = false
benchi-kafka     | 	confluent.metrics.reporter.bootstrap.servers = localhost:9092
benchi-kafka     | 	confluent.min.acks = 0
benchi-kafka     | 	confluent.min.connection.throttle.ms = 0
benchi-kafka     | 	confluent.min.segment.ms = 1
benchi-kafka     | 	confluent.missing.id.cache.ttl.sec = 60
benchi-kafka     | 	confluent.missing.id.query.range = 20000
benchi-kafka     | 	confluent.missing.schema.cache.ttl.sec = 60
benchi-kafka     | 	confluent.mtls.enable = false
benchi-kafka     | 	confluent.mtls.listener.name = EXTERNAL
benchi-kafka     | 	confluent.mtls.sasl.authenticator.request.max.bytes = 104857600
benchi-kafka     | 	confluent.mtls.truststore.manager.class.name = null
benchi-kafka     | 	confluent.multitenant.authorizer.enable.acl.state = false
benchi-kafka     | 	confluent.multitenant.interceptor.balancer.apis.enabled = false
benchi-kafka     | 	confluent.multitenant.interceptor.collect.client.apiversions.max.per.tenant = 1000
benchi-kafka     | 	confluent.multitenant.interceptor.collect.client.apiversions.metric = false
benchi-kafka     | 	confluent.multitenant.listener.hostname.cluster.prefix.enable = false
benchi-kafka     | 	confluent.multitenant.listener.hostname.subdomain.suffix.enable = false
benchi-kafka     | 	confluent.multitenant.listener.names = null
benchi-kafka     | 	confluent.multitenant.parse.lkc.id.enable = false
benchi-kafka     | 	confluent.multitenant.parse.sni.host.name.enable = false
benchi-kafka     | 	confluent.network.health.manager.enabled = false
benchi-kafka     | 	confluent.network.health.manager.external.listener.name = EXTERNAL
benchi-kafka     | 	confluent.network.health.manager.externalconnectivitystartup.enabled = false
benchi-kafka     | 	confluent.network.health.manager.min.healthy.network.samples = 3
benchi-kafka     | 	confluent.network.health.manager.min.percentage.healthy.network.samples = 3
benchi-kafka     | 	confluent.network.health.manager.mitigation.enabled = false
benchi-kafka     | 	confluent.network.health.manager.network.sample.window.size = 120
benchi-kafka     | 	confluent.network.health.manager.sample.duration.ms = 1000
benchi-kafka     | 	confluent.oauth.flat.networking.verification.enable = false
benchi-kafka     | 	confluent.offsets.log.cleaner.delete.retention.ms = 86400000
benchi-kafka     | 	confluent.offsets.log.cleaner.max.compaction.lag.ms = 9223372036854775807
benchi-kafka     | 	confluent.offsets.log.cleaner.min.cleanable.dirty.ratio = 0.5
benchi-kafka     | 	confluent.offsets.topic.placement.constraints = 
benchi-kafka     | 	confluent.omit.network.processor.metric.tag = false
benchi-kafka     | 	confluent.operator.managed = false
benchi-kafka     | 	confluent.password.encoder.old.secret.ttl.ms = 9223372036854775807
benchi-kafka     | 	confluent.plugins.cluster.link.policy.max.destination.links.per.tenant = 10
benchi-kafka     | 	confluent.plugins.cluster.link.policy.max.source.links.per.tenant = 10
benchi-kafka     | 	confluent.plugins.topic.policy.max.partitions.per.cluster = 2147483647
benchi-kafka     | 	confluent.plugins.topic.policy.max.partitions.per.tenant = 512
benchi-kafka     | 	confluent.plugins.topic.policy.max.replicas.per.broker = 2147483647
benchi-kafka     | 	confluent.plugins.topic.policy.max.topics.per.cluster = 2147483647
benchi-kafka     | 	confluent.prefer.tier.fetch.ms = -1
benchi-kafka     | 	confluent.produce.throttle.pre.check.enable = false
benchi-kafka     | 	confluent.producer.id.cache.broker.hard.limit = -1
benchi-kafka     | 	confluent.producer.id.cache.eviction.minimal.expiration.ms = 900000
benchi-kafka     | 	confluent.producer.id.cache.extra.eviction.percentage = 0
benchi-kafka     | 	confluent.producer.id.cache.limit = 2147483647
benchi-kafka     | 	confluent.producer.id.cache.partition.hard.limit = -1
benchi-kafka     | 	confluent.producer.id.cache.tenant.hard.limit = -1
benchi-kafka     | 	confluent.producer.id.quota.manager.enable = false
benchi-kafka     | 	confluent.producer.id.quota.window.num = 11
benchi-kafka     | 	confluent.producer.id.quota.window.size.seconds = 1
benchi-kafka     | 	confluent.producer.id.throttle.enable = false
benchi-kafka     | 	confluent.producer.id.throttle.enable.threshold.percentage = 100
benchi-kafka     | 	confluent.proxy.mode.local.default = false
benchi-kafka     | 	confluent.proxy.protocol.fallback.enabled = false
benchi-kafka     | 	confluent.proxy.protocol.version = NONE
benchi-kafka     | 	confluent.quota.computing.usage.adjustment = 0.5
benchi-kafka     | 	confluent.quota.dynamic.enable = false
benchi-kafka     | 	confluent.quota.dynamic.publishing.interval.ms = 60000
benchi-kafka     | 	confluent.quota.dynamic.reporting.interval.ms = 30000
benchi-kafka     | 	confluent.quota.dynamic.reporting.min.usage = 102400
benchi-kafka     | 	confluent.quota.tenant.broker.max.consumer.rate = 13107200
benchi-kafka     | 	confluent.quota.tenant.broker.max.producer.rate = 13107200
benchi-kafka     | 	confluent.quota.tenant.default.controller.mutation.rate = 2.147483647E9
benchi-kafka     | 	confluent.quota.tenant.default.producer.id.rate = 2.147483647E9
benchi-kafka     | 	confluent.quota.tenant.fetch.multiplier = 1.0
benchi-kafka     | 	confluent.quota.tenant.follower.broker.min.consumer.rate = 10485760
benchi-kafka     | 	confluent.quota.tenant.follower.broker.min.producer.rate = 10485760
benchi-kafka     | 	confluent.quota.tenant.produce.multiplier = 1.0
benchi-kafka     | 	confluent.quota.tenant.user.quotas.enable = false
benchi-kafka     | 	confluent.regional.metadata.client.class = null
benchi-kafka     | 	confluent.regional.resource.manager.client.scheduler.threads = 2
benchi-kafka     | 	confluent.regional.resource.manager.endpoint = null
benchi-kafka     | 	confluent.regional.resource.manager.watch.endpoint = null
benchi-kafka     | 	confluent.replica.fetch.backoff.max.ms = 1000
benchi-kafka     | 	confluent.replica.fetch.connections.mode = combined
benchi-kafka     | 	confluent.replication.mode = PULL
benchi-kafka     | 	confluent.replication.push.feature.enable = false
benchi-kafka     | 	confluent.reporters.telemetry.auto.enable = true
benchi-kafka     | 	confluent.request.log.filter.class = class org.apache.kafka.common.requests.SamplingRequestLogFilter
benchi-kafka     | 	confluent.request.pipelining.enable = true
benchi-kafka     | 	confluent.request.pipelining.max.in.flight.requests.per.connection = 5
benchi-kafka     | 	confluent.require.calling.resource.identity = false
benchi-kafka     | 	confluent.require.compatible.keystore.updates = true
benchi-kafka     | 	confluent.require.confluent.issuer = false
benchi-kafka     | 	confluent.roll.check.interval.ms = 300000
benchi-kafka     | 	confluent.schema.registry.max.cache.size = 10000
benchi-kafka     | 	confluent.schema.registry.max.retries = 1
benchi-kafka     | 	confluent.schema.registry.retries.wait.ms = 0
benchi-kafka     | 	confluent.schema.registry.url = null
benchi-kafka     | 	confluent.schema.validator.interceptor.class = io.confluent.kafka.schemaregistry.validator.RecordSchemaValidator
benchi-kafka     | 	confluent.schema.validator.multitenant.enable = false
benchi-kafka     | 	confluent.schema.validator.samples.per.min = 0
benchi-kafka     | 	confluent.security.bc.approved.mode.enable = false
benchi-kafka     | 	confluent.security.event.logger.authentication.enable = false
benchi-kafka     | 	confluent.security.event.logger.authentication.event.rate.limit = -1
benchi-kafka     | 	confluent.security.event.logger.authorization.event.rate.limit = -1
benchi-kafka     | 	confluent.security.event.logger.detailed.audit.logs.filter.class = class org.apache.kafka.common.requests.DetailedRequestAuditLogFilter
benchi-kafka     | 	confluent.security.event.logger.enable = true
benchi-kafka     | 	confluent.security.event.logger.kafka.request.rate.limit = -1
benchi-kafka     | 	confluent.security.event.logger.physical.cluster.id = 
benchi-kafka     | 	confluent.security.event.router.config = 
benchi-kafka     | 	confluent.security.revoked.certificate.ids = 
benchi-kafka     | 	confluent.segment.eager.roll.enable = false
benchi-kafka     | 	confluent.segment.speculative.prefetch.enable = false
benchi-kafka     | 	confluent.spiffe.id.principal.extraction.rules = 
benchi-kafka     | 	confluent.ssl.key.password = null
benchi-kafka     | 	confluent.ssl.keystore.location = null
benchi-kafka     | 	confluent.ssl.keystore.password = null
benchi-kafka     | 	confluent.ssl.keystore.type = null
benchi-kafka     | 	confluent.ssl.protocol = null
benchi-kafka     | 	confluent.ssl.truststore.location = null
benchi-kafka     | 	confluent.ssl.truststore.password = null
benchi-kafka     | 	confluent.ssl.truststore.type = null
benchi-kafka     | 	confluent.step.connection.rate.per.ip = -1.0
benchi-kafka     | 	confluent.step.connection.rate.per.tenant = -1.0
benchi-kafka     | 	confluent.storage.probe.period.ms = -1
benchi-kafka     | 	confluent.storage.probe.slow.write.threshold.ms = 5000
benchi-kafka     | 	confluent.stray.log.delete.delay.ms = 604800000
benchi-kafka     | 	confluent.stray.log.max.deletions.per.run = 72
benchi-kafka     | 	confluent.subdomain.prefix = null
benchi-kafka     | 	confluent.subdomain.separator.map = null
benchi-kafka     | 	confluent.subdomain.separator.variable = %sep
benchi-kafka     | 	confluent.system.time.roll.enable = false
benchi-kafka     | 	confluent.telemetry.enabled = false
benchi-kafka     | 	confluent.telemetry.external.client.metrics.push.enabled = false
benchi-kafka     | 	confluent.tenant.latency.metric.enabled = false
benchi-kafka     | 	confluent.tier.archiver.num.threads = 2
benchi-kafka     | 	confluent.tier.azure.block.blob.auto.abort.threshold.bytes = 500000
benchi-kafka     | 	confluent.tier.azure.block.blob.container = null
benchi-kafka     | 	confluent.tier.azure.block.blob.cred.file.path = null
benchi-kafka     | 	confluent.tier.azure.block.blob.endpoint = null
benchi-kafka     | 	confluent.tier.azure.block.blob.prefix = 
benchi-kafka     | 	confluent.tier.backend = 
benchi-kafka     | 	confluent.tier.bucket.probe.period.ms = -1
benchi-kafka     | 	confluent.tier.cleaner.compact.min.efficiency = 0.5
benchi-kafka     | 	confluent.tier.cleaner.compact.segment.min.bytes = 20971520
benchi-kafka     | 	confluent.tier.cleaner.dedupe.buffer.size = 134217728
benchi-kafka     | 	confluent.tier.cleaner.dual.compaction = false
benchi-kafka     | 	confluent.tier.cleaner.dual.compaction.validation.max.bytes = 1073741824
benchi-kafka     | 	confluent.tier.cleaner.dual.compaction.validation.percent = 0
benchi-kafka     | 	confluent.tier.cleaner.enable = false
benchi-kafka     | 	confluent.tier.cleaner.excluded.topics = [^_confluent.*]
benchi-kafka     | 	confluent.tier.cleaner.feature.enable = false
benchi-kafka     | 	confluent.tier.cleaner.io.buffer.load.factor = 0.9
benchi-kafka     | 	confluent.tier.cleaner.io.buffer.size = 10485760
benchi-kafka     | 	confluent.tier.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
benchi-kafka     | 	confluent.tier.cleaner.min.cleanable.ratio = 0.75
benchi-kafka     | 	confluent.tier.cleaner.num.threads = 2
benchi-kafka     | 	confluent.tier.enable = false
benchi-kafka     | 	confluent.tier.feature = false
benchi-kafka     | 	confluent.tier.fenced.segment.delete.delay.ms = 600000
benchi-kafka     | 	confluent.tier.fetcher.async.enable = false
benchi-kafka     | 	confluent.tier.fetcher.async.timestamp.offset.parallelism = 1
benchi-kafka     | 	confluent.tier.fetcher.fetch.based.on.segment_and_metadata_layout.field = false
benchi-kafka     | 	confluent.tier.fetcher.memorypool.bytes = 0
benchi-kafka     | 	confluent.tier.fetcher.num.threads = 4
benchi-kafka     | 	confluent.tier.fetcher.offset.cache.expiration.ms = 1800000
benchi-kafka     | 	confluent.tier.fetcher.offset.cache.period.ms = 60000
benchi-kafka     | 	confluent.tier.fetcher.offset.cache.size = 200000
benchi-kafka     | 	confluent.tier.gcs.bucket = null
benchi-kafka     | 	confluent.tier.gcs.cred.file.path = null
benchi-kafka     | 	confluent.tier.gcs.prefix = 
benchi-kafka     | 	confluent.tier.gcs.region = null
benchi-kafka     | 	confluent.tier.gcs.sse.customer.encryption.key = null
benchi-kafka     | 	confluent.tier.gcs.write.chunk.size = 0
benchi-kafka     | 	confluent.tier.local.hotset.bytes = -1
benchi-kafka     | 	confluent.tier.local.hotset.ms = 86400000
benchi-kafka     | 	confluent.tier.max.partition.fetch.bytes.override = 0
benchi-kafka     | 	confluent.tier.metadata.bootstrap.servers = null
benchi-kafka     | 	confluent.tier.metadata.max.poll.ms = 100
benchi-kafka     | 	confluent.tier.metadata.namespace = null
benchi-kafka     | 	confluent.tier.metadata.num.partitions = 50
benchi-kafka     | 	confluent.tier.metadata.replication.factor = 1
benchi-kafka     | 	confluent.tier.metadata.request.timeout.ms = 30000
benchi-kafka     | 	confluent.tier.metadata.snapshots.enable = false
benchi-kafka     | 	confluent.tier.metadata.snapshots.interval.ms = 86400000
benchi-kafka     | 	confluent.tier.metadata.snapshots.retention.days = 7
benchi-kafka     | 	confluent.tier.metadata.snapshots.threads = 2
benchi-kafka     | 	confluent.tier.object.fetcher.num.threads = 1
benchi-kafka     | 	confluent.tier.partition.state.cleanup.delay.ms = 2592000000
benchi-kafka     | 	confluent.tier.partition.state.cleanup.enable = false
benchi-kafka     | 	confluent.tier.partition.state.cleanup.interval.ms = 86400000
benchi-kafka     | 	confluent.tier.partition.state.commit.interval.ms = 15000
benchi-kafka     | 	confluent.tier.s3.assumerole.arn = null
benchi-kafka     | 	confluent.tier.s3.auto.abort.threshold.bytes = 500000
benchi-kafka     | 	confluent.tier.s3.aws.endpoint.override = null
benchi-kafka     | 	confluent.tier.s3.aws.signer.override = null
benchi-kafka     | 	confluent.tier.s3.bucket = null
benchi-kafka     | 	confluent.tier.s3.cred.file.path = null
benchi-kafka     | 	confluent.tier.s3.force.path.style.access = false
benchi-kafka     | 	confluent.tier.s3.prefix = 
benchi-kafka     | 	confluent.tier.s3.region = null
benchi-kafka     | 	confluent.tier.s3.security.providers = null
benchi-kafka     | 	confluent.tier.s3.sse.algorithm = AES256
benchi-kafka     | 	confluent.tier.s3.sse.customer.encryption.key = null
benchi-kafka     | 	confluent.tier.s3.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	confluent.tier.s3.ssl.key.password = null
benchi-kafka     | 	confluent.tier.s3.ssl.keystore.location = null
benchi-kafka     | 	confluent.tier.s3.ssl.keystore.password = null
benchi-kafka     | 	confluent.tier.s3.ssl.keystore.type = null
benchi-kafka     | 	confluent.tier.s3.ssl.protocol = TLSv1.3
benchi-kafka     | 	confluent.tier.s3.ssl.provider = null
benchi-kafka     | 	confluent.tier.s3.ssl.truststore.location = null
benchi-kafka     | 	confluent.tier.s3.ssl.truststore.password = null
benchi-kafka     | 	confluent.tier.s3.ssl.truststore.type = null
benchi-kafka     | 	confluent.tier.s3.user.agent.prefix = APN/1.0 Confluent/1.0 TieredStorageS3/1.0
benchi-kafka     | 	confluent.tier.segment.hotset.roll.min.bytes = 104857600
benchi-kafka     | 	confluent.tier.segment.metadata.layout.put.mode = LegacyMultiObject
benchi-kafka     | 	confluent.tier.topic.data.loss.validation.fencing.enable = false
benchi-kafka     | 	confluent.tier.topic.delete.backoff.ms = 21600000
benchi-kafka     | 	confluent.tier.topic.delete.check.interval.ms = 300000
benchi-kafka     | 	confluent.tier.topic.delete.max.inprogress.partitions = 100
benchi-kafka     | 	confluent.tier.topic.head.data.loss.validation.enable = true
benchi-kafka     | 	confluent.tier.topic.head.data.loss.validation.max.timeout.ms = 900000
benchi-kafka     | 	confluent.tier.topic.materialization.from.snapshot.enable = false
benchi-kafka     | 	confluent.tier.topic.producer.enable.idempotence = true
benchi-kafka     | 	confluent.tier.topic.snapshots.enable = false
benchi-kafka     | 	confluent.tier.topic.snapshots.interval.ms = 300000
benchi-kafka     | 	confluent.tier.topic.snapshots.max.records = 100000
benchi-kafka     | 	confluent.tier.topic.snapshots.retention.hours = 168
benchi-kafka     | 	confluent.topic.partition.default.placement = 2
benchi-kafka     | 	confluent.topic.policy.use.computed.assignments = false
benchi-kafka     | 	confluent.topic.replica.assignor.builder.class = 
benchi-kafka     | 	confluent.track.api.key.per.ip = false
benchi-kafka     | 	confluent.track.per.ip.max.size = 100000
benchi-kafka     | 	confluent.track.tenant.id.per.ip = false
benchi-kafka     | 	confluent.traffic.cdc.network.id.routes.enable = false
benchi-kafka     | 	confluent.traffic.cdc.network.id.routes.listener.names = EXTERNAL_BACKCHANNEL
benchi-kafka     | 	confluent.traffic.cdc.network.id.routes.periodic.start.task.ms = 300000
benchi-kafka     | 	confluent.traffic.cdc.network.id.routes.topic.name = _confluent-network_id_routes
benchi-kafka     | 	confluent.traffic.network.id = 
benchi-kafka     | 	confluent.transaction.2pc.timeout.ms = -1
benchi-kafka     | 	confluent.transaction.logging.verbosity = 0
benchi-kafka     | 	confluent.transaction.state.log.placement.constraints = 
benchi-kafka     | 	confluent.valid.broker.rack.set = null
benchi-kafka     | 	confluent.verify.group.subscription.prefix = false
benchi-kafka     | 	confluent.virtual.topic.creation.enabled = false
benchi-kafka     | 	confluent.zookeeper.metadata.migration.controller.check.disable = false
benchi-kafka     | 	confluent.zookeeper.metadata.migration.trigger.file.path = null
benchi-kafka     | 	connection.failed.authentication.delay.ms = 100
benchi-kafka     | 	connection.min.expire.interval.ms = 250
benchi-kafka     | 	connections.max.age.ms = 3153600000000
benchi-kafka     | 	connections.max.idle.ms = 600000
benchi-kafka     | 	connections.max.reauth.ms = 0
benchi-kafka     | 	control.plane.listener.name = null
benchi-kafka     | 	controlled.shutdown.enable = true
benchi-kafka     | 	controlled.shutdown.max.retries = 3
benchi-kafka     | 	controlled.shutdown.retry.backoff.ms = 5000
benchi-kafka     | 	controller.listener.names = CONTROLLER
benchi-kafka     | 	controller.quorum.append.linger.ms = 25
benchi-kafka     | 	controller.quorum.bootstrap.servers = []
benchi-kafka     | 	controller.quorum.election.backoff.max.ms = 1000
benchi-kafka     | 	controller.quorum.election.timeout.ms = 1000
benchi-kafka     | 	controller.quorum.fetch.timeout.ms = 2000
benchi-kafka     | 	controller.quorum.request.timeout.ms = 2000
benchi-kafka     | 	controller.quorum.retry.backoff.ms = 20
benchi-kafka     | 	controller.quorum.voters = [1@broker:29093]
benchi-kafka     | 	controller.quota.window.num = 11
benchi-kafka     | 	controller.quota.window.size.seconds = 1
benchi-kafka     | 	controller.socket.timeout.ms = 30000
benchi-kafka     | 	create.cluster.link.policy.class.name = null
benchi-kafka     | 	create.topic.policy.class.name = null
benchi-kafka     | 	default.replication.factor = 1
benchi-kafka     | 	delegation.token.expiry.check.interval.ms = 3600000
benchi-kafka     | 	delegation.token.expiry.time.ms = 86400000
benchi-kafka     | 	delegation.token.master.key = null
benchi-kafka     | 	delegation.token.max.lifetime.ms = 604800000
benchi-kafka     | 	delegation.token.secret.key = null
benchi-kafka     | 	delete.records.purgatory.purge.interval.requests = 1
benchi-kafka     | 	delete.topic.enable = true
benchi-kafka     | 	early.start.listeners = null
benchi-kafka     | 	eligible.leader.replicas.enable = false
benchi-kafka     | 	enable.fips = false
benchi-kafka     | 	fetch.max.bytes = 57671680
benchi-kafka     | 	fetch.purgatory.purge.interval.requests = 1000
benchi-kafka     | 	floor.max.connection.creation.rate = null
benchi-kafka     | 	follower.replication.throttled.rate = 9223372036854775807
benchi-kafka     | 	follower.replication.throttled.replicas = none
benchi-kafka     | 	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
benchi-kafka     | 	group.consumer.heartbeat.interval.ms = 5000
benchi-kafka     | 	group.consumer.max.heartbeat.interval.ms = 15000
benchi-kafka     | 	group.consumer.max.session.timeout.ms = 60000
benchi-kafka     | 	group.consumer.max.size = 2147483647
benchi-kafka     | 	group.consumer.migration.policy = disabled
benchi-kafka     | 	group.consumer.min.heartbeat.interval.ms = 5000
benchi-kafka     | 	group.consumer.min.session.timeout.ms = 45000
benchi-kafka     | 	group.consumer.session.timeout.ms = 45000
benchi-kafka     | 	group.coordinator.append.linger.ms = 10
benchi-kafka     | 	group.coordinator.new.enable = false
benchi-kafka     | 	group.coordinator.rebalance.protocols = [classic]
benchi-kafka     | 	group.coordinator.threads = 1
benchi-kafka     | 	group.initial.rebalance.delay.ms = 0
benchi-kafka     | 	group.max.session.timeout.ms = 1800000
benchi-kafka     | 	group.max.size = 2147483647
benchi-kafka     | 	group.min.session.timeout.ms = 6000
benchi-kafka     | 	initial.broker.registration.timeout.ms = 60000
benchi-kafka     | 	inter.broker.listener.name = PLAINTEXT
benchi-kafka     | 	inter.broker.protocol.version = 3.8-IV0A
benchi-kafka     | 	k2.stack.builder.class.name = null
benchi-kafka     | 	k2.startup.timeout.ms = 60000
benchi-kafka     | 	k2.topic.metadata.max.total.partition.count = 20000
benchi-kafka     | 	k2.topic.metadata.refresh.ms = 10000
benchi-kafka     | 	kafka.metrics.polling.interval.secs = 10
benchi-kafka     | 	kafka.metrics.reporters = []
benchi-kafka     | 	leader.imbalance.check.interval.seconds = 300
benchi-kafka     | 	leader.imbalance.per.broker.percentage = 10
benchi-kafka     | 	leader.replication.throttled.rate = 9223372036854775807
benchi-kafka     | 	leader.replication.throttled.replicas = none
benchi-kafka     | 	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
benchi-kafka     | 	listeners = PLAINTEXT://broker:29092,CONTROLLER://broker:29093,PLAINTEXT_HOST://0.0.0.0:9092
benchi-kafka     | 	log.cleaner.backoff.ms = 15000
benchi-kafka     | 	log.cleaner.dedupe.buffer.size = 134217728
benchi-kafka     | 	log.cleaner.delete.retention.ms = 86400000
benchi-kafka     | 	log.cleaner.enable = true
benchi-kafka     | 	log.cleaner.hash.algorithm = MD5
benchi-kafka     | 	log.cleaner.io.buffer.load.factor = 0.9
benchi-kafka     | 	log.cleaner.io.buffer.size = 524288
benchi-kafka     | 	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
benchi-kafka     | 	log.cleaner.max.compaction.lag.ms = 9223372036854775807
benchi-kafka     | 	log.cleaner.min.cleanable.ratio = 0.5
benchi-kafka     | 	log.cleaner.min.compaction.lag.ms = 0
benchi-kafka     | 	log.cleaner.threads = 1
benchi-kafka     | 	log.cleanup.policy = [delete]
benchi-kafka     | 	log.deletion.max.segments.per.run = 2147483647
benchi-kafka     | 	log.deletion.throttler.disk.free.headroom.bytes = 21474836480
benchi-kafka     | 	log.dir = /tmp/kafka-logs
benchi-kafka     | 	log.dir.failure.timeout.ms = 30000
benchi-kafka     | 	log.dirs = /tmp/kraft-combined-logs
benchi-kafka     | 	log.flush.interval.messages = 9223372036854775807
benchi-kafka     | 	log.flush.interval.ms = null
benchi-kafka     | 	log.flush.offset.checkpoint.interval.ms = 60000
benchi-kafka     | 	log.flush.scheduler.interval.ms = 9223372036854775807
benchi-kafka     | 	log.flush.start.offset.checkpoint.interval.ms = 60000
benchi-kafka     | 	log.index.interval.bytes = 4096
benchi-kafka     | 	log.index.size.max.bytes = 10485760
benchi-kafka     | 	log.initial.task.delay.ms = 30000
benchi-kafka     | 	log.local.retention.bytes = -2
benchi-kafka     | 	log.local.retention.ms = -2
benchi-kafka     | 	log.message.downconversion.enable = true
benchi-kafka     | 	log.message.format.version = 3.0-IV1
benchi-kafka     | 	log.message.timestamp.after.max.ms = 9223372036854775807
benchi-kafka     | 	log.message.timestamp.before.max.ms = 9223372036854775807
benchi-kafka     | 	log.message.timestamp.difference.max.ms = 9223372036854775807
benchi-kafka     | 	log.message.timestamp.type = CreateTime
benchi-kafka     | 	log.preallocate = false
benchi-kafka     | 	log.retention.bytes = -1
benchi-kafka     | 	log.retention.check.interval.ms = 300000
benchi-kafka     | 	log.retention.hours = 168
benchi-kafka     | 	log.retention.minutes = null
benchi-kafka     | 	log.retention.ms = null
benchi-kafka     | 	log.roll.hours = 168
benchi-kafka     | 	log.roll.jitter.hours = 0
benchi-kafka     | 	log.roll.jitter.ms = null
benchi-kafka     | 	log.roll.ms = null
benchi-kafka     | 	log.segment.bytes = 1073741824
benchi-kafka     | 	log.segment.delete.delay.ms = 60000
benchi-kafka     | 	max.connection.creation.rate = 1.7976931348623157E308
benchi-kafka     | 	max.connection.creation.rate.per.ip.enable.threshold = 0.0
benchi-kafka     | 	max.connection.creation.rate.per.tenant.enable.threshold = 0.0
benchi-kafka     | 	max.connections = 2147483647
benchi-kafka     | 	max.connections.per.ip = 2147483647
benchi-kafka     | 	max.connections.per.ip.overrides = 
benchi-kafka     | 	max.connections.per.tenant = 0
benchi-kafka     | 	max.connections.protected.listeners = []
benchi-kafka     | 	max.connections.reap.amount = 0
benchi-kafka     | 	max.incremental.fetch.session.cache.slots = 1000
benchi-kafka     | 	max.request.partition.size.limit = 2000
benchi-kafka     | 	message.max.bytes = 1048588
benchi-kafka     | 	metadata.log.dir = null
benchi-kafka     | 	metadata.log.max.record.bytes.between.snapshots = 20971520
benchi-kafka     | 	metadata.log.max.snapshot.interval.ms = 3600000
benchi-kafka     | 	metadata.log.segment.bytes = 1073741824
benchi-kafka     | 	metadata.log.segment.min.bytes = 8388608
benchi-kafka     | 	metadata.log.segment.ms = 604800000
benchi-kafka     | 	metadata.max.idle.interval.ms = 500
benchi-kafka     | 	metadata.max.retention.bytes = 104857600
benchi-kafka     | 	metadata.max.retention.ms = 604800000
benchi-kafka     | 	metric.reporters = [io.confluent.metrics.reporter.ConfluentMetricsReporter]
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.recording.level = INFO
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	min.insync.replicas = 1
benchi-kafka     | 	multitenant.authorizer.support.resource.ids = false
benchi-kafka     | 	multitenant.metadata.class = null
benchi-kafka     | 	multitenant.metadata.dir = null
benchi-kafka     | 	multitenant.metadata.reload.delay.ms = 120000
benchi-kafka     | 	multitenant.metadata.ssl.certs.path = null
benchi-kafka     | 	multitenant.tenant.delete.batch.size = 10
benchi-kafka     | 	multitenant.tenant.delete.check.ms = 120000
benchi-kafka     | 	multitenant.tenant.delete.delay = 604800000
benchi-kafka     | 	node.id = 1
benchi-kafka     | 	num.io.threads = 8
benchi-kafka     | 	num.network.threads = 3
benchi-kafka     | 	num.partitions = 1
benchi-kafka     | 	num.recovery.threads.per.data.dir = 1
benchi-kafka     | 	num.replica.alter.log.dirs.threads = null
benchi-kafka     | 	num.replica.fetchers = 1
benchi-kafka     | 	offset.metadata.max.bytes = 4096
benchi-kafka     | 	offsets.commit.required.acks = -1
benchi-kafka     | 	offsets.commit.timeout.ms = 5000
benchi-kafka     | 	offsets.load.buffer.size = 5242880
benchi-kafka     | 	offsets.retention.check.interval.ms = 600000
benchi-kafka     | 	offsets.retention.minutes = 10080
benchi-kafka     | 	offsets.topic.compression.codec = 0
benchi-kafka     | 	offsets.topic.num.partitions = 50
benchi-kafka     | 	offsets.topic.replication.factor = 1
benchi-kafka     | 	offsets.topic.segment.bytes = 104857600
benchi-kafka     | 	otel.exporter.otlp.custom.endpoint = default
benchi-kafka     | 	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
benchi-kafka     | 	password.encoder.iterations = 4096
benchi-kafka     | 	password.encoder.key.length = 128
benchi-kafka     | 	password.encoder.keyfactory.algorithm = null
benchi-kafka     | 	password.encoder.old.secret = null
benchi-kafka     | 	password.encoder.secret = null
benchi-kafka     | 	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
benchi-kafka     | 	process.roles = [broker, controller]
benchi-kafka     | 	producer.id.expiration.check.interval.ms = 600000
benchi-kafka     | 	producer.id.expiration.ms = 86400000
benchi-kafka     | 	producer.purgatory.purge.interval.requests = 1000
benchi-kafka     | 	queued.max.request.bytes = -1
benchi-kafka     | 	queued.max.requests = 500
benchi-kafka     | 	quota.window.num = 11
benchi-kafka     | 	quota.window.size.seconds = 1
benchi-kafka     | 	quotas.consumption.expiration.time.ms = 600000
benchi-kafka     | 	quotas.expiration.interval.ms = 3600000
benchi-kafka     | 	quotas.expiration.time.ms = 604800000
benchi-kafka     | 	quotas.lazy.evaluation.threshold = 0.5
benchi-kafka     | 	quotas.topic.append.timeout.ms = 5000
benchi-kafka     | 	quotas.topic.compression.codec = 3
benchi-kafka     | 	quotas.topic.load.buffer.size = 5242880
benchi-kafka     | 	quotas.topic.num.partitions = 50
benchi-kafka     | 	quotas.topic.placement.constraints = 
benchi-kafka     | 	quotas.topic.replication.factor = 3
benchi-kafka     | 	quotas.topic.segment.bytes = 104857600
benchi-kafka     | 	remote.fetch.max.wait.ms = 500
benchi-kafka     | 	remote.log.index.file.cache.total.size.bytes = 1073741824
benchi-kafka     | 	remote.log.manager.copier.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.copy.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.copy.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.expiration.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.fetch.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.fetch.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.task.interval.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.max.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.ms = 500
benchi-kafka     | 	remote.log.manager.task.retry.jitter = 0.2
benchi-kafka     | 	remote.log.manager.thread.pool.size = 10
benchi-kafka     | 	remote.log.metadata.custom.metadata.max.bytes = 128
benchi-kafka     | 	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
benchi-kafka     | 	remote.log.metadata.manager.class.path = null
benchi-kafka     | 	remote.log.metadata.manager.impl.prefix = rlmm.config.
benchi-kafka     | 	remote.log.metadata.manager.listener.name = null
benchi-kafka     | 	remote.log.reader.max.pending.tasks = 100
benchi-kafka     | 	remote.log.reader.threads = 10
benchi-kafka     | 	remote.log.storage.manager.class.name = null
benchi-kafka     | 	remote.log.storage.manager.class.path = null
benchi-kafka     | 	remote.log.storage.manager.impl.prefix = rsm.config.
benchi-kafka     | 	remote.log.storage.system.enable = false
benchi-kafka     | 	replica.fetch.backoff.ms = 1000
benchi-kafka     | 	replica.fetch.max.bytes = 1048576
benchi-kafka     | 	replica.fetch.min.bytes = 1
benchi-kafka     | 	replica.fetch.response.max.bytes = 10485760
benchi-kafka     | 	replica.fetch.wait.max.ms = 500
benchi-kafka     | 	replica.high.watermark.checkpoint.interval.ms = 5000
benchi-kafka     | 	replica.lag.time.max.ms = 30000
benchi-kafka     | 	replica.selector.class = null
benchi-kafka     | 	replica.socket.receive.buffer.bytes = 65536
benchi-kafka     | 	replica.socket.timeout.ms = 30000
benchi-kafka     | 	replication.quota.window.num = 11
benchi-kafka     | 	replication.quota.window.size.seconds = 1
benchi-kafka     | 	request.timeout.ms = 30000
benchi-kafka     | 	reserved.broker.max.id = 1000
benchi-kafka     | 	sasl.client.callback.handler.class = null
benchi-kafka     | 	sasl.enabled.mechanisms = [GSSAPI]
benchi-kafka     | 	sasl.jaas.config = null
benchi-kafka     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
benchi-kafka     | 	sasl.kerberos.min.time.before.relogin = 60000
benchi-kafka     | 	sasl.kerberos.principal.to.local.rules = [DEFAULT]
benchi-kafka     | 	sasl.kerberos.service.name = null
benchi-kafka     | 	sasl.kerberos.ticket.renew.jitter = 0.05
benchi-kafka     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
benchi-kafka     | 	sasl.login.callback.handler.class = null
benchi-kafka     | 	sasl.login.class = null
benchi-kafka     | 	sasl.login.connect.timeout.ms = null
benchi-kafka     | 	sasl.login.read.timeout.ms = null
benchi-kafka     | 	sasl.login.refresh.buffer.seconds = 300
benchi-kafka     | 	sasl.login.refresh.min.period.seconds = 60
benchi-kafka     | 	sasl.login.refresh.window.factor = 0.8
benchi-kafka     | 	sasl.login.refresh.window.jitter = 0.05
benchi-kafka     | 	sasl.login.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.login.retry.backoff.ms = 100
benchi-kafka     | 	sasl.mechanism.controller.protocol = GSSAPI
benchi-kafka     | 	sasl.mechanism.inter.broker.protocol = GSSAPI
benchi-kafka     | 	sasl.oauthbearer.clientassertion.audience = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.expiration = 5
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.jti.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.nbf.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.issuer = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.location = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key.passphrase = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.subject = null
benchi-kafka     | 	sasl.oauthbearer.clock.skew.seconds = 30
benchi-kafka     | 	sasl.oauthbearer.expected.audience = null
benchi-kafka     | 	sasl.oauthbearer.expected.issuer = null
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.url = null
benchi-kafka     | 	sasl.oauthbearer.scope.claim.name = scope
benchi-kafka     | 	sasl.oauthbearer.sub.claim.name = sub
benchi-kafka     | 	sasl.oauthbearer.token.endpoint.url = null
benchi-kafka     | 	sasl.server.authn.async.enable = false
benchi-kafka     | 	sasl.server.authn.async.max.threads = 1
benchi-kafka     | 	sasl.server.authn.async.timeout.ms = 30000
benchi-kafka     | 	sasl.server.callback.handler.class = null
benchi-kafka     | 	sasl.server.max.receive.size = 524288
benchi-kafka     | 	security.inter.broker.protocol = PLAINTEXT
benchi-kafka     | 	security.providers = null
benchi-kafka     | 	server.max.startup.time.ms = 9223372036854775807
benchi-kafka     | 	socket.connection.setup.timeout.max.ms = 30000
benchi-kafka     | 	socket.connection.setup.timeout.ms = 10000
benchi-kafka     | 	socket.listen.backlog.size = 50
benchi-kafka     | 	socket.receive.buffer.bytes = 102400
benchi-kafka     | 	socket.request.max.bytes = 104857600
benchi-kafka     | 	socket.send.buffer.bytes = 102400
benchi-kafka     | 	ssl.allow.dn.changes = false
benchi-kafka     | 	ssl.allow.san.changes = false
benchi-kafka     | 	ssl.cipher.suites = []
benchi-kafka     | 	ssl.client.auth = none
benchi-kafka     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	ssl.engine.factory.class = null
benchi-kafka     | 	ssl.key.password = null
benchi-kafka     | 	ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	ssl.keystore.certificate.chain = null
benchi-kafka     | 	ssl.keystore.key = null
benchi-kafka     | 	ssl.keystore.location = null
benchi-kafka     | 	ssl.keystore.password = null
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.principal.mapping.rules = DEFAULT
benchi-kafka     | 	ssl.protocol = TLSv1.3
benchi-kafka     | 	ssl.provider = null
benchi-kafka     | 	ssl.secure.random.implementation = null
benchi-kafka     | 	ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	ssl.truststore.certificates = null
benchi-kafka     | 	ssl.truststore.location = null
benchi-kafka     | 	ssl.truststore.password = null
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     | 	telemetry.max.bytes = 1048576
benchi-kafka     | 	throughput.quota.window.num = 11
benchi-kafka     | 	token.impersonation.validation = true
benchi-kafka     | 	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
benchi-kafka     | 	transaction.max.timeout.ms = 900000
benchi-kafka     | 	transaction.metadata.load.threads = 32
benchi-kafka     | 	transaction.partition.verification.enable = true
benchi-kafka     | 	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
benchi-kafka     | 	transaction.state.log.load.buffer.size = 5242880
benchi-kafka     | 	transaction.state.log.min.isr = 1
benchi-kafka     | 	transaction.state.log.num.partitions = 50
benchi-kafka     | 	transaction.state.log.replication.factor = 1
benchi-kafka     | 	transaction.state.log.segment.bytes = 104857600
benchi-kafka     | 	transactional.id.expiration.ms = 604800000
benchi-kafka     | 	unclean.leader.election.enable = false
benchi-kafka     | 	unstable.api.versions.enable = false
benchi-kafka     | 	unstable.feature.versions.enable = false
benchi-kafka     | 	zookeeper.acl.change.notification.expiration.ms = 900000
benchi-kafka     | 	zookeeper.clientCnxnSocket = null
benchi-kafka     | 	zookeeper.connect = 
benchi-kafka     | 	zookeeper.connection.timeout.ms = null
benchi-kafka     | 	zookeeper.max.in.flight.requests = 10
benchi-kafka     | 	zookeeper.metadata.migration.enable = false
benchi-kafka     | 	zookeeper.metadata.migration.min.batch.size = 200
benchi-kafka     | 	zookeeper.session.timeout.ms = 18000
benchi-kafka     | 	zookeeper.set.acl = false
benchi-kafka     | 	zookeeper.ssl.cipher.suites = null
benchi-kafka     | 	zookeeper.ssl.client.enable = false
benchi-kafka     | 	zookeeper.ssl.crl.enable = false
benchi-kafka     | 	zookeeper.ssl.enabled.protocols = null
benchi-kafka     | 	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
benchi-kafka     | 	zookeeper.ssl.keystore.location = null
benchi-kafka     | 	zookeeper.ssl.keystore.password = null
benchi-kafka     | 	zookeeper.ssl.keystore.type = null
benchi-kafka     | 	zookeeper.ssl.ocsp.enable = false
benchi-kafka     | 	zookeeper.ssl.protocol = TLSv1.2
benchi-kafka     | 	zookeeper.ssl.truststore.location = null
benchi-kafka     | 	zookeeper.ssl.truststore.password = null
benchi-kafka     | 	zookeeper.ssl.truststore.type = null
benchi-kafka     |  (kafka.server.KafkaConfig)
benchi-kafka     | [2025-03-18 16:03:20,368] INFO RemoteLogManagerConfig values: 
benchi-kafka     | 	log.local.retention.bytes = -2
benchi-kafka     | 	log.local.retention.ms = -2
benchi-kafka     | 	remote.fetch.max.wait.ms = 500
benchi-kafka     | 	remote.log.index.file.cache.total.size.bytes = 1073741824
benchi-kafka     | 	remote.log.manager.copier.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.copy.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.copy.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.expiration.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.fetch.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.fetch.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.task.interval.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.max.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.ms = 500
benchi-kafka     | 	remote.log.manager.task.retry.jitter = 0.2
benchi-kafka     | 	remote.log.manager.thread.pool.size = 10
benchi-kafka     | 	remote.log.metadata.custom.metadata.max.bytes = 128
benchi-kafka     | 	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
benchi-kafka     | 	remote.log.metadata.manager.class.path = null
benchi-kafka     | 	remote.log.metadata.manager.impl.prefix = rlmm.config.
benchi-kafka     | 	remote.log.metadata.manager.listener.name = null
benchi-kafka     | 	remote.log.reader.max.pending.tasks = 100
benchi-kafka     | 	remote.log.reader.threads = 10
benchi-kafka     | 	remote.log.storage.manager.class.name = null
benchi-kafka     | 	remote.log.storage.manager.class.path = null
benchi-kafka     | 	remote.log.storage.manager.impl.prefix = rsm.config.
benchi-kafka     | 	remote.log.storage.system.enable = false
benchi-kafka     |  (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
benchi-kafka     | [2025-03-18 16:03:20,371] INFO [BrokerServer id=1] Waiting for the broker to be unfenced (kafka.server.BrokerServer)
benchi-kafka     | [2025-03-18 16:03:20,372] INFO [ControllerServer id=1] The request from broker 1 to unfence has been granted because it has caught up with the offset of its register broker record 6. (org.apache.kafka.controller.BrokerHeartbeatManager)
benchi-kafka     | [2025-03-18 16:03:20,375] INFO [ControllerServer id=1] Replayed BrokerRegistrationChangeRecord modifying the registration for broker 1: BrokerRegistrationChangeRecord(brokerId=1, brokerEpoch=6, fenced=-1, inControlledShutdown=0, degradedComponents=null, logDirs=[]) (org.apache.kafka.controller.ClusterControlManager)
benchi-kafka     | [2025-03-18 16:03:20,403] INFO [BrokerLifecycleManager id=1] The broker has been unfenced. Transitioning from RECOVERY to RUNNING. (kafka.server.BrokerLifecycleManager)
benchi-kafka     | [2025-03-18 16:03:20,403] INFO [BrokerServer id=1] Finished waiting for the broker to be unfenced (kafka.server.BrokerServer)
benchi-kafka     | [2025-03-18 16:03:20,403] INFO [SocketServer listenerType=BROKER, nodeId=1] Enabling request processing. (kafka.network.SocketServer)
benchi-kafka     | [2025-03-18 16:03:20,403] INFO SBC Event SbcMetadataUpdateEvent-9 generated 1 more events to enqueue in the following order - [SbcKraftBrokerAdditionEvent-10]. Enqueuing... (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:03:20,403] INFO Awaiting socket connections on broker:29092. (kafka.network.DataPlaneAcceptor)
benchi-kafka     | [2025-03-18 16:03:20,403] INFO Handling event SbcConfigUpdateEvent-3 (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:03:20,404] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.DataPlaneAcceptor)
benchi-kafka     | [2025-03-18 16:03:20,404] INFO Balancer notified of a config change: ConfigurationsDelta(changes={}) (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:03:20,404] INFO There were 0 change(s) and 0 deletion(s) to balancer configs. Changed Configs: {}, Deleted Configs: [] (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:03:20,404] INFO Handling event SbcKraftStartupEvent-5 (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:03:20,404] INFO Configs metadata not yet available, SBC startup delayed. (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:03:20,404] INFO Handling event SbcKraftBrokerAdditionEvent-10 (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:03:20,404] INFO Topics Image not present, pausing broker addition event of brokers (new brokers: [1]) until it is received. (io.confluent.databalancer.event.SbcKraftBrokerAdditionEvent)
benchi-kafka     | [2025-03-18 16:03:20,405] INFO [BrokerServer id=1] Waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
benchi-kafka     | [2025-03-18 16:03:20,405] INFO [BrokerServer id=1] Finished waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
benchi-kafka     | [2025-03-18 16:03:20,405] INFO [BrokerServer id=1] Waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
benchi-kafka     | [2025-03-18 16:03:20,405] INFO [BrokerServer id=1] Finished waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
benchi-kafka     | [2025-03-18 16:03:20,424] INFO RestConfig values: 
benchi-kafka     | 	access.control.allow.headers = 
benchi-kafka     | 	access.control.allow.methods = 
benchi-kafka     | 	access.control.allow.origin = 
benchi-kafka     | 	access.control.skip.options = true
benchi-kafka     | 	authentication.method = NONE
benchi-kafka     | 	authentication.realm = 
benchi-kafka     | 	authentication.roles = [*]
benchi-kafka     | 	authentication.skip.paths = []
benchi-kafka     | 	compression.enable = true
benchi-kafka     | 	connector.connection.limit = 0
benchi-kafka     | 	csrf.prevention.enable = false
benchi-kafka     | 	csrf.prevention.token.endpoint = /csrf
benchi-kafka     | 	csrf.prevention.token.expiration.minutes = 30
benchi-kafka     | 	csrf.prevention.token.max.entries = 10000
benchi-kafka     | 	debug = false
benchi-kafka     | 	dos.filter.delay.ms = 100
benchi-kafka     | 	dos.filter.enabled = false
benchi-kafka     | 	dos.filter.insert.headers = true
benchi-kafka     | 	dos.filter.ip.whitelist = []
benchi-kafka     | 	dos.filter.managed.attr = false
benchi-kafka     | 	dos.filter.max.idle.tracker.ms = 30000
benchi-kafka     | 	dos.filter.max.requests.ms = 30000
benchi-kafka     | 	dos.filter.max.requests.per.connection.per.sec = 25
benchi-kafka     | 	dos.filter.max.requests.per.sec = 25
benchi-kafka     | 	dos.filter.max.wait.ms = 50
benchi-kafka     | 	dos.filter.throttle.ms = 30000
benchi-kafka     | 	dos.filter.throttled.requests = 5
benchi-kafka     | 	http2.enabled = true
benchi-kafka     | 	idle.timeout.ms = 30000
benchi-kafka     | 	listener.protocol.map = []
benchi-kafka     | 	listeners = []
benchi-kafka     | 	max.request.header.size = 8192
benchi-kafka     | 	max.response.header.size = 8192
benchi-kafka     | 	metric.reporters = []
benchi-kafka     | 	metrics.global.stats.request.tags.enable = false
benchi-kafka     | 	metrics.jmx.prefix = rest-utils
benchi-kafka     | 	metrics.latency.sla.ms = 50
benchi-kafka     | 	metrics.latency.slo.ms = 5
benchi-kafka     | 	metrics.latency.slo.sla.enable = false
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	metrics.tag.map = []
benchi-kafka     | 	network.traffic.rate.limit.backend = guava
benchi-kafka     | 	network.traffic.rate.limit.bytes.per.sec = 20971520
benchi-kafka     | 	network.traffic.rate.limit.enable = false
benchi-kafka     | 	nosniff.prevention.enable = false
benchi-kafka     | 	port = 8080
benchi-kafka     | 	proxy.protocol.enabled = false
benchi-kafka     | 	reject.options.request = false
benchi-kafka     | 	request.logger.name = io.confluent.rest-utils.requests
benchi-kafka     | 	request.queue.capacity = 2147483647
benchi-kafka     | 	request.queue.capacity.growby = 64
benchi-kafka     | 	request.queue.capacity.init = 128
benchi-kafka     | 	resource.extension.classes = []
benchi-kafka     | 	response.http.headers.config = 
benchi-kafka     | 	response.mediatype.default = application/json
benchi-kafka     | 	response.mediatype.preferred = [application/json]
benchi-kafka     | 	rest.servlet.initializor.classes = []
benchi-kafka     | 	server.connection.limit = 0
benchi-kafka     | 	shutdown.graceful.ms = 1000
benchi-kafka     | 	sni.check.enabled = false
benchi-kafka     | 	ssl.cipher.suites = []
benchi-kafka     | 	ssl.client.auth = false
benchi-kafka     | 	ssl.client.authentication = NONE
benchi-kafka     | 	ssl.enabled.protocols = []
benchi-kafka     | 	ssl.endpoint.identification.algorithm = null
benchi-kafka     | 	ssl.key.password = [hidden]
benchi-kafka     | 	ssl.keymanager.algorithm = 
benchi-kafka     | 	ssl.keystore.location = 
benchi-kafka     | 	ssl.keystore.password = [hidden]
benchi-kafka     | 	ssl.keystore.reload = false
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.keystore.watch.location = 
benchi-kafka     | 	ssl.protocol = TLS
benchi-kafka     | 	ssl.provider = 
benchi-kafka     | 	ssl.trustmanager.algorithm = 
benchi-kafka     | 	ssl.truststore.location = 
benchi-kafka     | 	ssl.truststore.password = [hidden]
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     | 	suppress.stack.trace.response = true
benchi-kafka     | 	thread.pool.max = 200
benchi-kafka     | 	thread.pool.min = 8
benchi-kafka     | 	websocket.path.prefix = /ws
benchi-kafka     | 	websocket.servlet.initializor.classes = []
benchi-kafka     |  (io.confluent.rest.RestConfig)
benchi-kafka     | [2025-03-18 16:03:20,428] INFO Logging initialized @2024ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log)
benchi-kafka     | [2025-03-18 16:03:20,442] INFO Application provider 'MetadataApiApplicationProvider' provided 1 instance(s). (io.confluent.http.server.KafkaHttpApplicationLoader)
benchi-kafka     | [2025-03-18 16:03:20,443] INFO MetadataServerConfig values: 
benchi-kafka     | 	confluent.http.server.listeners = [http://0.0.0.0:8090]
benchi-kafka     | 	confluent.metadata.server.advertised.listeners = null
benchi-kafka     | 	confluent.metadata.server.enable = false
benchi-kafka     | 	confluent.metadata.server.kraft.controller.enabled = false
benchi-kafka     | 	confluent.metadata.server.listeners = null
benchi-kafka     |  (org.apache.kafka.server.http.MetadataServerConfig)
benchi-kafka     | [2025-03-18 16:03:20,443] INFO Application provider 'RbacApplicationProvider' did not provide any instances. (io.confluent.http.server.KafkaHttpApplicationLoader)
benchi-kafka     | [2025-03-18 16:03:20,445] INFO KafkaConfig values: 
benchi-kafka     | 	advertised.listeners = PLAINTEXT://broker:29092,PLAINTEXT_HOST://localhost:9092
benchi-kafka     | 	alter.config.policy.class.name = null
benchi-kafka     | 	alter.log.dirs.replication.quota.window.num = 11
benchi-kafka     | 	alter.log.dirs.replication.quota.window.size.seconds = 1
benchi-kafka     | 	authorizer.class.name = 
benchi-kafka     | 	auto.create.topics.enable = true
benchi-kafka     | 	auto.include.jmx.reporter = true
benchi-kafka     | 	auto.leader.rebalance.enable = true
benchi-kafka     | 	background.threads = 10
benchi-kafka     | 	broker.heartbeat.interval.ms = 2000
benchi-kafka     | 	broker.id = 1
benchi-kafka     | 	broker.id.generation.enable = true
benchi-kafka     | 	broker.interceptor.class = class org.apache.kafka.server.interceptor.DefaultBrokerInterceptor
benchi-kafka     | 	broker.rack = null
benchi-kafka     | 	broker.session.timeout.ms = 9000
benchi-kafka     | 	broker.session.uuid = 22eDK4SjR7GWVEmuEgKyNQ
benchi-kafka     | 	client.quota.callback.class = null
benchi-kafka     | 	client.quota.max.throttle.time.ms = 5000
benchi-kafka     | 	compression.gzip.level = -1
benchi-kafka     | 	compression.lz4.level = 9
benchi-kafka     | 	compression.type = producer
benchi-kafka     | 	compression.zstd.level = 3
benchi-kafka     | 	confluent.alter.broker.health.max.demoted.brokers = 2147483647
benchi-kafka     | 	confluent.alter.broker.health.max.demoted.brokers.percentage = 0
benchi-kafka     | 	confluent.ansible.managed = false
benchi-kafka     | 	confluent.api.visibility = DEFAULT
benchi-kafka     | 	confluent.append.record.interceptor.classes = []
benchi-kafka     | 	confluent.apply.create.topic.policy.to.create.partitions = false
benchi-kafka     | 	confluent.authorizer.authority.name = 
benchi-kafka     | 	confluent.automatic.alter.broker.health.retry.backoff.ms = 2000
benchi-kafka     | 	confluent.backpressure.disk.enable = false
benchi-kafka     | 	confluent.backpressure.disk.free.threshold.bytes = 21474836480
benchi-kafka     | 	confluent.backpressure.disk.produce.bytes.per.second = 131072
benchi-kafka     | 	confluent.backpressure.disk.threshold.recovery.factor = 1.5
benchi-kafka     | 	confluent.backpressure.request.min.broker.limit = 200
benchi-kafka     | 	confluent.backpressure.request.queue.size.percentile = p95
benchi-kafka     | 	confluent.backpressure.types = null
benchi-kafka     | 	confluent.balancer.api.state.topic = _confluent_balancer_api_state
benchi-kafka     | 	confluent.balancer.broker.addition.elapsed.time.ms.completion.threshold = 57600000
benchi-kafka     | 	confluent.balancer.broker.addition.mean.cpu.percent.completion.threshold = 0.5
benchi-kafka     | 	confluent.balancer.capacity.threshold.upper.limit = 0.95
benchi-kafka     | 	confluent.balancer.cell.load.upper.bound = 0.7
benchi-kafka     | 	confluent.balancer.cell.overload.detection.interval.ms = 3600000
benchi-kafka     | 	confluent.balancer.cell.overload.duration.ms = 86400000
benchi-kafka     | 	confluent.balancer.class = io.confluent.databalancer.SbcDataBalanceManager
benchi-kafka     | 	confluent.balancer.consumer.out.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.balancer.cpu.balance.threshold = 1.1
benchi-kafka     | 	confluent.balancer.cpu.low.utilization.threshold = 0.2
benchi-kafka     | 	confluent.balancer.cpu.utilization.detector.duration.ms = 600000
benchi-kafka     | 	confluent.balancer.cpu.utilization.detector.enabled = false
benchi-kafka     | 	confluent.balancer.cpu.utilization.detector.overutilization.threshold = 80.0
benchi-kafka     | 	confluent.balancer.cpu.utilization.detector.underutilization.threshold = 50.0
benchi-kafka     | 	confluent.balancer.disk.max.load = 0.85
benchi-kafka     | 	confluent.balancer.disk.min.free.space.gb = 0
benchi-kafka     | 	confluent.balancer.disk.min.free.space.lower.limit.gb = 0
benchi-kafka     | 	confluent.balancer.disk.utilization.detector.duration.ms = 600000
benchi-kafka     | 	confluent.balancer.disk.utilization.detector.enabled = false
benchi-kafka     | 	confluent.balancer.disk.utilization.detector.overutilization.threshold = 80.0
benchi-kafka     | 	confluent.balancer.disk.utilization.detector.reserved.capacity = 150000.0
benchi-kafka     | 	confluent.balancer.disk.utilization.detector.underutilization.threshold = 35.0
benchi-kafka     | 	confluent.balancer.enable = true
benchi-kafka     | 	confluent.balancer.exclude.topic.names = []
benchi-kafka     | 	confluent.balancer.exclude.topic.prefixes = []
benchi-kafka     | 	confluent.balancer.goal.violation.delay.on.new.brokers.ms = 1800000
benchi-kafka     | 	confluent.balancer.goal.violation.distribution.threshold.multiplier = 1.1
benchi-kafka     | 	confluent.balancer.heal.broker.failure.threshold.ms = 3600000
benchi-kafka     | 	confluent.balancer.heal.uneven.load.trigger = EMPTY_BROKER
benchi-kafka     | 	confluent.balancer.incremental.balancing.cpu.top.proposal.tracking.enabled = true
benchi-kafka     | 	confluent.balancer.incremental.balancing.cpu.top.proposal.tracking.num.proposals = 15
benchi-kafka     | 	confluent.balancer.incremental.balancing.enabled = false
benchi-kafka     | 	confluent.balancer.incremental.balancing.goals = []
benchi-kafka     | 	confluent.balancer.incremental.balancing.lower.bound = 0.02
benchi-kafka     | 	confluent.balancer.incremental.balancing.min.valid.windows = 5
benchi-kafka     | 	confluent.balancer.incremental.balancing.step.ratio = 0.2
benchi-kafka     | 	confluent.balancer.inter.cell.balancing.enabled = false
benchi-kafka     | 	confluent.balancer.max.capacity.balancing.delta.percentage = 0.0
benchi-kafka     | 	confluent.balancer.max.replicas = 2147483647
benchi-kafka     | 	confluent.balancer.network.in.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.balancer.network.out.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.balancer.plan.computation.retry.timeout.ms = 3600000
benchi-kafka     | 	confluent.balancer.producer.in.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.balancer.rebalancing.goals = []
benchi-kafka     | 	confluent.balancer.replication.in.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.balancer.resource.utilization.detector.interval.ms = 60000
benchi-kafka     | 	confluent.balancer.sbc.metrics.parser.enabled = false
benchi-kafka     | 	confluent.balancer.self.healing.maximum.rounds = 1
benchi-kafka     | 	confluent.balancer.task.history.retention.days = 30
benchi-kafka     | 	confluent.balancer.tenant.maximum.movements = 0
benchi-kafka     | 	confluent.balancer.tenant.suspension.ms = 86400000
benchi-kafka     | 	confluent.balancer.throttle.bytes.per.second = 10485760
benchi-kafka     | 	confluent.balancer.topic.partition.maximum.movements = 5
benchi-kafka     | 	confluent.balancer.topic.partition.movement.expiration.ms = 10800000
benchi-kafka     | 	confluent.balancer.topic.partition.suspension.ms = 18000000
benchi-kafka     | 	confluent.balancer.topic.replication.factor = 1
benchi-kafka     | 	confluent.balancer.triggering.goals = []
benchi-kafka     | 	confluent.balancer.v2.addition.enabled = false
benchi-kafka     | 	confluent.basic.auth.credentials.source = null
benchi-kafka     | 	confluent.basic.auth.user.info = null
benchi-kafka     | 	confluent.bearer.auth.cache.expiry.buffer.seconds = 300
benchi-kafka     | 	confluent.bearer.auth.client.id = null
benchi-kafka     | 	confluent.bearer.auth.client.secret = null
benchi-kafka     | 	confluent.bearer.auth.credentials.source = null
benchi-kafka     | 	confluent.bearer.auth.identity.pool.id = null
benchi-kafka     | 	confluent.bearer.auth.issuer.endpoint.url = null
benchi-kafka     | 	confluent.bearer.auth.logical.cluster = null
benchi-kafka     | 	confluent.bearer.auth.scope = null
benchi-kafka     | 	confluent.bearer.auth.scope.claim.name = scope
benchi-kafka     | 	confluent.bearer.auth.sub.claim.name = sub
benchi-kafka     | 	confluent.bearer.auth.token = null
benchi-kafka     | 	confluent.broker.health.manager.enabled = true
benchi-kafka     | 	confluent.broker.health.manager.engine.request.handler.threads.stuck.criteria = AllThreadsStuck
benchi-kafka     | 	confluent.broker.health.manager.hard.kill.duration.ms = 60000
benchi-kafka     | 	confluent.broker.health.manager.mitigation.enabled = false
benchi-kafka     | 	confluent.broker.health.manager.num.samples.before.broker.suspect = 30
benchi-kafka     | 	confluent.broker.health.manager.num.samples.before.broker.unhealthy = 180
benchi-kafka     | 	confluent.broker.health.manager.percentage.unhealthy.samples.before.broker.suspect = 90
benchi-kafka     | 	confluent.broker.health.manager.percentage.unhealthy.samples.before.broker.unhealthy = 70
benchi-kafka     | 	confluent.broker.health.manager.sample.duration.ms = 1000
benchi-kafka     | 	confluent.broker.health.manager.storage.background.threads.stuck.criteria = AnyThreadStuck
benchi-kafka     | 	confluent.broker.health.manager.storage.network.threads.stuck.criteria = AnyThreadStuck
benchi-kafka     | 	confluent.broker.health.manager.storage.request.handler.threads.stuck.criteria = AnyThreadStuck
benchi-kafka     | 	confluent.broker.limit.consumer.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.broker.limit.producer.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.broker.load.advertised.limit.load = 0.8
benchi-kafka     | 	confluent.broker.load.average.service.request.time.ms = 0.1
benchi-kafka     | 	confluent.broker.load.delay.metric.start.ms = 180000
benchi-kafka     | 	confluent.broker.load.enabled = false
benchi-kafka     | 	confluent.broker.load.num.samples = 60
benchi-kafka     | 	confluent.broker.load.tenant.metric.enable = false
benchi-kafka     | 	confluent.broker.load.update.metric.tags.interval.ms = 60000
benchi-kafka     | 	confluent.broker.load.window.size.ms = 60000
benchi-kafka     | 	confluent.broker.load.workload.coefficient = 20.0
benchi-kafka     | 	confluent.broker.registration.delay.ms = 0
benchi-kafka     | 	confluent.catalog.collector.destination.topic = telemetry.events.data_catalog_source
benchi-kafka     | 	confluent.catalog.collector.enable = false
benchi-kafka     | 	confluent.catalog.collector.full.configs.enable = false
benchi-kafka     | 	confluent.catalog.collector.max.bytes.per.snapshot = 850000
benchi-kafka     | 	confluent.catalog.collector.max.topics.process = 500
benchi-kafka     | 	confluent.catalog.collector.max.zookeeper.request.per.sec = 100
benchi-kafka     | 	confluent.catalog.collector.snapshot.init.delay.sec = 60
benchi-kafka     | 	confluent.catalog.collector.snapshot.interval.sec = 300
benchi-kafka     | 	confluent.ccloud.host.suffixes = .confluent.cloud,.cpdev.cloud,.confluentgov.com,.confluentgov-internal.com
benchi-kafka     | 	confluent.ccloud.intranet.host.suffixes = .intranet.stag.cpdev.cloud,.intranet.stag.cpdev-untrusted.cloud,.intranet.devel.cpdev.cloud,.intranet.devel.cpdev-untrusted.cloud,.intranet.confluent.cloud,.intranet.confluent-untrusted.cloud
benchi-kafka     | 	confluent.cdc.api.keys.topic = 
benchi-kafka     | 	confluent.cdc.api.keys.topic.load.timeout.ms = 600000
benchi-kafka     | 	confluent.cdc.client.quotas.enable = false
benchi-kafka     | 	confluent.cdc.client.quotas.topic.name = 
benchi-kafka     | 	confluent.cdc.lkc.metadata.topic = 
benchi-kafka     | 	confluent.cdc.user.metadata.enable = false
benchi-kafka     | 	confluent.cdc.user.metadata.topic = _confluent-user_metadata
benchi-kafka     | 	confluent.cell.metrics.refresh.period.ms = 60000
benchi-kafka     | 	confluent.cells.default.size = 15
benchi-kafka     | 	confluent.cells.enable = false
benchi-kafka     | 	confluent.cells.implicit.creation.enable = false
benchi-kafka     | 	confluent.cells.load.refresher.enable = true
benchi-kafka     | 	confluent.cells.max.size = 15
benchi-kafka     | 	confluent.cells.min.size = 6
benchi-kafka     | 	confluent.checksum.enabled.files = [none]
benchi-kafka     | 	confluent.clm.enabled = false
benchi-kafka     | 	confluent.clm.frequency.in.hours = 6
benchi-kafka     | 	confluent.clm.list.object.thread_pool.size = 1
benchi-kafka     | 	confluent.clm.max.backup.days = 3
benchi-kafka     | 	confluent.clm.min.delay.in.minutes = 30
benchi-kafka     | 	confluent.clm.thread.pool.size = 2
benchi-kafka     | 	confluent.clm.topic.retention.days.to.backup.days = 0:0,3:3
benchi-kafka     | 	confluent.close.connections.on.credential.delete = false
benchi-kafka     | 	confluent.cluster.link.admin.max.in.flight.requests = 1000
benchi-kafka     | 	confluent.cluster.link.admin.request.batch.size = 1
benchi-kafka     | 	confluent.cluster.link.allow.config.providers = true
benchi-kafka     | 	confluent.cluster.link.allow.legacy.message.format = false
benchi-kafka     | 	confluent.cluster.link.allow.truncation.below.hwm = true
benchi-kafka     | 	confluent.cluster.link.availability.check.mode = ALL
benchi-kafka     | 	confluent.cluster.link.background.thread.affinity = LINK
benchi-kafka     | 	confluent.cluster.link.clients.max.idle.ms = 3153600000000
benchi-kafka     | 	confluent.cluster.link.enable = true
benchi-kafka     | 	confluent.cluster.link.enable.local.admin = false
benchi-kafka     | 	confluent.cluster.link.enable.metrics.reduction = false
benchi-kafka     | 	confluent.cluster.link.fetch.response.min.bytes = 1
benchi-kafka     | 	confluent.cluster.link.fetch.response.total.bytes = 2147483647
benchi-kafka     | 	confluent.cluster.link.fetcher.auto.tune.enable = false
benchi-kafka     | 	confluent.cluster.link.fetcher.thread.pool.mode = ENDPOINT
benchi-kafka     | 	confluent.cluster.link.insync.fetch.response.min.bytes = 1
benchi-kafka     | 	confluent.cluster.link.insync.fetch.response.total.bytes = 2147483647
benchi-kafka     | 	confluent.cluster.link.intranet.connectivity.enable = false
benchi-kafka     | 	confluent.cluster.link.io.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.cluster.link.local.reverse.connection.listener.map = null
benchi-kafka     | 	confluent.cluster.link.max.client.connections = 2147483647
benchi-kafka     | 	confluent.cluster.link.metadata.topic.create.retry.delay.ms = 1000
benchi-kafka     | 	confluent.cluster.link.metadata.topic.enable = false
benchi-kafka     | 	confluent.cluster.link.metadata.topic.min.isr = 2
benchi-kafka     | 	confluent.cluster.link.metadata.topic.partitions = 50
benchi-kafka     | 	confluent.cluster.link.metadata.topic.replication.factor = 1
benchi-kafka     | 	confluent.cluster.link.mirror.transition.batch.size = 10
benchi-kafka     | 	confluent.cluster.link.num.background.threads = 1
benchi-kafka     | 	confluent.cluster.link.periodic.task.batch.size = 2147483647
benchi-kafka     | 	confluent.cluster.link.periodic.task.min.interval.ms = 1000
benchi-kafka     | 	confluent.cluster.link.replica.fetch.connections.mode = combined
benchi-kafka     | 	confluent.cluster.link.replication.quota.mode = CLUSTER_LINK_ONLY
benchi-kafka     | 	confluent.cluster.link.replication.quota.mode.per.tenant.overrides = 
benchi-kafka     | 	confluent.cluster.link.replication.quota.window.num = 11
benchi-kafka     | 	confluent.cluster.link.replication.quota.window.size.seconds = 2
benchi-kafka     | 	confluent.cluster.link.request.quota.capacity = 400
benchi-kafka     | 	confluent.cluster.link.request.quota.request.percentage.multiplier = 1.0
benchi-kafka     | 	confluent.cluster.link.tenant.replication.quota.enable = false
benchi-kafka     | 	confluent.cluster.link.tenant.request.quota.enable = false
benchi-kafka     | 	confluent.cluster.metadata.snapshot.tier.delete.enable = false
benchi-kafka     | 	confluent.cluster.metadata.snapshot.tier.delete.maintain.min.snapshots = 3
benchi-kafka     | 	confluent.cluster.metadata.snapshot.tier.delete.retention.ms = 604800000
benchi-kafka     | 	confluent.cluster.metadata.snapshot.tier.upload.enable = false
benchi-kafka     | 	confluent.compacted.topic.prefer.tier.fetch.ms = -1
benchi-kafka     | 	confluent.connection.invalid.request.delay.enable = false
benchi-kafka     | 	confluent.connections.idle.expiry.manager.ignore.idleness.requests = []
benchi-kafka     | 	confluent.consumer.fetch.partition.pruning.enable = true
benchi-kafka     | 	confluent.consumer.lag.emitter.enabled = false
benchi-kafka     | 	confluent.consumer.lag.emitter.interval.ms = 60000
benchi-kafka     | 	confluent.dataflow.policy.watch.monitor.ms = 300000
benchi-kafka     | 	confluent.defer.isr.shrink.enable = false
benchi-kafka     | 	confluent.describe.topic.partitions.enabled = false
benchi-kafka     | 	confluent.disk.io.manager.enable = false
benchi-kafka     | 	confluent.disk.throughput.headroom = 10485760
benchi-kafka     | 	confluent.disk.throughput.limit = 10485760000
benchi-kafka     | 	confluent.disk.throughput.quota.tier.archive = 1048576000
benchi-kafka     | 	confluent.disk.throughput.quota.tier.archive.throttled = 104857600
benchi-kafka     | 	confluent.durability.audit.batch.flush.frequency.ms = 900000
benchi-kafka     | 	confluent.durability.audit.checks = PeriodicalAudit,ChecksumAudit
benchi-kafka     | 	confluent.durability.audit.enable = false
benchi-kafka     | 	confluent.durability.audit.idempotent.producer = false
benchi-kafka     | 	confluent.durability.audit.initial.job.delay.ms = 900000
benchi-kafka     | 	confluent.durability.audit.io.bytes.per.sec = 10485760
benchi-kafka     | 	confluent.durability.audit.log.ignored.event.types = 
benchi-kafka     | 	confluent.durability.audit.reporting.batch.ms = 1800000
benchi-kafka     | 	confluent.durability.audit.tier.compaction.audit.duration.ms = 14400000
benchi-kafka     | 	confluent.durability.events.allowed = OffsetChangeType,EpochChangeType,IsrExpandType,DeleteRecordsType,RetentionChangeType,StartOffsetChangeType,DeletePartitionType,HealthCheckType
benchi-kafka     | 	confluent.durability.topic.partition.count = 50
benchi-kafka     | 	confluent.durability.topic.replication.factor = 1
benchi-kafka     | 	confluent.e2e_checksum.protection.enabled = false
benchi-kafka     | 	confluent.e2e_checksum.protection.files = [none]
benchi-kafka     | 	confluent.e2e_checksum.protection.store.entry.ttl.ms = 2592000000
benchi-kafka     | 	confluent.elastic.cku.enabled = false
benchi-kafka     | 	confluent.elastic.cku.scaletozero.enabled = false
benchi-kafka     | 	confluent.eligible.controllers = []
benchi-kafka     | 	confluent.enable.stray.partition.deletion = false
benchi-kafka     | 	confluent.encryption.key.manager.rotation.interval.ms = 31536000000
benchi-kafka     | 	confluent.fetch.from.follower.require.leader.epoch.enable = false
benchi-kafka     | 	confluent.fetch.partition.pruning.enable = true
benchi-kafka     | 	confluent.floor.connection.rate.per.ip = -1.0
benchi-kafka     | 	confluent.floor.connection.rate.per.tenant = -1.0
benchi-kafka     | 	confluent.group.coordinator.offsets.batching.enable = false
benchi-kafka     | 	confluent.group.coordinator.offsets.writer.threads = 2
benchi-kafka     | 	confluent.group.metadata.load.threads = 32
benchi-kafka     | 	confluent.heap.tenured.notify.bytes = 0
benchi-kafka     | 	confluent.heap.tenured.notify.enabled = false
benchi-kafka     | 	confluent.hot.partition.ratio = 0.8
benchi-kafka     | 	confluent.http.server.start.timeout.ms = 60000
benchi-kafka     | 	confluent.http.server.stop.timeout.ms = 30000
benchi-kafka     | 	confluent.internal.metrics.enable = false
benchi-kafka     | 	confluent.internal.rest.server.bind.port = null
benchi-kafka     | 	confluent.internal.tenant.scoped.listener.name = INTERNAL_TENANT_SCOPED
benchi-kafka     | 	confluent.leader.epoch.checkpoint.checksum.enabled = false
benchi-kafka     | 	confluent.log.cleaner.timestamp.validation.enable = true
benchi-kafka     | 	confluent.log.placement.constraints = 
benchi-kafka     | 	confluent.max.connection.creation.rate.per.ip = 1.7976931348623157E308
benchi-kafka     | 	confluent.max.connection.creation.rate.per.tenant = 1.7976931348623157E308
benchi-kafka     | 	confluent.max.connection.rate.per.ip = -1.0
benchi-kafka     | 	confluent.max.connection.rate.per.tenant = -1.0
benchi-kafka     | 	confluent.max.connection.throttle.ms = null
benchi-kafka     | 	confluent.max.segment.ms = 9223372036854775807
benchi-kafka     | 	confluent.metadata.active.encryptor = null
benchi-kafka     | 	confluent.metadata.controlled.shutdown.partition.slice.delay.ms = 100
benchi-kafka     | 	confluent.metadata.encryptor.classes = null
benchi-kafka     | 	confluent.metadata.encryptor.required = false
benchi-kafka     | 	confluent.metadata.encryptor.secret.file = null
benchi-kafka     | 	confluent.metadata.encryptor.secrets = null
benchi-kafka     | 	confluent.metadata.jvm.warmup.ms = 60000
benchi-kafka     | 	confluent.metadata.leader.balance.slice.delay.ms = 100
benchi-kafka     | 	confluent.metadata.max.controlled.shutdown.partition.changes.per.slice = 1000
benchi-kafka     | 	confluent.metadata.max.leader.balance.changes.per.slice = 1000
benchi-kafka     | 	confluent.metadata.server.cluster.registry.clusters = []
benchi-kafka     | 	confluent.metadata.throttle.pre.check.for.non.wildcard.requests.enable = false
benchi-kafka     | 	confluent.metadata.throttle.pre.check.for.wildcard.requests.enable = false
benchi-kafka     | 	confluent.metrics.reporter.bootstrap.servers = localhost:9092
benchi-kafka     | 	confluent.min.acks = 0
benchi-kafka     | 	confluent.min.connection.throttle.ms = 0
benchi-kafka     | 	confluent.min.segment.ms = 1
benchi-kafka     | 	confluent.missing.id.cache.ttl.sec = 60
benchi-kafka     | 	confluent.missing.id.query.range = 20000
benchi-kafka     | 	confluent.missing.schema.cache.ttl.sec = 60
benchi-kafka     | 	confluent.mtls.enable = false
benchi-kafka     | 	confluent.mtls.listener.name = EXTERNAL
benchi-kafka     | 	confluent.mtls.sasl.authenticator.request.max.bytes = 104857600
benchi-kafka     | 	confluent.mtls.truststore.manager.class.name = null
benchi-kafka     | 	confluent.multitenant.authorizer.enable.acl.state = false
benchi-kafka     | 	confluent.multitenant.interceptor.balancer.apis.enabled = false
benchi-kafka     | 	confluent.multitenant.interceptor.collect.client.apiversions.max.per.tenant = 1000
benchi-kafka     | 	confluent.multitenant.interceptor.collect.client.apiversions.metric = false
benchi-kafka     | 	confluent.multitenant.listener.hostname.cluster.prefix.enable = false
benchi-kafka     | 	confluent.multitenant.listener.hostname.subdomain.suffix.enable = false
benchi-kafka     | 	confluent.multitenant.listener.names = null
benchi-kafka     | 	confluent.multitenant.parse.lkc.id.enable = false
benchi-kafka     | 	confluent.multitenant.parse.sni.host.name.enable = false
benchi-kafka     | 	confluent.network.health.manager.enabled = false
benchi-kafka     | 	confluent.network.health.manager.external.listener.name = EXTERNAL
benchi-kafka     | 	confluent.network.health.manager.externalconnectivitystartup.enabled = false
benchi-kafka     | 	confluent.network.health.manager.min.healthy.network.samples = 3
benchi-kafka     | 	confluent.network.health.manager.min.percentage.healthy.network.samples = 3
benchi-kafka     | 	confluent.network.health.manager.mitigation.enabled = false
benchi-kafka     | 	confluent.network.health.manager.network.sample.window.size = 120
benchi-kafka     | 	confluent.network.health.manager.sample.duration.ms = 1000
benchi-kafka     | 	confluent.oauth.flat.networking.verification.enable = false
benchi-kafka     | 	confluent.offsets.log.cleaner.delete.retention.ms = 86400000
benchi-kafka     | 	confluent.offsets.log.cleaner.max.compaction.lag.ms = 9223372036854775807
benchi-kafka     | 	confluent.offsets.log.cleaner.min.cleanable.dirty.ratio = 0.5
benchi-kafka     | 	confluent.offsets.topic.placement.constraints = 
benchi-kafka     | 	confluent.omit.network.processor.metric.tag = false
benchi-kafka     | 	confluent.operator.managed = false
benchi-kafka     | 	confluent.password.encoder.old.secret.ttl.ms = 9223372036854775807
benchi-kafka     | 	confluent.plugins.cluster.link.policy.max.destination.links.per.tenant = 10
benchi-kafka     | 	confluent.plugins.cluster.link.policy.max.source.links.per.tenant = 10
benchi-kafka     | 	confluent.plugins.topic.policy.max.partitions.per.cluster = 2147483647
benchi-kafka     | 	confluent.plugins.topic.policy.max.partitions.per.tenant = 512
benchi-kafka     | 	confluent.plugins.topic.policy.max.replicas.per.broker = 2147483647
benchi-kafka     | 	confluent.plugins.topic.policy.max.topics.per.cluster = 2147483647
benchi-kafka     | 	confluent.prefer.tier.fetch.ms = -1
benchi-kafka     | 	confluent.produce.throttle.pre.check.enable = false
benchi-kafka     | 	confluent.producer.id.cache.broker.hard.limit = -1
benchi-kafka     | 	confluent.producer.id.cache.eviction.minimal.expiration.ms = 900000
benchi-kafka     | 	confluent.producer.id.cache.extra.eviction.percentage = 0
benchi-kafka     | 	confluent.producer.id.cache.limit = 2147483647
benchi-kafka     | 	confluent.producer.id.cache.partition.hard.limit = -1
benchi-kafka     | 	confluent.producer.id.cache.tenant.hard.limit = -1
benchi-kafka     | 	confluent.producer.id.quota.manager.enable = false
benchi-kafka     | 	confluent.producer.id.quota.window.num = 11
benchi-kafka     | 	confluent.producer.id.quota.window.size.seconds = 1
benchi-kafka     | 	confluent.producer.id.throttle.enable = false
benchi-kafka     | 	confluent.producer.id.throttle.enable.threshold.percentage = 100
benchi-kafka     | 	confluent.proxy.mode.local.default = false
benchi-kafka     | 	confluent.proxy.protocol.fallback.enabled = false
benchi-kafka     | 	confluent.proxy.protocol.version = NONE
benchi-kafka     | 	confluent.quota.computing.usage.adjustment = 0.5
benchi-kafka     | 	confluent.quota.dynamic.enable = false
benchi-kafka     | 	confluent.quota.dynamic.publishing.interval.ms = 60000
benchi-kafka     | 	confluent.quota.dynamic.reporting.interval.ms = 30000
benchi-kafka     | 	confluent.quota.dynamic.reporting.min.usage = 102400
benchi-kafka     | 	confluent.quota.tenant.broker.max.consumer.rate = 13107200
benchi-kafka     | 	confluent.quota.tenant.broker.max.producer.rate = 13107200
benchi-kafka     | 	confluent.quota.tenant.default.controller.mutation.rate = 2.147483647E9
benchi-kafka     | 	confluent.quota.tenant.default.producer.id.rate = 2.147483647E9
benchi-kafka     | 	confluent.quota.tenant.fetch.multiplier = 1.0
benchi-kafka     | 	confluent.quota.tenant.follower.broker.min.consumer.rate = 10485760
benchi-kafka     | 	confluent.quota.tenant.follower.broker.min.producer.rate = 10485760
benchi-kafka     | 	confluent.quota.tenant.produce.multiplier = 1.0
benchi-kafka     | 	confluent.quota.tenant.user.quotas.enable = false
benchi-kafka     | 	confluent.regional.metadata.client.class = null
benchi-kafka     | 	confluent.regional.resource.manager.client.scheduler.threads = 2
benchi-kafka     | 	confluent.regional.resource.manager.endpoint = null
benchi-kafka     | 	confluent.regional.resource.manager.watch.endpoint = null
benchi-kafka     | 	confluent.replica.fetch.backoff.max.ms = 1000
benchi-kafka     | 	confluent.replica.fetch.connections.mode = combined
benchi-kafka     | 	confluent.replication.mode = PULL
benchi-kafka     | 	confluent.replication.push.feature.enable = false
benchi-kafka     | 	confluent.reporters.telemetry.auto.enable = true
benchi-kafka     | 	confluent.request.log.filter.class = class org.apache.kafka.common.requests.SamplingRequestLogFilter
benchi-kafka     | 	confluent.request.pipelining.enable = true
benchi-kafka     | 	confluent.request.pipelining.max.in.flight.requests.per.connection = 5
benchi-kafka     | 	confluent.require.calling.resource.identity = false
benchi-kafka     | 	confluent.require.compatible.keystore.updates = true
benchi-kafka     | 	confluent.require.confluent.issuer = false
benchi-kafka     | 	confluent.roll.check.interval.ms = 300000
benchi-kafka     | 	confluent.schema.registry.max.cache.size = 10000
benchi-kafka     | 	confluent.schema.registry.max.retries = 1
benchi-kafka     | 	confluent.schema.registry.retries.wait.ms = 0
benchi-kafka     | 	confluent.schema.registry.url = null
benchi-kafka     | 	confluent.schema.validator.interceptor.class = io.confluent.kafka.schemaregistry.validator.RecordSchemaValidator
benchi-kafka     | 	confluent.schema.validator.multitenant.enable = false
benchi-kafka     | 	confluent.schema.validator.samples.per.min = 0
benchi-kafka     | 	confluent.security.bc.approved.mode.enable = false
benchi-kafka     | 	confluent.security.event.logger.authentication.enable = false
benchi-kafka     | 	confluent.security.event.logger.authentication.event.rate.limit = -1
benchi-kafka     | 	confluent.security.event.logger.authorization.event.rate.limit = -1
benchi-kafka     | 	confluent.security.event.logger.detailed.audit.logs.filter.class = class org.apache.kafka.common.requests.DetailedRequestAuditLogFilter
benchi-kafka     | 	confluent.security.event.logger.enable = true
benchi-kafka     | 	confluent.security.event.logger.kafka.request.rate.limit = -1
benchi-kafka     | 	confluent.security.event.logger.physical.cluster.id = 
benchi-kafka     | 	confluent.security.event.router.config = 
benchi-kafka     | 	confluent.security.revoked.certificate.ids = 
benchi-kafka     | 	confluent.segment.eager.roll.enable = false
benchi-kafka     | 	confluent.segment.speculative.prefetch.enable = false
benchi-kafka     | 	confluent.spiffe.id.principal.extraction.rules = 
benchi-kafka     | 	confluent.ssl.key.password = null
benchi-kafka     | 	confluent.ssl.keystore.location = null
benchi-kafka     | 	confluent.ssl.keystore.password = null
benchi-kafka     | 	confluent.ssl.keystore.type = null
benchi-kafka     | 	confluent.ssl.protocol = null
benchi-kafka     | 	confluent.ssl.truststore.location = null
benchi-kafka     | 	confluent.ssl.truststore.password = null
benchi-kafka     | 	confluent.ssl.truststore.type = null
benchi-kafka     | 	confluent.step.connection.rate.per.ip = -1.0
benchi-kafka     | 	confluent.step.connection.rate.per.tenant = -1.0
benchi-kafka     | 	confluent.storage.probe.period.ms = -1
benchi-kafka     | 	confluent.storage.probe.slow.write.threshold.ms = 5000
benchi-kafka     | 	confluent.stray.log.delete.delay.ms = 604800000
benchi-kafka     | 	confluent.stray.log.max.deletions.per.run = 72
benchi-kafka     | 	confluent.subdomain.prefix = null
benchi-kafka     | 	confluent.subdomain.separator.map = null
benchi-kafka     | 	confluent.subdomain.separator.variable = %sep
benchi-kafka     | 	confluent.system.time.roll.enable = false
benchi-kafka     | 	confluent.telemetry.enabled = false
benchi-kafka     | 	confluent.telemetry.external.client.metrics.push.enabled = false
benchi-kafka     | 	confluent.tenant.latency.metric.enabled = false
benchi-kafka     | 	confluent.tier.archiver.num.threads = 2
benchi-kafka     | 	confluent.tier.azure.block.blob.auto.abort.threshold.bytes = 500000
benchi-kafka     | 	confluent.tier.azure.block.blob.container = null
benchi-kafka     | 	confluent.tier.azure.block.blob.cred.file.path = null
benchi-kafka     | 	confluent.tier.azure.block.blob.endpoint = null
benchi-kafka     | 	confluent.tier.azure.block.blob.prefix = 
benchi-kafka     | 	confluent.tier.backend = 
benchi-kafka     | 	confluent.tier.bucket.probe.period.ms = -1
benchi-kafka     | 	confluent.tier.cleaner.compact.min.efficiency = 0.5
benchi-kafka     | 	confluent.tier.cleaner.compact.segment.min.bytes = 20971520
benchi-kafka     | 	confluent.tier.cleaner.dedupe.buffer.size = 134217728
benchi-kafka     | 	confluent.tier.cleaner.dual.compaction = false
benchi-kafka     | 	confluent.tier.cleaner.dual.compaction.validation.max.bytes = 1073741824
benchi-kafka     | 	confluent.tier.cleaner.dual.compaction.validation.percent = 0
benchi-kafka     | 	confluent.tier.cleaner.enable = false
benchi-kafka     | 	confluent.tier.cleaner.excluded.topics = [^_confluent.*]
benchi-kafka     | 	confluent.tier.cleaner.feature.enable = false
benchi-kafka     | 	confluent.tier.cleaner.io.buffer.load.factor = 0.9
benchi-kafka     | 	confluent.tier.cleaner.io.buffer.size = 10485760
benchi-kafka     | 	confluent.tier.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
benchi-kafka     | 	confluent.tier.cleaner.min.cleanable.ratio = 0.75
benchi-kafka     | 	confluent.tier.cleaner.num.threads = 2
benchi-kafka     | 	confluent.tier.enable = false
benchi-kafka     | 	confluent.tier.feature = false
benchi-kafka     | 	confluent.tier.fenced.segment.delete.delay.ms = 600000
benchi-kafka     | 	confluent.tier.fetcher.async.enable = false
benchi-kafka     | 	confluent.tier.fetcher.async.timestamp.offset.parallelism = 1
benchi-kafka     | 	confluent.tier.fetcher.fetch.based.on.segment_and_metadata_layout.field = false
benchi-kafka     | 	confluent.tier.fetcher.memorypool.bytes = 0
benchi-kafka     | 	confluent.tier.fetcher.num.threads = 4
benchi-kafka     | 	confluent.tier.fetcher.offset.cache.expiration.ms = 1800000
benchi-kafka     | 	confluent.tier.fetcher.offset.cache.period.ms = 60000
benchi-kafka     | 	confluent.tier.fetcher.offset.cache.size = 200000
benchi-kafka     | 	confluent.tier.gcs.bucket = null
benchi-kafka     | 	confluent.tier.gcs.cred.file.path = null
benchi-kafka     | 	confluent.tier.gcs.prefix = 
benchi-kafka     | 	confluent.tier.gcs.region = null
benchi-kafka     | 	confluent.tier.gcs.sse.customer.encryption.key = null
benchi-kafka     | 	confluent.tier.gcs.write.chunk.size = 0
benchi-kafka     | 	confluent.tier.local.hotset.bytes = -1
benchi-kafka     | 	confluent.tier.local.hotset.ms = 86400000
benchi-kafka     | 	confluent.tier.max.partition.fetch.bytes.override = 0
benchi-kafka     | 	confluent.tier.metadata.bootstrap.servers = null
benchi-kafka     | 	confluent.tier.metadata.max.poll.ms = 100
benchi-kafka     | 	confluent.tier.metadata.namespace = null
benchi-kafka     | 	confluent.tier.metadata.num.partitions = 50
benchi-kafka     | 	confluent.tier.metadata.replication.factor = 1
benchi-kafka     | 	confluent.tier.metadata.request.timeout.ms = 30000
benchi-kafka     | 	confluent.tier.metadata.snapshots.enable = false
benchi-kafka     | 	confluent.tier.metadata.snapshots.interval.ms = 86400000
benchi-kafka     | 	confluent.tier.metadata.snapshots.retention.days = 7
benchi-kafka     | 	confluent.tier.metadata.snapshots.threads = 2
benchi-kafka     | 	confluent.tier.object.fetcher.num.threads = 1
benchi-kafka     | 	confluent.tier.partition.state.cleanup.delay.ms = 2592000000
benchi-kafka     | 	confluent.tier.partition.state.cleanup.enable = false
benchi-kafka     | 	confluent.tier.partition.state.cleanup.interval.ms = 86400000
benchi-kafka     | 	confluent.tier.partition.state.commit.interval.ms = 15000
benchi-kafka     | 	confluent.tier.s3.assumerole.arn = null
benchi-kafka     | 	confluent.tier.s3.auto.abort.threshold.bytes = 500000
benchi-kafka     | 	confluent.tier.s3.aws.endpoint.override = null
benchi-kafka     | 	confluent.tier.s3.aws.signer.override = null
benchi-kafka     | 	confluent.tier.s3.bucket = null
benchi-kafka     | 	confluent.tier.s3.cred.file.path = null
benchi-kafka     | 	confluent.tier.s3.force.path.style.access = false
benchi-kafka     | 	confluent.tier.s3.prefix = 
benchi-kafka     | 	confluent.tier.s3.region = null
benchi-kafka     | 	confluent.tier.s3.security.providers = null
benchi-kafka     | 	confluent.tier.s3.sse.algorithm = AES256
benchi-kafka     | 	confluent.tier.s3.sse.customer.encryption.key = null
benchi-kafka     | 	confluent.tier.s3.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	confluent.tier.s3.ssl.key.password = null
benchi-kafka     | 	confluent.tier.s3.ssl.keystore.location = null
benchi-kafka     | 	confluent.tier.s3.ssl.keystore.password = null
benchi-kafka     | 	confluent.tier.s3.ssl.keystore.type = null
benchi-kafka     | 	confluent.tier.s3.ssl.protocol = TLSv1.3
benchi-kafka     | 	confluent.tier.s3.ssl.provider = null
benchi-kafka     | 	confluent.tier.s3.ssl.truststore.location = null
benchi-kafka     | 	confluent.tier.s3.ssl.truststore.password = null
benchi-kafka     | 	confluent.tier.s3.ssl.truststore.type = null
benchi-kafka     | 	confluent.tier.s3.user.agent.prefix = APN/1.0 Confluent/1.0 TieredStorageS3/1.0
benchi-kafka     | 	confluent.tier.segment.hotset.roll.min.bytes = 104857600
benchi-kafka     | 	confluent.tier.segment.metadata.layout.put.mode = LegacyMultiObject
benchi-kafka     | 	confluent.tier.topic.data.loss.validation.fencing.enable = false
benchi-kafka     | 	confluent.tier.topic.delete.backoff.ms = 21600000
benchi-kafka     | 	confluent.tier.topic.delete.check.interval.ms = 300000
benchi-kafka     | 	confluent.tier.topic.delete.max.inprogress.partitions = 100
benchi-kafka     | 	confluent.tier.topic.head.data.loss.validation.enable = true
benchi-kafka     | 	confluent.tier.topic.head.data.loss.validation.max.timeout.ms = 900000
benchi-kafka     | 	confluent.tier.topic.materialization.from.snapshot.enable = false
benchi-kafka     | 	confluent.tier.topic.producer.enable.idempotence = true
benchi-kafka     | 	confluent.tier.topic.snapshots.enable = false
benchi-kafka     | 	confluent.tier.topic.snapshots.interval.ms = 300000
benchi-kafka     | 	confluent.tier.topic.snapshots.max.records = 100000
benchi-kafka     | 	confluent.tier.topic.snapshots.retention.hours = 168
benchi-kafka     | 	confluent.topic.partition.default.placement = 2
benchi-kafka     | 	confluent.topic.policy.use.computed.assignments = false
benchi-kafka     | 	confluent.topic.replica.assignor.builder.class = 
benchi-kafka     | 	confluent.track.api.key.per.ip = false
benchi-kafka     | 	confluent.track.per.ip.max.size = 100000
benchi-kafka     | 	confluent.track.tenant.id.per.ip = false
benchi-kafka     | 	confluent.traffic.cdc.network.id.routes.enable = false
benchi-kafka     | 	confluent.traffic.cdc.network.id.routes.listener.names = EXTERNAL_BACKCHANNEL
benchi-kafka     | 	confluent.traffic.cdc.network.id.routes.periodic.start.task.ms = 300000
benchi-kafka     | 	confluent.traffic.cdc.network.id.routes.topic.name = _confluent-network_id_routes
benchi-kafka     | 	confluent.traffic.network.id = 
benchi-kafka     | 	confluent.transaction.2pc.timeout.ms = -1
benchi-kafka     | 	confluent.transaction.logging.verbosity = 0
benchi-kafka     | 	confluent.transaction.state.log.placement.constraints = 
benchi-kafka     | 	confluent.valid.broker.rack.set = null
benchi-kafka     | 	confluent.verify.group.subscription.prefix = false
benchi-kafka     | 	confluent.virtual.topic.creation.enabled = false
benchi-kafka     | 	confluent.zookeeper.metadata.migration.controller.check.disable = false
benchi-kafka     | 	confluent.zookeeper.metadata.migration.trigger.file.path = null
benchi-kafka     | 	connection.failed.authentication.delay.ms = 100
benchi-kafka     | 	connection.min.expire.interval.ms = 250
benchi-kafka     | 	connections.max.age.ms = 3153600000000
benchi-kafka     | 	connections.max.idle.ms = 600000
benchi-kafka     | 	connections.max.reauth.ms = 0
benchi-kafka     | 	control.plane.listener.name = null
benchi-kafka     | 	controlled.shutdown.enable = true
benchi-kafka     | 	controlled.shutdown.max.retries = 3
benchi-kafka     | 	controlled.shutdown.retry.backoff.ms = 5000
benchi-kafka     | 	controller.listener.names = CONTROLLER
benchi-kafka     | 	controller.quorum.append.linger.ms = 25
benchi-kafka     | 	controller.quorum.bootstrap.servers = []
benchi-kafka     | 	controller.quorum.election.backoff.max.ms = 1000
benchi-kafka     | 	controller.quorum.election.timeout.ms = 1000
benchi-kafka     | 	controller.quorum.fetch.timeout.ms = 2000
benchi-kafka     | 	controller.quorum.request.timeout.ms = 2000
benchi-kafka     | 	controller.quorum.retry.backoff.ms = 20
benchi-kafka     | 	controller.quorum.voters = [1@broker:29093]
benchi-kafka     | 	controller.quota.window.num = 11
benchi-kafka     | 	controller.quota.window.size.seconds = 1
benchi-kafka     | 	controller.socket.timeout.ms = 30000
benchi-kafka     | 	create.cluster.link.policy.class.name = null
benchi-kafka     | 	create.topic.policy.class.name = null
benchi-kafka     | 	default.replication.factor = 1
benchi-kafka     | 	delegation.token.expiry.check.interval.ms = 3600000
benchi-kafka     | 	delegation.token.expiry.time.ms = 86400000
benchi-kafka     | 	delegation.token.master.key = null
benchi-kafka     | 	delegation.token.max.lifetime.ms = 604800000
benchi-kafka     | 	delegation.token.secret.key = null
benchi-kafka     | 	delete.records.purgatory.purge.interval.requests = 1
benchi-kafka     | 	delete.topic.enable = true
benchi-kafka     | 	early.start.listeners = null
benchi-kafka     | 	eligible.leader.replicas.enable = false
benchi-kafka     | 	enable.fips = false
benchi-kafka     | 	fetch.max.bytes = 57671680
benchi-kafka     | 	fetch.purgatory.purge.interval.requests = 1000
benchi-kafka     | 	floor.max.connection.creation.rate = null
benchi-kafka     | 	follower.replication.throttled.rate = 9223372036854775807
benchi-kafka     | 	follower.replication.throttled.replicas = none
benchi-kafka     | 	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
benchi-kafka     | 	group.consumer.heartbeat.interval.ms = 5000
benchi-kafka     | 	group.consumer.max.heartbeat.interval.ms = 15000
benchi-kafka     | 	group.consumer.max.session.timeout.ms = 60000
benchi-kafka     | 	group.consumer.max.size = 2147483647
benchi-kafka     | 	group.consumer.migration.policy = disabled
benchi-kafka     | 	group.consumer.min.heartbeat.interval.ms = 5000
benchi-kafka     | 	group.consumer.min.session.timeout.ms = 45000
benchi-kafka     | 	group.consumer.session.timeout.ms = 45000
benchi-kafka     | 	group.coordinator.append.linger.ms = 10
benchi-kafka     | 	group.coordinator.new.enable = false
benchi-kafka     | 	group.coordinator.rebalance.protocols = [classic]
benchi-kafka     | 	group.coordinator.threads = 1
benchi-kafka     | 	group.initial.rebalance.delay.ms = 0
benchi-kafka     | 	group.max.session.timeout.ms = 1800000
benchi-kafka     | 	group.max.size = 2147483647
benchi-kafka     | 	group.min.session.timeout.ms = 6000
benchi-kafka     | 	initial.broker.registration.timeout.ms = 60000
benchi-kafka     | 	inter.broker.listener.name = PLAINTEXT
benchi-kafka     | 	inter.broker.protocol.version = 3.8-IV0A
benchi-kafka     | 	k2.stack.builder.class.name = null
benchi-kafka     | 	k2.startup.timeout.ms = 60000
benchi-kafka     | 	k2.topic.metadata.max.total.partition.count = 20000
benchi-kafka     | 	k2.topic.metadata.refresh.ms = 10000
benchi-kafka     | 	kafka.metrics.polling.interval.secs = 10
benchi-kafka     | 	kafka.metrics.reporters = []
benchi-kafka     | 	leader.imbalance.check.interval.seconds = 300
benchi-kafka     | 	leader.imbalance.per.broker.percentage = 10
benchi-kafka     | 	leader.replication.throttled.rate = 9223372036854775807
benchi-kafka     | 	leader.replication.throttled.replicas = none
benchi-kafka     | 	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
benchi-kafka     | 	listeners = PLAINTEXT://broker:29092,CONTROLLER://broker:29093,PLAINTEXT_HOST://0.0.0.0:9092
benchi-kafka     | 	log.cleaner.backoff.ms = 15000
benchi-kafka     | 	log.cleaner.dedupe.buffer.size = 134217728
benchi-kafka     | 	log.cleaner.delete.retention.ms = 86400000
benchi-kafka     | 	log.cleaner.enable = true
benchi-kafka     | 	log.cleaner.hash.algorithm = MD5
benchi-kafka     | 	log.cleaner.io.buffer.load.factor = 0.9
benchi-kafka     | 	log.cleaner.io.buffer.size = 524288
benchi-kafka     | 	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
benchi-kafka     | 	log.cleaner.max.compaction.lag.ms = 9223372036854775807
benchi-kafka     | 	log.cleaner.min.cleanable.ratio = 0.5
benchi-kafka     | 	log.cleaner.min.compaction.lag.ms = 0
benchi-kafka     | 	log.cleaner.threads = 1
benchi-kafka     | 	log.cleanup.policy = [delete]
benchi-kafka     | 	log.deletion.max.segments.per.run = 2147483647
benchi-kafka     | 	log.deletion.throttler.disk.free.headroom.bytes = 21474836480
benchi-kafka     | 	log.dir = /tmp/kafka-logs
benchi-kafka     | 	log.dir.failure.timeout.ms = 30000
benchi-kafka     | 	log.dirs = /tmp/kraft-combined-logs
benchi-kafka     | 	log.flush.interval.messages = 9223372036854775807
benchi-kafka     | 	log.flush.interval.ms = null
benchi-kafka     | 	log.flush.offset.checkpoint.interval.ms = 60000
benchi-kafka     | 	log.flush.scheduler.interval.ms = 9223372036854775807
benchi-kafka     | 	log.flush.start.offset.checkpoint.interval.ms = 60000
benchi-kafka     | 	log.index.interval.bytes = 4096
benchi-kafka     | 	log.index.size.max.bytes = 10485760
benchi-kafka     | 	log.initial.task.delay.ms = 30000
benchi-kafka     | 	log.local.retention.bytes = -2
benchi-kafka     | 	log.local.retention.ms = -2
benchi-kafka     | 	log.message.downconversion.enable = true
benchi-kafka     | 	log.message.format.version = 3.0-IV1
benchi-kafka     | 	log.message.timestamp.after.max.ms = 9223372036854775807
benchi-kafka     | 	log.message.timestamp.before.max.ms = 9223372036854775807
benchi-kafka     | 	log.message.timestamp.difference.max.ms = 9223372036854775807
benchi-kafka     | 	log.message.timestamp.type = CreateTime
benchi-kafka     | 	log.preallocate = false
benchi-kafka     | 	log.retention.bytes = -1
benchi-kafka     | 	log.retention.check.interval.ms = 300000
benchi-kafka     | 	log.retention.hours = 168
benchi-kafka     | 	log.retention.minutes = null
benchi-kafka     | 	log.retention.ms = null
benchi-kafka     | 	log.roll.hours = 168
benchi-kafka     | 	log.roll.jitter.hours = 0
benchi-kafka     | 	log.roll.jitter.ms = null
benchi-kafka     | 	log.roll.ms = null
benchi-kafka     | 	log.segment.bytes = 1073741824
benchi-kafka     | 	log.segment.delete.delay.ms = 60000
benchi-kafka     | 	max.connection.creation.rate = 1.7976931348623157E308
benchi-kafka     | 	max.connection.creation.rate.per.ip.enable.threshold = 0.0
benchi-kafka     | 	max.connection.creation.rate.per.tenant.enable.threshold = 0.0
benchi-kafka     | 	max.connections = 2147483647
benchi-kafka     | 	max.connections.per.ip = 2147483647
benchi-kafka     | 	max.connections.per.ip.overrides = 
benchi-kafka     | 	max.connections.per.tenant = 0
benchi-kafka     | 	max.connections.protected.listeners = []
benchi-kafka     | 	max.connections.reap.amount = 0
benchi-kafka     | 	max.incremental.fetch.session.cache.slots = 1000
benchi-kafka     | 	max.request.partition.size.limit = 2000
benchi-kafka     | 	message.max.bytes = 1048588
benchi-kafka     | 	metadata.log.dir = null
benchi-kafka     | 	metadata.log.max.record.bytes.between.snapshots = 20971520
benchi-kafka     | 	metadata.log.max.snapshot.interval.ms = 3600000
benchi-kafka     | 	metadata.log.segment.bytes = 1073741824
benchi-kafka     | 	metadata.log.segment.min.bytes = 8388608
benchi-kafka     | 	metadata.log.segment.ms = 604800000
benchi-kafka     | 	metadata.max.idle.interval.ms = 500
benchi-kafka     | 	metadata.max.retention.bytes = 104857600
benchi-kafka     | 	metadata.max.retention.ms = 604800000
benchi-kafka     | 	metric.reporters = [io.confluent.metrics.reporter.ConfluentMetricsReporter]
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.recording.level = INFO
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	min.insync.replicas = 1
benchi-kafka     | 	multitenant.authorizer.support.resource.ids = false
benchi-kafka     | 	multitenant.metadata.class = null
benchi-kafka     | 	multitenant.metadata.dir = null
benchi-kafka     | 	multitenant.metadata.reload.delay.ms = 120000
benchi-kafka     | 	multitenant.metadata.ssl.certs.path = null
benchi-kafka     | 	multitenant.tenant.delete.batch.size = 10
benchi-kafka     | 	multitenant.tenant.delete.check.ms = 120000
benchi-kafka     | 	multitenant.tenant.delete.delay = 604800000
benchi-kafka     | 	node.id = 1
benchi-kafka     | 	num.io.threads = 8
benchi-kafka     | 	num.network.threads = 3
benchi-kafka     | 	num.partitions = 1
benchi-kafka     | 	num.recovery.threads.per.data.dir = 1
benchi-kafka     | 	num.replica.alter.log.dirs.threads = null
benchi-kafka     | 	num.replica.fetchers = 1
benchi-kafka     | 	offset.metadata.max.bytes = 4096
benchi-kafka     | 	offsets.commit.required.acks = -1
benchi-kafka     | 	offsets.commit.timeout.ms = 5000
benchi-kafka     | 	offsets.load.buffer.size = 5242880
benchi-kafka     | 	offsets.retention.check.interval.ms = 600000
benchi-kafka     | 	offsets.retention.minutes = 10080
benchi-kafka     | 	offsets.topic.compression.codec = 0
benchi-kafka     | 	offsets.topic.num.partitions = 50
benchi-kafka     | 	offsets.topic.replication.factor = 1
benchi-kafka     | 	offsets.topic.segment.bytes = 104857600
benchi-kafka     | 	otel.exporter.otlp.custom.endpoint = default
benchi-kafka     | 	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
benchi-kafka     | 	password.encoder.iterations = 4096
benchi-kafka     | 	password.encoder.key.length = 128
benchi-kafka     | 	password.encoder.keyfactory.algorithm = null
benchi-kafka     | 	password.encoder.old.secret = null
benchi-kafka     | 	password.encoder.secret = null
benchi-kafka     | 	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
benchi-kafka     | 	process.roles = [broker, controller]
benchi-kafka     | 	producer.id.expiration.check.interval.ms = 600000
benchi-kafka     | 	producer.id.expiration.ms = 86400000
benchi-kafka     | 	producer.purgatory.purge.interval.requests = 1000
benchi-kafka     | 	queued.max.request.bytes = -1
benchi-kafka     | 	queued.max.requests = 500
benchi-kafka     | 	quota.window.num = 11
benchi-kafka     | 	quota.window.size.seconds = 1
benchi-kafka     | 	quotas.consumption.expiration.time.ms = 600000
benchi-kafka     | 	quotas.expiration.interval.ms = 3600000
benchi-kafka     | 	quotas.expiration.time.ms = 604800000
benchi-kafka     | 	quotas.lazy.evaluation.threshold = 0.5
benchi-kafka     | 	quotas.topic.append.timeout.ms = 5000
benchi-kafka     | 	quotas.topic.compression.codec = 3
benchi-kafka     | 	quotas.topic.load.buffer.size = 5242880
benchi-kafka     | 	quotas.topic.num.partitions = 50
benchi-kafka     | 	quotas.topic.placement.constraints = 
benchi-kafka     | 	quotas.topic.replication.factor = 3
benchi-kafka     | 	quotas.topic.segment.bytes = 104857600
benchi-kafka     | 	remote.fetch.max.wait.ms = 500
benchi-kafka     | 	remote.log.index.file.cache.total.size.bytes = 1073741824
benchi-kafka     | 	remote.log.manager.copier.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.copy.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.copy.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.expiration.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.fetch.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.fetch.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.task.interval.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.max.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.ms = 500
benchi-kafka     | 	remote.log.manager.task.retry.jitter = 0.2
benchi-kafka     | 	remote.log.manager.thread.pool.size = 10
benchi-kafka     | 	remote.log.metadata.custom.metadata.max.bytes = 128
benchi-kafka     | 	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
benchi-kafka     | 	remote.log.metadata.manager.class.path = null
benchi-kafka     | 	remote.log.metadata.manager.impl.prefix = rlmm.config.
benchi-kafka     | 	remote.log.metadata.manager.listener.name = null
benchi-kafka     | 	remote.log.reader.max.pending.tasks = 100
benchi-kafka     | 	remote.log.reader.threads = 10
benchi-kafka     | 	remote.log.storage.manager.class.name = null
benchi-kafka     | 	remote.log.storage.manager.class.path = null
benchi-kafka     | 	remote.log.storage.manager.impl.prefix = rsm.config.
benchi-kafka     | 	remote.log.storage.system.enable = false
benchi-kafka     | 	replica.fetch.backoff.ms = 1000
benchi-kafka     | 	replica.fetch.max.bytes = 1048576
benchi-kafka     | 	replica.fetch.min.bytes = 1
benchi-kafka     | 	replica.fetch.response.max.bytes = 10485760
benchi-kafka     | 	replica.fetch.wait.max.ms = 500
benchi-kafka     | 	replica.high.watermark.checkpoint.interval.ms = 5000
benchi-kafka     | 	replica.lag.time.max.ms = 30000
benchi-kafka     | 	replica.selector.class = null
benchi-kafka     | 	replica.socket.receive.buffer.bytes = 65536
benchi-kafka     | 	replica.socket.timeout.ms = 30000
benchi-kafka     | 	replication.quota.window.num = 11
benchi-kafka     | 	replication.quota.window.size.seconds = 1
benchi-kafka     | 	request.timeout.ms = 30000
benchi-kafka     | 	reserved.broker.max.id = 1000
benchi-kafka     | 	sasl.client.callback.handler.class = null
benchi-kafka     | 	sasl.enabled.mechanisms = [GSSAPI]
benchi-kafka     | 	sasl.jaas.config = null
benchi-kafka     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
benchi-kafka     | 	sasl.kerberos.min.time.before.relogin = 60000
benchi-kafka     | 	sasl.kerberos.principal.to.local.rules = [DEFAULT]
benchi-kafka     | 	sasl.kerberos.service.name = null
benchi-kafka     | 	sasl.kerberos.ticket.renew.jitter = 0.05
benchi-kafka     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
benchi-kafka     | 	sasl.login.callback.handler.class = null
benchi-kafka     | 	sasl.login.class = null
benchi-kafka     | 	sasl.login.connect.timeout.ms = null
benchi-kafka     | 	sasl.login.read.timeout.ms = null
benchi-kafka     | 	sasl.login.refresh.buffer.seconds = 300
benchi-kafka     | 	sasl.login.refresh.min.period.seconds = 60
benchi-kafka     | 	sasl.login.refresh.window.factor = 0.8
benchi-kafka     | 	sasl.login.refresh.window.jitter = 0.05
benchi-kafka     | 	sasl.login.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.login.retry.backoff.ms = 100
benchi-kafka     | 	sasl.mechanism.controller.protocol = GSSAPI
benchi-kafka     | 	sasl.mechanism.inter.broker.protocol = GSSAPI
benchi-kafka     | 	sasl.oauthbearer.clientassertion.audience = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.expiration = 5
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.jti.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.nbf.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.issuer = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.location = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key.passphrase = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.subject = null
benchi-kafka     | 	sasl.oauthbearer.clock.skew.seconds = 30
benchi-kafka     | 	sasl.oauthbearer.expected.audience = null
benchi-kafka     | 	sasl.oauthbearer.expected.issuer = null
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.url = null
benchi-kafka     | 	sasl.oauthbearer.scope.claim.name = scope
benchi-kafka     | 	sasl.oauthbearer.sub.claim.name = sub
benchi-kafka     | 	sasl.oauthbearer.token.endpoint.url = null
benchi-kafka     | 	sasl.server.authn.async.enable = false
benchi-kafka     | 	sasl.server.authn.async.max.threads = 1
benchi-kafka     | 	sasl.server.authn.async.timeout.ms = 30000
benchi-kafka     | 	sasl.server.callback.handler.class = null
benchi-kafka     | 	sasl.server.max.receive.size = 524288
benchi-kafka     | 	security.inter.broker.protocol = PLAINTEXT
benchi-kafka     | 	security.providers = null
benchi-kafka     | 	server.max.startup.time.ms = 9223372036854775807
benchi-kafka     | 	socket.connection.setup.timeout.max.ms = 30000
benchi-kafka     | 	socket.connection.setup.timeout.ms = 10000
benchi-kafka     | 	socket.listen.backlog.size = 50
benchi-kafka     | 	socket.receive.buffer.bytes = 102400
benchi-kafka     | 	socket.request.max.bytes = 104857600
benchi-kafka     | 	socket.send.buffer.bytes = 102400
benchi-kafka     | 	ssl.allow.dn.changes = false
benchi-kafka     | 	ssl.allow.san.changes = false
benchi-kafka     | 	ssl.cipher.suites = []
benchi-kafka     | 	ssl.client.auth = none
benchi-kafka     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	ssl.engine.factory.class = null
benchi-kafka     | 	ssl.key.password = null
benchi-kafka     | 	ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	ssl.keystore.certificate.chain = null
benchi-kafka     | 	ssl.keystore.key = null
benchi-kafka     | 	ssl.keystore.location = null
benchi-kafka     | 	ssl.keystore.password = null
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.principal.mapping.rules = DEFAULT
benchi-kafka     | 	ssl.protocol = TLSv1.3
benchi-kafka     | 	ssl.provider = null
benchi-kafka     | 	ssl.secure.random.implementation = null
benchi-kafka     | 	ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	ssl.truststore.certificates = null
benchi-kafka     | 	ssl.truststore.location = null
benchi-kafka     | 	ssl.truststore.password = null
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     | 	telemetry.max.bytes = 1048576
benchi-kafka     | 	throughput.quota.window.num = 11
benchi-kafka     | 	token.impersonation.validation = true
benchi-kafka     | 	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
benchi-kafka     | 	transaction.max.timeout.ms = 900000
benchi-kafka     | 	transaction.metadata.load.threads = 32
benchi-kafka     | 	transaction.partition.verification.enable = true
benchi-kafka     | 	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
benchi-kafka     | 	transaction.state.log.load.buffer.size = 5242880
benchi-kafka     | 	transaction.state.log.min.isr = 1
benchi-kafka     | 	transaction.state.log.num.partitions = 50
benchi-kafka     | 	transaction.state.log.replication.factor = 1
benchi-kafka     | 	transaction.state.log.segment.bytes = 104857600
benchi-kafka     | 	transactional.id.expiration.ms = 604800000
benchi-kafka     | 	unclean.leader.election.enable = false
benchi-kafka     | 	unstable.api.versions.enable = false
benchi-kafka     | 	unstable.feature.versions.enable = false
benchi-kafka     | 	zookeeper.acl.change.notification.expiration.ms = 900000
benchi-kafka     | 	zookeeper.clientCnxnSocket = null
benchi-kafka     | 	zookeeper.connect = 
benchi-kafka     | 	zookeeper.connection.timeout.ms = null
benchi-kafka     | 	zookeeper.max.in.flight.requests = 10
benchi-kafka     | 	zookeeper.metadata.migration.enable = false
benchi-kafka     | 	zookeeper.metadata.migration.min.batch.size = 200
benchi-kafka     | 	zookeeper.session.timeout.ms = 18000
benchi-kafka     | 	zookeeper.set.acl = false
benchi-kafka     | 	zookeeper.ssl.cipher.suites = null
benchi-kafka     | 	zookeeper.ssl.client.enable = false
benchi-kafka     | 	zookeeper.ssl.crl.enable = false
benchi-kafka     | 	zookeeper.ssl.enabled.protocols = null
benchi-kafka     | 	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
benchi-kafka     | 	zookeeper.ssl.keystore.location = null
benchi-kafka     | 	zookeeper.ssl.keystore.password = null
benchi-kafka     | 	zookeeper.ssl.keystore.type = null
benchi-kafka     | 	zookeeper.ssl.ocsp.enable = false
benchi-kafka     | 	zookeeper.ssl.protocol = TLSv1.2
benchi-kafka     | 	zookeeper.ssl.truststore.location = null
benchi-kafka     | 	zookeeper.ssl.truststore.password = null
benchi-kafka     | 	zookeeper.ssl.truststore.type = null
benchi-kafka     |  (kafka.server.KafkaConfig)
benchi-kafka     | [2025-03-18 16:03:20,445] INFO RemoteLogManagerConfig values: 
benchi-kafka     | 	log.local.retention.bytes = -2
benchi-kafka     | 	log.local.retention.ms = -2
benchi-kafka     | 	remote.fetch.max.wait.ms = 500
benchi-kafka     | 	remote.log.index.file.cache.total.size.bytes = 1073741824
benchi-kafka     | 	remote.log.manager.copier.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.copy.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.copy.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.expiration.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.fetch.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.fetch.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.task.interval.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.max.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.ms = 500
benchi-kafka     | 	remote.log.manager.task.retry.jitter = 0.2
benchi-kafka     | 	remote.log.manager.thread.pool.size = 10
benchi-kafka     | 	remote.log.metadata.custom.metadata.max.bytes = 128
benchi-kafka     | 	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
benchi-kafka     | 	remote.log.metadata.manager.class.path = null
benchi-kafka     | 	remote.log.metadata.manager.impl.prefix = rlmm.config.
benchi-kafka     | 	remote.log.metadata.manager.listener.name = null
benchi-kafka     | 	remote.log.reader.max.pending.tasks = 100
benchi-kafka     | 	remote.log.reader.threads = 10
benchi-kafka     | 	remote.log.storage.manager.class.name = null
benchi-kafka     | 	remote.log.storage.manager.class.path = null
benchi-kafka     | 	remote.log.storage.manager.impl.prefix = rsm.config.
benchi-kafka     | 	remote.log.storage.system.enable = false
benchi-kafka     |  (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
benchi-kafka     | [2025-03-18 16:03:20,455] INFO Unexpected credentials store injected: null (io.confluent.kafkarest.servlet.KafkaRestApplicationProvider)
benchi-kafka     | [2025-03-18 16:03:20,458] INFO For rest-app with listener null, configuring custom request logging (io.confluent.kafkarest.KafkaRestApplication)
benchi-kafka     | [2025-03-18 16:03:20,459] INFO EventEmitterConfig values: 
benchi-kafka     |  (io.confluent.shaded.io.confluent.telemetry.events.EventEmitterConfig)
benchi-kafka     | [2025-03-18 16:03:20,459] INFO EventEmitterConfig values: 
benchi-kafka     |  (io.confluent.shaded.io.confluent.telemetry.events.EventEmitterConfig)
benchi-kafka     | [2025-03-18 16:03:20,459] INFO ConfluentTelemetryConfig values: 
benchi-kafka     | 	confluent.telemetry.api.key = null
benchi-kafka     | 	confluent.telemetry.api.secret = null
benchi-kafka     | 	confluent.telemetry.cluster.id = null
benchi-kafka     | 	confluent.telemetry.debug.enabled = false
benchi-kafka     | 	confluent.telemetry.enabled = false
benchi-kafka     | 	confluent.telemetry.events.collector.include = .*transaction.remove.expired.transaction.cleanup.interval.ms.*|.*transaction.state.log.load.buffer.size.*|.*transaction.state.log.min.isr.*|.*transaction.state.log.num.partitions.*|.*transaction.state.log.replication.factor.*|.*transaction.state.log.segment.bytes.*|.*transactional.id.expiration.ms.*|.*controlled.shutdown.enable.*|.*controlled.shutdown.max.retries.*|.*controlled.shutdown.retry.backoff.ms.*|.*fetch.max.bytes.*|.*fetch.purgatory.purge.interval.requests.*|.*group.initial.rebalance.delay.ms.*|.*group.max.session.timeout.ms.*|.*group.max.size.*|.*group.min.session.timeout.ms.*|.*log.cleaner.backoff.ms.*|.*log.cleaner.dedupe.buffer.size.*|.*log.cleaner.delete.retention.ms.*|.*log.cleaner.enable.*|.*log.cleaner.io.buffer.load.factor.*|.*log.cleaner.io.buffer.size.*|.*log.cleaner.io.max.bytes.per.second.*|.*log.cleaner.max.compaction.lag.ms.*|.*log.cleaner.min.cleanable.ratio.*|.*log.cleaner.min.compaction.lag.ms.*|.*log.cleaner.threads.*|.*log.cleanup.policy.*|.*log.index.interval.bytes.*|.*log.index.size.max.bytes.*|.*log.message.downconversion.enable.*|.*log.message.format.version.*|.*log.message.timestamp.difference.max.ms.*|.*log.message.timestamp.type.*|.*max.connection.creation.rate.*|.*max.connections.*|.*max.connections.per.ip.*|.*max.incremental.fetch.session.cache.slots.*|.*replica.fetch.backoff.ms.*|.*replica.fetch.max.bytes.*|.*replica.fetch.min.bytes.*|.*replica.fetch.response.max.bytes.*|.*replica.fetch.wait.max.ms.*|.*replica.high.watermark.checkpoint.interval.ms.*|.*replica.lag.time.max.ms.*|.*replica.selector.class.*|.*replica.socket.receive.buffer.bytes.*|.*replica.socket.timeout.ms.*|.*alter.config.policy.class.name.*|.*alter.log.dirs.replication.quota.window.num.*|.*alter.log.dirs.replication.quota.window.size.seconds.*|.*metrics.num.samples.*|.*metrics.recording.level.*|.*metrics.sample.window.ms.*|.*quota.window.num.*|.*quota.window.size.seconds.*|.*replication.quota.window.num.*|.*replication.quota.window.size.seconds.*|.*confluent.balancer.enable.*|.*confluent.balancer.throttle.bytes.per.second.*|.*confluent.balancer.heal.uneven.load.trigger.*|.*confluent.balancer.heal.broker.failure.threshold.ms.*|.*confluent.balancer.disk.max.load.*|.*confluent.balancer.exclude.topic.prefixes.*|.*confluent.balancer.exclude.topic.names.*|.*confluent.tier.local.hotset.bytes.*|.*confluent.tier.local.hotset.ms.*|.*confluent.tier.archiver.num.threads.*|.*confluent.tier.backend.*|.*confluent.tier.enable.*|.*confluent.tier.feature.*|.*confluent.tier.fetcher.num.threads.*|.*confluent.tier.max.partition.fetch.bytes.override.*|.*confluent.tier.metadata.replication.factor.*|.*confluent.operator.managed.*|.*confluent.ansible.managed.*|.*confluent.license.*
benchi-kafka     | 	confluent.telemetry.events.enable = true
benchi-kafka     | 	confluent.telemetry.metrics.collector.include = .*io.confluent.system/.*(process_cpu_load|max_file_descriptor_count|open_file_descriptor_count|system_cpu_load|system_load_average|free_physical_memory_size|total_physical_memory_size|disk_total_bytes|disk_usable_bytes|jvm/mem|jvm/gc).*
benchi-kafka     | 	confluent.telemetry.metrics.collector.interval.ms = 60000
benchi-kafka     | 	confluent.telemetry.metrics.collector.slo.enabled = false
benchi-kafka     | 	confluent.telemetry.proxy.password = null
benchi-kafka     | 	confluent.telemetry.proxy.url = null
benchi-kafka     | 	confluent.telemetry.proxy.username = null
benchi-kafka     |  (io.confluent.telemetry.ConfluentTelemetryConfig)
benchi-kafka     | [2025-03-18 16:03:20,459] INFO VolumeMetricsCollectorConfig values: 
benchi-kafka     | 	confluent.telemetry.metrics.collector.volume.update.ms = 15000
benchi-kafka     |  (io.confluent.telemetry.collector.VolumeMetricsCollector$VolumeMetricsCollectorConfig)
benchi-kafka     | [2025-03-18 16:03:20,459] INFO HttpClientConfig values: 
benchi-kafka     | 	api.key = null
benchi-kafka     | 	api.secret = null
benchi-kafka     | 	buffer.batch.duration.max.ms = null
benchi-kafka     | 	buffer.batch.items.max = null
benchi-kafka     | 	buffer.inflight.submissions.max = null
benchi-kafka     | 	buffer.pending.batches.max = null
benchi-kafka     | 	client.attempts.max = null
benchi-kafka     | 	client.base.url = https://collector.telemetry.confluent.cloud
benchi-kafka     | 	client.compression = null
benchi-kafka     | 	client.connect.timeout.ms = null
benchi-kafka     | 	client.metrics.path.override = /v1/metrics
benchi-kafka     | 	client.request.timeout.ms = null
benchi-kafka     | 	client.retry.delay.seconds = null
benchi-kafka     | 	proxy.password = null
benchi-kafka     | 	proxy.url = null
benchi-kafka     | 	proxy.username = null
benchi-kafka     | 	type = httpTelemetryClient
benchi-kafka     |  (io.confluent.telemetry.exporter.http.HttpClientConfig)
benchi-kafka     | [2025-03-18 16:03:20,459] INFO HttpExporterConfig values: 
benchi-kafka     | 	api.key = null
benchi-kafka     | 	api.secret = null
benchi-kafka     | 	buffer.batch.duration.max.ms = null
benchi-kafka     | 	buffer.batch.items.max = null
benchi-kafka     | 	buffer.inflight.submissions.max = null
benchi-kafka     | 	buffer.pending.batches.max = null
benchi-kafka     | 	client = _confluentClient
benchi-kafka     | 	client.attempts.max = null
benchi-kafka     | 	client.base.url = null
benchi-kafka     | 	client.compression = null
benchi-kafka     | 	client.connect.timeout.ms = null
benchi-kafka     | 	client.metrics.path.override = /v1/metrics
benchi-kafka     | 	client.request.timeout.ms = null
benchi-kafka     | 	client.retry.delay.seconds = null
benchi-kafka     | 	enabled = false
benchi-kafka     | 	events.enabled = true
benchi-kafka     | 	metrics.enabled = true
benchi-kafka     | 	metrics.include = null
benchi-kafka     | 	proxy.password = null
benchi-kafka     | 	proxy.url = null
benchi-kafka     | 	proxy.username = null
benchi-kafka     | 	remote.configurable = false
benchi-kafka     | 	type = http
benchi-kafka     |  (io.confluent.telemetry.exporter.http.HttpExporterConfig)
benchi-kafka     | [2025-03-18 16:03:20,459] INFO Configuring named client _confluentClient for exporter _confluent (io.confluent.telemetry.exporter.http.HttpExporterConfig)
benchi-kafka     | [2025-03-18 16:03:20,459] WARN no telemetry exporters are enabled (io.confluent.telemetry.ConfluentTelemetryConfig)
benchi-kafka     | [2025-03-18 16:03:20,460] INFO PollingRemoteConfigurationConfig values: 
benchi-kafka     | 	enabled = true
benchi-kafka     | 	refresh.interval.ms = 60000
benchi-kafka     |  (io.confluent.shaded.io.confluent.telemetry.config.remote.polling.PollingRemoteConfigurationConfig)
benchi-kafka     | [2025-03-18 16:03:20,460] WARN Ignoring redefinition of existing telemetry label kafka_rest.version (io.confluent.shaded.io.confluent.telemetry.ResourceBuilderFacade)
benchi-kafka     | [2025-03-18 16:03:20,460] INFO ConfluentTelemetryConfig values: 
benchi-kafka     | 	confluent.telemetry.api.key = null
benchi-kafka     | 	confluent.telemetry.api.secret = null
benchi-kafka     | 	confluent.telemetry.cluster.id = null
benchi-kafka     | 	confluent.telemetry.debug.enabled = false
benchi-kafka     | 	confluent.telemetry.enabled = false
benchi-kafka     | 	confluent.telemetry.events.collector.include = .*transaction.remove.expired.transaction.cleanup.interval.ms.*|.*transaction.state.log.load.buffer.size.*|.*transaction.state.log.min.isr.*|.*transaction.state.log.num.partitions.*|.*transaction.state.log.replication.factor.*|.*transaction.state.log.segment.bytes.*|.*transactional.id.expiration.ms.*|.*controlled.shutdown.enable.*|.*controlled.shutdown.max.retries.*|.*controlled.shutdown.retry.backoff.ms.*|.*fetch.max.bytes.*|.*fetch.purgatory.purge.interval.requests.*|.*group.initial.rebalance.delay.ms.*|.*group.max.session.timeout.ms.*|.*group.max.size.*|.*group.min.session.timeout.ms.*|.*log.cleaner.backoff.ms.*|.*log.cleaner.dedupe.buffer.size.*|.*log.cleaner.delete.retention.ms.*|.*log.cleaner.enable.*|.*log.cleaner.io.buffer.load.factor.*|.*log.cleaner.io.buffer.size.*|.*log.cleaner.io.max.bytes.per.second.*|.*log.cleaner.max.compaction.lag.ms.*|.*log.cleaner.min.cleanable.ratio.*|.*log.cleaner.min.compaction.lag.ms.*|.*log.cleaner.threads.*|.*log.cleanup.policy.*|.*log.index.interval.bytes.*|.*log.index.size.max.bytes.*|.*log.message.downconversion.enable.*|.*log.message.format.version.*|.*log.message.timestamp.difference.max.ms.*|.*log.message.timestamp.type.*|.*max.connection.creation.rate.*|.*max.connections.*|.*max.connections.per.ip.*|.*max.incremental.fetch.session.cache.slots.*|.*replica.fetch.backoff.ms.*|.*replica.fetch.max.bytes.*|.*replica.fetch.min.bytes.*|.*replica.fetch.response.max.bytes.*|.*replica.fetch.wait.max.ms.*|.*replica.high.watermark.checkpoint.interval.ms.*|.*replica.lag.time.max.ms.*|.*replica.selector.class.*|.*replica.socket.receive.buffer.bytes.*|.*replica.socket.timeout.ms.*|.*alter.config.policy.class.name.*|.*alter.log.dirs.replication.quota.window.num.*|.*alter.log.dirs.replication.quota.window.size.seconds.*|.*metrics.num.samples.*|.*metrics.recording.level.*|.*metrics.sample.window.ms.*|.*quota.window.num.*|.*quota.window.size.seconds.*|.*replication.quota.window.num.*|.*replication.quota.window.size.seconds.*|.*confluent.balancer.enable.*|.*confluent.balancer.throttle.bytes.per.second.*|.*confluent.balancer.heal.uneven.load.trigger.*|.*confluent.balancer.heal.broker.failure.threshold.ms.*|.*confluent.balancer.disk.max.load.*|.*confluent.balancer.exclude.topic.prefixes.*|.*confluent.balancer.exclude.topic.names.*|.*confluent.tier.local.hotset.bytes.*|.*confluent.tier.local.hotset.ms.*|.*confluent.tier.archiver.num.threads.*|.*confluent.tier.backend.*|.*confluent.tier.enable.*|.*confluent.tier.feature.*|.*confluent.tier.fetcher.num.threads.*|.*confluent.tier.max.partition.fetch.bytes.override.*|.*confluent.tier.metadata.replication.factor.*|.*confluent.operator.managed.*|.*confluent.ansible.managed.*|.*confluent.license.*
benchi-kafka     | 	confluent.telemetry.events.enable = true
benchi-kafka     | 	confluent.telemetry.metrics.collector.include = .*io.confluent.telemetry/.*.*|.*io.confluent.system/.*(process_cpu_load|max_file_descriptor_count|open_file_descriptor_count|system_cpu_load|system_load_average|free_physical_memory_size|total_physical_memory_size|disk_total_bytes|disk_usable_bytes|jvm/mem|jvm/gc).*|.*io.confluent.kafka.rest/.*(connections_active|connections_closed_rate|request_error_rate|request_latency_avg|request_latency_max|request_rate|response_size_avg|response_size_max).*
benchi-kafka     | 	confluent.telemetry.metrics.collector.interval.ms = 60000
benchi-kafka     | 	confluent.telemetry.metrics.collector.slo.enabled = false
benchi-kafka     | 	confluent.telemetry.proxy.password = null
benchi-kafka     | 	confluent.telemetry.proxy.url = null
benchi-kafka     | 	confluent.telemetry.proxy.username = null
benchi-kafka     |  (io.confluent.telemetry.ConfluentTelemetryConfig)
benchi-kafka     | [2025-03-18 16:03:20,460] INFO VolumeMetricsCollectorConfig values: 
benchi-kafka     | 	confluent.telemetry.metrics.collector.volume.update.ms = 15000
benchi-kafka     |  (io.confluent.telemetry.collector.VolumeMetricsCollector$VolumeMetricsCollectorConfig)
benchi-kafka     | [2025-03-18 16:03:20,460] INFO HttpClientConfig values: 
benchi-kafka     | 	api.key = null
benchi-kafka     | 	api.secret = null
benchi-kafka     | 	buffer.batch.duration.max.ms = null
benchi-kafka     | 	buffer.batch.items.max = null
benchi-kafka     | 	buffer.inflight.submissions.max = null
benchi-kafka     | 	buffer.pending.batches.max = null
benchi-kafka     | 	client.attempts.max = null
benchi-kafka     | 	client.base.url = https://collector.telemetry.confluent.cloud
benchi-kafka     | 	client.compression = null
benchi-kafka     | 	client.connect.timeout.ms = null
benchi-kafka     | 	client.metrics.path.override = /v1/metrics
benchi-kafka     | 	client.request.timeout.ms = null
benchi-kafka     | 	client.retry.delay.seconds = null
benchi-kafka     | 	proxy.password = null
benchi-kafka     | 	proxy.url = null
benchi-kafka     | 	proxy.username = null
benchi-kafka     | 	type = httpTelemetryClient
benchi-kafka     |  (io.confluent.telemetry.exporter.http.HttpClientConfig)
benchi-kafka     | [2025-03-18 16:03:20,460] INFO HttpExporterConfig values: 
benchi-kafka     | 	api.key = null
benchi-kafka     | 	api.secret = null
benchi-kafka     | 	buffer.batch.duration.max.ms = null
benchi-kafka     | 	buffer.batch.items.max = null
benchi-kafka     | 	buffer.inflight.submissions.max = null
benchi-kafka     | 	buffer.pending.batches.max = null
benchi-kafka     | 	client = _confluentClient
benchi-kafka     | 	client.attempts.max = null
benchi-kafka     | 	client.base.url = null
benchi-kafka     | 	client.compression = null
benchi-kafka     | 	client.connect.timeout.ms = null
benchi-kafka     | 	client.metrics.path.override = /v1/metrics
benchi-kafka     | 	client.request.timeout.ms = null
benchi-kafka     | 	client.retry.delay.seconds = null
benchi-kafka     | 	enabled = false
benchi-kafka     | 	events.enabled = true
benchi-kafka     | 	metrics.enabled = true
benchi-kafka     | 	metrics.include = null
benchi-kafka     | 	proxy.password = null
benchi-kafka     | 	proxy.url = null
benchi-kafka     | 	proxy.username = null
benchi-kafka     | 	remote.configurable = false
benchi-kafka     | 	type = http
benchi-kafka     |  (io.confluent.telemetry.exporter.http.HttpExporterConfig)
benchi-kafka     | [2025-03-18 16:03:20,460] INFO Configuring named client _confluentClient for exporter _confluent (io.confluent.telemetry.exporter.http.HttpExporterConfig)
benchi-kafka     | [2025-03-18 16:03:20,460] WARN no telemetry exporters are enabled (io.confluent.telemetry.ConfluentTelemetryConfig)
benchi-kafka     | [2025-03-18 16:03:20,461] INFO PollingRemoteConfigurationConfig values: 
benchi-kafka     | 	enabled = true
benchi-kafka     | 	refresh.interval.ms = 60000
benchi-kafka     |  (io.confluent.shaded.io.confluent.telemetry.config.remote.polling.PollingRemoteConfigurationConfig)
benchi-kafka     | [2025-03-18 16:03:20,461] INFO Initializing the event logger (io.confluent.telemetry.reporter.TelemetryReporter)
benchi-kafka     | [2025-03-18 16:03:20,461] INFO EventLoggerConfig values: 
benchi-kafka     | 	event.logger.cloudevent.codec = structured
benchi-kafka     | 	event.logger.exporter.class = class io.confluent.shaded.io.confluent.telemetry.events.exporter.http.EventHttpExporter
benchi-kafka     |  (io.confluent.shaded.io.confluent.telemetry.events.EventLoggerConfig)
benchi-kafka     | [2025-03-18 16:03:20,461] INFO HttpExporterConfig values: 
benchi-kafka     | 	api.key = null
benchi-kafka     | 	api.secret = null
benchi-kafka     | 	buffer.batch.duration.max.ms = null
benchi-kafka     | 	buffer.batch.items.max = null
benchi-kafka     | 	buffer.inflight.submissions.max = null
benchi-kafka     | 	buffer.pending.batches.max = null
benchi-kafka     | 	client.attempts.max = null
benchi-kafka     | 	client.base.url = https://collector.telemetry.confluent.cloud
benchi-kafka     | 	client.compression = null
benchi-kafka     | 	client.connect.timeout.ms = null
benchi-kafka     | 	client.request.timeout.ms = null
benchi-kafka     | 	client.retry.delay.seconds = null
benchi-kafka     | 	enabled = true
benchi-kafka     | 	events.enabled = true
benchi-kafka     | 	metrics.enabled = true
benchi-kafka     | 	proxy.password = null
benchi-kafka     | 	proxy.url = null
benchi-kafka     | 	proxy.username = null
benchi-kafka     | 	type = http
benchi-kafka     |  (io.confluent.shaded.io.confluent.telemetry.events.exporter.http.HttpExporterConfig)
benchi-kafka     | [2025-03-18 16:03:20,464] INFO Starting Confluent telemetry reporter with an interval of 60000 ms) (io.confluent.telemetry.reporter.TelemetryReporter)
benchi-kafka     | [2025-03-18 16:03:20,465] INFO Application provider 'KafkaRestApplicationProvider' provided 1 instance(s). (io.confluent.http.server.KafkaHttpApplicationLoader)
benchi-kafka     | [2025-03-18 16:03:20,468] INFO Initial capacity 128, increased by 64, maximum capacity 2147483647. (io.confluent.rest.ApplicationServer)
benchi-kafka     | [2025-03-18 16:03:20,492] INFO [ControllerServer id=1] CreateTopics result(s): CreatableTopic(name='_confluent-link-metadata', numPartitions=50, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact'), CreateableTopicConfig(name='min.insync.replicas', value='2')], linkName=null, mirrorTopic=null, sourceTopicId=AAAAAAAAAAAAAAAAAAAAAA, mirrorStartOffsetSpec=-9223372036854775808, mirrorStartOffsets=[], stoppedSequenceNumber=0): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:03:20,492] INFO [ControllerServer id=1] Replayed TopicRecord for topic _confluent-link-metadata with topic ID gBYNT1z0TfW3wKSf7yKxpA. (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:03:20,492] INFO [ControllerServer id=1] Replayed ConfigRecord for ConfigResource(type=TOPIC, name='_confluent-link-metadata') which set configuration cleanup.policy to compact (org.apache.kafka.controller.ConfigurationControlManager)
benchi-kafka     | [2025-03-18 16:03:20,492] INFO [ControllerServer id=1] Replayed ConfigRecord for ConfigResource(type=TOPIC, name='_confluent-link-metadata') which set configuration min.insync.replicas to 2 (org.apache.kafka.controller.ConfigurationControlManager)
benchi-kafka     | [2025-03-18 16:03:20,493] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-0 with topic ID gBYNT1z0TfW3wKSf7yKxpA and PartitionRegistration(replicas=[1], observers=[], directories=[xbl_6JROMTfs6aQ8gB3fjw], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:03:20,494] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-1 with topic ID gBYNT1z0TfW3wKSf7yKxpA and PartitionRegistration(replicas=[1], observers=[], directories=[xbl_6JROMTfs6aQ8gB3fjw], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:03:20,494] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-2 with topic ID gBYNT1z0TfW3wKSf7yKxpA and PartitionRegistration(replicas=[1], observers=[], directories=[xbl_6JROMTfs6aQ8gB3fjw], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:03:20,494] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-3 with topic ID gBYNT1z0TfW3wKSf7yKxpA and PartitionRegistration(replicas=[1], observers=[], directories=[xbl_6JROMTfs6aQ8gB3fjw], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:03:20,494] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-4 with topic ID gBYNT1z0TfW3wKSf7yKxpA and PartitionRegistration(replicas=[1], observers=[], directories=[xbl_6JROMTfs6aQ8gB3fjw], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:03:20,495] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-5 with topic ID gBYNT1z0TfW3wKSf7yKxpA and PartitionRegistration(replicas=[1], observers=[], directories=[xbl_6JROMTfs6aQ8gB3fjw], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:03:20,495] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-6 with topic ID gBYNT1z0TfW3wKSf7yKxpA and PartitionRegistration(replicas=[1], observers=[], directories=[xbl_6JROMTfs6aQ8gB3fjw], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:03:20,495] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-7 with topic ID gBYNT1z0TfW3wKSf7yKxpA and PartitionRegistration(replicas=[1], observers=[], directories=[xbl_6JROMTfs6aQ8gB3fjw], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:03:20,495] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-8 with topic ID gBYNT1z0TfW3wKSf7yKxpA and PartitionRegistration(replicas=[1], observers=[], directories=[xbl_6JROMTfs6aQ8gB3fjw], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:03:20,495] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-9 with topic ID gBYNT1z0TfW3wKSf7yKxpA and PartitionRegistration(replicas=[1], observers=[], directories=[xbl_6JROMTfs6aQ8gB3fjw], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:03:20,495] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-10 with topic ID gBYNT1z0TfW3wKSf7yKxpA and PartitionRegistration(replicas=[1], observers=[], directories=[xbl_6JROMTfs6aQ8gB3fjw], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:03:20,495] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-11 with topic ID gBYNT1z0TfW3wKSf7yKxpA and PartitionRegistration(replicas=[1], observers=[], directories=[xbl_6JROMTfs6aQ8gB3fjw], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:03:20,495] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-12 with topic ID gBYNT1z0TfW3wKSf7yKxpA and PartitionRegistration(replicas=[1], observers=[], directories=[xbl_6JROMTfs6aQ8gB3fjw], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:03:20,495] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-13 with topic ID gBYNT1z0TfW3wKSf7yKxpA and PartitionRegistration(replicas=[1], observers=[], directories=[xbl_6JROMTfs6aQ8gB3fjw], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:03:20,495] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-14 with topic ID gBYNT1z0TfW3wKSf7yKxpA and PartitionRegistration(replicas=[1], observers=[], directories=[xbl_6JROMTfs6aQ8gB3fjw], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:03:20,495] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-15 with topic ID gBYNT1z0TfW3wKSf7yKxpA and PartitionRegistration(replicas=[1], observers=[], directories=[xbl_6JROMTfs6aQ8gB3fjw], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:03:20,496] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-16 with topic ID gBYNT1z0TfW3wKSf7yKxpA and PartitionRegistration(replicas=[1], observers=[], directories=[xbl_6JROMTfs6aQ8gB3fjw], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:03:20,496] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-17 with topic ID gBYNT1z0TfW3wKSf7yKxpA and PartitionRegistration(replicas=[1], observers=[], directories=[xbl_6JROMTfs6aQ8gB3fjw], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:03:20,496] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-18 with topic ID gBYNT1z0TfW3wKSf7yKxpA and PartitionRegistration(replicas=[1], observers=[], directories=[xbl_6JROMTfs6aQ8gB3fjw], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:03:20,496] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-19 with topic ID gBYNT1z0TfW3wKSf7yKxpA and PartitionRegistration(replicas=[1], observers=[], directories=[xbl_6JROMTfs6aQ8gB3fjw], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:03:20,496] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-20 with topic ID gBYNT1z0TfW3wKSf7yKxpA and PartitionRegistration(replicas=[1], observers=[], directories=[xbl_6JROMTfs6aQ8gB3fjw], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:03:20,496] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-21 with topic ID gBYNT1z0TfW3wKSf7yKxpA and PartitionRegistration(replicas=[1], observers=[], directories=[xbl_6JROMTfs6aQ8gB3fjw], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:03:20,496] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-22 with topic ID gBYNT1z0TfW3wKSf7yKxpA and PartitionRegistration(replicas=[1], observers=[], directories=[xbl_6JROMTfs6aQ8gB3fjw], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:03:20,496] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-23 with topic ID gBYNT1z0TfW3wKSf7yKxpA and PartitionRegistration(replicas=[1], observers=[], directories=[xbl_6JROMTfs6aQ8gB3fjw], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:03:20,496] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-24 with topic ID gBYNT1z0TfW3wKSf7yKxpA and PartitionRegistration(replicas=[1], observers=[], directories=[xbl_6JROMTfs6aQ8gB3fjw], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:03:20,496] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-25 with topic ID gBYNT1z0TfW3wKSf7yKxpA and PartitionRegistration(replicas=[1], observers=[], directories=[xbl_6JROMTfs6aQ8gB3fjw], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:03:20,496] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-26 with topic ID gBYNT1z0TfW3wKSf7yKxpA and PartitionRegistration(replicas=[1], observers=[], directories=[xbl_6JROMTfs6aQ8gB3fjw], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:03:20,497] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-27 with topic ID gBYNT1z0TfW3wKSf7yKxpA and PartitionRegistration(replicas=[1], observers=[], directories=[xbl_6JROMTfs6aQ8gB3fjw], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:03:20,497] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-28 with topic ID gBYNT1z0TfW3wKSf7yKxpA and PartitionRegistration(replicas=[1], observers=[], directories=[xbl_6JROMTfs6aQ8gB3fjw], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:03:20,497] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-29 with topic ID gBYNT1z0TfW3wKSf7yKxpA and PartitionRegistration(replicas=[1], observers=[], directories=[xbl_6JROMTfs6aQ8gB3fjw], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:03:20,497] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-30 with topic ID gBYNT1z0TfW3wKSf7yKxpA and PartitionRegistration(replicas=[1], observers=[], directories=[xbl_6JROMTfs6aQ8gB3fjw], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:03:20,497] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-31 with topic ID gBYNT1z0TfW3wKSf7yKxpA and PartitionRegistration(replicas=[1], observers=[], directories=[xbl_6JROMTfs6aQ8gB3fjw], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:03:20,497] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-32 with topic ID gBYNT1z0TfW3wKSf7yKxpA and PartitionRegistration(replicas=[1], observers=[], directories=[xbl_6JROMTfs6aQ8gB3fjw], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:03:20,497] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-33 with topic ID gBYNT1z0TfW3wKSf7yKxpA and PartitionRegistration(replicas=[1], observers=[], directories=[xbl_6JROMTfs6aQ8gB3fjw], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:03:20,497] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-34 with topic ID gBYNT1z0TfW3wKSf7yKxpA and PartitionRegistration(replicas=[1], observers=[], directories=[xbl_6JROMTfs6aQ8gB3fjw], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:03:20,497] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-35 with topic ID gBYNT1z0TfW3wKSf7yKxpA and PartitionRegistration(replicas=[1], observers=[], directories=[xbl_6JROMTfs6aQ8gB3fjw], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:03:20,498] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-36 with topic ID gBYNT1z0TfW3wKSf7yKxpA and PartitionRegistration(replicas=[1], observers=[], directories=[xbl_6JROMTfs6aQ8gB3fjw], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:03:20,498] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-37 with topic ID gBYNT1z0TfW3wKSf7yKxpA and PartitionRegistration(replicas=[1], observers=[], directories=[xbl_6JROMTfs6aQ8gB3fjw], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:03:20,498] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-38 with topic ID gBYNT1z0TfW3wKSf7yKxpA and PartitionRegistration(replicas=[1], observers=[], directories=[xbl_6JROMTfs6aQ8gB3fjw], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:03:20,498] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-39 with topic ID gBYNT1z0TfW3wKSf7yKxpA and PartitionRegistration(replicas=[1], observers=[], directories=[xbl_6JROMTfs6aQ8gB3fjw], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:03:20,498] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-40 with topic ID gBYNT1z0TfW3wKSf7yKxpA and PartitionRegistration(replicas=[1], observers=[], directories=[xbl_6JROMTfs6aQ8gB3fjw], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:03:20,498] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-41 with topic ID gBYNT1z0TfW3wKSf7yKxpA and PartitionRegistration(replicas=[1], observers=[], directories=[xbl_6JROMTfs6aQ8gB3fjw], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:03:20,498] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-42 with topic ID gBYNT1z0TfW3wKSf7yKxpA and PartitionRegistration(replicas=[1], observers=[], directories=[xbl_6JROMTfs6aQ8gB3fjw], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:03:20,498] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-43 with topic ID gBYNT1z0TfW3wKSf7yKxpA and PartitionRegistration(replicas=[1], observers=[], directories=[xbl_6JROMTfs6aQ8gB3fjw], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:03:20,498] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-44 with topic ID gBYNT1z0TfW3wKSf7yKxpA and PartitionRegistration(replicas=[1], observers=[], directories=[xbl_6JROMTfs6aQ8gB3fjw], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:03:20,498] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-45 with topic ID gBYNT1z0TfW3wKSf7yKxpA and PartitionRegistration(replicas=[1], observers=[], directories=[xbl_6JROMTfs6aQ8gB3fjw], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:03:20,499] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-46 with topic ID gBYNT1z0TfW3wKSf7yKxpA and PartitionRegistration(replicas=[1], observers=[], directories=[xbl_6JROMTfs6aQ8gB3fjw], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:03:20,499] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-47 with topic ID gBYNT1z0TfW3wKSf7yKxpA and PartitionRegistration(replicas=[1], observers=[], directories=[xbl_6JROMTfs6aQ8gB3fjw], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:03:20,499] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-48 with topic ID gBYNT1z0TfW3wKSf7yKxpA and PartitionRegistration(replicas=[1], observers=[], directories=[xbl_6JROMTfs6aQ8gB3fjw], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:03:20,499] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-49 with topic ID gBYNT1z0TfW3wKSf7yKxpA and PartitionRegistration(replicas=[1], observers=[], directories=[xbl_6JROMTfs6aQ8gB3fjw], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:03:20,504] INFO SBC Event SbcMetadataUpdateEvent-11 generated 1 more events to enqueue in the following order - [SbcConfigUpdateEvent-12]. Enqueuing... (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:03:20,504] INFO Handling event SbcKraftStartupEvent-5 (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:03:20,504] INFO Balancer Status state for brokers [1] transitioned from BALANCER_EVENT_RECEIVED to STARTING due to event INITIALIZING_CRUISE_CONTROL. (io.confluent.databalancer.operation.StateMachine)
benchi-kafka     | [2025-03-18 16:03:20,504] INFO DataBalancer: Activating SBC with io.confluent.databalancer.BrokersMetadataSnapshot@619782b2 (io.confluent.databalancer.KafkaDataBalanceManager)
benchi-kafka     | [2025-03-18 16:03:20,505] INFO [Broker id=1] Transitioning 50 partition(s) to local leaders. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,505] INFO DataBalancer: Scheduling DataBalanceEngine Startup (io.confluent.databalancer.ConfluentDataBalanceEngine)
benchi-kafka     | [2025-03-18 16:03:20,505] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-link-metadata-43, _confluent-link-metadata-10, _confluent-link-metadata-39, _confluent-link-metadata-6, _confluent-link-metadata-18, _confluent-link-metadata-47, _confluent-link-metadata-14, _confluent-link-metadata-27, _confluent-link-metadata-23, _confluent-link-metadata-35, _confluent-link-metadata-2, _confluent-link-metadata-31, _confluent-link-metadata-42, _confluent-link-metadata-13, _confluent-link-metadata-38, _confluent-link-metadata-9, _confluent-link-metadata-21, _confluent-link-metadata-46, _confluent-link-metadata-17, _confluent-link-metadata-26, _confluent-link-metadata-22, _confluent-link-metadata-34, _confluent-link-metadata-5, _confluent-link-metadata-30, _confluent-link-metadata-1, _confluent-link-metadata-45, _confluent-link-metadata-12, _confluent-link-metadata-41, _confluent-link-metadata-8, _confluent-link-metadata-20, _confluent-link-metadata-49, _confluent-link-metadata-16, _confluent-link-metadata-29, _confluent-link-metadata-25, _confluent-link-metadata-37, _confluent-link-metadata-4, _confluent-link-metadata-33, _confluent-link-metadata-0, _confluent-link-metadata-11, _confluent-link-metadata-44, _confluent-link-metadata-7, _confluent-link-metadata-40, _confluent-link-metadata-19, _confluent-link-metadata-15, _confluent-link-metadata-48, _confluent-link-metadata-28, _confluent-link-metadata-24, _confluent-link-metadata-3, _confluent-link-metadata-36, _confluent-link-metadata-32) (kafka.server.ReplicaFetcherManager)
benchi-kafka     | [2025-03-18 16:03:20,506] INFO [Broker id=1] Creating new partition _confluent-link-metadata-43 with topic id gBYNT1z0TfW3wKSf7yKxpA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,508] INFO Handling event SbcKraftBrokerAdditionEvent-10 (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:03:20,509] INFO Processing SbcKraftBrokerAdditionEvent-10 event with data: empty_brokers: [], new_brokers: [1] (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:03:20,509] WARN Notified of broker additions (empty broker ids [], new brokers [1]) but DataBalancer is disabled -- ignoring for now (io.confluent.databalancer.KafkaDataBalanceManager)
benchi-kafka     | [2025-03-18 16:03:20,509] INFO Handling event SbcConfigUpdateEvent-12 (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:03:20,509] INFO Balancer notified of a config change: ConfigurationsDelta(changes={ConfigResource(type=TOPIC, name='_confluent-link-metadata')=ConfigurationDelta(changedKeys=[cleanup.policy, min.insync.replicas])}) (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:03:20,509] INFO There were 0 change(s) and 0 deletion(s) to balancer configs. Changed Configs: {}, Deleted Configs: [] (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:03:20,509] INFO DataBalancer: Bootstrap server endpoint is Endpoint(listenerName='PLAINTEXT', securityProtocol=PLAINTEXT, host='broker', port=29092) (io.confluent.databalancer.startup.CruiseControlStartable)
benchi-kafka     | [2025-03-18 16:03:20,509] INFO DataBalancer: BOOTSTRAP_SERVERS determined to be broker:29092 (io.confluent.databalancer.startup.CruiseControlStartable)
benchi-kafka     | [2025-03-18 16:03:20,510] INFO KafkaCruiseControlConfig values: 
benchi-kafka     | 	alter.configs.response.timeout.ms = 30000
benchi-kafka     | 	anomaly.detection.allow.capacity.estimation = true
benchi-kafka     | 	anomaly.detection.goals = [io.confluent.cruisecontrol.analyzer.goals.ReplicaPlacementGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.RackAwareGoal, io.confluent.cruisecontrol.analyzer.goals.CellAwareGoal, io.confluent.cruisecontrol.analyzer.goals.TenantAwareGoal, io.confluent.cruisecontrol.analyzer.goals.MaxReplicaMovementParallelismGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.ReplicaCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.DiskCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.NetworkInboundCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.NetworkOutboundCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.ReplicationInboundCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.ProducerInboundCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.ReplicaDistributionGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.DiskUsageDistributionGoal]
benchi-kafka     | 	anomaly.detection.interval.ms = 60000
benchi-kafka     | 	bootstrap.servers = [broker:29092]
benchi-kafka     | 	broker.addition.elapsed.time.ms.completion.threshold = 57600000
benchi-kafka     | 	broker.addition.mean.cpu.percent.completion.threshold = 0.5
benchi-kafka     | 	broker.capacity.config.resolver.class = class com.linkedin.kafka.cruisecontrol.config.BrokerCapacityResolver
benchi-kafka     | 	broker.failure.alert.threshold.ms = 0
benchi-kafka     | 	broker.failure.exclude.recently.removed.brokers = true
benchi-kafka     | 	broker.failure.self.healing.threshold.ms = 3600000
benchi-kafka     | 	broker.metric.sample.aggregator.completeness.cache.size = 5
benchi-kafka     | 	broker.metric.sample.store.topic = _confluent_balancer_broker_samples
benchi-kafka     | 	broker.removal.shutdown.timeout.ms = 600000
benchi-kafka     | 	broker.replica.exclusion.timeout.ms = 120000
benchi-kafka     | 	bytes.cpu.contribution.weight = 0.2
benchi-kafka     | 	calculated.throttle.ratio = 0.8
benchi-kafka     | 	capacity.threshold.upper.limit = 0.95
benchi-kafka     | 	cdbe.shutdown.wait.ms = 15000
benchi-kafka     | 	cell.load.upper.bound = 0.7
benchi-kafka     | 	cell.overload.detection.interval.ms = 3600000
benchi-kafka     | 	cell.overload.duration.ms = 86400000
benchi-kafka     | 	client.id = kafka-cruise-control
benchi-kafka     | 	confluent.balancer.additional.invalidation.duration.ms = 60000
benchi-kafka     | 	confluent.balancer.plan.computation.retry.timeout.ms = 3600000
benchi-kafka     | 	confluent.cells.enable = false
benchi-kafka     | 	connections.max.idle.ms = 540000
benchi-kafka     | 	consume.out.bound.should.balance.FFF.traffic = false
benchi-kafka     | 	consumer.out.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	consumer.outbound.capacity.threshold = 0.9
benchi-kafka     | 	cpu.balance.threshold = 1.1
benchi-kafka     | 	cpu.capacity.threshold = 1.0
benchi-kafka     | 	cpu.low.utilization.threshold = 0.2
benchi-kafka     | 	cpu.low.utilization.threshold.for.broker.addition = 0.2
benchi-kafka     | 	cpu.utilization.detector.duration.ms = 600000
benchi-kafka     | 	cpu.utilization.detector.enabled = false
benchi-kafka     | 	cpu.utilization.detector.overutilization.threshold = 80.0
benchi-kafka     | 	cpu.utilization.detector.underutilization.threshold = 50.0
benchi-kafka     | 	default.api.timeout.ms = 60000
benchi-kafka     | 	default.replica.movement.strategies = [com.linkedin.kafka.cruisecontrol.executor.strategy.BaseReplicaMovementStrategy]
benchi-kafka     | 	describe.broker.exclusion.timeout.ms = 60000
benchi-kafka     | 	describe.cluster.response.timeout.ms = 30000
benchi-kafka     | 	describe.configs.batch.size = 1000
benchi-kafka     | 	describe.configs.response.timeout.ms = 30000
benchi-kafka     | 	describe.topics.response.timeout.ms = 30000
benchi-kafka     | 	disk.balance.threshold = 1.1
benchi-kafka     | 	disk.low.utilization.threshold = 0.2
benchi-kafka     | 	disk.max.load = 0.85
benchi-kafka     | 	disk.min.free.space.gb = 0
benchi-kafka     | 	disk.min.free.space.lower.limit.gb = 0
benchi-kafka     | 	disk.read.ratio = 0.2
benchi-kafka     | 	disk.utilization.detector.duration.ms = 600000
benchi-kafka     | 	disk.utilization.detector.enabled = false
benchi-kafka     | 	disk.utilization.detector.overutilization.threshold = 80.0
benchi-kafka     | 	disk.utilization.detector.reserved.capacity = 150000.0
benchi-kafka     | 	disk.utilization.detector.underutilization.threshold = 35.0
benchi-kafka     | 	dynamic.throttling.enabled = true
benchi-kafka     | 	execution.progress.check.interval.ms = 7000
benchi-kafka     | 	executor.leader.action.timeout.ms = 180000
benchi-kafka     | 	executor.notifier.class = class com.linkedin.kafka.cruisecontrol.executor.ExecutorNoopNotifier
benchi-kafka     | 	executor.reservation.refresh.time.ms = 60000
benchi-kafka     | 	follower.network.inbound.weight.for.cpu.util = 0.15
benchi-kafka     | 	goal.balancedness.priority.weight = 1.1
benchi-kafka     | 	goal.balancedness.strictness.weight = 1.5
benchi-kafka     | 	goal.violation.delay.on.new.brokers.ms = 1800000
benchi-kafka     | 	goal.violation.distribution.threshold.multiplier = 1.1
benchi-kafka     | 	goal.violation.exclude.recently.removed.brokers = true
benchi-kafka     | 	goals = [com.linkedin.kafka.cruisecontrol.analyzer.goals.MovementExclusionGoal, io.confluent.cruisecontrol.analyzer.goals.ReplicaPlacementGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.RackAwareGoal, io.confluent.cruisecontrol.analyzer.goals.CellAwareGoal, io.confluent.cruisecontrol.analyzer.goals.TenantAwareGoal, io.confluent.cruisecontrol.analyzer.goals.MaxReplicaMovementParallelismGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.ReplicaCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.DiskCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.NetworkInboundCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.NetworkOutboundCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.ReplicationInboundCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.ProducerInboundCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.MirrorInboundCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.ConsumerOutboundCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.SystemTopicEvenDistributionGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.ReplicaDistributionGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.DiskUsageDistributionGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.LeaderReplicaDistributionGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.NetworkInboundUsageDistributionGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.NetworkOutboundUsageDistributionGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.TopicReplicaDistributionGoal]
benchi-kafka     | 	hot.partition.capacity.utilization.threshold = 0.2
benchi-kafka     | 	incremental.balancing.cpu.top.proposal.tracking.enabled = true
benchi-kafka     | 	incremental.balancing.cpu.top.proposal.tracking.num.proposals = 15
benchi-kafka     | 	incremental.balancing.enabled = false
benchi-kafka     | 	incremental.balancing.goals = [com.linkedin.kafka.cruisecontrol.analyzer.goals.MovementExclusionGoal, io.confluent.cruisecontrol.analyzer.goals.ReplicaPlacementGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.RackAwareGoal, io.confluent.cruisecontrol.analyzer.goals.CellAwareGoal, io.confluent.cruisecontrol.analyzer.goals.TenantAwareGoal, io.confluent.cruisecontrol.analyzer.goals.MaxReplicaMovementParallelismGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.ReplicaCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.DiskCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.NetworkInboundCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.NetworkOutboundCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.ReplicationInboundCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.ProducerInboundCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.MirrorInboundCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.ConsumerOutboundCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.IncrementalCPUResourceDistributionGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.IncrementalTopicReplicaDistributionGoal]
benchi-kafka     | 	incremental.balancing.lower.bound = 0.02
benchi-kafka     | 	incremental.balancing.min.valid.windows = 5
benchi-kafka     | 	incremental.balancing.step.ratio = 0.2
benchi-kafka     | 	inter.cell.balancing.enabled = false
benchi-kafka     | 	invalid.replica.assignment.retry.timeout.ms = 300000
benchi-kafka     | 	leader.network.inbound.weight.for.cpu.util = 0.7
benchi-kafka     | 	leader.network.outbound.weight.for.cpu.util = 0.15
benchi-kafka     | 	leader.replica.count.balance.threshold = 1.1
benchi-kafka     | 	logdir.response.timeout.ms = 30000
benchi-kafka     | 	max.allowed.extrapolations.per.broker = 5
benchi-kafka     | 	max.allowed.extrapolations.per.partition = 5
benchi-kafka     | 	max.capacity.balancing.delta.percentage = 0.0
benchi-kafka     | 	max.replicas = 2147483647
benchi-kafka     | 	max.volume.throughput.mb = 0
benchi-kafka     | 	metadata.client.timeout.ms = 180000
benchi-kafka     | 	metadata.ttl = 10000
benchi-kafka     | 	metric.sampler.class = class io.confluent.cruisecontrol.metricsreporter.ConfluentTelemetryReporterSampler
benchi-kafka     | 	min.samples.per.partition.metrics.window = 1
benchi-kafka     | 	min.valid.partition.ratio = 0.95
benchi-kafka     | 	network.in.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	network.inbound.balance.threshold = 1.1
benchi-kafka     | 	network.inbound.capacity.threshold = 0.8
benchi-kafka     | 	network.inbound.low.utilization.threshold = 0.2
benchi-kafka     | 	network.out.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	network.outbound.balance.threshold = 1.1
benchi-kafka     | 	network.outbound.capacity.threshold = 0.8
benchi-kafka     | 	network.outbound.low.utilization.threshold = 0.2
benchi-kafka     | 	num.cached.recent.anomaly.states = 10
benchi-kafka     | 	num.concurrent.leader.movements = 1000
benchi-kafka     | 	num.concurrent.partition.movements.per.broker = 5
benchi-kafka     | 	num.metric.fetchers = 1
benchi-kafka     | 	num.partition.metrics.windows = 12
benchi-kafka     | 	partition.metric.sample.aggregator.completeness.cache.size = 5
benchi-kafka     | 	partition.metric.sample.store.topic = _confluent_balancer_partition_samples
benchi-kafka     | 	partition.metrics.window.ms = 180000
benchi-kafka     | 	populate.default.disk.capacity.from.local = true
benchi-kafka     | 	producer.in.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	producer.inbound.capacity.threshold = 0.9
benchi-kafka     | 	read.throughput.multiplier = 1.0
benchi-kafka     | 	receive.buffer.bytes = 32768
benchi-kafka     | 	reconnect.backoff.ms = 50
benchi-kafka     | 	removal.history.retention.time.ms = 86400000
benchi-kafka     | 	replica.count.balance.threshold = 1.1
benchi-kafka     | 	replica.movement.strategies = [com.linkedin.kafka.cruisecontrol.executor.strategy.PostponeUrpReplicaMovementStrategy, com.linkedin.kafka.cruisecontrol.executor.strategy.PrioritizeLargeReplicaMovementStrategy, com.linkedin.kafka.cruisecontrol.executor.strategy.PrioritizeSmallReplicaMovementStrategy, com.linkedin.kafka.cruisecontrol.executor.strategy.BaseReplicaMovementStrategy]
benchi-kafka     | 	replication.in.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	replication.inbound.capacity.threshold = 0.9
benchi-kafka     | 	request.cpu.contribution.weight = 0.8
benchi-kafka     | 	request.timeout.ms = 30000
benchi-kafka     | 	resource.utilization.detector.interval.ms = 60000
benchi-kafka     | 	sampling.allow.cpu.capacity.estimation = true
benchi-kafka     | 	sasl.client.callback.handler.class = null
benchi-kafka     | 	sasl.jaas.config = null
benchi-kafka     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
benchi-kafka     | 	sasl.kerberos.min.time.before.relogin = 60000
benchi-kafka     | 	sasl.kerberos.service.name = null
benchi-kafka     | 	sasl.kerberos.ticket.renew.jitter = 0.05
benchi-kafka     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
benchi-kafka     | 	sasl.login.callback.handler.class = null
benchi-kafka     | 	sasl.login.class = null
benchi-kafka     | 	sasl.login.connect.timeout.ms = null
benchi-kafka     | 	sasl.login.read.timeout.ms = null
benchi-kafka     | 	sasl.login.refresh.buffer.seconds = 300
benchi-kafka     | 	sasl.login.refresh.min.period.seconds = 60
benchi-kafka     | 	sasl.login.refresh.window.factor = 0.8
benchi-kafka     | 	sasl.login.refresh.window.jitter = 0.05
benchi-kafka     | 	sasl.login.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.login.retry.backoff.ms = 100
benchi-kafka     | 	sasl.mechanism = GSSAPI
benchi-kafka     | 	sasl.oauthbearer.clientassertion.audience = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.expiration = 5
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.jti.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.nbf.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.issuer = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.location = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key.passphrase = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.subject = null
benchi-kafka     | 	sasl.oauthbearer.clock.skew.seconds = 30
benchi-kafka     | 	sasl.oauthbearer.expected.audience = null
benchi-kafka     | 	sasl.oauthbearer.expected.issuer = null
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.url = null
benchi-kafka     | 	sasl.oauthbearer.scope.claim.name = scope
benchi-kafka     | 	sasl.oauthbearer.sub.claim.name = sub
benchi-kafka     | 	sasl.oauthbearer.token.endpoint.url = null
benchi-kafka     | 	sbc.metrics.parser.enabled = false
benchi-kafka     | 	security.protocol = PLAINTEXT
benchi-kafka     | 	self.healing.broker.failure.enabled = true
benchi-kafka     | 	self.healing.goal.violation.enabled = false
benchi-kafka     | 	self.healing.maximum.rounds = 1
benchi-kafka     | 	send.buffer.bytes = 131072
benchi-kafka     | 	ssl.cipher.suites = null
benchi-kafka     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	ssl.engine.factory.class = null
benchi-kafka     | 	ssl.key.password = null
benchi-kafka     | 	ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	ssl.keystore.certificate.chain = null
benchi-kafka     | 	ssl.keystore.key = null
benchi-kafka     | 	ssl.keystore.location = null
benchi-kafka     | 	ssl.keystore.password = null
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.protocol = TLSv1.3
benchi-kafka     | 	ssl.provider = null
benchi-kafka     | 	ssl.secure.random.implementation = null
benchi-kafka     | 	ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	ssl.truststore.certificates = null
benchi-kafka     | 	ssl.truststore.location = null
benchi-kafka     | 	ssl.truststore.password = null
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     | 	startup.retry.delay.minutes = 5
benchi-kafka     | 	startup.retry.max.hours = 2
benchi-kafka     | 	static.throttle.rate.override.enabled = false
benchi-kafka     | 	tenant.maximum.movements = 0
benchi-kafka     | 	tenant.suspension.ms = 86400000
benchi-kafka     | 	throttle.bytes.per.second = 10485760
benchi-kafka     | 	topic.balancing.badly.imbalanced.topic.imbalance.score.threshold = 0.3
benchi-kafka     | 	topic.balancing.balance.threshold.multiplier = 1.0
benchi-kafka     | 	topic.balancing.broker.addition.completion.percentage = 0.8
benchi-kafka     | 	topic.balancing.broker.addition.detector.with.trdg.enabled = false
benchi-kafka     | 	topic.balancing.imbalanced.score.threshold = 0.07
benchi-kafka     | 	topic.balancing.max.reassignments.per.iteration = -1
benchi-kafka     | 	topic.balancing.slightly.imbalanced.topic.imbalance.score.threshold = 0.05
benchi-kafka     | 	topic.balancing.slightly.imbalanced.topics.percentage.trigger = 0.2
benchi-kafka     | 	topic.balancing.trigger.threshold.multiplier = 3.0
benchi-kafka     | 	topic.partition.maximum.movements = 5
benchi-kafka     | 	topic.partition.movement.expiration.ms = 10800000
benchi-kafka     | 	topic.partition.suspension.ms = 18000000
benchi-kafka     | 	topics.excluded.from.partition.movement = 
benchi-kafka     | 	v2.addition.enabled = false
benchi-kafka     | 	write.throughput.multiplier = 1.0
benchi-kafka     | 	zookeeper.connect = 
benchi-kafka     | 	zookeeper.security.enabled = false
benchi-kafka     |  (com.linkedin.kafka.cruisecontrol.config.KafkaCruiseControlConfig)
benchi-kafka     | [2025-03-18 16:03:20,510] INFO DataBalancer: Instantiating DataBalanceEngine (io.confluent.databalancer.ConfluentDataBalanceEngine)
benchi-kafka     | [2025-03-18 16:03:20,514] INFO DataBalancer: Checking startup components (io.confluent.databalancer.startup.CruiseControlStartable)
benchi-kafka     | [2025-03-18 16:03:20,514] INFO DataBalancer: Checking startup component StartupComponent ConfluentTelemetryReporterSampler (io.confluent.databalancer.startup.StartupComponents)
benchi-kafka     | [2025-03-18 16:03:20,515] INFO [Broker id=1] Creating new partition _confluent-link-metadata-10 with topic id gBYNT1z0TfW3wKSf7yKxpA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,516] INFO [Broker id=1] Creating new partition _confluent-link-metadata-39 with topic id gBYNT1z0TfW3wKSf7yKxpA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,516] INFO [Broker id=1] Creating new partition _confluent-link-metadata-6 with topic id gBYNT1z0TfW3wKSf7yKxpA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,517] INFO [Broker id=1] Creating new partition _confluent-link-metadata-18 with topic id gBYNT1z0TfW3wKSf7yKxpA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,517] INFO [Broker id=1] Creating new partition _confluent-link-metadata-47 with topic id gBYNT1z0TfW3wKSf7yKxpA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,518] INFO [Broker id=1] Creating new partition _confluent-link-metadata-14 with topic id gBYNT1z0TfW3wKSf7yKxpA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,519] INFO [Broker id=1] Creating new partition _confluent-link-metadata-27 with topic id gBYNT1z0TfW3wKSf7yKxpA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,519] INFO [Broker id=1] Creating new partition _confluent-link-metadata-23 with topic id gBYNT1z0TfW3wKSf7yKxpA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,519] INFO [Broker id=1] Creating new partition _confluent-link-metadata-35 with topic id gBYNT1z0TfW3wKSf7yKxpA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,519] INFO [Broker id=1] Creating new partition _confluent-link-metadata-2 with topic id gBYNT1z0TfW3wKSf7yKxpA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,520] INFO [Broker id=1] Creating new partition _confluent-link-metadata-31 with topic id gBYNT1z0TfW3wKSf7yKxpA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,520] INFO [Broker id=1] Creating new partition _confluent-link-metadata-42 with topic id gBYNT1z0TfW3wKSf7yKxpA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,520] INFO [Broker id=1] Creating new partition _confluent-link-metadata-13 with topic id gBYNT1z0TfW3wKSf7yKxpA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,521] WARN Disabling exponential reconnect backoff because reconnect.backoff.ms is set, but reconnect.backoff.max.ms is not. (org.apache.kafka.clients.CommonClientConfigs)
benchi-kafka     | [2025-03-18 16:03:20,521] INFO ConsumerConfig values: 
benchi-kafka     | 	allow.auto.create.topics = true
benchi-kafka     | 	auto.commit.interval.ms = 5000
benchi-kafka     | 	auto.include.jmx.reporter = true
benchi-kafka     | 	auto.offset.reset = latest
benchi-kafka     | 	bootstrap.servers = [broker:29092]
benchi-kafka     | 	check.crcs = true
benchi-kafka     | 	client.dns.lookup = use_all_dns_ips
benchi-kafka     | 	client.id = kafka-cruise-control
benchi-kafka     | 	client.rack = 
benchi-kafka     | 	confluent.lkc.id = null
benchi-kafka     | 	confluent.proxy.protocol.client.address = null
benchi-kafka     | 	confluent.proxy.protocol.client.mode = PROXY
benchi-kafka     | 	confluent.proxy.protocol.client.port = null
benchi-kafka     | 	confluent.proxy.protocol.client.version = NONE
benchi-kafka     | 	connections.max.idle.ms = 540000
benchi-kafka     | 	default.api.timeout.ms = 60000
benchi-kafka     | 	enable.auto.commit = false
benchi-kafka     | 	enable.metrics.push = true
benchi-kafka     | 	exclude.internal.topics = true
benchi-kafka     | 	fetch.max.bytes = 52428800
benchi-kafka     | 	fetch.max.wait.ms = 500
benchi-kafka     | 	fetch.min.bytes = 1
benchi-kafka     | 	group.id = ConfluentTelemetryReporterSampler--2083203430002767771
benchi-kafka     | 	group.instance.id = null
benchi-kafka     | 	group.protocol = classic
benchi-kafka     | 	group.remote.assignor = null
benchi-kafka     | 	heartbeat.interval.ms = 3000
benchi-kafka     | 	interceptor.classes = []
benchi-kafka     | 	internal.leave.group.on.close = true
benchi-kafka     | 	internal.throw.on.fetch.stable.offset.unsupported = false
benchi-kafka     | 	isolation.level = read_uncommitted
benchi-kafka     | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
benchi-kafka     | 	max.partition.fetch.bytes = 1048576
benchi-kafka     | 	max.poll.interval.ms = 2147483647
benchi-kafka     | 	max.poll.records = 2147483647
benchi-kafka     | 	metadata.max.age.ms = 300000
benchi-kafka     | 	metadata.recovery.strategy = none
benchi-kafka     | 	metric.reporters = []
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.recording.level = INFO
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
benchi-kafka     | 	receive.buffer.bytes = 32768
benchi-kafka     | 	reconnect.backoff.max.ms = 50
benchi-kafka     | 	reconnect.backoff.ms = 50
benchi-kafka     | 	request.timeout.ms = 30000
benchi-kafka     | 	retry.backoff.max.ms = 1000
benchi-kafka     | 	retry.backoff.ms = 100
benchi-kafka     | 	sasl.client.callback.handler.class = null
benchi-kafka     | 	sasl.jaas.config = null
benchi-kafka     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
benchi-kafka     | 	sasl.kerberos.min.time.before.relogin = 60000
benchi-kafka     | 	sasl.kerberos.service.name = null
benchi-kafka     | 	sasl.kerberos.ticket.renew.jitter = 0.05
benchi-kafka     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
benchi-kafka     | 	sasl.login.callback.handler.class = null
benchi-kafka     | 	sasl.login.class = null
benchi-kafka     | 	sasl.login.connect.timeout.ms = null
benchi-kafka     | 	sasl.login.read.timeout.ms = null
benchi-kafka     | 	sasl.login.refresh.buffer.seconds = 300
benchi-kafka     | 	sasl.login.refresh.min.period.seconds = 60
benchi-kafka     | 	sasl.login.refresh.window.factor = 0.8
benchi-kafka     | 	sasl.login.refresh.window.jitter = 0.05
benchi-kafka     | 	sasl.login.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.login.retry.backoff.ms = 100
benchi-kafka     | 	sasl.mechanism = GSSAPI
benchi-kafka     | 	sasl.oauthbearer.clientassertion.audience = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.expiration = 5
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.jti.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.nbf.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.issuer = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.location = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key.passphrase = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.subject = null
benchi-kafka     | 	sasl.oauthbearer.clock.skew.seconds = 30
benchi-kafka     | 	sasl.oauthbearer.expected.audience = null
benchi-kafka     | 	sasl.oauthbearer.expected.issuer = null
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.url = null
benchi-kafka     | 	sasl.oauthbearer.scope.claim.name = scope
benchi-kafka     | 	sasl.oauthbearer.sub.claim.name = sub
benchi-kafka     | 	sasl.oauthbearer.token.endpoint.url = null
benchi-kafka     | 	security.protocol = PLAINTEXT
benchi-kafka     | 	security.providers = null
benchi-kafka     | 	send.buffer.bytes = 131072
benchi-kafka     | 	session.timeout.ms = 45000
benchi-kafka     | 	socket.connection.setup.timeout.max.ms = 30000
benchi-kafka     | 	socket.connection.setup.timeout.ms = 10000
benchi-kafka     | 	ssl.cipher.suites = null
benchi-kafka     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	ssl.engine.factory.class = null
benchi-kafka     | 	ssl.key.password = null
benchi-kafka     | 	ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	ssl.keystore.certificate.chain = null
benchi-kafka     | 	ssl.keystore.key = null
benchi-kafka     | 	ssl.keystore.location = null
benchi-kafka     | 	ssl.keystore.password = null
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.protocol = TLSv1.3
benchi-kafka     | 	ssl.provider = null
benchi-kafka     | 	ssl.secure.random.implementation = null
benchi-kafka     | 	ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	ssl.truststore.certificates = null
benchi-kafka     | 	ssl.truststore.location = null
benchi-kafka     | 	ssl.truststore.password = null
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
benchi-kafka     |  (org.apache.kafka.clients.consumer.ConsumerConfig)
benchi-kafka     | [2025-03-18 16:03:20,521] INFO [Broker id=1] Creating new partition _confluent-link-metadata-38 with topic id gBYNT1z0TfW3wKSf7yKxpA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,521] INFO [Broker id=1] Creating new partition _confluent-link-metadata-9 with topic id gBYNT1z0TfW3wKSf7yKxpA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,522] INFO [Broker id=1] Creating new partition _confluent-link-metadata-21 with topic id gBYNT1z0TfW3wKSf7yKxpA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,522] INFO [Broker id=1] Creating new partition _confluent-link-metadata-46 with topic id gBYNT1z0TfW3wKSf7yKxpA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,522] INFO [Broker id=1] Creating new partition _confluent-link-metadata-17 with topic id gBYNT1z0TfW3wKSf7yKxpA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,522] INFO [Broker id=1] Creating new partition _confluent-link-metadata-26 with topic id gBYNT1z0TfW3wKSf7yKxpA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,523] INFO [Broker id=1] Creating new partition _confluent-link-metadata-22 with topic id gBYNT1z0TfW3wKSf7yKxpA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,523] INFO [Broker id=1] Creating new partition _confluent-link-metadata-34 with topic id gBYNT1z0TfW3wKSf7yKxpA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,523] INFO [Broker id=1] Creating new partition _confluent-link-metadata-5 with topic id gBYNT1z0TfW3wKSf7yKxpA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,524] INFO [Broker id=1] Creating new partition _confluent-link-metadata-30 with topic id gBYNT1z0TfW3wKSf7yKxpA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,524] INFO [Broker id=1] Creating new partition _confluent-link-metadata-1 with topic id gBYNT1z0TfW3wKSf7yKxpA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,524] INFO [Broker id=1] Creating new partition _confluent-link-metadata-45 with topic id gBYNT1z0TfW3wKSf7yKxpA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,525] INFO [Broker id=1] Creating new partition _confluent-link-metadata-12 with topic id gBYNT1z0TfW3wKSf7yKxpA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,525] INFO [Broker id=1] Creating new partition _confluent-link-metadata-41 with topic id gBYNT1z0TfW3wKSf7yKxpA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,525] INFO [Broker id=1] Creating new partition _confluent-link-metadata-8 with topic id gBYNT1z0TfW3wKSf7yKxpA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,525] INFO [Broker id=1] Creating new partition _confluent-link-metadata-20 with topic id gBYNT1z0TfW3wKSf7yKxpA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,525] INFO [Broker id=1] Creating new partition _confluent-link-metadata-49 with topic id gBYNT1z0TfW3wKSf7yKxpA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,526] INFO [Broker id=1] Creating new partition _confluent-link-metadata-16 with topic id gBYNT1z0TfW3wKSf7yKxpA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,526] INFO [Broker id=1] Creating new partition _confluent-link-metadata-29 with topic id gBYNT1z0TfW3wKSf7yKxpA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,526] INFO [Broker id=1] Creating new partition _confluent-link-metadata-25 with topic id gBYNT1z0TfW3wKSf7yKxpA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,527] INFO [Broker id=1] Creating new partition _confluent-link-metadata-37 with topic id gBYNT1z0TfW3wKSf7yKxpA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,527] INFO [Broker id=1] Creating new partition _confluent-link-metadata-4 with topic id gBYNT1z0TfW3wKSf7yKxpA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,527] INFO [Broker id=1] Creating new partition _confluent-link-metadata-33 with topic id gBYNT1z0TfW3wKSf7yKxpA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,527] INFO initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector)
benchi-kafka     | [2025-03-18 16:03:20,527] INFO [Broker id=1] Creating new partition _confluent-link-metadata-0 with topic id gBYNT1z0TfW3wKSf7yKxpA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,527] INFO [Broker id=1] Creating new partition _confluent-link-metadata-11 with topic id gBYNT1z0TfW3wKSf7yKxpA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,528] INFO [Broker id=1] Creating new partition _confluent-link-metadata-44 with topic id gBYNT1z0TfW3wKSf7yKxpA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,528] INFO [Broker id=1] Creating new partition _confluent-link-metadata-7 with topic id gBYNT1z0TfW3wKSf7yKxpA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,528] INFO [Broker id=1] Creating new partition _confluent-link-metadata-40 with topic id gBYNT1z0TfW3wKSf7yKxpA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,528] INFO [Broker id=1] Creating new partition _confluent-link-metadata-19 with topic id gBYNT1z0TfW3wKSf7yKxpA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,529] INFO [Broker id=1] Creating new partition _confluent-link-metadata-15 with topic id gBYNT1z0TfW3wKSf7yKxpA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,529] INFO RestConfig values: 
benchi-kafka     | 	access.control.allow.headers = 
benchi-kafka     | 	access.control.allow.methods = 
benchi-kafka     | 	access.control.allow.origin = 
benchi-kafka     | 	access.control.skip.options = true
benchi-kafka     | 	authentication.method = NONE
benchi-kafka     | 	authentication.realm = 
benchi-kafka     | 	authentication.roles = [*]
benchi-kafka     | 	authentication.skip.paths = []
benchi-kafka     | 	compression.enable = true
benchi-kafka     | 	connector.connection.limit = 0
benchi-kafka     | 	csrf.prevention.enable = false
benchi-kafka     | 	csrf.prevention.token.endpoint = /csrf
benchi-kafka     | 	csrf.prevention.token.expiration.minutes = 30
benchi-kafka     | 	csrf.prevention.token.max.entries = 10000
benchi-kafka     | 	debug = false
benchi-kafka     | 	dos.filter.delay.ms = 100
benchi-kafka     | 	dos.filter.enabled = false
benchi-kafka     | 	dos.filter.insert.headers = true
benchi-kafka     | 	dos.filter.ip.whitelist = []
benchi-kafka     | 	dos.filter.managed.attr = false
benchi-kafka     | 	dos.filter.max.idle.tracker.ms = 30000
benchi-kafka     | 	dos.filter.max.requests.ms = 30000
benchi-kafka     | 	dos.filter.max.requests.per.connection.per.sec = 25
benchi-kafka     | 	dos.filter.max.requests.per.sec = 25
benchi-kafka     | 	dos.filter.max.wait.ms = 50
benchi-kafka     | 	dos.filter.throttle.ms = 30000
benchi-kafka     | 	dos.filter.throttled.requests = 5
benchi-kafka     | 	http2.enabled = true
benchi-kafka     | 	idle.timeout.ms = 30000
benchi-kafka     | 	listener.protocol.map = []
benchi-kafka     | 	listeners = []
benchi-kafka     | 	max.request.header.size = 8192
benchi-kafka     | 	max.response.header.size = 8192
benchi-kafka     | 	metric.reporters = []
benchi-kafka     | 	metrics.global.stats.request.tags.enable = false
benchi-kafka     | 	metrics.jmx.prefix = rest-utils
benchi-kafka     | 	metrics.latency.sla.ms = 50
benchi-kafka     | 	metrics.latency.slo.ms = 5
benchi-kafka     | 	metrics.latency.slo.sla.enable = false
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	metrics.tag.map = []
benchi-kafka     | 	network.traffic.rate.limit.backend = guava
benchi-kafka     | 	network.traffic.rate.limit.bytes.per.sec = 20971520
benchi-kafka     | 	network.traffic.rate.limit.enable = false
benchi-kafka     | 	nosniff.prevention.enable = false
benchi-kafka     | 	port = 8080
benchi-kafka     | 	proxy.protocol.enabled = false
benchi-kafka     | 	reject.options.request = false
benchi-kafka     | 	request.logger.name = io.confluent.rest-utils.requests
benchi-kafka     | 	request.queue.capacity = 2147483647
benchi-kafka     | 	request.queue.capacity.growby = 64
benchi-kafka     | 	request.queue.capacity.init = 128
benchi-kafka     | 	resource.extension.classes = []
benchi-kafka     | 	response.http.headers.config = 
benchi-kafka     | 	response.mediatype.default = application/json
benchi-kafka     | 	response.mediatype.preferred = [application/json]
benchi-kafka     | 	rest.servlet.initializor.classes = []
benchi-kafka     | 	server.connection.limit = 0
benchi-kafka     | 	shutdown.graceful.ms = 1000
benchi-kafka     | 	sni.check.enabled = false
benchi-kafka     | 	ssl.cipher.suites = []
benchi-kafka     | 	ssl.client.auth = false
benchi-kafka     | 	ssl.client.authentication = NONE
benchi-kafka     | 	ssl.enabled.protocols = []
benchi-kafka     | 	ssl.endpoint.identification.algorithm = null
benchi-kafka     | 	ssl.key.password = [hidden]
benchi-kafka     | 	ssl.keymanager.algorithm = 
benchi-kafka     | 	ssl.keystore.location = 
benchi-kafka     | 	ssl.keystore.password = [hidden]
benchi-kafka     | 	ssl.keystore.reload = false
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.keystore.watch.location = 
benchi-kafka     | 	ssl.protocol = TLS
benchi-kafka     | 	ssl.provider = 
benchi-kafka     | 	ssl.trustmanager.algorithm = 
benchi-kafka     | 	ssl.truststore.location = 
benchi-kafka     | 	ssl.truststore.password = [hidden]
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     | 	suppress.stack.trace.response = true
benchi-kafka     | 	thread.pool.max = 200
benchi-kafka     | 	thread.pool.min = 8
benchi-kafka     | 	websocket.path.prefix = /ws
benchi-kafka     | 	websocket.servlet.initializor.classes = []
benchi-kafka     |  (io.confluent.rest.RestConfig)
benchi-kafka     | [2025-03-18 16:03:20,529] INFO Adding listener with HTTP/2: NamedURI{uri=http://0.0.0.0:8090, name='null'} (io.confluent.rest.ApplicationServer)
benchi-kafka     | [2025-03-18 16:03:20,529] INFO [Broker id=1] Creating new partition _confluent-link-metadata-48 with topic id gBYNT1z0TfW3wKSf7yKxpA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,529] INFO [Broker id=1] Creating new partition _confluent-link-metadata-28 with topic id gBYNT1z0TfW3wKSf7yKxpA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,529] INFO [Broker id=1] Creating new partition _confluent-link-metadata-24 with topic id gBYNT1z0TfW3wKSf7yKxpA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,530] INFO [Broker id=1] Creating new partition _confluent-link-metadata-3 with topic id gBYNT1z0TfW3wKSf7yKxpA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,530] INFO [Broker id=1] Creating new partition _confluent-link-metadata-36 with topic id gBYNT1z0TfW3wKSf7yKxpA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,530] INFO [Broker id=1] Creating new partition _confluent-link-metadata-32 with topic id gBYNT1z0TfW3wKSf7yKxpA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,532] INFO [Broker id=1] Stopped fetchers as part of become-leader transition for 50 partitions (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,543] INFO [MergedLog partition=_confluent-link-metadata-19, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:03:20,544] INFO Created log for partition _confluent-link-metadata-19 in /tmp/kraft-combined-logs/_confluent-link-metadata-19 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:03:20,545] INFO [Partition _confluent-link-metadata-19 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-19 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,545] INFO [Partition _confluent-link-metadata-19 broker=1] Log loaded for partition _confluent-link-metadata-19 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,546] INFO [MergedLog partition=_confluent-link-metadata-43, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:03:20,546] INFO Setting topicIdPartition gBYNT1z0TfW3wKSf7yKxpA:_confluent-link-metadata-19 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:03:20,546] INFO Created log for partition _confluent-link-metadata-43 in /tmp/kraft-combined-logs/_confluent-link-metadata-43 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:03:20,546] INFO [Partition _confluent-link-metadata-43 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-43 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,547] INFO [Partition _confluent-link-metadata-43 broker=1] Log loaded for partition _confluent-link-metadata-43 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,547] INFO [MergedLog partition=_confluent-link-metadata-19, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-19 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:03:20,547] INFO Setting topicIdPartition gBYNT1z0TfW3wKSf7yKxpA:_confluent-link-metadata-43 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:03:20,547] INFO [MergedLog partition=_confluent-link-metadata-43, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-43 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:03:20,548] INFO [MergedLog partition=_confluent-link-metadata-39, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:03:20,549] INFO Created log for partition _confluent-link-metadata-39 in /tmp/kraft-combined-logs/_confluent-link-metadata-39 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:03:20,549] INFO [Partition _confluent-link-metadata-39 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-39 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,549] INFO [Partition _confluent-link-metadata-39 broker=1] Log loaded for partition _confluent-link-metadata-39 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,549] INFO Setting topicIdPartition gBYNT1z0TfW3wKSf7yKxpA:_confluent-link-metadata-39 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:03:20,549] INFO [MergedLog partition=_confluent-link-metadata-39, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-39 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:03:20,550] INFO [Broker id=1] Leader _confluent-link-metadata-39 with topic id Some(gBYNT1z0TfW3wKSf7yKxpA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,550] INFO [Broker id=1] Leader _confluent-link-metadata-43 with topic id Some(gBYNT1z0TfW3wKSf7yKxpA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,550] INFO [Broker id=1] Leader _confluent-link-metadata-19 with topic id Some(gBYNT1z0TfW3wKSf7yKxpA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,550] INFO [MergedLog partition=_confluent-link-metadata-24, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:03:20,551] INFO These configurations '[sasl.oauthbearer.jwks.endpoint.refresh.ms, sasl.oauthbearer.clientassertion.include.jti.claim, sasl.login.refresh.min.period.seconds, sasl.oauthbearer.scope.claim.name, sasl.login.refresh.window.factor, sasl.kerberos.ticket.renew.window.factor, sasl.login.retry.backoff.ms, sasl.oauthbearer.clientassertion.expiration, sasl.kerberos.kinit.cmd, sasl.kerberos.ticket.renew.jitter, ssl.keystore.type, ssl.trustmanager.algorithm, sasl.kerberos.min.time.before.relogin, ssl.endpoint.identification.algorithm, ssl.protocol, sasl.login.refresh.buffer.seconds, sasl.login.retry.backoff.max.ms, ssl.enabled.protocols, sasl.oauthbearer.sub.claim.name, ssl.truststore.type, sasl.oauthbearer.jwks.endpoint.retry.backoff.ms, sasl.oauthbearer.clock.skew.seconds, ssl.keymanager.algorithm, sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms, sasl.oauthbearer.clientassertion.include.nbf.claim, sasl.login.refresh.window.jitter]' were supplied but are not used yet. (org.apache.kafka.clients.consumer.ConsumerConfig)
benchi-kafka     | [2025-03-18 16:03:20,551] INFO Kafka version: 7.8.1-ce (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:03:20,551] INFO Kafka commitId: eefb51c6f2f1b888b8b03296353338afbac8e213 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:03:20,551] INFO Kafka startTimeMs: 1742313800551 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:03:20,551] INFO Created log for partition _confluent-link-metadata-24 in /tmp/kraft-combined-logs/_confluent-link-metadata-24 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:03:20,551] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2083203430002767771] Subscribed to pattern: '_confluent-telemetry-metrics' (org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer)
benchi-kafka     | [2025-03-18 16:03:20,551] INFO [Partition _confluent-link-metadata-24 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-24 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,551] INFO [Partition _confluent-link-metadata-24 broker=1] Log loaded for partition _confluent-link-metadata-24 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,551] INFO Setting topicIdPartition gBYNT1z0TfW3wKSf7yKxpA:_confluent-link-metadata-24 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:03:20,551] INFO [MergedLog partition=_confluent-link-metadata-24, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-24 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:03:20,551] INFO [Broker id=1] Leader _confluent-link-metadata-24 with topic id Some(gBYNT1z0TfW3wKSf7yKxpA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,553] INFO Loaded KafkaHttpServer implementation class io.confluent.http.server.KafkaHttpServerImpl (io.confluent.kafka.http.server.KafkaHttpServerLoader)
benchi-kafka     | [2025-03-18 16:03:20,553] INFO [MergedLog partition=_confluent-link-metadata-25, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:03:20,553] INFO KafkaHttpServer transitioned from NEW to STARTING.. (io.confluent.http.server.KafkaHttpServerImpl)
benchi-kafka     | [2025-03-18 16:03:20,553] INFO Created log for partition _confluent-link-metadata-25 in /tmp/kraft-combined-logs/_confluent-link-metadata-25 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:03:20,554] INFO [Partition _confluent-link-metadata-25 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-25 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,554] INFO [Partition _confluent-link-metadata-25 broker=1] Log loaded for partition _confluent-link-metadata-25 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,554] INFO Setting topicIdPartition gBYNT1z0TfW3wKSf7yKxpA:_confluent-link-metadata-25 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:03:20,554] INFO [MergedLog partition=_confluent-link-metadata-25, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-25 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:03:20,554] INFO [Broker id=1] Leader _confluent-link-metadata-25 with topic id Some(gBYNT1z0TfW3wKSf7yKxpA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,555] INFO [MergedLog partition=_confluent-link-metadata-21, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:03:20,555] INFO Created log for partition _confluent-link-metadata-21 in /tmp/kraft-combined-logs/_confluent-link-metadata-21 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:03:20,555] INFO [Partition _confluent-link-metadata-21 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-21 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,555] INFO [Partition _confluent-link-metadata-21 broker=1] Log loaded for partition _confluent-link-metadata-21 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,555] INFO Setting topicIdPartition gBYNT1z0TfW3wKSf7yKxpA:_confluent-link-metadata-21 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:03:20,556] INFO [MergedLog partition=_confluent-link-metadata-21, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-21 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:03:20,556] INFO [Broker id=1] Leader _confluent-link-metadata-21 with topic id Some(gBYNT1z0TfW3wKSf7yKxpA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,558] INFO [MergedLog partition=_confluent-link-metadata-20, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:03:20,559] INFO Registered MetricsListener to connector of listener: null (io.confluent.rest.ApplicationServer)
benchi-kafka     | [2025-03-18 16:03:20,559] INFO Created log for partition _confluent-link-metadata-20 in /tmp/kraft-combined-logs/_confluent-link-metadata-20 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:03:20,559] INFO [Partition _confluent-link-metadata-20 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-20 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,559] INFO [Partition _confluent-link-metadata-20 broker=1] Log loaded for partition _confluent-link-metadata-20 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,559] INFO Setting topicIdPartition gBYNT1z0TfW3wKSf7yKxpA:_confluent-link-metadata-20 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:03:20,559] INFO [MergedLog partition=_confluent-link-metadata-20, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-20 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:03:20,559] INFO [Broker id=1] Leader _confluent-link-metadata-20 with topic id Some(gBYNT1z0TfW3wKSf7yKxpA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,561] INFO [MergedLog partition=_confluent-link-metadata-23, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:03:20,561] INFO Created log for partition _confluent-link-metadata-23 in /tmp/kraft-combined-logs/_confluent-link-metadata-23 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:03:20,561] INFO [Partition _confluent-link-metadata-23 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-23 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,561] INFO [Partition _confluent-link-metadata-23 broker=1] Log loaded for partition _confluent-link-metadata-23 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,561] INFO Setting topicIdPartition gBYNT1z0TfW3wKSf7yKxpA:_confluent-link-metadata-23 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:03:20,561] INFO [MergedLog partition=_confluent-link-metadata-23, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-23 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:03:20,561] INFO [Broker id=1] Leader _confluent-link-metadata-23 with topic id Some(gBYNT1z0TfW3wKSf7yKxpA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,563] INFO [MergedLog partition=_confluent-link-metadata-18, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:03:20,563] INFO Created log for partition _confluent-link-metadata-18 in /tmp/kraft-combined-logs/_confluent-link-metadata-18 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:03:20,563] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2083203430002767771] Cluster ID: MkU3OEVBNTcwNTJENDM2Qk (org.apache.kafka.clients.Metadata)
benchi-kafka     | [2025-03-18 16:03:20,563] INFO [Partition _confluent-link-metadata-18 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-18 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,563] INFO [Partition _confluent-link-metadata-18 broker=1] Log loaded for partition _confluent-link-metadata-18 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,564] INFO Setting topicIdPartition gBYNT1z0TfW3wKSf7yKxpA:_confluent-link-metadata-18 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:03:20,564] INFO [MergedLog partition=_confluent-link-metadata-18, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-18 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:03:20,564] INFO [Broker id=1] Leader _confluent-link-metadata-18 with topic id Some(gBYNT1z0TfW3wKSf7yKxpA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,564] INFO Waiting for 1 seconds for metric reporter topic _confluent-telemetry-metrics to become available. (io.confluent.cruisecontrol.metricsreporter.ConfluentMetricsSamplerBase)
benchi-kafka     | [2025-03-18 16:03:20,565] INFO [MergedLog partition=_confluent-link-metadata-41, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:03:20,565] INFO Created log for partition _confluent-link-metadata-41 in /tmp/kraft-combined-logs/_confluent-link-metadata-41 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:03:20,565] INFO [Partition _confluent-link-metadata-41 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-41 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,565] INFO [Partition _confluent-link-metadata-41 broker=1] Log loaded for partition _confluent-link-metadata-41 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,565] INFO Setting topicIdPartition gBYNT1z0TfW3wKSf7yKxpA:_confluent-link-metadata-41 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:03:20,565] INFO [MergedLog partition=_confluent-link-metadata-41, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-41 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:03:20,566] INFO [Broker id=1] Leader _confluent-link-metadata-41 with topic id Some(gBYNT1z0TfW3wKSf7yKxpA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,567] INFO [MergedLog partition=_confluent-link-metadata-47, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:03:20,568] INFO Created log for partition _confluent-link-metadata-47 in /tmp/kraft-combined-logs/_confluent-link-metadata-47 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:03:20,568] INFO [Partition _confluent-link-metadata-47 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-47 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,568] INFO [Partition _confluent-link-metadata-47 broker=1] Log loaded for partition _confluent-link-metadata-47 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,568] INFO Setting topicIdPartition gBYNT1z0TfW3wKSf7yKxpA:_confluent-link-metadata-47 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:03:20,568] INFO [MergedLog partition=_confluent-link-metadata-47, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-47 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:03:20,568] INFO [Broker id=1] Leader _confluent-link-metadata-47 with topic id Some(gBYNT1z0TfW3wKSf7yKxpA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,569] INFO [MergedLog partition=_confluent-link-metadata-27, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:03:20,570] INFO Created log for partition _confluent-link-metadata-27 in /tmp/kraft-combined-logs/_confluent-link-metadata-27 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:03:20,570] INFO [Partition _confluent-link-metadata-27 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-27 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,570] INFO [Partition _confluent-link-metadata-27 broker=1] Log loaded for partition _confluent-link-metadata-27 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,570] INFO Setting topicIdPartition gBYNT1z0TfW3wKSf7yKxpA:_confluent-link-metadata-27 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:03:20,570] INFO [MergedLog partition=_confluent-link-metadata-27, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-27 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:03:20,570] INFO [Broker id=1] Leader _confluent-link-metadata-27 with topic id Some(gBYNT1z0TfW3wKSf7yKxpA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,571] INFO [MergedLog partition=_confluent-link-metadata-49, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:03:20,571] INFO Created log for partition _confluent-link-metadata-49 in /tmp/kraft-combined-logs/_confluent-link-metadata-49 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:03:20,571] INFO [Partition _confluent-link-metadata-49 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-49 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,571] INFO [Partition _confluent-link-metadata-49 broker=1] Log loaded for partition _confluent-link-metadata-49 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,572] INFO Setting topicIdPartition gBYNT1z0TfW3wKSf7yKxpA:_confluent-link-metadata-49 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:03:20,572] INFO [MergedLog partition=_confluent-link-metadata-49, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-49 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:03:20,572] INFO [Broker id=1] Leader _confluent-link-metadata-49 with topic id Some(gBYNT1z0TfW3wKSf7yKxpA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,573] INFO [MergedLog partition=_confluent-link-metadata-15, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:03:20,573] INFO Created log for partition _confluent-link-metadata-15 in /tmp/kraft-combined-logs/_confluent-link-metadata-15 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:03:20,573] INFO [Partition _confluent-link-metadata-15 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-15 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,573] INFO [Partition _confluent-link-metadata-15 broker=1] Log loaded for partition _confluent-link-metadata-15 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,573] INFO Setting topicIdPartition gBYNT1z0TfW3wKSf7yKxpA:_confluent-link-metadata-15 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:03:20,573] INFO [MergedLog partition=_confluent-link-metadata-15, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-15 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:03:20,573] INFO [Broker id=1] Leader _confluent-link-metadata-15 with topic id Some(gBYNT1z0TfW3wKSf7yKxpA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,574] INFO [MergedLog partition=_confluent-link-metadata-46, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:03:20,575] INFO Created log for partition _confluent-link-metadata-46 in /tmp/kraft-combined-logs/_confluent-link-metadata-46 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:03:20,575] INFO [Partition _confluent-link-metadata-46 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-46 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,575] INFO [Partition _confluent-link-metadata-46 broker=1] Log loaded for partition _confluent-link-metadata-46 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,575] INFO Setting topicIdPartition gBYNT1z0TfW3wKSf7yKxpA:_confluent-link-metadata-46 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:03:20,575] INFO [MergedLog partition=_confluent-link-metadata-46, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-46 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:03:20,575] INFO [Broker id=1] Leader _confluent-link-metadata-46 with topic id Some(gBYNT1z0TfW3wKSf7yKxpA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,576] INFO [MergedLog partition=_confluent-link-metadata-28, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:03:20,577] INFO Created log for partition _confluent-link-metadata-28 in /tmp/kraft-combined-logs/_confluent-link-metadata-28 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:03:20,577] INFO [Partition _confluent-link-metadata-28 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-28 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,577] INFO [Partition _confluent-link-metadata-28 broker=1] Log loaded for partition _confluent-link-metadata-28 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,577] INFO Setting topicIdPartition gBYNT1z0TfW3wKSf7yKxpA:_confluent-link-metadata-28 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:03:20,577] INFO [MergedLog partition=_confluent-link-metadata-28, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-28 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:03:20,577] INFO [Broker id=1] Leader _confluent-link-metadata-28 with topic id Some(gBYNT1z0TfW3wKSf7yKxpA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,578] INFO [MergedLog partition=_confluent-link-metadata-29, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:03:20,578] INFO Created log for partition _confluent-link-metadata-29 in /tmp/kraft-combined-logs/_confluent-link-metadata-29 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:03:20,578] INFO [Partition _confluent-link-metadata-29 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-29 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,578] INFO [Partition _confluent-link-metadata-29 broker=1] Log loaded for partition _confluent-link-metadata-29 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,578] INFO Setting topicIdPartition gBYNT1z0TfW3wKSf7yKxpA:_confluent-link-metadata-29 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:03:20,578] INFO [MergedLog partition=_confluent-link-metadata-29, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-29 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:03:20,578] INFO [Broker id=1] Leader _confluent-link-metadata-29 with topic id Some(gBYNT1z0TfW3wKSf7yKxpA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,580] INFO [MergedLog partition=_confluent-link-metadata-10, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:03:20,580] INFO Created log for partition _confluent-link-metadata-10 in /tmp/kraft-combined-logs/_confluent-link-metadata-10 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:03:20,580] INFO [Partition _confluent-link-metadata-10 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-10 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,580] INFO [Partition _confluent-link-metadata-10 broker=1] Log loaded for partition _confluent-link-metadata-10 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,580] INFO Setting topicIdPartition gBYNT1z0TfW3wKSf7yKxpA:_confluent-link-metadata-10 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:03:20,580] INFO [MergedLog partition=_confluent-link-metadata-10, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-10 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:03:20,580] INFO [Broker id=1] Leader _confluent-link-metadata-10 with topic id Some(gBYNT1z0TfW3wKSf7yKxpA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,582] INFO [MergedLog partition=_confluent-link-metadata-6, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:03:20,582] INFO Created log for partition _confluent-link-metadata-6 in /tmp/kraft-combined-logs/_confluent-link-metadata-6 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:03:20,582] INFO [Partition _confluent-link-metadata-6 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-6 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,582] INFO [Partition _confluent-link-metadata-6 broker=1] Log loaded for partition _confluent-link-metadata-6 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,582] INFO Setting topicIdPartition gBYNT1z0TfW3wKSf7yKxpA:_confluent-link-metadata-6 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:03:20,582] INFO [MergedLog partition=_confluent-link-metadata-6, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-6 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:03:20,582] INFO [Broker id=1] Leader _confluent-link-metadata-6 with topic id Some(gBYNT1z0TfW3wKSf7yKxpA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,583] INFO [MergedLog partition=_confluent-link-metadata-7, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:03:20,583] INFO Created log for partition _confluent-link-metadata-7 in /tmp/kraft-combined-logs/_confluent-link-metadata-7 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:03:20,584] INFO [Partition _confluent-link-metadata-7 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-7 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,584] INFO [Partition _confluent-link-metadata-7 broker=1] Log loaded for partition _confluent-link-metadata-7 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,584] INFO Setting topicIdPartition gBYNT1z0TfW3wKSf7yKxpA:_confluent-link-metadata-7 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:03:20,584] INFO [MergedLog partition=_confluent-link-metadata-7, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-7 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:03:20,584] INFO [Broker id=1] Leader _confluent-link-metadata-7 with topic id Some(gBYNT1z0TfW3wKSf7yKxpA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,585] INFO [MergedLog partition=_confluent-link-metadata-33, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:03:20,586] INFO Created log for partition _confluent-link-metadata-33 in /tmp/kraft-combined-logs/_confluent-link-metadata-33 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:03:20,586] INFO [Partition _confluent-link-metadata-33 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-33 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,586] INFO [Partition _confluent-link-metadata-33 broker=1] Log loaded for partition _confluent-link-metadata-33 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,586] INFO Setting topicIdPartition gBYNT1z0TfW3wKSf7yKxpA:_confluent-link-metadata-33 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:03:20,586] INFO [MergedLog partition=_confluent-link-metadata-33, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-33 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:03:20,586] INFO [Broker id=1] Leader _confluent-link-metadata-33 with topic id Some(gBYNT1z0TfW3wKSf7yKxpA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,587] INFO [MergedLog partition=_confluent-link-metadata-32, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:03:20,588] INFO Created log for partition _confluent-link-metadata-32 in /tmp/kraft-combined-logs/_confluent-link-metadata-32 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:03:20,588] INFO [Partition _confluent-link-metadata-32 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-32 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,588] INFO [Partition _confluent-link-metadata-32 broker=1] Log loaded for partition _confluent-link-metadata-32 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,588] INFO Setting topicIdPartition gBYNT1z0TfW3wKSf7yKxpA:_confluent-link-metadata-32 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:03:20,588] INFO [MergedLog partition=_confluent-link-metadata-32, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-32 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:03:20,588] INFO [Broker id=1] Leader _confluent-link-metadata-32 with topic id Some(gBYNT1z0TfW3wKSf7yKxpA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,589] INFO [MergedLog partition=_confluent-link-metadata-17, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:03:20,589] INFO Created log for partition _confluent-link-metadata-17 in /tmp/kraft-combined-logs/_confluent-link-metadata-17 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:03:20,589] INFO [Partition _confluent-link-metadata-17 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-17 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,589] INFO [Partition _confluent-link-metadata-17 broker=1] Log loaded for partition _confluent-link-metadata-17 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,589] INFO Setting topicIdPartition gBYNT1z0TfW3wKSf7yKxpA:_confluent-link-metadata-17 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:03:20,589] INFO [MergedLog partition=_confluent-link-metadata-17, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-17 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:03:20,589] INFO [Broker id=1] Leader _confluent-link-metadata-17 with topic id Some(gBYNT1z0TfW3wKSf7yKxpA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,590] INFO [MergedLog partition=_confluent-link-metadata-48, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:03:20,591] INFO Created log for partition _confluent-link-metadata-48 in /tmp/kraft-combined-logs/_confluent-link-metadata-48 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:03:20,591] INFO [Partition _confluent-link-metadata-48 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-48 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,591] INFO [Partition _confluent-link-metadata-48 broker=1] Log loaded for partition _confluent-link-metadata-48 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,591] INFO Setting topicIdPartition gBYNT1z0TfW3wKSf7yKxpA:_confluent-link-metadata-48 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:03:20,591] INFO [MergedLog partition=_confluent-link-metadata-48, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-48 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:03:20,591] INFO [Broker id=1] Leader _confluent-link-metadata-48 with topic id Some(gBYNT1z0TfW3wKSf7yKxpA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,592] INFO [MergedLog partition=_confluent-link-metadata-16, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:03:20,592] INFO Created log for partition _confluent-link-metadata-16 in /tmp/kraft-combined-logs/_confluent-link-metadata-16 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:03:20,592] INFO [Partition _confluent-link-metadata-16 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-16 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,592] INFO [Partition _confluent-link-metadata-16 broker=1] Log loaded for partition _confluent-link-metadata-16 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,592] INFO Setting topicIdPartition gBYNT1z0TfW3wKSf7yKxpA:_confluent-link-metadata-16 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:03:20,593] INFO [MergedLog partition=_confluent-link-metadata-16, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-16 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:03:20,593] INFO [Broker id=1] Leader _confluent-link-metadata-16 with topic id Some(gBYNT1z0TfW3wKSf7yKxpA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,594] INFO [MergedLog partition=_confluent-link-metadata-31, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:03:20,594] INFO Created log for partition _confluent-link-metadata-31 in /tmp/kraft-combined-logs/_confluent-link-metadata-31 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:03:20,594] INFO [Partition _confluent-link-metadata-31 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-31 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,594] INFO [Partition _confluent-link-metadata-31 broker=1] Log loaded for partition _confluent-link-metadata-31 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,594] INFO Setting topicIdPartition gBYNT1z0TfW3wKSf7yKxpA:_confluent-link-metadata-31 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:03:20,594] INFO [MergedLog partition=_confluent-link-metadata-31, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-31 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:03:20,594] INFO [Broker id=1] Leader _confluent-link-metadata-31 with topic id Some(gBYNT1z0TfW3wKSf7yKxpA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,596] INFO [MergedLog partition=_confluent-link-metadata-14, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:03:20,596] INFO Created log for partition _confluent-link-metadata-14 in /tmp/kraft-combined-logs/_confluent-link-metadata-14 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:03:20,596] INFO [Partition _confluent-link-metadata-14 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-14 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,596] INFO [Partition _confluent-link-metadata-14 broker=1] Log loaded for partition _confluent-link-metadata-14 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,596] INFO Setting topicIdPartition gBYNT1z0TfW3wKSf7yKxpA:_confluent-link-metadata-14 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:03:20,596] INFO [MergedLog partition=_confluent-link-metadata-14, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-14 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:03:20,597] INFO [Broker id=1] Leader _confluent-link-metadata-14 with topic id Some(gBYNT1z0TfW3wKSf7yKxpA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,597] INFO [MergedLog partition=_confluent-link-metadata-8, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:03:20,598] INFO Created log for partition _confluent-link-metadata-8 in /tmp/kraft-combined-logs/_confluent-link-metadata-8 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:03:20,598] INFO [Partition _confluent-link-metadata-8 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-8 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,598] INFO [Partition _confluent-link-metadata-8 broker=1] Log loaded for partition _confluent-link-metadata-8 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,598] INFO Setting topicIdPartition gBYNT1z0TfW3wKSf7yKxpA:_confluent-link-metadata-8 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:03:20,598] INFO [MergedLog partition=_confluent-link-metadata-8, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-8 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:03:20,598] INFO [Broker id=1] Leader _confluent-link-metadata-8 with topic id Some(gBYNT1z0TfW3wKSf7yKxpA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,599] INFO [MergedLog partition=_confluent-link-metadata-45, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:03:20,599] INFO Created log for partition _confluent-link-metadata-45 in /tmp/kraft-combined-logs/_confluent-link-metadata-45 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:03:20,599] INFO [Partition _confluent-link-metadata-45 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-45 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,599] INFO [Partition _confluent-link-metadata-45 broker=1] Log loaded for partition _confluent-link-metadata-45 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,599] INFO Setting topicIdPartition gBYNT1z0TfW3wKSf7yKxpA:_confluent-link-metadata-45 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:03:20,599] INFO [MergedLog partition=_confluent-link-metadata-45, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-45 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:03:20,599] INFO [Broker id=1] Leader _confluent-link-metadata-45 with topic id Some(gBYNT1z0TfW3wKSf7yKxpA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,601] INFO [MergedLog partition=_confluent-link-metadata-42, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:03:20,601] INFO Created log for partition _confluent-link-metadata-42 in /tmp/kraft-combined-logs/_confluent-link-metadata-42 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:03:20,601] INFO [Partition _confluent-link-metadata-42 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-42 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,601] INFO [Partition _confluent-link-metadata-42 broker=1] Log loaded for partition _confluent-link-metadata-42 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,601] INFO Setting topicIdPartition gBYNT1z0TfW3wKSf7yKxpA:_confluent-link-metadata-42 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:03:20,601] INFO [MergedLog partition=_confluent-link-metadata-42, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-42 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:03:20,601] INFO [Broker id=1] Leader _confluent-link-metadata-42 with topic id Some(gBYNT1z0TfW3wKSf7yKxpA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,602] INFO [MergedLog partition=_confluent-link-metadata-37, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:03:20,603] INFO Created log for partition _confluent-link-metadata-37 in /tmp/kraft-combined-logs/_confluent-link-metadata-37 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:03:20,603] INFO [Partition _confluent-link-metadata-37 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-37 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,603] INFO [Partition _confluent-link-metadata-37 broker=1] Log loaded for partition _confluent-link-metadata-37 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,603] INFO Setting topicIdPartition gBYNT1z0TfW3wKSf7yKxpA:_confluent-link-metadata-37 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:03:20,603] INFO [MergedLog partition=_confluent-link-metadata-37, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-37 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:03:20,603] INFO [Broker id=1] Leader _confluent-link-metadata-37 with topic id Some(gBYNT1z0TfW3wKSf7yKxpA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,604] INFO [MergedLog partition=_confluent-link-metadata-35, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:03:20,604] INFO Created log for partition _confluent-link-metadata-35 in /tmp/kraft-combined-logs/_confluent-link-metadata-35 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:03:20,604] INFO [Partition _confluent-link-metadata-35 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-35 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,604] INFO [Partition _confluent-link-metadata-35 broker=1] Log loaded for partition _confluent-link-metadata-35 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,604] INFO Setting topicIdPartition gBYNT1z0TfW3wKSf7yKxpA:_confluent-link-metadata-35 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:03:20,604] INFO [MergedLog partition=_confluent-link-metadata-35, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-35 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:03:20,604] INFO [Broker id=1] Leader _confluent-link-metadata-35 with topic id Some(gBYNT1z0TfW3wKSf7yKxpA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,605] INFO [MergedLog partition=_confluent-link-metadata-38, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:03:20,605] INFO Created log for partition _confluent-link-metadata-38 in /tmp/kraft-combined-logs/_confluent-link-metadata-38 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:03:20,605] INFO [Partition _confluent-link-metadata-38 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-38 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,605] INFO [Partition _confluent-link-metadata-38 broker=1] Log loaded for partition _confluent-link-metadata-38 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,605] INFO Setting topicIdPartition gBYNT1z0TfW3wKSf7yKxpA:_confluent-link-metadata-38 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:03:20,605] INFO [MergedLog partition=_confluent-link-metadata-38, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-38 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:03:20,605] INFO [Broker id=1] Leader _confluent-link-metadata-38 with topic id Some(gBYNT1z0TfW3wKSf7yKxpA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,607] INFO [MergedLog partition=_confluent-link-metadata-3, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:03:20,607] INFO Created log for partition _confluent-link-metadata-3 in /tmp/kraft-combined-logs/_confluent-link-metadata-3 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:03:20,607] INFO [Partition _confluent-link-metadata-3 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-3 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,607] INFO [Partition _confluent-link-metadata-3 broker=1] Log loaded for partition _confluent-link-metadata-3 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,607] INFO Setting topicIdPartition gBYNT1z0TfW3wKSf7yKxpA:_confluent-link-metadata-3 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:03:20,607] INFO [MergedLog partition=_confluent-link-metadata-3, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-3 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:03:20,607] INFO [Broker id=1] Leader _confluent-link-metadata-3 with topic id Some(gBYNT1z0TfW3wKSf7yKxpA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,608] INFO [MergedLog partition=_confluent-link-metadata-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:03:20,608] INFO Created log for partition _confluent-link-metadata-0 in /tmp/kraft-combined-logs/_confluent-link-metadata-0 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:03:20,608] INFO [Partition _confluent-link-metadata-0 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,608] INFO [Partition _confluent-link-metadata-0 broker=1] Log loaded for partition _confluent-link-metadata-0 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,608] INFO Setting topicIdPartition gBYNT1z0TfW3wKSf7yKxpA:_confluent-link-metadata-0 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:03:20,608] INFO [MergedLog partition=_confluent-link-metadata-0, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:03:20,609] INFO [Broker id=1] Leader _confluent-link-metadata-0 with topic id Some(gBYNT1z0TfW3wKSf7yKxpA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,610] INFO [MergedLog partition=_confluent-link-metadata-40, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:03:20,610] INFO Created log for partition _confluent-link-metadata-40 in /tmp/kraft-combined-logs/_confluent-link-metadata-40 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:03:20,610] INFO [Partition _confluent-link-metadata-40 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-40 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,610] INFO [Partition _confluent-link-metadata-40 broker=1] Log loaded for partition _confluent-link-metadata-40 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,610] INFO Setting topicIdPartition gBYNT1z0TfW3wKSf7yKxpA:_confluent-link-metadata-40 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:03:20,610] INFO [MergedLog partition=_confluent-link-metadata-40, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-40 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:03:20,610] INFO [Broker id=1] Leader _confluent-link-metadata-40 with topic id Some(gBYNT1z0TfW3wKSf7yKxpA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,611] INFO [MergedLog partition=_confluent-link-metadata-34, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:03:20,612] INFO Created log for partition _confluent-link-metadata-34 in /tmp/kraft-combined-logs/_confluent-link-metadata-34 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:03:20,612] INFO [Partition _confluent-link-metadata-34 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-34 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,612] INFO [Partition _confluent-link-metadata-34 broker=1] Log loaded for partition _confluent-link-metadata-34 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,612] INFO Setting topicIdPartition gBYNT1z0TfW3wKSf7yKxpA:_confluent-link-metadata-34 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:03:20,612] INFO [MergedLog partition=_confluent-link-metadata-34, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-34 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:03:20,612] INFO [Broker id=1] Leader _confluent-link-metadata-34 with topic id Some(gBYNT1z0TfW3wKSf7yKxpA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,613] INFO [MergedLog partition=_confluent-link-metadata-11, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:03:20,613] INFO Created log for partition _confluent-link-metadata-11 in /tmp/kraft-combined-logs/_confluent-link-metadata-11 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:03:20,613] INFO [Partition _confluent-link-metadata-11 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-11 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,613] INFO [Partition _confluent-link-metadata-11 broker=1] Log loaded for partition _confluent-link-metadata-11 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,613] INFO Setting topicIdPartition gBYNT1z0TfW3wKSf7yKxpA:_confluent-link-metadata-11 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:03:20,613] INFO [MergedLog partition=_confluent-link-metadata-11, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-11 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:03:20,613] INFO [Broker id=1] Leader _confluent-link-metadata-11 with topic id Some(gBYNT1z0TfW3wKSf7yKxpA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,614] INFO [MergedLog partition=_confluent-link-metadata-26, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:03:20,615] INFO Created log for partition _confluent-link-metadata-26 in /tmp/kraft-combined-logs/_confluent-link-metadata-26 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:03:20,615] INFO [Partition _confluent-link-metadata-26 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-26 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,615] INFO [Partition _confluent-link-metadata-26 broker=1] Log loaded for partition _confluent-link-metadata-26 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,615] INFO Setting topicIdPartition gBYNT1z0TfW3wKSf7yKxpA:_confluent-link-metadata-26 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:03:20,615] INFO [MergedLog partition=_confluent-link-metadata-26, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-26 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:03:20,615] INFO [Broker id=1] Leader _confluent-link-metadata-26 with topic id Some(gBYNT1z0TfW3wKSf7yKxpA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,616] INFO [MergedLog partition=_confluent-link-metadata-36, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:03:20,616] INFO Created log for partition _confluent-link-metadata-36 in /tmp/kraft-combined-logs/_confluent-link-metadata-36 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:03:20,616] INFO [Partition _confluent-link-metadata-36 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-36 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,616] INFO [Partition _confluent-link-metadata-36 broker=1] Log loaded for partition _confluent-link-metadata-36 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,616] INFO Setting topicIdPartition gBYNT1z0TfW3wKSf7yKxpA:_confluent-link-metadata-36 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:03:20,616] INFO [MergedLog partition=_confluent-link-metadata-36, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-36 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:03:20,616] INFO [Broker id=1] Leader _confluent-link-metadata-36 with topic id Some(gBYNT1z0TfW3wKSf7yKxpA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,617] INFO [MergedLog partition=_confluent-link-metadata-9, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:03:20,617] INFO Created log for partition _confluent-link-metadata-9 in /tmp/kraft-combined-logs/_confluent-link-metadata-9 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:03:20,617] INFO [Partition _confluent-link-metadata-9 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-9 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,617] INFO [Partition _confluent-link-metadata-9 broker=1] Log loaded for partition _confluent-link-metadata-9 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,617] INFO Setting topicIdPartition gBYNT1z0TfW3wKSf7yKxpA:_confluent-link-metadata-9 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:03:20,618] INFO [MergedLog partition=_confluent-link-metadata-9, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-9 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:03:20,618] INFO [Broker id=1] Leader _confluent-link-metadata-9 with topic id Some(gBYNT1z0TfW3wKSf7yKxpA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,619] INFO [MergedLog partition=_confluent-link-metadata-2, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:03:20,619] INFO Created log for partition _confluent-link-metadata-2 in /tmp/kraft-combined-logs/_confluent-link-metadata-2 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:03:20,619] INFO [Partition _confluent-link-metadata-2 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-2 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,619] INFO [Partition _confluent-link-metadata-2 broker=1] Log loaded for partition _confluent-link-metadata-2 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,619] INFO Setting topicIdPartition gBYNT1z0TfW3wKSf7yKxpA:_confluent-link-metadata-2 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:03:20,619] INFO [MergedLog partition=_confluent-link-metadata-2, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-2 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:03:20,619] INFO [Broker id=1] Leader _confluent-link-metadata-2 with topic id Some(gBYNT1z0TfW3wKSf7yKxpA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,621] INFO [MergedLog partition=_confluent-link-metadata-4, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:03:20,621] INFO Created log for partition _confluent-link-metadata-4 in /tmp/kraft-combined-logs/_confluent-link-metadata-4 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:03:20,621] INFO [Partition _confluent-link-metadata-4 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-4 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,621] INFO [Partition _confluent-link-metadata-4 broker=1] Log loaded for partition _confluent-link-metadata-4 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,621] INFO Setting topicIdPartition gBYNT1z0TfW3wKSf7yKxpA:_confluent-link-metadata-4 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:03:20,621] INFO [MergedLog partition=_confluent-link-metadata-4, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-4 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:03:20,622] INFO [Broker id=1] Leader _confluent-link-metadata-4 with topic id Some(gBYNT1z0TfW3wKSf7yKxpA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,623] INFO [MergedLog partition=_confluent-link-metadata-13, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:03:20,623] INFO Created log for partition _confluent-link-metadata-13 in /tmp/kraft-combined-logs/_confluent-link-metadata-13 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:03:20,623] INFO [Partition _confluent-link-metadata-13 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-13 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,623] INFO [Partition _confluent-link-metadata-13 broker=1] Log loaded for partition _confluent-link-metadata-13 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,623] INFO Setting topicIdPartition gBYNT1z0TfW3wKSf7yKxpA:_confluent-link-metadata-13 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:03:20,623] INFO [MergedLog partition=_confluent-link-metadata-13, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-13 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:03:20,623] INFO [Broker id=1] Leader _confluent-link-metadata-13 with topic id Some(gBYNT1z0TfW3wKSf7yKxpA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,624] INFO [MergedLog partition=_confluent-link-metadata-12, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:03:20,624] INFO Created log for partition _confluent-link-metadata-12 in /tmp/kraft-combined-logs/_confluent-link-metadata-12 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:03:20,625] INFO [Partition _confluent-link-metadata-12 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-12 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,625] INFO [Partition _confluent-link-metadata-12 broker=1] Log loaded for partition _confluent-link-metadata-12 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,625] INFO Setting topicIdPartition gBYNT1z0TfW3wKSf7yKxpA:_confluent-link-metadata-12 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:03:20,625] INFO [MergedLog partition=_confluent-link-metadata-12, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-12 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:03:20,625] INFO [Broker id=1] Leader _confluent-link-metadata-12 with topic id Some(gBYNT1z0TfW3wKSf7yKxpA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,626] INFO [MergedLog partition=_confluent-link-metadata-22, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:03:20,626] INFO Created log for partition _confluent-link-metadata-22 in /tmp/kraft-combined-logs/_confluent-link-metadata-22 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:03:20,626] INFO [Partition _confluent-link-metadata-22 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-22 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,626] INFO [Partition _confluent-link-metadata-22 broker=1] Log loaded for partition _confluent-link-metadata-22 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,626] INFO Setting topicIdPartition gBYNT1z0TfW3wKSf7yKxpA:_confluent-link-metadata-22 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:03:20,626] INFO [MergedLog partition=_confluent-link-metadata-22, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-22 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:03:20,626] INFO [Broker id=1] Leader _confluent-link-metadata-22 with topic id Some(gBYNT1z0TfW3wKSf7yKxpA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,628] INFO [MergedLog partition=_confluent-link-metadata-30, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:03:20,628] INFO Created log for partition _confluent-link-metadata-30 in /tmp/kraft-combined-logs/_confluent-link-metadata-30 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:03:20,628] INFO [Partition _confluent-link-metadata-30 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-30 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,628] INFO [Partition _confluent-link-metadata-30 broker=1] Log loaded for partition _confluent-link-metadata-30 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,628] INFO Binding MetadataApiApplication to all listeners. (io.confluent.rest.Application)
benchi-kafka     | [2025-03-18 16:03:20,628] INFO Setting topicIdPartition gBYNT1z0TfW3wKSf7yKxpA:_confluent-link-metadata-30 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:03:20,628] INFO [MergedLog partition=_confluent-link-metadata-30, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-30 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:03:20,628] INFO [Broker id=1] Leader _confluent-link-metadata-30 with topic id Some(gBYNT1z0TfW3wKSf7yKxpA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,629] INFO [MergedLog partition=_confluent-link-metadata-44, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:03:20,630] INFO Created log for partition _confluent-link-metadata-44 in /tmp/kraft-combined-logs/_confluent-link-metadata-44 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:03:20,630] INFO [Partition _confluent-link-metadata-44 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-44 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,630] INFO [Partition _confluent-link-metadata-44 broker=1] Log loaded for partition _confluent-link-metadata-44 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,630] INFO Setting topicIdPartition gBYNT1z0TfW3wKSf7yKxpA:_confluent-link-metadata-44 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:03:20,630] INFO [MergedLog partition=_confluent-link-metadata-44, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-44 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:03:20,630] INFO [Broker id=1] Leader _confluent-link-metadata-44 with topic id Some(gBYNT1z0TfW3wKSf7yKxpA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,631] INFO [MergedLog partition=_confluent-link-metadata-5, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:03:20,631] INFO Created log for partition _confluent-link-metadata-5 in /tmp/kraft-combined-logs/_confluent-link-metadata-5 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:03:20,631] INFO [Partition _confluent-link-metadata-5 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-5 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,631] INFO [Partition _confluent-link-metadata-5 broker=1] Log loaded for partition _confluent-link-metadata-5 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,631] INFO Setting topicIdPartition gBYNT1z0TfW3wKSf7yKxpA:_confluent-link-metadata-5 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:03:20,631] INFO [MergedLog partition=_confluent-link-metadata-5, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-5 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:03:20,631] INFO [Broker id=1] Leader _confluent-link-metadata-5 with topic id Some(gBYNT1z0TfW3wKSf7yKxpA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,632] INFO [MergedLog partition=_confluent-link-metadata-1, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:03:20,632] INFO Created log for partition _confluent-link-metadata-1 in /tmp/kraft-combined-logs/_confluent-link-metadata-1 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:03:20,632] INFO [Partition _confluent-link-metadata-1 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-1 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,632] INFO [Partition _confluent-link-metadata-1 broker=1] Log loaded for partition _confluent-link-metadata-1 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:20,632] INFO Setting topicIdPartition gBYNT1z0TfW3wKSf7yKxpA:_confluent-link-metadata-1 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:03:20,632] INFO [MergedLog partition=_confluent-link-metadata-1, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-1 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:03:20,632] INFO [Broker id=1] Leader _confluent-link-metadata-1 with topic id Some(gBYNT1z0TfW3wKSf7yKxpA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:20,638] INFO [DynamicConfigPublisher broker id=1] Updating topic _confluent-link-metadata with new configuration : cleanup.policy -> compact,min.insync.replicas -> 2 (kafka.server.metadata.DynamicConfigPublisher)
benchi-kafka     | [2025-03-18 16:03:20,666] INFO Registered MetricsListener to connector of listener: null (io.confluent.rest.ApplicationServer)
benchi-kafka     | [2025-03-18 16:03:20,694] INFO [Producer clientId=confluent-metrics-reporter] Cluster ID: MkU3OEVBNTcwNTJENDM2Qk (org.apache.kafka.clients.Metadata)
benchi-kafka     | [2025-03-18 16:03:20,695] INFO [Producer clientId=confluent-telemetry-reporter-local-producer] Cluster ID: MkU3OEVBNTcwNTJENDM2Qk (org.apache.kafka.clients.Metadata)
benchi-kafka     | [2025-03-18 16:03:20,698] WARN Using default value http://localhost:8081 for config schema.registry.url. In a future release this config won't have a default value anymore. If you are using Schema Registry, please, specify schema.registry.url explicitly. Requests will fail in a future release if you try to use Schema Registry but have not specified a value for schema.registry.url. An empty value for this property means that the Schema Registry is disabled. (io.confluent.kafkarest.KafkaRestConfig)
benchi-kafka     | [2025-03-18 16:03:20,699] INFO SchemaRegistryConfig values: 
benchi-kafka     | 	auto.register.schemas = false
benchi-kafka     | 	basic.auth.credentials.source = URL
benchi-kafka     | 	basic.auth.user.info = [hidden]
benchi-kafka     | 	bearer.auth.cache.expiry.buffer.seconds = 300
benchi-kafka     | 	bearer.auth.client.id = null
benchi-kafka     | 	bearer.auth.client.secret = null
benchi-kafka     | 	bearer.auth.credentials.source = STATIC_TOKEN
benchi-kafka     | 	bearer.auth.custom.provider.class = null
benchi-kafka     | 	bearer.auth.identity.pool.id = null
benchi-kafka     | 	bearer.auth.issuer.endpoint.url = null
benchi-kafka     | 	bearer.auth.logical.cluster = null
benchi-kafka     | 	bearer.auth.scope = null
benchi-kafka     | 	bearer.auth.scope.claim.name = scope
benchi-kafka     | 	bearer.auth.sub.claim.name = sub
benchi-kafka     | 	bearer.auth.token = [hidden]
benchi-kafka     | 	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
benchi-kafka     | 	http.connect.timeout.ms = 60000
benchi-kafka     | 	http.read.timeout.ms = 60000
benchi-kafka     | 	id.compatibility.strict = true
benchi-kafka     | 	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
benchi-kafka     | 	latest.cache.size = 1000
benchi-kafka     | 	latest.cache.ttl.sec = -1
benchi-kafka     | 	latest.compatibility.strict = true
benchi-kafka     | 	max.schemas.per.subject = 1000
benchi-kafka     | 	normalize.schemas = false
benchi-kafka     | 	propagate.schema.tags = false
benchi-kafka     | 	proxy.host = 
benchi-kafka     | 	proxy.port = -1
benchi-kafka     | 	rule.actions = []
benchi-kafka     | 	rule.executors = []
benchi-kafka     | 	rule.service.loader.enable = true
benchi-kafka     | 	schema.format = null
benchi-kafka     | 	schema.reflection = false
benchi-kafka     | 	schema.registry.basic.auth.user.info = [hidden]
benchi-kafka     | 	schema.registry.ssl.cipher.suites = null
benchi-kafka     | 	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	schema.registry.ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	schema.registry.ssl.engine.factory.class = null
benchi-kafka     | 	schema.registry.ssl.key.password = null
benchi-kafka     | 	schema.registry.ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	schema.registry.ssl.keystore.certificate.chain = null
benchi-kafka     | 	schema.registry.ssl.keystore.key = null
benchi-kafka     | 	schema.registry.ssl.keystore.location = null
benchi-kafka     | 	schema.registry.ssl.keystore.password = null
benchi-kafka     | 	schema.registry.ssl.keystore.type = JKS
benchi-kafka     | 	schema.registry.ssl.protocol = TLSv1.3
benchi-kafka     | 	schema.registry.ssl.provider = null
benchi-kafka     | 	schema.registry.ssl.secure.random.implementation = null
benchi-kafka     | 	schema.registry.ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	schema.registry.ssl.truststore.certificates = null
benchi-kafka     | 	schema.registry.ssl.truststore.location = null
benchi-kafka     | 	schema.registry.ssl.truststore.password = null
benchi-kafka     | 	schema.registry.ssl.truststore.type = JKS
benchi-kafka     | 	schema.registry.url = [http://localhost:8081]
benchi-kafka     | 	use.latest.version = false
benchi-kafka     | 	use.latest.with.metadata = null
benchi-kafka     | 	use.schema.id = -1
benchi-kafka     | 	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
benchi-kafka     |  (io.confluent.kafkarest.config.SchemaRegistryConfig)
benchi-kafka     | [2025-03-18 16:03:20,704] INFO Binding EmbeddedKafkaRestApplication to all listeners. (io.confluent.rest.Application)
benchi-kafka     | [2025-03-18 16:03:20,730] INFO jetty-9.4.54.v20240208; built: 2024-02-08T19:42:39.027Z; git: cef3fbd6d736a21e7d541a5db490381d95a2047d; jvm 17.0.13+11 (org.eclipse.jetty.server.Server)
benchi-kafka     | [2025-03-18 16:03:20,748] INFO DefaultSessionIdManager workerName=node0 (org.eclipse.jetty.server.session)
benchi-kafka     | [2025-03-18 16:03:20,748] INFO No SessionScavenger set, using defaults (org.eclipse.jetty.server.session)
benchi-kafka     | [2025-03-18 16:03:20,748] INFO node0 Scavenging every 660000ms (org.eclipse.jetty.server.session)
benchi-kafka     | Mar 18, 2025 4:03:20 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
benchi-kafka     | WARNING: A provider io.confluent.metadataapi.resources.MetadataResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.metadataapi.resources.MetadataResource will be ignored. 
benchi-kafka     | [2025-03-18 16:03:20,924] INFO HV000001: Hibernate Validator 6.1.7.Final (org.hibernate.validator.internal.util.Version)
benchi-kafka     | [2025-03-18 16:03:21,018] INFO Started o.e.j.s.ServletContextHandler@6b9ecf7a{/v1/metadata,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler)
benchi-kafka     | [2025-03-18 16:03:21,031] WARN Using default value http://localhost:8081 for config schema.registry.url. In a future release this config won't have a default value anymore. If you are using Schema Registry, please, specify schema.registry.url explicitly. Requests will fail in a future release if you try to use Schema Registry but have not specified a value for schema.registry.url. An empty value for this property means that the Schema Registry is disabled. (io.confluent.kafkarest.KafkaRestConfig)
benchi-kafka     | [2025-03-18 16:03:21,033] WARN Using default value http://localhost:8081 for config schema.registry.url. In a future release this config won't have a default value anymore. If you are using Schema Registry, please, specify schema.registry.url explicitly. Requests will fail in a future release if you try to use Schema Registry but have not specified a value for schema.registry.url. An empty value for this property means that the Schema Registry is disabled. (io.confluent.kafkarest.KafkaRestConfig)
benchi-kafka     | [2025-03-18 16:03:21,037] WARN Using default value http://localhost:8081 for config schema.registry.url. In a future release this config won't have a default value anymore. If you are using Schema Registry, please, specify schema.registry.url explicitly. Requests will fail in a future release if you try to use Schema Registry but have not specified a value for schema.registry.url. An empty value for this property means that the Schema Registry is disabled. (io.confluent.kafkarest.KafkaRestConfig)
benchi-kafka     | [2025-03-18 16:03:21,042] WARN Using default value http://localhost:8081 for config schema.registry.url. In a future release this config won't have a default value anymore. If you are using Schema Registry, please, specify schema.registry.url explicitly. Requests will fail in a future release if you try to use Schema Registry but have not specified a value for schema.registry.url. An empty value for this property means that the Schema Registry is disabled. (io.confluent.kafkarest.KafkaRestConfig)
benchi-kafka     | [2025-03-18 16:03:21,242] INFO Started o.e.j.s.ServletContextHandler@542bd7fa{/kafka,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler)
benchi-kafka     | [2025-03-18 16:03:21,249] INFO Started o.e.j.s.ServletContextHandler@2943bf92{/ws,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler)
benchi-kafka     | [2025-03-18 16:03:21,249] INFO Started o.e.j.s.ServletContextHandler@7b2ac7aa{/ws,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler)
benchi-kafka     | [2025-03-18 16:03:21,256] INFO Started NetworkTrafficServerConnector@77e1dacd{HTTP/1.1, (http/1.1, h2c)}{0.0.0.0:8090} (org.eclipse.jetty.server.AbstractConnector)
benchi-kafka     | [2025-03-18 16:03:21,256] INFO Started @2852ms (org.eclipse.jetty.server.Server)
benchi-kafka     | [2025-03-18 16:03:21,256] INFO KafkaHttpServer transitioned from STARTING to RUNNING.. (io.confluent.http.server.KafkaHttpServerImpl)
benchi-kafka     | [2025-03-18 16:03:21,262] INFO LicenseConfig values: 
benchi-kafka     | 	confluent.license = [hidden]
benchi-kafka     | 	confluent.license.retry.backoff.max.ms = 100000
benchi-kafka     | 	confluent.license.retry.backoff.min.ms = 1000
benchi-kafka     | 	confluent.license.topic = _confluent-license
benchi-kafka     | 	confluent.license.topic.create.timeout.ms = 600000
benchi-kafka     | 	confluent.license.topic.replication.factor = 1
benchi-kafka     |  (io.confluent.license.validator.LicenseConfig)
benchi-kafka     | [2025-03-18 16:03:21,262] INFO LicenseConfig values: 
benchi-kafka     | 	confluent.license = [hidden]
benchi-kafka     | 	confluent.license.retry.backoff.max.ms = 100000
benchi-kafka     | 	confluent.license.retry.backoff.min.ms = 1000
benchi-kafka     | 	confluent.license.topic = _confluent-license
benchi-kafka     | 	confluent.license.topic.create.timeout.ms = 600000
benchi-kafka     | 	confluent.license.topic.replication.factor = 1
benchi-kafka     |  (io.confluent.license.validator.LicenseConfig)
benchi-kafka     | [2025-03-18 16:03:21,274] INFO AdminClientConfig values: 
benchi-kafka     | 	auto.include.jmx.reporter = true
benchi-kafka     | 	bootstrap.controllers = []
benchi-kafka     | 	bootstrap.servers = [broker:29092]
benchi-kafka     | 	client.dns.lookup = use_all_dns_ips
benchi-kafka     | 	client.id = _confluent-license-admin-1
benchi-kafka     | 	confluent.lkc.id = null
benchi-kafka     | 	confluent.metrics.reporter.bootstrap.servers = localhost:9092
benchi-kafka     | 	confluent.proxy.protocol.client.address = null
benchi-kafka     | 	confluent.proxy.protocol.client.mode = PROXY
benchi-kafka     | 	confluent.proxy.protocol.client.port = null
benchi-kafka     | 	confluent.proxy.protocol.client.version = NONE
benchi-kafka     | 	connections.max.idle.ms = 300000
benchi-kafka     | 	default.api.timeout.ms = 60000
benchi-kafka     | 	enable.metrics.push = true
benchi-kafka     | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
benchi-kafka     | 	metadata.max.age.ms = 300000
benchi-kafka     | 	metadata.recovery.strategy = none
benchi-kafka     | 	metric.reporters = []
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.recording.level = INFO
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	receive.buffer.bytes = 65536
benchi-kafka     | 	reconnect.backoff.max.ms = 1000
benchi-kafka     | 	reconnect.backoff.ms = 50
benchi-kafka     | 	request.timeout.ms = 30000
benchi-kafka     | 	retries = 2147483647
benchi-kafka     | 	retry.backoff.max.ms = 1000
benchi-kafka     | 	retry.backoff.ms = 100
benchi-kafka     | 	sasl.client.callback.handler.class = null
benchi-kafka     | 	sasl.jaas.config = null
benchi-kafka     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
benchi-kafka     | 	sasl.kerberos.min.time.before.relogin = 60000
benchi-kafka     | 	sasl.kerberos.service.name = null
benchi-kafka     | 	sasl.kerberos.ticket.renew.jitter = 0.05
benchi-kafka     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
benchi-kafka     | 	sasl.login.callback.handler.class = null
benchi-kafka     | 	sasl.login.class = null
benchi-kafka     | 	sasl.login.connect.timeout.ms = null
benchi-kafka     | 	sasl.login.read.timeout.ms = null
benchi-kafka     | 	sasl.login.refresh.buffer.seconds = 300
benchi-kafka     | 	sasl.login.refresh.min.period.seconds = 60
benchi-kafka     | 	sasl.login.refresh.window.factor = 0.8
benchi-kafka     | 	sasl.login.refresh.window.jitter = 0.05
benchi-kafka     | 	sasl.login.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.login.retry.backoff.ms = 100
benchi-kafka     | 	sasl.mechanism = GSSAPI
benchi-kafka     | 	sasl.oauthbearer.clientassertion.audience = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.expiration = 5
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.jti.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.nbf.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.issuer = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.location = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key.passphrase = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.subject = null
benchi-kafka     | 	sasl.oauthbearer.clock.skew.seconds = 30
benchi-kafka     | 	sasl.oauthbearer.expected.audience = null
benchi-kafka     | 	sasl.oauthbearer.expected.issuer = null
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.url = null
benchi-kafka     | 	sasl.oauthbearer.scope.claim.name = scope
benchi-kafka     | 	sasl.oauthbearer.sub.claim.name = sub
benchi-kafka     | 	sasl.oauthbearer.token.endpoint.url = null
benchi-kafka     | 	security.protocol = PLAINTEXT
benchi-kafka     | 	security.providers = null
benchi-kafka     | 	send.buffer.bytes = 131072
benchi-kafka     | 	socket.connection.setup.timeout.max.ms = 30000
benchi-kafka     | 	socket.connection.setup.timeout.ms = 10000
benchi-kafka     | 	ssl.cipher.suites = null
benchi-kafka     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	ssl.engine.factory.class = null
benchi-kafka     | 	ssl.key.password = null
benchi-kafka     | 	ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	ssl.keystore.certificate.chain = null
benchi-kafka     | 	ssl.keystore.key = null
benchi-kafka     | 	ssl.keystore.location = null
benchi-kafka     | 	ssl.keystore.password = null
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.protocol = TLSv1.3
benchi-kafka     | 	ssl.provider = null
benchi-kafka     | 	ssl.secure.random.implementation = null
benchi-kafka     | 	ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	ssl.truststore.certificates = null
benchi-kafka     | 	ssl.truststore.location = null
benchi-kafka     | 	ssl.truststore.password = null
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     |  (org.apache.kafka.clients.admin.AdminClientConfig)
benchi-kafka     | [2025-03-18 16:03:21,275] INFO These configurations '[confluent.metrics.topic.replicas, replication.factor, confluent.metrics.reporter.bootstrap.servers, jmx.hostname, min.insync.replicas, cluster.link.metadata.topic.replication.factor, jmx.port]' were supplied but are not used yet. (org.apache.kafka.clients.admin.AdminClientConfig)
benchi-kafka     | [2025-03-18 16:03:21,275] INFO Kafka version: 7.8.1-ce (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:03:21,275] INFO Kafka commitId: eefb51c6f2f1b888b8b03296353338afbac8e213 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:03:21,275] INFO Kafka startTimeMs: 1742313801275 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:03:21,281] INFO [AdminClient clientId=_confluent-license-admin-1] The DescribeTopicPartitions API is not supported, using Metadata API to describe topics. (org.apache.kafka.clients.admin.KafkaAdminClient)
benchi-kafka     | [2025-03-18 16:03:21,285] INFO App info kafka.admin.client for _confluent-license-admin-1 unregistered (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:03:21,291] INFO Starting License Store (io.confluent.license.LicenseStore)
benchi-kafka     | [2025-03-18 16:03:21,291] INFO Starting KafkaBasedLog with topic _confluent-command reportErrorsToCallback=false (org.apache.kafka.connect.util.KafkaBasedLog)
benchi-kafka     | [2025-03-18 16:03:21,291] INFO AdminClientConfig values: 
benchi-kafka     | 	auto.include.jmx.reporter = true
benchi-kafka     | 	bootstrap.controllers = []
benchi-kafka     | 	bootstrap.servers = [broker:29092]
benchi-kafka     | 	client.dns.lookup = use_all_dns_ips
benchi-kafka     | 	client.id = _confluent-license-admin-1
benchi-kafka     | 	confluent.lkc.id = null
benchi-kafka     | 	confluent.metrics.reporter.bootstrap.servers = localhost:9092
benchi-kafka     | 	confluent.proxy.protocol.client.address = null
benchi-kafka     | 	confluent.proxy.protocol.client.mode = PROXY
benchi-kafka     | 	confluent.proxy.protocol.client.port = null
benchi-kafka     | 	confluent.proxy.protocol.client.version = NONE
benchi-kafka     | 	connections.max.idle.ms = 300000
benchi-kafka     | 	default.api.timeout.ms = 60000
benchi-kafka     | 	enable.metrics.push = true
benchi-kafka     | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
benchi-kafka     | 	metadata.max.age.ms = 300000
benchi-kafka     | 	metadata.recovery.strategy = none
benchi-kafka     | 	metric.reporters = []
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.recording.level = INFO
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	receive.buffer.bytes = 65536
benchi-kafka     | 	reconnect.backoff.max.ms = 1000
benchi-kafka     | 	reconnect.backoff.ms = 50
benchi-kafka     | 	request.timeout.ms = 30000
benchi-kafka     | 	retries = 2147483647
benchi-kafka     | 	retry.backoff.max.ms = 1000
benchi-kafka     | 	retry.backoff.ms = 100
benchi-kafka     | 	sasl.client.callback.handler.class = null
benchi-kafka     | 	sasl.jaas.config = null
benchi-kafka     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
benchi-kafka     | 	sasl.kerberos.min.time.before.relogin = 60000
benchi-kafka     | 	sasl.kerberos.service.name = null
benchi-kafka     | 	sasl.kerberos.ticket.renew.jitter = 0.05
benchi-kafka     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
benchi-kafka     | 	sasl.login.callback.handler.class = null
benchi-kafka     | 	sasl.login.class = null
benchi-kafka     | 	sasl.login.connect.timeout.ms = null
benchi-kafka     | 	sasl.login.read.timeout.ms = null
benchi-kafka     | 	sasl.login.refresh.buffer.seconds = 300
benchi-kafka     | 	sasl.login.refresh.min.period.seconds = 60
benchi-kafka     | 	sasl.login.refresh.window.factor = 0.8
benchi-kafka     | 	sasl.login.refresh.window.jitter = 0.05
benchi-kafka     | 	sasl.login.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.login.retry.backoff.ms = 100
benchi-kafka     | 	sasl.mechanism = GSSAPI
benchi-kafka     | 	sasl.oauthbearer.clientassertion.audience = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.expiration = 5
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.jti.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.nbf.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.issuer = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.location = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key.passphrase = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.subject = null
benchi-kafka     | 	sasl.oauthbearer.clock.skew.seconds = 30
benchi-kafka     | 	sasl.oauthbearer.expected.audience = null
benchi-kafka     | 	sasl.oauthbearer.expected.issuer = null
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.url = null
benchi-kafka     | 	sasl.oauthbearer.scope.claim.name = scope
benchi-kafka     | 	sasl.oauthbearer.sub.claim.name = sub
benchi-kafka     | 	sasl.oauthbearer.token.endpoint.url = null
benchi-kafka     | 	security.protocol = PLAINTEXT
benchi-kafka     | 	security.providers = null
benchi-kafka     | 	send.buffer.bytes = 131072
benchi-kafka     | 	socket.connection.setup.timeout.max.ms = 30000
benchi-kafka     | 	socket.connection.setup.timeout.ms = 10000
benchi-kafka     | 	ssl.cipher.suites = null
benchi-kafka     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	ssl.engine.factory.class = null
benchi-kafka     | 	ssl.key.password = null
benchi-kafka     | 	ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	ssl.keystore.certificate.chain = null
benchi-kafka     | 	ssl.keystore.key = null
benchi-kafka     | 	ssl.keystore.location = null
benchi-kafka     | 	ssl.keystore.password = null
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.protocol = TLSv1.3
benchi-kafka     | 	ssl.provider = null
benchi-kafka     | 	ssl.secure.random.implementation = null
benchi-kafka     | 	ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	ssl.truststore.certificates = null
benchi-kafka     | 	ssl.truststore.location = null
benchi-kafka     | 	ssl.truststore.password = null
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     |  (org.apache.kafka.clients.admin.AdminClientConfig)
benchi-kafka     | [2025-03-18 16:03:21,291] INFO These configurations '[confluent.metrics.topic.replicas, replication.factor, confluent.metrics.reporter.bootstrap.servers, jmx.hostname, min.insync.replicas, cluster.link.metadata.topic.replication.factor, jmx.port]' were supplied but are not used yet. (org.apache.kafka.clients.admin.AdminClientConfig)
benchi-kafka     | [2025-03-18 16:03:21,291] INFO Kafka version: 7.8.1-ce (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:03:21,291] INFO Kafka commitId: eefb51c6f2f1b888b8b03296353338afbac8e213 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:03:21,291] INFO Kafka startTimeMs: 1742313801291 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:03:21,295] INFO [AdminClient clientId=_confluent-license-admin-1] The DescribeTopicPartitions API is not supported, using Metadata API to describe topics. (org.apache.kafka.clients.admin.KafkaAdminClient)
benchi-kafka     | [2025-03-18 16:03:21,298] INFO [ControllerServer id=1] CreateTopics result(s): CreatableTopic(name='_confluent-command', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact'), CreateableTopicConfig(name='min.insync.replicas', value='1')], linkName=null, mirrorTopic=null, sourceTopicId=AAAAAAAAAAAAAAAAAAAAAA, mirrorStartOffsetSpec=-9223372036854775808, mirrorStartOffsets=[], stoppedSequenceNumber=0): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:03:21,298] INFO [ControllerServer id=1] Replayed TopicRecord for topic _confluent-command with topic ID RJ8JLOQ-RYOWaslNQxNJpQ. (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:03:21,298] INFO [ControllerServer id=1] Replayed ConfigRecord for ConfigResource(type=TOPIC, name='_confluent-command') which set configuration cleanup.policy to compact (org.apache.kafka.controller.ConfigurationControlManager)
benchi-kafka     | [2025-03-18 16:03:21,298] INFO [ControllerServer id=1] Replayed ConfigRecord for ConfigResource(type=TOPIC, name='_confluent-command') which set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
benchi-kafka     | [2025-03-18 16:03:21,298] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-command-0 with topic ID RJ8JLOQ-RYOWaslNQxNJpQ and PartitionRegistration(replicas=[1], observers=[], directories=[xbl_6JROMTfs6aQ8gB3fjw], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:03:21,325] INFO SBC Event SbcMetadataUpdateEvent-14 generated 1 more events to enqueue in the following order - [SbcConfigUpdateEvent-15]. Enqueuing... (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:03:21,325] INFO Handling event SbcConfigUpdateEvent-15 (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:03:21,325] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:21,325] INFO Balancer notified of a config change: ConfigurationsDelta(changes={ConfigResource(type=TOPIC, name='_confluent-command')=ConfigurationDelta(changedKeys=[cleanup.policy, min.insync.replicas])}) (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:03:21,325] INFO There were 0 change(s) and 0 deletion(s) to balancer configs. Changed Configs: {}, Deleted Configs: [] (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:03:21,325] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-command-0) (kafka.server.ReplicaFetcherManager)
benchi-kafka     | [2025-03-18 16:03:21,325] INFO [Broker id=1] Creating new partition _confluent-command-0 with topic id RJ8JLOQ-RYOWaslNQxNJpQ. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:21,326] INFO [Broker id=1] Stopped fetchers as part of become-leader transition for 1 partitions (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:21,327] INFO App info kafka.admin.client for _confluent-license-admin-1 unregistered (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:03:21,328] INFO ConsumerConfig values: 
benchi-kafka     | 	allow.auto.create.topics = true
benchi-kafka     | 	auto.commit.interval.ms = 5000
benchi-kafka     | 	auto.include.jmx.reporter = true
benchi-kafka     | 	auto.offset.reset = latest
benchi-kafka     | 	bootstrap.servers = [broker:29092]
benchi-kafka     | 	check.crcs = true
benchi-kafka     | 	client.dns.lookup = use_all_dns_ips
benchi-kafka     | 	client.id = _confluent-license-consumer-1
benchi-kafka     | 	client.rack = 
benchi-kafka     | 	confluent.lkc.id = null
benchi-kafka     | 	confluent.proxy.protocol.client.address = null
benchi-kafka     | 	confluent.proxy.protocol.client.mode = PROXY
benchi-kafka     | 	confluent.proxy.protocol.client.port = null
benchi-kafka     | 	confluent.proxy.protocol.client.version = NONE
benchi-kafka     | 	connections.max.idle.ms = 540000
benchi-kafka     | 	default.api.timeout.ms = 60000
benchi-kafka     | 	enable.auto.commit = false
benchi-kafka     | 	enable.metrics.push = true
benchi-kafka     | 	exclude.internal.topics = true
benchi-kafka     | 	fetch.max.bytes = 52428800
benchi-kafka     | 	fetch.max.wait.ms = 500
benchi-kafka     | 	fetch.min.bytes = 1
benchi-kafka     | 	group.id = null
benchi-kafka     | 	group.instance.id = null
benchi-kafka     | 	group.protocol = classic
benchi-kafka     | 	group.remote.assignor = null
benchi-kafka     | 	heartbeat.interval.ms = 3000
benchi-kafka     | 	interceptor.classes = []
benchi-kafka     | 	internal.leave.group.on.close = true
benchi-kafka     | 	internal.throw.on.fetch.stable.offset.unsupported = false
benchi-kafka     | 	isolation.level = read_uncommitted
benchi-kafka     | 	key.deserializer = class io.confluent.license.LicenseStore$LicenseKeySerde
benchi-kafka     | 	max.partition.fetch.bytes = 1048576
benchi-kafka     | 	max.poll.interval.ms = 300000
benchi-kafka     | 	max.poll.records = 500
benchi-kafka     | 	metadata.max.age.ms = 300000
benchi-kafka     | 	metadata.recovery.strategy = none
benchi-kafka     | 	metric.reporters = []
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.recording.level = INFO
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
benchi-kafka     | 	receive.buffer.bytes = 65536
benchi-kafka     | 	reconnect.backoff.max.ms = 1000
benchi-kafka     | 	reconnect.backoff.ms = 50
benchi-kafka     | 	request.timeout.ms = 120000
benchi-kafka     | 	retry.backoff.max.ms = 1000
benchi-kafka     | 	retry.backoff.ms = 100
benchi-kafka     | 	sasl.client.callback.handler.class = null
benchi-kafka     | 	sasl.jaas.config = null
benchi-kafka     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
benchi-kafka     | 	sasl.kerberos.min.time.before.relogin = 60000
benchi-kafka     | 	sasl.kerberos.service.name = null
benchi-kafka     | 	sasl.kerberos.ticket.renew.jitter = 0.05
benchi-kafka     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
benchi-kafka     | 	sasl.login.callback.handler.class = null
benchi-kafka     | 	sasl.login.class = null
benchi-kafka     | 	sasl.login.connect.timeout.ms = null
benchi-kafka     | 	sasl.login.read.timeout.ms = null
benchi-kafka     | 	sasl.login.refresh.buffer.seconds = 300
benchi-kafka     | 	sasl.login.refresh.min.period.seconds = 60
benchi-kafka     | 	sasl.login.refresh.window.factor = 0.8
benchi-kafka     | 	sasl.login.refresh.window.jitter = 0.05
benchi-kafka     | 	sasl.login.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.login.retry.backoff.ms = 100
benchi-kafka     | 	sasl.mechanism = GSSAPI
benchi-kafka     | 	sasl.oauthbearer.clientassertion.audience = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.expiration = 5
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.jti.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.nbf.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.issuer = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.location = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key.passphrase = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.subject = null
benchi-kafka     | 	sasl.oauthbearer.clock.skew.seconds = 30
benchi-kafka     | 	sasl.oauthbearer.expected.audience = null
benchi-kafka     | 	sasl.oauthbearer.expected.issuer = null
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.url = null
benchi-kafka     | 	sasl.oauthbearer.scope.claim.name = scope
benchi-kafka     | 	sasl.oauthbearer.sub.claim.name = sub
benchi-kafka     | 	sasl.oauthbearer.token.endpoint.url = null
benchi-kafka     | 	security.protocol = PLAINTEXT
benchi-kafka     | 	security.providers = null
benchi-kafka     | 	send.buffer.bytes = 131072
benchi-kafka     | 	session.timeout.ms = 45000
benchi-kafka     | 	socket.connection.setup.timeout.max.ms = 30000
benchi-kafka     | 	socket.connection.setup.timeout.ms = 10000
benchi-kafka     | 	ssl.cipher.suites = null
benchi-kafka     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	ssl.engine.factory.class = null
benchi-kafka     | 	ssl.key.password = null
benchi-kafka     | 	ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	ssl.keystore.certificate.chain = null
benchi-kafka     | 	ssl.keystore.key = null
benchi-kafka     | 	ssl.keystore.location = null
benchi-kafka     | 	ssl.keystore.password = null
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.protocol = TLSv1.3
benchi-kafka     | 	ssl.provider = null
benchi-kafka     | 	ssl.secure.random.implementation = null
benchi-kafka     | 	ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	ssl.truststore.certificates = null
benchi-kafka     | 	ssl.truststore.location = null
benchi-kafka     | 	ssl.truststore.password = null
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     | 	value.deserializer = class io.confluent.license.LicenseStore$LicenseMessageSerde
benchi-kafka     |  (org.apache.kafka.clients.consumer.ConsumerConfig)
benchi-kafka     | [2025-03-18 16:03:21,328] INFO initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector)
benchi-kafka     | [2025-03-18 16:03:21,328] INFO [MergedLog partition=_confluent-command-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:03:21,328] INFO Created log for partition _confluent-command-0 in /tmp/kraft-combined-logs/_confluent-command-0 with properties {cleanup.policy=compact, min.insync.replicas=1} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:03:21,328] INFO [Partition _confluent-command-0 broker=1] No checkpointed highwatermark is found for partition _confluent-command-0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:21,328] INFO [Partition _confluent-command-0 broker=1] Log loaded for partition _confluent-command-0 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:03:21,328] INFO Setting topicIdPartition RJ8JLOQ-RYOWaslNQxNJpQ:_confluent-command-0 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:03:21,329] INFO [MergedLog partition=_confluent-command-0, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-command-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:03:21,329] INFO [Broker id=1] Leader _confluent-command-0 with topic id Some(RJ8JLOQ-RYOWaslNQxNJpQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:03:21,329] INFO [DynamicConfigPublisher broker id=1] Updating topic _confluent-command with new configuration : cleanup.policy -> compact,min.insync.replicas -> 1 (kafka.server.metadata.DynamicConfigPublisher)
benchi-kafka     | [2025-03-18 16:03:21,333] INFO These configurations '[confluent.metrics.topic.replicas, confluent.metrics.reporter.bootstrap.servers, jmx.hostname, cluster.link.metadata.topic.replication.factor, jmx.port]' were supplied but are not used yet. (org.apache.kafka.clients.consumer.ConsumerConfig)
benchi-kafka     | [2025-03-18 16:03:21,333] INFO Kafka version: 7.8.1-ce (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:03:21,333] INFO Kafka commitId: eefb51c6f2f1b888b8b03296353338afbac8e213 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:03:21,333] INFO Kafka startTimeMs: 1742313801333 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:03:21,335] INFO [Consumer clientId=_confluent-license-consumer-1, groupId=null] Cluster ID: MkU3OEVBNTcwNTJENDM2Qk (org.apache.kafka.clients.Metadata)
benchi-kafka     | [2025-03-18 16:03:21,339] INFO App info kafka.consumer for _confluent-license-consumer-1 unregistered (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:03:21,339] INFO ProducerConfig values: 
benchi-kafka     | 	acks = -1
benchi-kafka     | 	auto.include.jmx.reporter = true
benchi-kafka     | 	batch.size = 16384
benchi-kafka     | 	bootstrap.servers = [broker:29092]
benchi-kafka     | 	buffer.memory = 33554432
benchi-kafka     | 	client.dns.lookup = use_all_dns_ips
benchi-kafka     | 	client.id = _confluent-license-producer-1
benchi-kafka     | 	compression.gzip.level = -1
benchi-kafka     | 	compression.lz4.level = 9
benchi-kafka     | 	compression.type = none
benchi-kafka     | 	compression.zstd.level = 3
benchi-kafka     | 	confluent.lkc.id = null
benchi-kafka     | 	confluent.proxy.protocol.client.address = null
benchi-kafka     | 	confluent.proxy.protocol.client.mode = PROXY
benchi-kafka     | 	confluent.proxy.protocol.client.port = null
benchi-kafka     | 	confluent.proxy.protocol.client.version = NONE
benchi-kafka     | 	connections.max.idle.ms = 540000
benchi-kafka     | 	delivery.timeout.ms = 120000
benchi-kafka     | 	enable.idempotence = false
benchi-kafka     | 	enable.metrics.push = true
benchi-kafka     | 	interceptor.classes = []
benchi-kafka     | 	key.serializer = class io.confluent.license.LicenseStore$LicenseKeySerde
benchi-kafka     | 	linger.ms = 0
benchi-kafka     | 	max.block.ms = 60000
benchi-kafka     | 	max.in.flight.requests.per.connection = 1
benchi-kafka     | 	max.request.size = 1048576
benchi-kafka     | 	metadata.max.age.ms = 300000
benchi-kafka     | 	metadata.max.idle.ms = 300000
benchi-kafka     | 	metadata.recovery.strategy = none
benchi-kafka     | 	metric.reporters = []
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.recording.level = INFO
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	partitioner.adaptive.partitioning.enable = true
benchi-kafka     | 	partitioner.availability.timeout.ms = 0
benchi-kafka     | 	partitioner.class = null
benchi-kafka     | 	partitioner.ignore.keys = false
benchi-kafka     | 	receive.buffer.bytes = 32768
benchi-kafka     | 	reconnect.backoff.max.ms = 1000
benchi-kafka     | 	reconnect.backoff.ms = 50
benchi-kafka     | 	request.timeout.ms = 30000
benchi-kafka     | 	retries = 2147483647
benchi-kafka     | 	retry.backoff.max.ms = 1000
benchi-kafka     | 	retry.backoff.ms = 100
benchi-kafka     | 	sasl.client.callback.handler.class = null
benchi-kafka     | 	sasl.jaas.config = null
benchi-kafka     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
benchi-kafka     | 	sasl.kerberos.min.time.before.relogin = 60000
benchi-kafka     | 	sasl.kerberos.service.name = null
benchi-kafka     | 	sasl.kerberos.ticket.renew.jitter = 0.05
benchi-kafka     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
benchi-kafka     | 	sasl.login.callback.handler.class = null
benchi-kafka     | 	sasl.login.class = null
benchi-kafka     | 	sasl.login.connect.timeout.ms = null
benchi-kafka     | 	sasl.login.read.timeout.ms = null
benchi-kafka     | 	sasl.login.refresh.buffer.seconds = 300
benchi-kafka     | 	sasl.login.refresh.min.period.seconds = 60
benchi-kafka     | 	sasl.login.refresh.window.factor = 0.8
benchi-kafka     | 	sasl.login.refresh.window.jitter = 0.05
benchi-kafka     | 	sasl.login.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.login.retry.backoff.ms = 100
benchi-kafka     | 	sasl.mechanism = GSSAPI
benchi-kafka     | 	sasl.oauthbearer.clientassertion.audience = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.expiration = 5
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.jti.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.nbf.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.issuer = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.location = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key.passphrase = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.subject = null
benchi-kafka     | 	sasl.oauthbearer.clock.skew.seconds = 30
benchi-kafka     | 	sasl.oauthbearer.expected.audience = null
benchi-kafka     | 	sasl.oauthbearer.expected.issuer = null
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.url = null
benchi-kafka     | 	sasl.oauthbearer.scope.claim.name = scope
benchi-kafka     | 	sasl.oauthbearer.sub.claim.name = sub
benchi-kafka     | 	sasl.oauthbearer.token.endpoint.url = null
benchi-kafka     | 	security.protocol = PLAINTEXT
benchi-kafka     | 	security.providers = null
benchi-kafka     | 	send.buffer.bytes = 131072
benchi-kafka     | 	socket.connection.setup.timeout.max.ms = 30000
benchi-kafka     | 	socket.connection.setup.timeout.ms = 10000
benchi-kafka     | 	ssl.cipher.suites = null
benchi-kafka     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	ssl.engine.factory.class = null
benchi-kafka     | 	ssl.key.password = null
benchi-kafka     | 	ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	ssl.keystore.certificate.chain = null
benchi-kafka     | 	ssl.keystore.key = null
benchi-kafka     | 	ssl.keystore.location = null
benchi-kafka     | 	ssl.keystore.password = null
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.protocol = TLSv1.3
benchi-kafka     | 	ssl.provider = null
benchi-kafka     | 	ssl.secure.random.implementation = null
benchi-kafka     | 	ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	ssl.truststore.certificates = null
benchi-kafka     | 	ssl.truststore.location = null
benchi-kafka     | 	ssl.truststore.password = null
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     | 	transaction.timeout.ms = 60000
benchi-kafka     | 	transactional.id = null
benchi-kafka     | 	value.serializer = class io.confluent.license.LicenseStore$LicenseMessageSerde
benchi-kafka     |  (org.apache.kafka.clients.producer.ProducerConfig)
benchi-kafka     | [2025-03-18 16:03:21,339] INFO initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector)
benchi-kafka     | [2025-03-18 16:03:21,340] INFO These configurations '[confluent.metrics.topic.replicas, confluent.metrics.reporter.bootstrap.servers, jmx.hostname, cluster.link.metadata.topic.replication.factor, jmx.port]' were supplied but are not used yet. (org.apache.kafka.clients.producer.ProducerConfig)
benchi-kafka     | [2025-03-18 16:03:21,340] INFO Kafka version: 7.8.1-ce (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:03:21,340] INFO Kafka commitId: eefb51c6f2f1b888b8b03296353338afbac8e213 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:03:21,340] INFO Kafka startTimeMs: 1742313801340 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:03:21,340] INFO ConsumerConfig values: 
benchi-kafka     | 	allow.auto.create.topics = true
benchi-kafka     | 	auto.commit.interval.ms = 5000
benchi-kafka     | 	auto.include.jmx.reporter = true
benchi-kafka     | 	auto.offset.reset = earliest
benchi-kafka     | 	bootstrap.servers = [broker:29092]
benchi-kafka     | 	check.crcs = true
benchi-kafka     | 	client.dns.lookup = use_all_dns_ips
benchi-kafka     | 	client.id = _confluent-license-consumer-1
benchi-kafka     | 	client.rack = 
benchi-kafka     | 	confluent.lkc.id = null
benchi-kafka     | 	confluent.proxy.protocol.client.address = null
benchi-kafka     | 	confluent.proxy.protocol.client.mode = PROXY
benchi-kafka     | 	confluent.proxy.protocol.client.port = null
benchi-kafka     | 	confluent.proxy.protocol.client.version = NONE
benchi-kafka     | 	connections.max.idle.ms = 540000
benchi-kafka     | 	default.api.timeout.ms = 60000
benchi-kafka     | 	enable.auto.commit = false
benchi-kafka     | 	enable.metrics.push = true
benchi-kafka     | 	exclude.internal.topics = true
benchi-kafka     | 	fetch.max.bytes = 52428800
benchi-kafka     | 	fetch.max.wait.ms = 500
benchi-kafka     | 	fetch.min.bytes = 1
benchi-kafka     | 	group.id = null
benchi-kafka     | 	group.instance.id = null
benchi-kafka     | 	group.protocol = classic
benchi-kafka     | 	group.remote.assignor = null
benchi-kafka     | 	heartbeat.interval.ms = 3000
benchi-kafka     | 	interceptor.classes = []
benchi-kafka     | 	internal.leave.group.on.close = true
benchi-kafka     | 	internal.throw.on.fetch.stable.offset.unsupported = false
benchi-kafka     | 	isolation.level = read_uncommitted
benchi-kafka     | 	key.deserializer = class io.confluent.license.LicenseStore$LicenseKeySerde
benchi-kafka     | 	max.partition.fetch.bytes = 1048576
benchi-kafka     | 	max.poll.interval.ms = 300000
benchi-kafka     | 	max.poll.records = 500
benchi-kafka     | 	metadata.max.age.ms = 300000
benchi-kafka     | 	metadata.recovery.strategy = none
benchi-kafka     | 	metric.reporters = []
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.recording.level = INFO
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
benchi-kafka     | 	receive.buffer.bytes = 65536
benchi-kafka     | 	reconnect.backoff.max.ms = 1000
benchi-kafka     | 	reconnect.backoff.ms = 50
benchi-kafka     | 	request.timeout.ms = 120000
benchi-kafka     | 	retry.backoff.max.ms = 1000
benchi-kafka     | 	retry.backoff.ms = 100
benchi-kafka     | 	sasl.client.callback.handler.class = null
benchi-kafka     | 	sasl.jaas.config = null
benchi-kafka     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
benchi-kafka     | 	sasl.kerberos.min.time.before.relogin = 60000
benchi-kafka     | 	sasl.kerberos.service.name = null
benchi-kafka     | 	sasl.kerberos.ticket.renew.jitter = 0.05
benchi-kafka     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
benchi-kafka     | 	sasl.login.callback.handler.class = null
benchi-kafka     | 	sasl.login.class = null
benchi-kafka     | 	sasl.login.connect.timeout.ms = null
benchi-kafka     | 	sasl.login.read.timeout.ms = null
benchi-kafka     | 	sasl.login.refresh.buffer.seconds = 300
benchi-kafka     | 	sasl.login.refresh.min.period.seconds = 60
benchi-kafka     | 	sasl.login.refresh.window.factor = 0.8
benchi-kafka     | 	sasl.login.refresh.window.jitter = 0.05
benchi-kafka     | 	sasl.login.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.login.retry.backoff.ms = 100
benchi-kafka     | 	sasl.mechanism = GSSAPI
benchi-kafka     | 	sasl.oauthbearer.clientassertion.audience = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.expiration = 5
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.jti.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.nbf.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.issuer = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.location = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key.passphrase = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.subject = null
benchi-kafka     | 	sasl.oauthbearer.clock.skew.seconds = 30
benchi-kafka     | 	sasl.oauthbearer.expected.audience = null
benchi-kafka     | 	sasl.oauthbearer.expected.issuer = null
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.url = null
benchi-kafka     | 	sasl.oauthbearer.scope.claim.name = scope
benchi-kafka     | 	sasl.oauthbearer.sub.claim.name = sub
benchi-kafka     | 	sasl.oauthbearer.token.endpoint.url = null
benchi-kafka     | 	security.protocol = PLAINTEXT
benchi-kafka     | 	security.providers = null
benchi-kafka     | 	send.buffer.bytes = 131072
benchi-kafka     | 	session.timeout.ms = 45000
benchi-kafka     | 	socket.connection.setup.timeout.max.ms = 30000
benchi-kafka     | 	socket.connection.setup.timeout.ms = 10000
benchi-kafka     | 	ssl.cipher.suites = null
benchi-kafka     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	ssl.engine.factory.class = null
benchi-kafka     | 	ssl.key.password = null
benchi-kafka     | 	ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	ssl.keystore.certificate.chain = null
benchi-kafka     | 	ssl.keystore.key = null
benchi-kafka     | 	ssl.keystore.location = null
benchi-kafka     | 	ssl.keystore.password = null
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.protocol = TLSv1.3
benchi-kafka     | 	ssl.provider = null
benchi-kafka     | 	ssl.secure.random.implementation = null
benchi-kafka     | 	ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	ssl.truststore.certificates = null
benchi-kafka     | 	ssl.truststore.location = null
benchi-kafka     | 	ssl.truststore.password = null
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     | 	value.deserializer = class io.confluent.license.LicenseStore$LicenseMessageSerde
benchi-kafka     |  (org.apache.kafka.clients.consumer.ConsumerConfig)
benchi-kafka     | [2025-03-18 16:03:21,340] INFO initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector)
benchi-kafka     | [2025-03-18 16:03:21,341] INFO These configurations '[confluent.metrics.topic.replicas, confluent.metrics.reporter.bootstrap.servers, jmx.hostname, cluster.link.metadata.topic.replication.factor, jmx.port]' were supplied but are not used yet. (org.apache.kafka.clients.consumer.ConsumerConfig)
benchi-kafka     | [2025-03-18 16:03:21,341] INFO Kafka version: 7.8.1-ce (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:03:21,341] INFO Kafka commitId: eefb51c6f2f1b888b8b03296353338afbac8e213 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:03:21,341] INFO Kafka startTimeMs: 1742313801341 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:03:21,343] INFO [Producer clientId=_confluent-license-producer-1] Cluster ID: MkU3OEVBNTcwNTJENDM2Qk (org.apache.kafka.clients.Metadata)
benchi-kafka     | [2025-03-18 16:03:21,344] INFO [Consumer clientId=_confluent-license-consumer-1, groupId=null] Cluster ID: MkU3OEVBNTcwNTJENDM2Qk (org.apache.kafka.clients.Metadata)
benchi-kafka     | [2025-03-18 16:03:21,345] INFO [Consumer clientId=_confluent-license-consumer-1, groupId=null] Assigned to partition(s): _confluent-command-0 (org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer)
benchi-kafka     | [2025-03-18 16:03:21,347] INFO [Consumer clientId=_confluent-license-consumer-1, groupId=null] Seeking to earliest offset of partition _confluent-command-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
benchi-kafka     | [2025-03-18 16:03:21,359] INFO Finished reading KafkaBasedLog for topic _confluent-command (org.apache.kafka.connect.util.KafkaBasedLog)
benchi-kafka     | [2025-03-18 16:03:21,359] INFO Started KafkaBasedLog for topic _confluent-command (org.apache.kafka.connect.util.KafkaBasedLog)
benchi-kafka     | [2025-03-18 16:03:21,359] INFO Started License Store (io.confluent.license.LicenseStore)
benchi-kafka     | [2025-03-18 16:03:21,576] INFO Waiting for 2 seconds for metric reporter topic _confluent-telemetry-metrics to become available. (io.confluent.cruisecontrol.metricsreporter.ConfluentMetricsSamplerBase)
benchi-kafka     | [2025-03-18 16:03:21,898] INFO AdminClientConfig values: 
benchi-kafka     | 	auto.include.jmx.reporter = true
benchi-kafka     | 	bootstrap.controllers = []
benchi-kafka     | 	bootstrap.servers = [broker:29092]
benchi-kafka     | 	client.dns.lookup = use_all_dns_ips
benchi-kafka     | 	client.id = _confluent-license-admin-1
benchi-kafka     | 	confluent.lkc.id = null
benchi-kafka     | 	confluent.metrics.reporter.bootstrap.servers = localhost:9092
benchi-kafka     | 	confluent.proxy.protocol.client.address = null
benchi-kafka     | 	confluent.proxy.protocol.client.mode = PROXY
benchi-kafka     | 	confluent.proxy.protocol.client.port = null
benchi-kafka     | 	confluent.proxy.protocol.client.version = NONE
benchi-kafka     | 	connections.max.idle.ms = 300000
benchi-kafka     | 	default.api.timeout.ms = 60000
benchi-kafka     | 	enable.metrics.push = true
benchi-kafka     | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
benchi-kafka     | 	metadata.max.age.ms = 300000
benchi-kafka     | 	metadata.recovery.strategy = none
benchi-kafka     | 	metric.reporters = []
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.recording.level = INFO
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	receive.buffer.bytes = 65536
benchi-kafka     | 	reconnect.backoff.max.ms = 1000
benchi-kafka     | 	reconnect.backoff.ms = 50
benchi-kafka     | 	request.timeout.ms = 30000
benchi-kafka     | 	retries = 2147483647
benchi-kafka     | 	retry.backoff.max.ms = 1000
benchi-kafka     | 	retry.backoff.ms = 100
benchi-kafka     | 	sasl.client.callback.handler.class = null
benchi-kafka     | 	sasl.jaas.config = null
benchi-kafka     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
benchi-kafka     | 	sasl.kerberos.min.time.before.relogin = 60000
benchi-kafka     | 	sasl.kerberos.service.name = null
benchi-kafka     | 	sasl.kerberos.ticket.renew.jitter = 0.05
benchi-kafka     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
benchi-kafka     | 	sasl.login.callback.handler.class = null
benchi-kafka     | 	sasl.login.class = null
benchi-kafka     | 	sasl.login.connect.timeout.ms = null
benchi-kafka     | 	sasl.login.read.timeout.ms = null
benchi-kafka     | 	sasl.login.refresh.buffer.seconds = 300
benchi-kafka     | 	sasl.login.refresh.min.period.seconds = 60
benchi-kafka     | 	sasl.login.refresh.window.factor = 0.8
benchi-kafka     | 	sasl.login.refresh.window.jitter = 0.05
benchi-kafka     | 	sasl.login.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.login.retry.backoff.ms = 100
benchi-kafka     | 	sasl.mechanism = GSSAPI
benchi-kafka     | 	sasl.oauthbearer.clientassertion.audience = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.expiration = 5
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.jti.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.nbf.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.issuer = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.location = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key.passphrase = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.subject = null
benchi-kafka     | 	sasl.oauthbearer.clock.skew.seconds = 30
benchi-kafka     | 	sasl.oauthbearer.expected.audience = null
benchi-kafka     | 	sasl.oauthbearer.expected.issuer = null
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.url = null
benchi-kafka     | 	sasl.oauthbearer.scope.claim.name = scope
benchi-kafka     | 	sasl.oauthbearer.sub.claim.name = sub
benchi-kafka     | 	sasl.oauthbearer.token.endpoint.url = null
benchi-kafka     | 	security.protocol = PLAINTEXT
benchi-kafka     | 	security.providers = null
benchi-kafka     | 	send.buffer.bytes = 131072
benchi-kafka     | 	socket.connection.setup.timeout.max.ms = 30000
benchi-kafka     | 	socket.connection.setup.timeout.ms = 10000
benchi-kafka     | 	ssl.cipher.suites = null
benchi-kafka     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	ssl.engine.factory.class = null
benchi-kafka     | 	ssl.key.password = null
benchi-kafka     | 	ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	ssl.keystore.certificate.chain = null
benchi-kafka     | 	ssl.keystore.key = null
benchi-kafka     | 	ssl.keystore.location = null
benchi-kafka     | 	ssl.keystore.password = null
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.protocol = TLSv1.3
benchi-kafka     | 	ssl.provider = null
benchi-kafka     | 	ssl.secure.random.implementation = null
benchi-kafka     | 	ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	ssl.truststore.certificates = null
benchi-kafka     | 	ssl.truststore.location = null
benchi-kafka     | 	ssl.truststore.password = null
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     |  (org.apache.kafka.clients.admin.AdminClientConfig)
benchi-kafka     | [2025-03-18 16:03:21,899] INFO These configurations '[confluent.metrics.topic.replicas, replication.factor, confluent.metrics.reporter.bootstrap.servers, jmx.hostname, min.insync.replicas, cluster.link.metadata.topic.replication.factor, jmx.port]' were supplied but are not used yet. (org.apache.kafka.clients.admin.AdminClientConfig)
benchi-kafka     | [2025-03-18 16:03:21,899] INFO Kafka version: 7.8.1-ce (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:03:21,899] INFO Kafka commitId: eefb51c6f2f1b888b8b03296353338afbac8e213 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:03:21,899] INFO Kafka startTimeMs: 1742313801899 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:03:21,905] INFO App info kafka.admin.client for _confluent-license-admin-1 unregistered (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:03:21,943] INFO AdminClientConfig values: 
benchi-kafka     | 	auto.include.jmx.reporter = true
benchi-kafka     | 	bootstrap.controllers = []
benchi-kafka     | 	bootstrap.servers = [broker:29092]
benchi-kafka     | 	client.dns.lookup = use_all_dns_ips
benchi-kafka     | 	client.id = _confluent-license-admin-1
benchi-kafka     | 	confluent.lkc.id = null
benchi-kafka     | 	confluent.metrics.reporter.bootstrap.servers = localhost:9092
benchi-kafka     | 	confluent.proxy.protocol.client.address = null
benchi-kafka     | 	confluent.proxy.protocol.client.mode = PROXY
benchi-kafka     | 	confluent.proxy.protocol.client.port = null
benchi-kafka     | 	confluent.proxy.protocol.client.version = NONE
benchi-kafka     | 	connections.max.idle.ms = 300000
benchi-kafka     | 	default.api.timeout.ms = 60000
benchi-kafka     | 	enable.metrics.push = true
benchi-kafka     | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
benchi-kafka     | 	metadata.max.age.ms = 300000
benchi-kafka     | 	metadata.recovery.strategy = none
benchi-kafka     | 	metric.reporters = []
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.recording.level = INFO
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	receive.buffer.bytes = 65536
benchi-kafka     | 	reconnect.backoff.max.ms = 1000
benchi-kafka     | 	reconnect.backoff.ms = 50
benchi-kafka     | 	request.timeout.ms = 30000
benchi-kafka     | 	retries = 2147483647
benchi-kafka     | 	retry.backoff.max.ms = 1000
benchi-kafka     | 	retry.backoff.ms = 100
benchi-kafka     | 	sasl.client.callback.handler.class = null
benchi-kafka     | 	sasl.jaas.config = null
benchi-kafka     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
benchi-kafka     | 	sasl.kerberos.min.time.before.relogin = 60000
benchi-kafka     | 	sasl.kerberos.service.name = null
benchi-kafka     | 	sasl.kerberos.ticket.renew.jitter = 0.05
benchi-kafka     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
benchi-kafka     | 	sasl.login.callback.handler.class = null
benchi-kafka     | 	sasl.login.class = null
benchi-kafka     | 	sasl.login.connect.timeout.ms = null
benchi-kafka     | 	sasl.login.read.timeout.ms = null
benchi-kafka     | 	sasl.login.refresh.buffer.seconds = 300
benchi-kafka     | 	sasl.login.refresh.min.period.seconds = 60
benchi-kafka     | 	sasl.login.refresh.window.factor = 0.8
benchi-kafka     | 	sasl.login.refresh.window.jitter = 0.05
benchi-kafka     | 	sasl.login.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.login.retry.backoff.ms = 100
benchi-kafka     | 	sasl.mechanism = GSSAPI
benchi-kafka     | 	sasl.oauthbearer.clientassertion.audience = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.expiration = 5
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.jti.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.nbf.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.issuer = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.location = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key.passphrase = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.subject = null
benchi-kafka     | 	sasl.oauthbearer.clock.skew.seconds = 30
benchi-kafka     | 	sasl.oauthbearer.expected.audience = null
benchi-kafka     | 	sasl.oauthbearer.expected.issuer = null
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.url = null
benchi-kafka     | 	sasl.oauthbearer.scope.claim.name = scope
benchi-kafka     | 	sasl.oauthbearer.sub.claim.name = sub
benchi-kafka     | 	sasl.oauthbearer.token.endpoint.url = null
benchi-kafka     | 	security.protocol = PLAINTEXT
benchi-kafka     | 	security.providers = null
benchi-kafka     | 	send.buffer.bytes = 131072
benchi-kafka     | 	socket.connection.setup.timeout.max.ms = 30000
benchi-kafka     | 	socket.connection.setup.timeout.ms = 10000
benchi-kafka     | 	ssl.cipher.suites = null
benchi-kafka     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	ssl.engine.factory.class = null
benchi-kafka     | 	ssl.key.password = null
benchi-kafka     | 	ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	ssl.keystore.certificate.chain = null
benchi-kafka     | 	ssl.keystore.key = null
benchi-kafka     | 	ssl.keystore.location = null
benchi-kafka     | 	ssl.keystore.password = null
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.protocol = TLSv1.3
benchi-kafka     | 	ssl.provider = null
benchi-kafka     | 	ssl.secure.random.implementation = null
benchi-kafka     | 	ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	ssl.truststore.certificates = null
benchi-kafka     | 	ssl.truststore.location = null
benchi-kafka     | 	ssl.truststore.password = null
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     |  (org.apache.kafka.clients.admin.AdminClientConfig)
benchi-kafka     | [2025-03-18 16:03:21,944] INFO These configurations '[confluent.metrics.topic.replicas, replication.factor, confluent.metrics.reporter.bootstrap.servers, jmx.hostname, min.insync.replicas, cluster.link.metadata.topic.replication.factor, jmx.port]' were supplied but are not used yet. (org.apache.kafka.clients.admin.AdminClientConfig)
benchi-kafka     | [2025-03-18 16:03:21,944] INFO Kafka version: 7.8.1-ce (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:03:21,944] INFO Kafka commitId: eefb51c6f2f1b888b8b03296353338afbac8e213 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:03:21,944] INFO Kafka startTimeMs: 1742313801944 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:03:21,948] INFO App info kafka.admin.client for _confluent-license-admin-1 unregistered (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:03:21,949] INFO License for single cluster, single node (io.confluent.license.LicenseManager)
benchi-kafka     | [2025-03-18 16:03:21,951] INFO [BrokerServer id=1] Transition from STARTING to STARTED (kafka.server.BrokerServer)
benchi-kafka     | [2025-03-18 16:03:21,951] INFO Kafka version: 7.8.1-ce (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:03:21,951] INFO Kafka commitId: eefb51c6f2f1b888b8b03296353338afbac8e213 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:03:21,951] INFO Kafka startTimeMs: 1742313801951 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:03:21,951] INFO [KafkaRaftServer nodeId=1] Kafka Server started (kafka.server.KafkaRaftServer)
benchi-kafka     | [2025-03-18 16:03:23,587] INFO Waiting for 4 seconds for metric reporter topic _confluent-telemetry-metrics to become available. (io.confluent.cruisecontrol.metricsreporter.ConfluentMetricsSamplerBase)
benchi-kafka     | [2025-03-18 16:03:27,591] INFO Waiting for 8 seconds for metric reporter topic _confluent-telemetry-metrics to become available. (io.confluent.cruisecontrol.metricsreporter.ConfluentMetricsSamplerBase)
benchi-kafka     | [2025-03-18 16:03:35,295] INFO AdminClientConfig values: 
benchi-kafka     | 	auto.include.jmx.reporter = true
benchi-kafka     | 	bootstrap.controllers = []
benchi-kafka     | 	bootstrap.servers = [localhost:9092]
benchi-kafka     | 	client.dns.lookup = use_all_dns_ips
benchi-kafka     | 	client.id = 
benchi-kafka     | 	confluent.lkc.id = null
benchi-kafka     | 	confluent.metrics.reporter.bootstrap.servers = kafka-0:9071
benchi-kafka     | 	confluent.proxy.protocol.client.address = null
benchi-kafka     | 	confluent.proxy.protocol.client.mode = PROXY
benchi-kafka     | 	confluent.proxy.protocol.client.port = null
benchi-kafka     | 	confluent.proxy.protocol.client.version = NONE
benchi-kafka     | 	connections.max.idle.ms = 300000
benchi-kafka     | 	default.api.timeout.ms = 60000
benchi-kafka     | 	enable.metrics.push = true
benchi-kafka     | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
benchi-kafka     | 	metadata.max.age.ms = 300000
benchi-kafka     | 	metadata.recovery.strategy = none
benchi-kafka     | 	metric.reporters = []
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.recording.level = INFO
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	receive.buffer.bytes = 65536
benchi-kafka     | 	reconnect.backoff.max.ms = 1000
benchi-kafka     | 	reconnect.backoff.ms = 50
benchi-kafka     | 	request.timeout.ms = 30000
benchi-kafka     | 	retries = 2147483647
benchi-kafka     | 	retry.backoff.max.ms = 1000
benchi-kafka     | 	retry.backoff.ms = 100
benchi-kafka     | 	sasl.client.callback.handler.class = null
benchi-kafka     | 	sasl.jaas.config = null
benchi-kafka     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
benchi-kafka     | 	sasl.kerberos.min.time.before.relogin = 60000
benchi-kafka     | 	sasl.kerberos.service.name = null
benchi-kafka     | 	sasl.kerberos.ticket.renew.jitter = 0.05
benchi-kafka     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
benchi-kafka     | 	sasl.login.callback.handler.class = null
benchi-kafka     | 	sasl.login.class = null
benchi-kafka     | 	sasl.login.connect.timeout.ms = null
benchi-kafka     | 	sasl.login.read.timeout.ms = null
benchi-kafka     | 	sasl.login.refresh.buffer.seconds = 300
benchi-kafka     | 	sasl.login.refresh.min.period.seconds = 60
benchi-kafka     | 	sasl.login.refresh.window.factor = 0.8
benchi-kafka     | 	sasl.login.refresh.window.jitter = 0.05
benchi-kafka     | 	sasl.login.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.login.retry.backoff.ms = 100
benchi-kafka     | 	sasl.mechanism = GSSAPI
benchi-kafka     | 	sasl.oauthbearer.clientassertion.audience = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.expiration = 5
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.jti.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.nbf.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.issuer = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.location = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key.passphrase = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.subject = null
benchi-kafka     | 	sasl.oauthbearer.clock.skew.seconds = 30
benchi-kafka     | 	sasl.oauthbearer.expected.audience = null
benchi-kafka     | 	sasl.oauthbearer.expected.issuer = null
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.url = null
benchi-kafka     | 	sasl.oauthbearer.scope.claim.name = scope
benchi-kafka     | 	sasl.oauthbearer.sub.claim.name = sub
benchi-kafka     | 	sasl.oauthbearer.token.endpoint.url = null
benchi-kafka     | 	security.protocol = PLAINTEXT
benchi-kafka     | 	security.providers = null
benchi-kafka     | 	send.buffer.bytes = 131072
benchi-kafka     | 	socket.connection.setup.timeout.max.ms = 30000
benchi-kafka     | 	socket.connection.setup.timeout.ms = 10000
benchi-kafka     | 	ssl.cipher.suites = null
benchi-kafka     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	ssl.engine.factory.class = null
benchi-kafka     | 	ssl.key.password = null
benchi-kafka     | 	ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	ssl.keystore.certificate.chain = null
benchi-kafka     | 	ssl.keystore.key = null
benchi-kafka     | 	ssl.keystore.location = null
benchi-kafka     | 	ssl.keystore.password = null
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.protocol = TLSv1.3
benchi-kafka     | 	ssl.provider = null
benchi-kafka     | 	ssl.secure.random.implementation = null
benchi-kafka     | 	ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	ssl.truststore.certificates = null
benchi-kafka     | 	ssl.truststore.location = null
benchi-kafka     | 	ssl.truststore.password = null
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     |  (org.apache.kafka.clients.admin.AdminClientConfig)
benchi-kafka     | [2025-03-18 16:03:35,296] INFO Kafka version: 7.8.1-ce (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:03:35,296] INFO Kafka commitId: eefb51c6f2f1b888b8b03296353338afbac8e213 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:03:35,296] INFO Kafka startTimeMs: 1742313815296 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:03:35,300] INFO [AdminClient clientId=adminclient-1] The DescribeTopicPartitions API is not supported, using Metadata API to describe topics. (org.apache.kafka.clients.admin.KafkaAdminClient)
benchi-kafka     | [2025-03-18 16:03:35,303] INFO [ControllerServer id=1] CreateTopics result(s): CreatableTopic(name='_confluent-metrics', numPartitions=12, replicationFactor=3, assignments=[], configs=[CreateableTopicConfig(name='max.message.bytes', value='10485760'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='259200000'), CreateableTopicConfig(name='segment.ms', value='14400000'), CreateableTopicConfig(name='retention.bytes', value='-1')], linkName=null, mirrorTopic=null, sourceTopicId=AAAAAAAAAAAAAAAAAAAAAA, mirrorStartOffsetSpec=-9223372036854775808, mirrorStartOffsets=[], stoppedSequenceNumber=0): INVALID_REPLICATION_FACTOR (Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.) (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:03:35,304] INFO App info kafka.admin.client for adminclient-1 unregistered (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:03:35,305] ERROR Error checking or creating metrics topic (io.confluent.metrics.reporter.ConfluentMetricsReporter)
benchi-kafka     | org.apache.kafka.common.errors.InvalidReplicationFactorException: Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.
benchi-kafka     | [2025-03-18 16:03:35,595] INFO Waiting for 16 seconds for metric reporter topic _confluent-telemetry-metrics to become available. (io.confluent.cruisecontrol.metricsreporter.ConfluentMetricsSamplerBase)
benchi-kafka     | [2025-03-18 16:03:50,296] INFO AdminClientConfig values: 
benchi-kafka     | 	auto.include.jmx.reporter = true
benchi-kafka     | 	bootstrap.controllers = []
benchi-kafka     | 	bootstrap.servers = [localhost:9092]
benchi-kafka     | 	client.dns.lookup = use_all_dns_ips
benchi-kafka     | 	client.id = 
benchi-kafka     | 	confluent.lkc.id = null
benchi-kafka     | 	confluent.metrics.reporter.bootstrap.servers = kafka-0:9071
benchi-kafka     | 	confluent.proxy.protocol.client.address = null
benchi-kafka     | 	confluent.proxy.protocol.client.mode = PROXY
benchi-kafka     | 	confluent.proxy.protocol.client.port = null
benchi-kafka     | 	confluent.proxy.protocol.client.version = NONE
benchi-kafka     | 	connections.max.idle.ms = 300000
benchi-kafka     | 	default.api.timeout.ms = 60000
benchi-kafka     | 	enable.metrics.push = true
benchi-kafka     | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
benchi-kafka     | 	metadata.max.age.ms = 300000
benchi-kafka     | 	metadata.recovery.strategy = none
benchi-kafka     | 	metric.reporters = []
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.recording.level = INFO
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	receive.buffer.bytes = 65536
benchi-kafka     | 	reconnect.backoff.max.ms = 1000
benchi-kafka     | 	reconnect.backoff.ms = 50
benchi-kafka     | 	request.timeout.ms = 30000
benchi-kafka     | 	retries = 2147483647
benchi-kafka     | 	retry.backoff.max.ms = 1000
benchi-kafka     | 	retry.backoff.ms = 100
benchi-kafka     | 	sasl.client.callback.handler.class = null
benchi-kafka     | 	sasl.jaas.config = null
benchi-kafka     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
benchi-kafka     | 	sasl.kerberos.min.time.before.relogin = 60000
benchi-kafka     | 	sasl.kerberos.service.name = null
benchi-kafka     | 	sasl.kerberos.ticket.renew.jitter = 0.05
benchi-kafka     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
benchi-kafka     | 	sasl.login.callback.handler.class = null
benchi-kafka     | 	sasl.login.class = null
benchi-kafka     | 	sasl.login.connect.timeout.ms = null
benchi-kafka     | 	sasl.login.read.timeout.ms = null
benchi-kafka     | 	sasl.login.refresh.buffer.seconds = 300
benchi-kafka     | 	sasl.login.refresh.min.period.seconds = 60
benchi-kafka     | 	sasl.login.refresh.window.factor = 0.8
benchi-kafka     | 	sasl.login.refresh.window.jitter = 0.05
benchi-kafka     | 	sasl.login.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.login.retry.backoff.ms = 100
benchi-kafka     | 	sasl.mechanism = GSSAPI
benchi-kafka     | 	sasl.oauthbearer.clientassertion.audience = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.expiration = 5
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.jti.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.nbf.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.issuer = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.location = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key.passphrase = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.subject = null
benchi-kafka     | 	sasl.oauthbearer.clock.skew.seconds = 30
benchi-kafka     | 	sasl.oauthbearer.expected.audience = null
benchi-kafka     | 	sasl.oauthbearer.expected.issuer = null
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.url = null
benchi-kafka     | 	sasl.oauthbearer.scope.claim.name = scope
benchi-kafka     | 	sasl.oauthbearer.sub.claim.name = sub
benchi-kafka     | 	sasl.oauthbearer.token.endpoint.url = null
benchi-kafka     | 	security.protocol = PLAINTEXT
benchi-kafka     | 	security.providers = null
benchi-kafka     | 	send.buffer.bytes = 131072
benchi-kafka     | 	socket.connection.setup.timeout.max.ms = 30000
benchi-kafka     | 	socket.connection.setup.timeout.ms = 10000
benchi-kafka     | 	ssl.cipher.suites = null
benchi-kafka     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	ssl.engine.factory.class = null
benchi-kafka     | 	ssl.key.password = null
benchi-kafka     | 	ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	ssl.keystore.certificate.chain = null
benchi-kafka     | 	ssl.keystore.key = null
benchi-kafka     | 	ssl.keystore.location = null
benchi-kafka     | 	ssl.keystore.password = null
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.protocol = TLSv1.3
benchi-kafka     | 	ssl.provider = null
benchi-kafka     | 	ssl.secure.random.implementation = null
benchi-kafka     | 	ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	ssl.truststore.certificates = null
benchi-kafka     | 	ssl.truststore.location = null
benchi-kafka     | 	ssl.truststore.password = null
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     |  (org.apache.kafka.clients.admin.AdminClientConfig)
benchi-kafka     | [2025-03-18 16:03:50,299] INFO Kafka version: 7.8.1-ce (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:03:50,299] INFO Kafka commitId: eefb51c6f2f1b888b8b03296353338afbac8e213 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:03:50,299] INFO Kafka startTimeMs: 1742313830299 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:03:50,305] INFO Beginning log roller... (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:03:50,305] INFO Log roller completed in 0 seconds (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:03:50,314] INFO [AdminClient clientId=adminclient-2] The DescribeTopicPartitions API is not supported, using Metadata API to describe topics. (org.apache.kafka.clients.admin.KafkaAdminClient)
benchi-kafka     | [2025-03-18 16:03:50,317] INFO [ControllerServer id=1] CreateTopics result(s): CreatableTopic(name='_confluent-metrics', numPartitions=12, replicationFactor=3, assignments=[], configs=[CreateableTopicConfig(name='max.message.bytes', value='10485760'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='259200000'), CreateableTopicConfig(name='segment.ms', value='14400000'), CreateableTopicConfig(name='retention.bytes', value='-1')], linkName=null, mirrorTopic=null, sourceTopicId=AAAAAAAAAAAAAAAAAAAAAA, mirrorStartOffsetSpec=-9223372036854775808, mirrorStartOffsets=[], stoppedSequenceNumber=0): INVALID_REPLICATION_FACTOR (Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.) (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:03:50,319] INFO App info kafka.admin.client for adminclient-2 unregistered (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:03:50,320] ERROR Error checking or creating metrics topic (io.confluent.metrics.reporter.ConfluentMetricsReporter)
benchi-kafka     | org.apache.kafka.common.errors.InvalidReplicationFactorException: Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.
benchi-kafka     | [2025-03-18 16:03:51,607] INFO Waiting for 32 seconds for metric reporter topic _confluent-telemetry-metrics to become available. (io.confluent.cruisecontrol.metricsreporter.ConfluentMetricsSamplerBase)
benchi-kafka     | [2025-03-18 16:04:05,293] INFO AdminClientConfig values: 
benchi-kafka     | 	auto.include.jmx.reporter = true
benchi-kafka     | 	bootstrap.controllers = []
benchi-kafka     | 	bootstrap.servers = [localhost:9092]
benchi-kafka     | 	client.dns.lookup = use_all_dns_ips
benchi-kafka     | 	client.id = 
benchi-kafka     | 	confluent.lkc.id = null
benchi-kafka     | 	confluent.metrics.reporter.bootstrap.servers = kafka-0:9071
benchi-kafka     | 	confluent.proxy.protocol.client.address = null
benchi-kafka     | 	confluent.proxy.protocol.client.mode = PROXY
benchi-kafka     | 	confluent.proxy.protocol.client.port = null
benchi-kafka     | 	confluent.proxy.protocol.client.version = NONE
benchi-kafka     | 	connections.max.idle.ms = 300000
benchi-kafka     | 	default.api.timeout.ms = 60000
benchi-kafka     | 	enable.metrics.push = true
benchi-kafka     | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
benchi-kafka     | 	metadata.max.age.ms = 300000
benchi-kafka     | 	metadata.recovery.strategy = none
benchi-kafka     | 	metric.reporters = []
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.recording.level = INFO
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	receive.buffer.bytes = 65536
benchi-kafka     | 	reconnect.backoff.max.ms = 1000
benchi-kafka     | 	reconnect.backoff.ms = 50
benchi-kafka     | 	request.timeout.ms = 30000
benchi-kafka     | 	retries = 2147483647
benchi-kafka     | 	retry.backoff.max.ms = 1000
benchi-kafka     | 	retry.backoff.ms = 100
benchi-kafka     | 	sasl.client.callback.handler.class = null
benchi-kafka     | 	sasl.jaas.config = null
benchi-kafka     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
benchi-kafka     | 	sasl.kerberos.min.time.before.relogin = 60000
benchi-kafka     | 	sasl.kerberos.service.name = null
benchi-kafka     | 	sasl.kerberos.ticket.renew.jitter = 0.05
benchi-kafka     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
benchi-kafka     | 	sasl.login.callback.handler.class = null
benchi-kafka     | 	sasl.login.class = null
benchi-kafka     | 	sasl.login.connect.timeout.ms = null
benchi-kafka     | 	sasl.login.read.timeout.ms = null
benchi-kafka     | 	sasl.login.refresh.buffer.seconds = 300
benchi-kafka     | 	sasl.login.refresh.min.period.seconds = 60
benchi-kafka     | 	sasl.login.refresh.window.factor = 0.8
benchi-kafka     | 	sasl.login.refresh.window.jitter = 0.05
benchi-kafka     | 	sasl.login.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.login.retry.backoff.ms = 100
benchi-kafka     | 	sasl.mechanism = GSSAPI
benchi-kafka     | 	sasl.oauthbearer.clientassertion.audience = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.expiration = 5
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.jti.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.nbf.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.issuer = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.location = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key.passphrase = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.subject = null
benchi-kafka     | 	sasl.oauthbearer.clock.skew.seconds = 30
benchi-kafka     | 	sasl.oauthbearer.expected.audience = null
benchi-kafka     | 	sasl.oauthbearer.expected.issuer = null
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.url = null
benchi-kafka     | 	sasl.oauthbearer.scope.claim.name = scope
benchi-kafka     | 	sasl.oauthbearer.sub.claim.name = sub
benchi-kafka     | 	sasl.oauthbearer.token.endpoint.url = null
benchi-kafka     | 	security.protocol = PLAINTEXT
benchi-kafka     | 	security.providers = null
benchi-kafka     | 	send.buffer.bytes = 131072
benchi-kafka     | 	socket.connection.setup.timeout.max.ms = 30000
benchi-kafka     | 	socket.connection.setup.timeout.ms = 10000
benchi-kafka     | 	ssl.cipher.suites = null
benchi-kafka     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	ssl.engine.factory.class = null
benchi-kafka     | 	ssl.key.password = null
benchi-kafka     | 	ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	ssl.keystore.certificate.chain = null
benchi-kafka     | 	ssl.keystore.key = null
benchi-kafka     | 	ssl.keystore.location = null
benchi-kafka     | 	ssl.keystore.password = null
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.protocol = TLSv1.3
benchi-kafka     | 	ssl.provider = null
benchi-kafka     | 	ssl.secure.random.implementation = null
benchi-kafka     | 	ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	ssl.truststore.certificates = null
benchi-kafka     | 	ssl.truststore.location = null
benchi-kafka     | 	ssl.truststore.password = null
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     |  (org.apache.kafka.clients.admin.AdminClientConfig)
benchi-kafka     | [2025-03-18 16:04:05,294] INFO Kafka version: 7.8.1-ce (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:04:05,294] INFO Kafka commitId: eefb51c6f2f1b888b8b03296353338afbac8e213 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:04:05,294] INFO Kafka startTimeMs: 1742313845294 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:04:05,298] INFO [AdminClient clientId=adminclient-3] The DescribeTopicPartitions API is not supported, using Metadata API to describe topics. (org.apache.kafka.clients.admin.KafkaAdminClient)
benchi-kafka     | [2025-03-18 16:04:05,300] INFO [ControllerServer id=1] CreateTopics result(s): CreatableTopic(name='_confluent-metrics', numPartitions=12, replicationFactor=3, assignments=[], configs=[CreateableTopicConfig(name='max.message.bytes', value='10485760'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='259200000'), CreateableTopicConfig(name='segment.ms', value='14400000'), CreateableTopicConfig(name='retention.bytes', value='-1')], linkName=null, mirrorTopic=null, sourceTopicId=AAAAAAAAAAAAAAAAAAAAAA, mirrorStartOffsetSpec=-9223372036854775808, mirrorStartOffsets=[], stoppedSequenceNumber=0): INVALID_REPLICATION_FACTOR (Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.) (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:04:05,301] INFO App info kafka.admin.client for adminclient-3 unregistered (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:04:05,302] ERROR Error checking or creating metrics topic (io.confluent.metrics.reporter.ConfluentMetricsReporter)
benchi-kafka     | org.apache.kafka.common.errors.InvalidReplicationFactorException: Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.
benchi-kafka     | [2025-03-18 16:04:19,513] INFO [CelltControllerMetricsPublisher id=1] No cells found. Skipping cell metrics refresh. (org.apache.kafka.controller.metrics.CellControllerMetricsPublisher)
benchi-kafka     | [2025-03-18 16:04:20,300] INFO AdminClientConfig values: 
benchi-kafka     | 	auto.include.jmx.reporter = true
benchi-kafka     | 	bootstrap.controllers = []
benchi-kafka     | 	bootstrap.servers = [localhost:9092]
benchi-kafka     | 	client.dns.lookup = use_all_dns_ips
benchi-kafka     | 	client.id = 
benchi-kafka     | 	confluent.lkc.id = null
benchi-kafka     | 	confluent.metrics.reporter.bootstrap.servers = kafka-0:9071
benchi-kafka     | 	confluent.proxy.protocol.client.address = null
benchi-kafka     | 	confluent.proxy.protocol.client.mode = PROXY
benchi-kafka     | 	confluent.proxy.protocol.client.port = null
benchi-kafka     | 	confluent.proxy.protocol.client.version = NONE
benchi-kafka     | 	connections.max.idle.ms = 300000
benchi-kafka     | 	default.api.timeout.ms = 60000
benchi-kafka     | 	enable.metrics.push = true
benchi-kafka     | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
benchi-kafka     | 	metadata.max.age.ms = 300000
benchi-kafka     | 	metadata.recovery.strategy = none
benchi-kafka     | 	metric.reporters = []
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.recording.level = INFO
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	receive.buffer.bytes = 65536
benchi-kafka     | 	reconnect.backoff.max.ms = 1000
benchi-kafka     | 	reconnect.backoff.ms = 50
benchi-kafka     | 	request.timeout.ms = 30000
benchi-kafka     | 	retries = 2147483647
benchi-kafka     | 	retry.backoff.max.ms = 1000
benchi-kafka     | 	retry.backoff.ms = 100
benchi-kafka     | 	sasl.client.callback.handler.class = null
benchi-kafka     | 	sasl.jaas.config = null
benchi-kafka     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
benchi-kafka     | 	sasl.kerberos.min.time.before.relogin = 60000
benchi-kafka     | 	sasl.kerberos.service.name = null
benchi-kafka     | 	sasl.kerberos.ticket.renew.jitter = 0.05
benchi-kafka     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
benchi-kafka     | 	sasl.login.callback.handler.class = null
benchi-kafka     | 	sasl.login.class = null
benchi-kafka     | 	sasl.login.connect.timeout.ms = null
benchi-kafka     | 	sasl.login.read.timeout.ms = null
benchi-kafka     | 	sasl.login.refresh.buffer.seconds = 300
benchi-kafka     | 	sasl.login.refresh.min.period.seconds = 60
benchi-kafka     | 	sasl.login.refresh.window.factor = 0.8
benchi-kafka     | 	sasl.login.refresh.window.jitter = 0.05
benchi-kafka     | 	sasl.login.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.login.retry.backoff.ms = 100
benchi-kafka     | 	sasl.mechanism = GSSAPI
benchi-kafka     | 	sasl.oauthbearer.clientassertion.audience = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.expiration = 5
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.jti.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.nbf.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.issuer = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.location = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key.passphrase = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.subject = null
benchi-kafka     | 	sasl.oauthbearer.clock.skew.seconds = 30
benchi-kafka     | 	sasl.oauthbearer.expected.audience = null
benchi-kafka     | 	sasl.oauthbearer.expected.issuer = null
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.url = null
benchi-kafka     | 	sasl.oauthbearer.scope.claim.name = scope
benchi-kafka     | 	sasl.oauthbearer.sub.claim.name = sub
benchi-kafka     | 	sasl.oauthbearer.token.endpoint.url = null
benchi-kafka     | 	security.protocol = PLAINTEXT
benchi-kafka     | 	security.providers = null
benchi-kafka     | 	send.buffer.bytes = 131072
benchi-kafka     | 	socket.connection.setup.timeout.max.ms = 30000
benchi-kafka     | 	socket.connection.setup.timeout.ms = 10000
benchi-kafka     | 	ssl.cipher.suites = null
benchi-kafka     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	ssl.engine.factory.class = null
benchi-kafka     | 	ssl.key.password = null
benchi-kafka     | 	ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	ssl.keystore.certificate.chain = null
benchi-kafka     | 	ssl.keystore.key = null
benchi-kafka     | 	ssl.keystore.location = null
benchi-kafka     | 	ssl.keystore.password = null
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.protocol = TLSv1.3
benchi-kafka     | 	ssl.provider = null
benchi-kafka     | 	ssl.secure.random.implementation = null
benchi-kafka     | 	ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	ssl.truststore.certificates = null
benchi-kafka     | 	ssl.truststore.location = null
benchi-kafka     | 	ssl.truststore.password = null
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     |  (org.apache.kafka.clients.admin.AdminClientConfig)
benchi-kafka     | [2025-03-18 16:04:20,301] INFO Kafka version: 7.8.1-ce (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:04:20,301] INFO Kafka commitId: eefb51c6f2f1b888b8b03296353338afbac8e213 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:04:20,301] INFO Kafka startTimeMs: 1742313860301 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:04:20,307] INFO [AdminClient clientId=adminclient-4] The DescribeTopicPartitions API is not supported, using Metadata API to describe topics. (org.apache.kafka.clients.admin.KafkaAdminClient)
benchi-kafka     | [2025-03-18 16:04:20,309] INFO [ControllerServer id=1] CreateTopics result(s): CreatableTopic(name='_confluent-metrics', numPartitions=12, replicationFactor=3, assignments=[], configs=[CreateableTopicConfig(name='max.message.bytes', value='10485760'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='259200000'), CreateableTopicConfig(name='segment.ms', value='14400000'), CreateableTopicConfig(name='retention.bytes', value='-1')], linkName=null, mirrorTopic=null, sourceTopicId=AAAAAAAAAAAAAAAAAAAAAA, mirrorStartOffsetSpec=-9223372036854775808, mirrorStartOffsets=[], stoppedSequenceNumber=0): INVALID_REPLICATION_FACTOR (Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.) (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:04:20,321] INFO App info kafka.admin.client for adminclient-4 unregistered (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:04:20,321] ERROR Error checking or creating metrics topic (io.confluent.metrics.reporter.ConfluentMetricsReporter)
benchi-kafka     | org.apache.kafka.common.errors.InvalidReplicationFactorException: Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.
benchi-kafka     | [2025-03-18 16:04:20,327] INFO AdminClientConfig values: 
benchi-kafka     | 	auto.include.jmx.reporter = true
benchi-kafka     | 	bootstrap.controllers = []
benchi-kafka     | 	bootstrap.servers = [broker:29092]
benchi-kafka     | 	client.dns.lookup = use_all_dns_ips
benchi-kafka     | 	client.id = confluent-telemetry-reporter-local-producer
benchi-kafka     | 	confluent.lkc.id = null
benchi-kafka     | 	confluent.metrics.reporter.bootstrap.servers = localhost:9092
benchi-kafka     | 	confluent.proxy.protocol.client.address = null
benchi-kafka     | 	confluent.proxy.protocol.client.mode = PROXY
benchi-kafka     | 	confluent.proxy.protocol.client.port = null
benchi-kafka     | 	confluent.proxy.protocol.client.version = NONE
benchi-kafka     | 	connections.max.idle.ms = 300000
benchi-kafka     | 	default.api.timeout.ms = 60000
benchi-kafka     | 	enable.metrics.push = true
benchi-kafka     | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
benchi-kafka     | 	metadata.max.age.ms = 300000
benchi-kafka     | 	metadata.recovery.strategy = none
benchi-kafka     | 	metric.reporters = []
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.recording.level = INFO
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	receive.buffer.bytes = 65536
benchi-kafka     | 	reconnect.backoff.max.ms = 1000
benchi-kafka     | 	reconnect.backoff.ms = 50
benchi-kafka     | 	request.timeout.ms = 30000
benchi-kafka     | 	retries = 2147483647
benchi-kafka     | 	retry.backoff.max.ms = 1000
benchi-kafka     | 	retry.backoff.ms = 500
benchi-kafka     | 	sasl.client.callback.handler.class = null
benchi-kafka     | 	sasl.jaas.config = null
benchi-kafka     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
benchi-kafka     | 	sasl.kerberos.min.time.before.relogin = 60000
benchi-kafka     | 	sasl.kerberos.service.name = null
benchi-kafka     | 	sasl.kerberos.ticket.renew.jitter = 0.05
benchi-kafka     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
benchi-kafka     | 	sasl.login.callback.handler.class = null
benchi-kafka     | 	sasl.login.class = null
benchi-kafka     | 	sasl.login.connect.timeout.ms = null
benchi-kafka     | 	sasl.login.read.timeout.ms = null
benchi-kafka     | 	sasl.login.refresh.buffer.seconds = 300
benchi-kafka     | 	sasl.login.refresh.min.period.seconds = 60
benchi-kafka     | 	sasl.login.refresh.window.factor = 0.8
benchi-kafka     | 	sasl.login.refresh.window.jitter = 0.05
benchi-kafka     | 	sasl.login.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.login.retry.backoff.ms = 100
benchi-kafka     | 	sasl.mechanism = GSSAPI
benchi-kafka     | 	sasl.oauthbearer.clientassertion.audience = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.expiration = 5
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.jti.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.nbf.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.issuer = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.location = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key.passphrase = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.subject = null
benchi-kafka     | 	sasl.oauthbearer.clock.skew.seconds = 30
benchi-kafka     | 	sasl.oauthbearer.expected.audience = null
benchi-kafka     | 	sasl.oauthbearer.expected.issuer = null
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.url = null
benchi-kafka     | 	sasl.oauthbearer.scope.claim.name = scope
benchi-kafka     | 	sasl.oauthbearer.sub.claim.name = sub
benchi-kafka     | 	sasl.oauthbearer.token.endpoint.url = null
benchi-kafka     | 	security.protocol = PLAINTEXT
benchi-kafka     | 	security.providers = null
benchi-kafka     | 	send.buffer.bytes = 131072
benchi-kafka     | 	socket.connection.setup.timeout.max.ms = 30000
benchi-kafka     | 	socket.connection.setup.timeout.ms = 10000
benchi-kafka     | 	ssl.cipher.suites = null
benchi-kafka     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	ssl.engine.factory.class = null
benchi-kafka     | 	ssl.key.password = null
benchi-kafka     | 	ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	ssl.keystore.certificate.chain = null
benchi-kafka     | 	ssl.keystore.key = null
benchi-kafka     | 	ssl.keystore.location = null
benchi-kafka     | 	ssl.keystore.password = null
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.protocol = TLSv1.3
benchi-kafka     | 	ssl.provider = null
benchi-kafka     | 	ssl.secure.random.implementation = null
benchi-kafka     | 	ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	ssl.truststore.certificates = null
benchi-kafka     | 	ssl.truststore.location = null
benchi-kafka     | 	ssl.truststore.password = null
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     |  (org.apache.kafka.clients.admin.AdminClientConfig)
benchi-kafka     | [2025-03-18 16:04:20,327] INFO These configurations '[compression.type, confluent.metrics.reporter.bootstrap.servers, enable.idempotence, acks, key.serializer, max.request.size, value.serializer, partitioner.class, interceptor.classes, max.in.flight.requests.per.connection, linger.ms]' were supplied but are not used yet. (org.apache.kafka.clients.admin.AdminClientConfig)
benchi-kafka     | [2025-03-18 16:04:20,327] INFO Kafka version: 7.8.1-ce (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:04:20,327] INFO Kafka commitId: eefb51c6f2f1b888b8b03296353338afbac8e213 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:04:20,327] INFO Kafka startTimeMs: 1742313860327 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:04:20,332] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] The DescribeTopicPartitions API is not supported, using Metadata API to describe topics. (org.apache.kafka.clients.admin.KafkaAdminClient)
benchi-kafka     | [2025-03-18 16:04:20,334] INFO [ControllerServer id=1] CreateTopics result(s): CreatableTopic(name='_confluent-telemetry-metrics', numPartitions=12, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='max.message.bytes', value='10485760'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='259200000'), CreateableTopicConfig(name='segment.ms', value='14400000'), CreateableTopicConfig(name='retention.bytes', value='-1')], linkName=null, mirrorTopic=null, sourceTopicId=AAAAAAAAAAAAAAAAAAAAAA, mirrorStartOffsetSpec=-9223372036854775808, mirrorStartOffsets=[], stoppedSequenceNumber=0): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:04:20,334] INFO [ControllerServer id=1] Replayed TopicRecord for topic _confluent-telemetry-metrics with topic ID OjlqIXo5SBe-rYGlWAcA1Q. (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:04:20,334] INFO [ControllerServer id=1] Replayed ConfigRecord for ConfigResource(type=TOPIC, name='_confluent-telemetry-metrics') which set configuration max.message.bytes to 10485760 (org.apache.kafka.controller.ConfigurationControlManager)
benchi-kafka     | [2025-03-18 16:04:20,334] INFO [ControllerServer id=1] Replayed ConfigRecord for ConfigResource(type=TOPIC, name='_confluent-telemetry-metrics') which set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
benchi-kafka     | [2025-03-18 16:04:20,334] INFO [ControllerServer id=1] Replayed ConfigRecord for ConfigResource(type=TOPIC, name='_confluent-telemetry-metrics') which set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
benchi-kafka     | [2025-03-18 16:04:20,334] INFO [ControllerServer id=1] Replayed ConfigRecord for ConfigResource(type=TOPIC, name='_confluent-telemetry-metrics') which set configuration retention.ms to 259200000 (org.apache.kafka.controller.ConfigurationControlManager)
benchi-kafka     | [2025-03-18 16:04:20,334] INFO [ControllerServer id=1] Replayed ConfigRecord for ConfigResource(type=TOPIC, name='_confluent-telemetry-metrics') which set configuration segment.ms to 14400000 (org.apache.kafka.controller.ConfigurationControlManager)
benchi-kafka     | [2025-03-18 16:04:20,334] INFO [ControllerServer id=1] Replayed ConfigRecord for ConfigResource(type=TOPIC, name='_confluent-telemetry-metrics') which set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
benchi-kafka     | [2025-03-18 16:04:20,334] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-telemetry-metrics-0 with topic ID OjlqIXo5SBe-rYGlWAcA1Q and PartitionRegistration(replicas=[1], observers=[], directories=[xbl_6JROMTfs6aQ8gB3fjw], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:04:20,334] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-telemetry-metrics-1 with topic ID OjlqIXo5SBe-rYGlWAcA1Q and PartitionRegistration(replicas=[1], observers=[], directories=[xbl_6JROMTfs6aQ8gB3fjw], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:04:20,335] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-telemetry-metrics-2 with topic ID OjlqIXo5SBe-rYGlWAcA1Q and PartitionRegistration(replicas=[1], observers=[], directories=[xbl_6JROMTfs6aQ8gB3fjw], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:04:20,335] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-telemetry-metrics-3 with topic ID OjlqIXo5SBe-rYGlWAcA1Q and PartitionRegistration(replicas=[1], observers=[], directories=[xbl_6JROMTfs6aQ8gB3fjw], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:04:20,335] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-telemetry-metrics-4 with topic ID OjlqIXo5SBe-rYGlWAcA1Q and PartitionRegistration(replicas=[1], observers=[], directories=[xbl_6JROMTfs6aQ8gB3fjw], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:04:20,335] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-telemetry-metrics-5 with topic ID OjlqIXo5SBe-rYGlWAcA1Q and PartitionRegistration(replicas=[1], observers=[], directories=[xbl_6JROMTfs6aQ8gB3fjw], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:04:20,335] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-telemetry-metrics-6 with topic ID OjlqIXo5SBe-rYGlWAcA1Q and PartitionRegistration(replicas=[1], observers=[], directories=[xbl_6JROMTfs6aQ8gB3fjw], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:04:20,335] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-telemetry-metrics-7 with topic ID OjlqIXo5SBe-rYGlWAcA1Q and PartitionRegistration(replicas=[1], observers=[], directories=[xbl_6JROMTfs6aQ8gB3fjw], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:04:20,335] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-telemetry-metrics-8 with topic ID OjlqIXo5SBe-rYGlWAcA1Q and PartitionRegistration(replicas=[1], observers=[], directories=[xbl_6JROMTfs6aQ8gB3fjw], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:04:20,335] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-telemetry-metrics-9 with topic ID OjlqIXo5SBe-rYGlWAcA1Q and PartitionRegistration(replicas=[1], observers=[], directories=[xbl_6JROMTfs6aQ8gB3fjw], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:04:20,335] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-telemetry-metrics-10 with topic ID OjlqIXo5SBe-rYGlWAcA1Q and PartitionRegistration(replicas=[1], observers=[], directories=[xbl_6JROMTfs6aQ8gB3fjw], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:04:20,335] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-telemetry-metrics-11 with topic ID OjlqIXo5SBe-rYGlWAcA1Q and PartitionRegistration(replicas=[1], observers=[], directories=[xbl_6JROMTfs6aQ8gB3fjw], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:04:20,363] INFO SBC Event SbcMetadataUpdateEvent-134 generated 1 more events to enqueue in the following order - [SbcConfigUpdateEvent-135]. Enqueuing... (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:04:20,363] INFO Handling event SbcConfigUpdateEvent-135 (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:04:20,363] INFO [Broker id=1] Transitioning 12 partition(s) to local leaders. (state.change.logger)
benchi-kafka     | [2025-03-18 16:04:20,363] INFO Balancer notified of a config change: ConfigurationsDelta(changes={ConfigResource(type=TOPIC, name='_confluent-telemetry-metrics')=ConfigurationDelta(changedKeys=[max.message.bytes, message.timestamp.type, min.insync.replicas, retention.ms, segment.ms, retention.bytes])}) (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:04:20,363] INFO There were 0 change(s) and 0 deletion(s) to balancer configs. Changed Configs: {}, Deleted Configs: [] (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:04:20,363] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-telemetry-metrics-3, _confluent-telemetry-metrics-4, _confluent-telemetry-metrics-5, _confluent-telemetry-metrics-6, _confluent-telemetry-metrics-7, _confluent-telemetry-metrics-8, _confluent-telemetry-metrics-9, _confluent-telemetry-metrics-10, _confluent-telemetry-metrics-11, _confluent-telemetry-metrics-0, _confluent-telemetry-metrics-1, _confluent-telemetry-metrics-2) (kafka.server.ReplicaFetcherManager)
benchi-kafka     | [2025-03-18 16:04:20,363] INFO [Broker id=1] Creating new partition _confluent-telemetry-metrics-3 with topic id OjlqIXo5SBe-rYGlWAcA1Q. (state.change.logger)
benchi-kafka     | [2025-03-18 16:04:20,364] INFO Created telemetry topic _confluent-telemetry-metrics (io.confluent.telemetry.exporter.kafka.KafkaExporter)
benchi-kafka     | [2025-03-18 16:04:20,364] INFO App info kafka.admin.client for confluent-telemetry-reporter-local-producer unregistered (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:04:20,364] INFO [Broker id=1] Creating new partition _confluent-telemetry-metrics-4 with topic id OjlqIXo5SBe-rYGlWAcA1Q. (state.change.logger)
benchi-kafka     | [2025-03-18 16:04:20,365] INFO [Broker id=1] Creating new partition _confluent-telemetry-metrics-5 with topic id OjlqIXo5SBe-rYGlWAcA1Q. (state.change.logger)
benchi-kafka     | [2025-03-18 16:04:20,366] INFO [Broker id=1] Creating new partition _confluent-telemetry-metrics-6 with topic id OjlqIXo5SBe-rYGlWAcA1Q. (state.change.logger)
benchi-kafka     | [2025-03-18 16:04:20,366] INFO [Broker id=1] Creating new partition _confluent-telemetry-metrics-7 with topic id OjlqIXo5SBe-rYGlWAcA1Q. (state.change.logger)
benchi-kafka     | [2025-03-18 16:04:20,366] INFO [Broker id=1] Creating new partition _confluent-telemetry-metrics-8 with topic id OjlqIXo5SBe-rYGlWAcA1Q. (state.change.logger)
benchi-kafka     | [2025-03-18 16:04:20,367] INFO [Broker id=1] Creating new partition _confluent-telemetry-metrics-9 with topic id OjlqIXo5SBe-rYGlWAcA1Q. (state.change.logger)
benchi-kafka     | [2025-03-18 16:04:20,367] INFO [Broker id=1] Creating new partition _confluent-telemetry-metrics-10 with topic id OjlqIXo5SBe-rYGlWAcA1Q. (state.change.logger)
benchi-kafka     | [2025-03-18 16:04:20,367] INFO [Broker id=1] Creating new partition _confluent-telemetry-metrics-11 with topic id OjlqIXo5SBe-rYGlWAcA1Q. (state.change.logger)
benchi-kafka     | [2025-03-18 16:04:20,367] INFO [Broker id=1] Creating new partition _confluent-telemetry-metrics-0 with topic id OjlqIXo5SBe-rYGlWAcA1Q. (state.change.logger)
benchi-kafka     | [2025-03-18 16:04:20,368] INFO [Broker id=1] Creating new partition _confluent-telemetry-metrics-1 with topic id OjlqIXo5SBe-rYGlWAcA1Q. (state.change.logger)
benchi-kafka     | [2025-03-18 16:04:20,368] INFO [Broker id=1] Creating new partition _confluent-telemetry-metrics-2 with topic id OjlqIXo5SBe-rYGlWAcA1Q. (state.change.logger)
benchi-kafka     | [2025-03-18 16:04:20,368] INFO [Broker id=1] Stopped fetchers as part of become-leader transition for 12 partitions (state.change.logger)
benchi-kafka     | [2025-03-18 16:04:20,370] INFO [MergedLog partition=_confluent-telemetry-metrics-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:04:20,371] INFO Created log for partition _confluent-telemetry-metrics-0 in /tmp/kraft-combined-logs/_confluent-telemetry-metrics-0 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:04:20,372] INFO [MergedLog partition=_confluent-telemetry-metrics-3, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:04:20,372] INFO [Partition _confluent-telemetry-metrics-0 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:04:20,372] INFO [Partition _confluent-telemetry-metrics-0 broker=1] Log loaded for partition _confluent-telemetry-metrics-0 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:04:20,372] INFO Setting topicIdPartition OjlqIXo5SBe-rYGlWAcA1Q:_confluent-telemetry-metrics-0 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:04:20,372] INFO Created log for partition _confluent-telemetry-metrics-3 in /tmp/kraft-combined-logs/_confluent-telemetry-metrics-3 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:04:20,372] INFO [Partition _confluent-telemetry-metrics-3 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-3 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:04:20,372] INFO [Partition _confluent-telemetry-metrics-3 broker=1] Log loaded for partition _confluent-telemetry-metrics-3 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:04:20,372] INFO [MergedLog partition=_confluent-telemetry-metrics-0, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-telemetry-metrics-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:04:20,372] INFO Setting topicIdPartition OjlqIXo5SBe-rYGlWAcA1Q:_confluent-telemetry-metrics-3 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:04:20,372] INFO [MergedLog partition=_confluent-telemetry-metrics-3, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-telemetry-metrics-3 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:04:20,372] INFO [Broker id=1] Leader _confluent-telemetry-metrics-0 with topic id Some(OjlqIXo5SBe-rYGlWAcA1Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:04:20,372] INFO [Broker id=1] Leader _confluent-telemetry-metrics-3 with topic id Some(OjlqIXo5SBe-rYGlWAcA1Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:04:20,373] INFO [MergedLog partition=_confluent-telemetry-metrics-11, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:04:20,373] INFO Created log for partition _confluent-telemetry-metrics-11 in /tmp/kraft-combined-logs/_confluent-telemetry-metrics-11 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:04:20,373] INFO [Partition _confluent-telemetry-metrics-11 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-11 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:04:20,373] INFO [Partition _confluent-telemetry-metrics-11 broker=1] Log loaded for partition _confluent-telemetry-metrics-11 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:04:20,374] INFO Setting topicIdPartition OjlqIXo5SBe-rYGlWAcA1Q:_confluent-telemetry-metrics-11 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:04:20,374] INFO [MergedLog partition=_confluent-telemetry-metrics-11, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-telemetry-metrics-11 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:04:20,374] INFO [Broker id=1] Leader _confluent-telemetry-metrics-11 with topic id Some(OjlqIXo5SBe-rYGlWAcA1Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:04:20,376] INFO [MergedLog partition=_confluent-telemetry-metrics-7, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:04:20,376] INFO Created log for partition _confluent-telemetry-metrics-7 in /tmp/kraft-combined-logs/_confluent-telemetry-metrics-7 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:04:20,376] INFO [Partition _confluent-telemetry-metrics-7 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-7 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:04:20,376] INFO [Partition _confluent-telemetry-metrics-7 broker=1] Log loaded for partition _confluent-telemetry-metrics-7 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:04:20,376] INFO Setting topicIdPartition OjlqIXo5SBe-rYGlWAcA1Q:_confluent-telemetry-metrics-7 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:04:20,376] INFO [MergedLog partition=_confluent-telemetry-metrics-7, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-telemetry-metrics-7 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:04:20,377] INFO [Broker id=1] Leader _confluent-telemetry-metrics-7 with topic id Some(OjlqIXo5SBe-rYGlWAcA1Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:04:20,377] INFO Partitioner has null list of partitions to produce to. Calculating partitions to produce to (io.confluent.shaded.io.confluent.telemetry.events.exporter.kafka.RandomBrokerPartitionSubsetPartitioner)
benchi-kafka     | [2025-03-18 16:04:20,378] INFO [MergedLog partition=_confluent-telemetry-metrics-1, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:04:20,378] INFO Kafka Producer producing to the following subset partitions: {_confluent-telemetry-metrics=[0, 7]} (io.confluent.shaded.io.confluent.telemetry.events.exporter.kafka.RandomBrokerPartitionSubsetPartitioner)
benchi-kafka     | [2025-03-18 16:04:20,378] INFO Created log for partition _confluent-telemetry-metrics-1 in /tmp/kraft-combined-logs/_confluent-telemetry-metrics-1 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:04:20,378] INFO [Partition _confluent-telemetry-metrics-1 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-1 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:04:20,378] INFO [Partition _confluent-telemetry-metrics-1 broker=1] Log loaded for partition _confluent-telemetry-metrics-1 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:04:20,378] INFO Setting topicIdPartition OjlqIXo5SBe-rYGlWAcA1Q:_confluent-telemetry-metrics-1 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:04:20,378] INFO [MergedLog partition=_confluent-telemetry-metrics-1, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-telemetry-metrics-1 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:04:20,378] INFO [Broker id=1] Leader _confluent-telemetry-metrics-1 with topic id Some(OjlqIXo5SBe-rYGlWAcA1Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:04:20,380] INFO [MergedLog partition=_confluent-telemetry-metrics-4, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:04:20,380] INFO Created log for partition _confluent-telemetry-metrics-4 in /tmp/kraft-combined-logs/_confluent-telemetry-metrics-4 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:04:20,380] INFO [Partition _confluent-telemetry-metrics-4 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-4 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:04:20,380] INFO [Partition _confluent-telemetry-metrics-4 broker=1] Log loaded for partition _confluent-telemetry-metrics-4 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:04:20,380] INFO Setting topicIdPartition OjlqIXo5SBe-rYGlWAcA1Q:_confluent-telemetry-metrics-4 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:04:20,380] INFO [MergedLog partition=_confluent-telemetry-metrics-4, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-telemetry-metrics-4 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:04:20,380] INFO [Broker id=1] Leader _confluent-telemetry-metrics-4 with topic id Some(OjlqIXo5SBe-rYGlWAcA1Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:04:20,381] INFO [MergedLog partition=_confluent-telemetry-metrics-2, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:04:20,381] INFO Created log for partition _confluent-telemetry-metrics-2 in /tmp/kraft-combined-logs/_confluent-telemetry-metrics-2 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:04:20,381] INFO [Partition _confluent-telemetry-metrics-2 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-2 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:04:20,381] INFO [Partition _confluent-telemetry-metrics-2 broker=1] Log loaded for partition _confluent-telemetry-metrics-2 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:04:20,381] INFO Setting topicIdPartition OjlqIXo5SBe-rYGlWAcA1Q:_confluent-telemetry-metrics-2 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:04:20,381] INFO [MergedLog partition=_confluent-telemetry-metrics-2, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-telemetry-metrics-2 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:04:20,381] INFO [Broker id=1] Leader _confluent-telemetry-metrics-2 with topic id Some(OjlqIXo5SBe-rYGlWAcA1Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:04:20,383] INFO [MergedLog partition=_confluent-telemetry-metrics-8, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:04:20,383] INFO Created log for partition _confluent-telemetry-metrics-8 in /tmp/kraft-combined-logs/_confluent-telemetry-metrics-8 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:04:20,383] INFO [Partition _confluent-telemetry-metrics-8 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-8 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:04:20,383] INFO [Partition _confluent-telemetry-metrics-8 broker=1] Log loaded for partition _confluent-telemetry-metrics-8 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:04:20,383] INFO Setting topicIdPartition OjlqIXo5SBe-rYGlWAcA1Q:_confluent-telemetry-metrics-8 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:04:20,383] INFO [MergedLog partition=_confluent-telemetry-metrics-8, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-telemetry-metrics-8 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:04:20,383] INFO [Broker id=1] Leader _confluent-telemetry-metrics-8 with topic id Some(OjlqIXo5SBe-rYGlWAcA1Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:04:20,385] INFO [MergedLog partition=_confluent-telemetry-metrics-5, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:04:20,385] INFO Created log for partition _confluent-telemetry-metrics-5 in /tmp/kraft-combined-logs/_confluent-telemetry-metrics-5 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:04:20,385] INFO [Partition _confluent-telemetry-metrics-5 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-5 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:04:20,385] INFO [Partition _confluent-telemetry-metrics-5 broker=1] Log loaded for partition _confluent-telemetry-metrics-5 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:04:20,385] INFO Setting topicIdPartition OjlqIXo5SBe-rYGlWAcA1Q:_confluent-telemetry-metrics-5 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:04:20,385] INFO [MergedLog partition=_confluent-telemetry-metrics-5, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-telemetry-metrics-5 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:04:20,385] INFO [Broker id=1] Leader _confluent-telemetry-metrics-5 with topic id Some(OjlqIXo5SBe-rYGlWAcA1Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:04:20,386] INFO [MergedLog partition=_confluent-telemetry-metrics-9, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:04:20,387] INFO Created log for partition _confluent-telemetry-metrics-9 in /tmp/kraft-combined-logs/_confluent-telemetry-metrics-9 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:04:20,387] INFO [Partition _confluent-telemetry-metrics-9 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-9 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:04:20,387] INFO [Partition _confluent-telemetry-metrics-9 broker=1] Log loaded for partition _confluent-telemetry-metrics-9 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:04:20,387] INFO Setting topicIdPartition OjlqIXo5SBe-rYGlWAcA1Q:_confluent-telemetry-metrics-9 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:04:20,387] INFO [MergedLog partition=_confluent-telemetry-metrics-9, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-telemetry-metrics-9 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:04:20,387] INFO [Broker id=1] Leader _confluent-telemetry-metrics-9 with topic id Some(OjlqIXo5SBe-rYGlWAcA1Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:04:20,388] INFO [MergedLog partition=_confluent-telemetry-metrics-6, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:04:20,388] INFO Created log for partition _confluent-telemetry-metrics-6 in /tmp/kraft-combined-logs/_confluent-telemetry-metrics-6 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:04:20,388] INFO [Partition _confluent-telemetry-metrics-6 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-6 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:04:20,388] INFO [Partition _confluent-telemetry-metrics-6 broker=1] Log loaded for partition _confluent-telemetry-metrics-6 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:04:20,388] INFO Setting topicIdPartition OjlqIXo5SBe-rYGlWAcA1Q:_confluent-telemetry-metrics-6 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:04:20,388] INFO [MergedLog partition=_confluent-telemetry-metrics-6, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-telemetry-metrics-6 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:04:20,388] INFO [Broker id=1] Leader _confluent-telemetry-metrics-6 with topic id Some(OjlqIXo5SBe-rYGlWAcA1Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:04:20,390] INFO [MergedLog partition=_confluent-telemetry-metrics-10, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:04:20,390] INFO Created log for partition _confluent-telemetry-metrics-10 in /tmp/kraft-combined-logs/_confluent-telemetry-metrics-10 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:04:20,390] INFO [Partition _confluent-telemetry-metrics-10 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-10 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:04:20,390] INFO [Partition _confluent-telemetry-metrics-10 broker=1] Log loaded for partition _confluent-telemetry-metrics-10 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:04:20,390] INFO Setting topicIdPartition OjlqIXo5SBe-rYGlWAcA1Q:_confluent-telemetry-metrics-10 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:04:20,390] INFO [MergedLog partition=_confluent-telemetry-metrics-10, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-telemetry-metrics-10 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:04:20,390] INFO [Broker id=1] Leader _confluent-telemetry-metrics-10 with topic id Some(OjlqIXo5SBe-rYGlWAcA1Q) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:04:20,391] INFO [DynamicConfigPublisher broker id=1] Updating topic _confluent-telemetry-metrics with new configuration : max.message.bytes -> 10485760,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,retention.ms -> 259200000,segment.ms -> 14400000,retention.bytes -> -1 (kafka.server.metadata.DynamicConfigPublisher)
benchi-kafka     | [2025-03-18 16:04:20,408] INFO Broker Addition context is yet to be initialized, hence BrokerAddCount metrics will be reported as 0. (io.confluent.databalancer.ConfluentDataBalanceEngine)
benchi-kafka     | [2025-03-18 16:04:20,440] INFO [Partition _confluent-telemetry-metrics-7 broker=1] roll: _confluent-telemetry-metrics-7: first produce received, lastOffset: 22, leaderEpoch: 0, numMessages:23, time diff: 71 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:04:20,441] INFO [Partition _confluent-telemetry-metrics-7 broker=1] roll: _confluent-telemetry-metrics-7: first HWM advanced, leaderEpoch: 0, old HW: 0, new HW: 23, become leader time: 1742313860369 ms, time diff: 72 ms (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:04:20,442] INFO [Partition _confluent-telemetry-metrics-0 broker=1] roll: _confluent-telemetry-metrics-0: first produce received, lastOffset: 22, leaderEpoch: 0, numMessages:23, time diff: 73 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:04:20,442] INFO [Partition _confluent-telemetry-metrics-0 broker=1] roll: _confluent-telemetry-metrics-0: first HWM advanced, leaderEpoch: 0, old HW: 0, new HW: 23, become leader time: 1742313860369 ms, time diff: 73 ms (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:04:23,621] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2083203430002767771] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
benchi-kafka     | [2025-03-18 16:04:23,621] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2083203430002767771] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
benchi-kafka     | [2025-03-18 16:04:23,623] INFO App info kafka.consumer for kafka-cruise-control unregistered (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:04:23,623] INFO Metric Reporter Sampler ready to start. (io.confluent.cruisecontrol.metricsreporter.ConfluentMetricsSamplerBase)
benchi-kafka     | [2025-03-18 16:04:23,623] INFO DataBalancer: Startup component StartupComponent ConfluentTelemetryReporterSampler ready to proceed (io.confluent.databalancer.startup.StartupComponents)
benchi-kafka     | [2025-03-18 16:04:23,623] INFO DataBalancer: Checking startup component StartupComponent SampleStoreTopicCleanUp (io.confluent.databalancer.startup.StartupComponents)
benchi-kafka     | [2025-03-18 16:04:23,624] INFO AdminClientConfig values: 
benchi-kafka     | 	auto.include.jmx.reporter = true
benchi-kafka     | 	bootstrap.controllers = []
benchi-kafka     | 	bootstrap.servers = [broker:29092]
benchi-kafka     | 	client.dns.lookup = use_all_dns_ips
benchi-kafka     | 	client.id = kafka-cruise-control
benchi-kafka     | 	confluent.lkc.id = null
benchi-kafka     | 	confluent.metrics.reporter.bootstrap.servers = localhost:9092
benchi-kafka     | 	confluent.proxy.protocol.client.address = null
benchi-kafka     | 	confluent.proxy.protocol.client.mode = PROXY
benchi-kafka     | 	confluent.proxy.protocol.client.port = null
benchi-kafka     | 	confluent.proxy.protocol.client.version = NONE
benchi-kafka     | 	connections.max.idle.ms = 540000
benchi-kafka     | 	default.api.timeout.ms = 60000
benchi-kafka     | 	enable.metrics.push = true
benchi-kafka     | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
benchi-kafka     | 	metadata.max.age.ms = 300000
benchi-kafka     | 	metadata.recovery.strategy = none
benchi-kafka     | 	metric.reporters = []
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.recording.level = INFO
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	receive.buffer.bytes = 32768
benchi-kafka     | 	reconnect.backoff.max.ms = 5000
benchi-kafka     | 	reconnect.backoff.ms = 500
benchi-kafka     | 	request.timeout.ms = 30000
benchi-kafka     | 	retries = 2147483647
benchi-kafka     | 	retry.backoff.max.ms = 1000
benchi-kafka     | 	retry.backoff.ms = 500
benchi-kafka     | 	sasl.client.callback.handler.class = null
benchi-kafka     | 	sasl.jaas.config = null
benchi-kafka     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
benchi-kafka     | 	sasl.kerberos.min.time.before.relogin = 60000
benchi-kafka     | 	sasl.kerberos.service.name = null
benchi-kafka     | 	sasl.kerberos.ticket.renew.jitter = 0.05
benchi-kafka     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
benchi-kafka     | 	sasl.login.callback.handler.class = null
benchi-kafka     | 	sasl.login.class = null
benchi-kafka     | 	sasl.login.connect.timeout.ms = null
benchi-kafka     | 	sasl.login.read.timeout.ms = null
benchi-kafka     | 	sasl.login.refresh.buffer.seconds = 300
benchi-kafka     | 	sasl.login.refresh.min.period.seconds = 60
benchi-kafka     | 	sasl.login.refresh.window.factor = 0.8
benchi-kafka     | 	sasl.login.refresh.window.jitter = 0.05
benchi-kafka     | 	sasl.login.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.login.retry.backoff.ms = 100
benchi-kafka     | 	sasl.mechanism = GSSAPI
benchi-kafka     | 	sasl.oauthbearer.clientassertion.audience = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.expiration = 5
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.jti.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.nbf.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.issuer = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.location = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key.passphrase = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.subject = null
benchi-kafka     | 	sasl.oauthbearer.clock.skew.seconds = 30
benchi-kafka     | 	sasl.oauthbearer.expected.audience = null
benchi-kafka     | 	sasl.oauthbearer.expected.issuer = null
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.url = null
benchi-kafka     | 	sasl.oauthbearer.scope.claim.name = scope
benchi-kafka     | 	sasl.oauthbearer.sub.claim.name = sub
benchi-kafka     | 	sasl.oauthbearer.token.endpoint.url = null
benchi-kafka     | 	security.protocol = PLAINTEXT
benchi-kafka     | 	security.providers = null
benchi-kafka     | 	send.buffer.bytes = 131072
benchi-kafka     | 	socket.connection.setup.timeout.max.ms = 30000
benchi-kafka     | 	socket.connection.setup.timeout.ms = 10000
benchi-kafka     | 	ssl.cipher.suites = null
benchi-kafka     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	ssl.engine.factory.class = null
benchi-kafka     | 	ssl.key.password = null
benchi-kafka     | 	ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	ssl.keystore.certificate.chain = null
benchi-kafka     | 	ssl.keystore.key = null
benchi-kafka     | 	ssl.keystore.location = null
benchi-kafka     | 	ssl.keystore.password = null
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.protocol = TLSv1.3
benchi-kafka     | 	ssl.provider = null
benchi-kafka     | 	ssl.secure.random.implementation = null
benchi-kafka     | 	ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	ssl.truststore.certificates = null
benchi-kafka     | 	ssl.truststore.location = null
benchi-kafka     | 	ssl.truststore.password = null
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     |  (org.apache.kafka.clients.admin.AdminClientConfig)
benchi-kafka     | [2025-03-18 16:04:23,625] INFO These configurations '[sasl.oauthbearer.jwks.endpoint.refresh.ms, sasl.oauthbearer.clientassertion.include.jti.claim, sasl.login.refresh.min.period.seconds, sasl.oauthbearer.scope.claim.name, sasl.login.refresh.window.factor, sasl.kerberos.ticket.renew.window.factor, sasl.login.retry.backoff.ms, sasl.oauthbearer.clientassertion.expiration, sasl.kerberos.kinit.cmd, sasl.kerberos.ticket.renew.jitter, ssl.keystore.type, ssl.trustmanager.algorithm, sasl.kerberos.min.time.before.relogin, ssl.endpoint.identification.algorithm, ssl.protocol, confluent.metrics.reporter.bootstrap.servers, sasl.login.refresh.buffer.seconds, sasl.login.retry.backoff.max.ms, ssl.enabled.protocols, sasl.oauthbearer.sub.claim.name, ssl.truststore.type, sasl.oauthbearer.jwks.endpoint.retry.backoff.ms, sasl.oauthbearer.clock.skew.seconds, ssl.keymanager.algorithm, sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms, sasl.oauthbearer.clientassertion.include.nbf.claim, sasl.login.refresh.window.jitter]' were supplied but are not used yet. (org.apache.kafka.clients.admin.AdminClientConfig)
benchi-kafka     | [2025-03-18 16:04:23,625] INFO Kafka version: 7.8.1-ce (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:04:23,625] INFO Kafka commitId: eefb51c6f2f1b888b8b03296353338afbac8e213 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:04:23,625] INFO Kafka startTimeMs: 1742313863625 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:04:23,632] INFO DataBalancer: No topics to be deleted. (com.linkedin.kafka.cruisecontrol.SbkTopicUtils)
benchi-kafka     | [2025-03-18 16:04:23,632] INFO App info kafka.admin.client for kafka-cruise-control unregistered (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:04:23,633] INFO DataBalancer: Startup component StartupComponent SampleStoreTopicCleanUp ready to proceed (io.confluent.databalancer.startup.StartupComponents)
benchi-kafka     | [2025-03-18 16:04:23,633] INFO DataBalancer: Checking startup component StartupComponent ApiStatePersistenceStore (io.confluent.databalancer.startup.StartupComponents)
benchi-kafka     | [2025-03-18 16:04:23,633] INFO AdminClientConfig values: 
benchi-kafka     | 	auto.include.jmx.reporter = true
benchi-kafka     | 	bootstrap.controllers = []
benchi-kafka     | 	bootstrap.servers = [broker:29092]
benchi-kafka     | 	client.dns.lookup = use_all_dns_ips
benchi-kafka     | 	client.id = kafka-cruise-control
benchi-kafka     | 	confluent.lkc.id = null
benchi-kafka     | 	confluent.metrics.reporter.bootstrap.servers = localhost:9092
benchi-kafka     | 	confluent.proxy.protocol.client.address = null
benchi-kafka     | 	confluent.proxy.protocol.client.mode = PROXY
benchi-kafka     | 	confluent.proxy.protocol.client.port = null
benchi-kafka     | 	confluent.proxy.protocol.client.version = NONE
benchi-kafka     | 	connections.max.idle.ms = 540000
benchi-kafka     | 	default.api.timeout.ms = 60000
benchi-kafka     | 	enable.metrics.push = true
benchi-kafka     | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
benchi-kafka     | 	metadata.max.age.ms = 300000
benchi-kafka     | 	metadata.recovery.strategy = none
benchi-kafka     | 	metric.reporters = []
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.recording.level = INFO
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	receive.buffer.bytes = 32768
benchi-kafka     | 	reconnect.backoff.max.ms = 5000
benchi-kafka     | 	reconnect.backoff.ms = 500
benchi-kafka     | 	request.timeout.ms = 30000
benchi-kafka     | 	retries = 2147483647
benchi-kafka     | 	retry.backoff.max.ms = 1000
benchi-kafka     | 	retry.backoff.ms = 500
benchi-kafka     | 	sasl.client.callback.handler.class = null
benchi-kafka     | 	sasl.jaas.config = null
benchi-kafka     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
benchi-kafka     | 	sasl.kerberos.min.time.before.relogin = 60000
benchi-kafka     | 	sasl.kerberos.service.name = null
benchi-kafka     | 	sasl.kerberos.ticket.renew.jitter = 0.05
benchi-kafka     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
benchi-kafka     | 	sasl.login.callback.handler.class = null
benchi-kafka     | 	sasl.login.class = null
benchi-kafka     | 	sasl.login.connect.timeout.ms = null
benchi-kafka     | 	sasl.login.read.timeout.ms = null
benchi-kafka     | 	sasl.login.refresh.buffer.seconds = 300
benchi-kafka     | 	sasl.login.refresh.min.period.seconds = 60
benchi-kafka     | 	sasl.login.refresh.window.factor = 0.8
benchi-kafka     | 	sasl.login.refresh.window.jitter = 0.05
benchi-kafka     | 	sasl.login.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.login.retry.backoff.ms = 100
benchi-kafka     | 	sasl.mechanism = GSSAPI
benchi-kafka     | 	sasl.oauthbearer.clientassertion.audience = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.expiration = 5
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.jti.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.nbf.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.issuer = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.location = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key.passphrase = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.subject = null
benchi-kafka     | 	sasl.oauthbearer.clock.skew.seconds = 30
benchi-kafka     | 	sasl.oauthbearer.expected.audience = null
benchi-kafka     | 	sasl.oauthbearer.expected.issuer = null
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.url = null
benchi-kafka     | 	sasl.oauthbearer.scope.claim.name = scope
benchi-kafka     | 	sasl.oauthbearer.sub.claim.name = sub
benchi-kafka     | 	sasl.oauthbearer.token.endpoint.url = null
benchi-kafka     | 	security.protocol = PLAINTEXT
benchi-kafka     | 	security.providers = null
benchi-kafka     | 	send.buffer.bytes = 131072
benchi-kafka     | 	socket.connection.setup.timeout.max.ms = 30000
benchi-kafka     | 	socket.connection.setup.timeout.ms = 10000
benchi-kafka     | 	ssl.cipher.suites = null
benchi-kafka     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	ssl.engine.factory.class = null
benchi-kafka     | 	ssl.key.password = null
benchi-kafka     | 	ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	ssl.keystore.certificate.chain = null
benchi-kafka     | 	ssl.keystore.key = null
benchi-kafka     | 	ssl.keystore.location = null
benchi-kafka     | 	ssl.keystore.password = null
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.protocol = TLSv1.3
benchi-kafka     | 	ssl.provider = null
benchi-kafka     | 	ssl.secure.random.implementation = null
benchi-kafka     | 	ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	ssl.truststore.certificates = null
benchi-kafka     | 	ssl.truststore.location = null
benchi-kafka     | 	ssl.truststore.password = null
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     |  (org.apache.kafka.clients.admin.AdminClientConfig)
benchi-kafka     | [2025-03-18 16:04:23,634] INFO These configurations '[sasl.oauthbearer.jwks.endpoint.refresh.ms, sasl.oauthbearer.clientassertion.include.jti.claim, sasl.login.refresh.min.period.seconds, sasl.oauthbearer.scope.claim.name, sasl.login.refresh.window.factor, sasl.kerberos.ticket.renew.window.factor, sasl.login.retry.backoff.ms, sasl.oauthbearer.clientassertion.expiration, sasl.kerberos.kinit.cmd, sasl.kerberos.ticket.renew.jitter, ssl.keystore.type, ssl.trustmanager.algorithm, sasl.kerberos.min.time.before.relogin, ssl.endpoint.identification.algorithm, ssl.protocol, confluent.metrics.reporter.bootstrap.servers, sasl.login.refresh.buffer.seconds, sasl.login.retry.backoff.max.ms, ssl.enabled.protocols, sasl.oauthbearer.sub.claim.name, ssl.truststore.type, sasl.oauthbearer.jwks.endpoint.retry.backoff.ms, sasl.oauthbearer.clock.skew.seconds, ssl.keymanager.algorithm, sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms, sasl.oauthbearer.clientassertion.include.nbf.claim, sasl.login.refresh.window.jitter]' were supplied but are not used yet. (org.apache.kafka.clients.admin.AdminClientConfig)
benchi-kafka     | [2025-03-18 16:04:23,634] INFO Kafka version: 7.8.1-ce (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:04:23,634] INFO Kafka commitId: eefb51c6f2f1b888b8b03296353338afbac8e213 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:04:23,634] INFO Kafka startTimeMs: 1742313863634 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:04:23,640] INFO DataBalancer: Creating topic _confluent_balancer_api_state  (com.linkedin.kafka.cruisecontrol.SbkTopicUtils)
benchi-kafka     | [2025-03-18 16:04:23,642] INFO [ControllerServer id=1] CreateTopics result(s): CreatableTopic(name='_confluent_balancer_api_state', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='delete,compact'), CreateableTopicConfig(name='retention.ms', value='2592000000')], linkName=null, mirrorTopic=null, sourceTopicId=AAAAAAAAAAAAAAAAAAAAAA, mirrorStartOffsetSpec=-9223372036854775808, mirrorStartOffsets=[], stoppedSequenceNumber=0): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:04:23,642] INFO [ControllerServer id=1] Replayed TopicRecord for topic _confluent_balancer_api_state with topic ID nk5Zk1I2TWKWZEDQQ08-VQ. (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:04:23,642] INFO [ControllerServer id=1] Replayed ConfigRecord for ConfigResource(type=TOPIC, name='_confluent_balancer_api_state') which set configuration cleanup.policy to delete,compact (org.apache.kafka.controller.ConfigurationControlManager)
benchi-kafka     | [2025-03-18 16:04:23,642] INFO [ControllerServer id=1] Replayed ConfigRecord for ConfigResource(type=TOPIC, name='_confluent_balancer_api_state') which set configuration retention.ms to 2592000000 (org.apache.kafka.controller.ConfigurationControlManager)
benchi-kafka     | [2025-03-18 16:04:23,642] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent_balancer_api_state-0 with topic ID nk5Zk1I2TWKWZEDQQ08-VQ and PartitionRegistration(replicas=[1], observers=[], directories=[xbl_6JROMTfs6aQ8gB3fjw], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:04:23,670] INFO SBC Event SbcMetadataUpdateEvent-142 generated 1 more events to enqueue in the following order - [SbcConfigUpdateEvent-143]. Enqueuing... (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:04:23,670] INFO Handling event SbcConfigUpdateEvent-143 (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:04:23,670] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
benchi-kafka     | [2025-03-18 16:04:23,670] INFO Balancer notified of a config change: ConfigurationsDelta(changes={ConfigResource(type=TOPIC, name='_confluent_balancer_api_state')=ConfigurationDelta(changedKeys=[cleanup.policy, retention.ms])}) (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:04:23,670] INFO There were 0 change(s) and 0 deletion(s) to balancer configs. Changed Configs: {}, Deleted Configs: [] (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:04:23,670] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent_balancer_api_state-0) (kafka.server.ReplicaFetcherManager)
benchi-kafka     | [2025-03-18 16:04:23,670] INFO [Broker id=1] Creating new partition _confluent_balancer_api_state-0 with topic id nk5Zk1I2TWKWZEDQQ08-VQ. (state.change.logger)
benchi-kafka     | [2025-03-18 16:04:23,671] INFO [Broker id=1] Stopped fetchers as part of become-leader transition for 1 partitions (state.change.logger)
benchi-kafka     | [2025-03-18 16:04:23,672] INFO App info kafka.admin.client for kafka-cruise-control unregistered (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:04:23,672] INFO Waiting for 1 seconds to ensure that api persistent store topic is created/exists. (io.confluent.databalancer.persistence.ApiStatePersistenceStore)
benchi-kafka     | [2025-03-18 16:04:23,672] INFO [MergedLog partition=_confluent_balancer_api_state-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:04:23,673] INFO Created log for partition _confluent_balancer_api_state-0 in /tmp/kraft-combined-logs/_confluent_balancer_api_state-0 with properties {cleanup.policy=delete,compact, retention.ms=2592000000} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:04:23,673] INFO [Partition _confluent_balancer_api_state-0 broker=1] No checkpointed highwatermark is found for partition _confluent_balancer_api_state-0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:04:23,673] INFO [Partition _confluent_balancer_api_state-0 broker=1] Log loaded for partition _confluent_balancer_api_state-0 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:04:23,673] INFO Setting topicIdPartition nk5Zk1I2TWKWZEDQQ08-VQ:_confluent_balancer_api_state-0 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:04:23,673] INFO [MergedLog partition=_confluent_balancer_api_state-0, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent_balancer_api_state-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:04:23,673] INFO [Broker id=1] Leader _confluent_balancer_api_state-0 with topic id Some(nk5Zk1I2TWKWZEDQQ08-VQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:04:23,674] INFO [DynamicConfigPublisher broker id=1] Updating topic _confluent_balancer_api_state with new configuration : cleanup.policy -> delete,compact,retention.ms -> 2592000000 (kafka.server.metadata.DynamicConfigPublisher)
Gracefully stopping... (press Ctrl+C again to force)
 Container benchi-kafka  Stopping
 Container benchi-postgres  Stopping
 Container benchi-postgres  Stopped
 Container benchi-kafka  Stopped
canceled

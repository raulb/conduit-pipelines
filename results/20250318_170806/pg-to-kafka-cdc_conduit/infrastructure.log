 Network infra_default  Creating
 Network infra_default  Created
 Container benchi-postgres  Creating
 Container benchi-kafka  Creating
 Container benchi-kafka  Created
 Container benchi-postgres  Created
Attaching to benchi-kafka, benchi-postgres
benchi-kafka     | ===> User
benchi-kafka     | uid=1000(appuser) gid=1000(appuser) groups=1000(appuser)
benchi-kafka     | ===> Configuring ...
benchi-kafka     | Running in KRaft mode...
benchi-postgres  | The files belonging to this database system will be owned by user "postgres".
benchi-postgres  | This user must also own the server process.
benchi-postgres  | 
benchi-postgres  | The database cluster will be initialized with locale "en_US.utf8".
benchi-postgres  | The default database encoding has accordingly been set to "UTF8".
benchi-postgres  | The default text search configuration will be set to "english".
benchi-postgres  | 
benchi-postgres  | Data page checksums are disabled.
benchi-postgres  | 
benchi-postgres  | fixing permissions on existing directory /var/lib/postgresql/data ... ok
benchi-postgres  | creating subdirectories ... ok
benchi-postgres  | selecting dynamic shared memory implementation ... posix
benchi-postgres  | selecting default max_connections ... 100
benchi-postgres  | selecting default shared_buffers ... 128MB
benchi-postgres  | selecting default time zone ... Etc/UTC
benchi-postgres  | creating configuration files ... ok
benchi-postgres  | running bootstrap script ... ok
benchi-postgres  | performing post-bootstrap initialization ... ok
benchi-postgres  | initdb: warning: enabling "trust" authentication for local connections
benchi-postgres  | initdb: hint: You can change this by editing pg_hba.conf or using the option -A, or --auth-local and --auth-host, the next time you run initdb.
benchi-postgres  | syncing data to disk ... ok
benchi-postgres  | 
benchi-postgres  | 
benchi-postgres  | Success. You can now start the database server using:
benchi-postgres  | 
benchi-postgres  |     pg_ctl -D /var/lib/postgresql/data -l logfile start
benchi-postgres  | 
benchi-postgres  | waiting for server to start....2025-03-18 16:08:07.900 UTC [48] LOG:  starting PostgreSQL 16.4 (Debian 16.4-1.pgdg110+2) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 10.2.1-6) 10.2.1 20210110, 64-bit
benchi-postgres  | 2025-03-18 16:08:07.900 UTC [48] LOG:  listening on Unix socket "/var/run/postgresql/.s.PGSQL.5432"
benchi-postgres  | 2025-03-18 16:08:07.902 UTC [51] LOG:  database system was shut down at 2025-03-18 16:08:07 UTC
benchi-postgres  | 2025-03-18 16:08:07.904 UTC [48] LOG:  database system is ready to accept connections
benchi-postgres  |  done
benchi-postgres  | server started
benchi-postgres  | CREATE DATABASE
benchi-postgres  | 
benchi-postgres  | 
benchi-postgres  | /usr/local/bin/docker-entrypoint.sh: sourcing /docker-entrypoint-initdb.d/init-permissions.sh
benchi-postgres  | 
benchi-postgres  | /usr/local/bin/docker-entrypoint.sh: running /docker-entrypoint-initdb.d/init.sql
benchi-postgres  | psql:/docker-entrypoint-initdb.d/init.sql:1: NOTICE:  table "employees" does not exist, skipping
benchi-postgres  | psql:/docker-entrypoint-initdb.d/init.sql:2: NOTICE:  sequence "employees_id_seq" does not exist, skipping
benchi-postgres  | DROP TABLE
benchi-postgres  | DROP SEQUENCE
benchi-postgres  | CREATE TABLE
benchi-postgres  | CREATE SEQUENCE
benchi-postgres  | ALTER TABLE
benchi-postgres  | ALTER TABLE
benchi-postgres  | 
benchi-postgres  | 
benchi-postgres  | waiting for server to shut down...2025-03-18 16:08:08.051 UTC [48] LOG:  received fast shutdown request
benchi-postgres  | .2025-03-18 16:08:08.052 UTC [48] LOG:  aborting any active transactions
benchi-postgres  | 2025-03-18 16:08:08.053 UTC [48] LOG:  background worker "logical replication launcher" (PID 54) exited with exit code 1
benchi-postgres  | 2025-03-18 16:08:08.053 UTC [49] LOG:  shutting down
benchi-postgres  | 2025-03-18 16:08:08.054 UTC [49] LOG:  checkpoint starting: shutdown immediate
benchi-postgres  | 2025-03-18 16:08:08.080 UTC [49] LOG:  checkpoint complete: wrote 928 buffers (5.7%); 0 WAL file(s) added, 0 removed, 0 recycled; write=0.006 s, sync=0.019 s, total=0.027 s; sync files=304, longest=0.007 s, average=0.001 s; distance=4267 kB, estimate=4267 kB; lsn=0/19D91E8, redo lsn=0/19D91E8
benchi-postgres  | 2025-03-18 16:08:08.082 UTC [48] LOG:  database system is shut down
benchi-postgres  |  done
benchi-postgres  | server stopped
benchi-postgres  | 
benchi-postgres  | PostgreSQL init process complete; ready for start up.
benchi-postgres  | 
benchi-postgres  | 2025-03-18 16:08:08.162 UTC [1] LOG:  starting PostgreSQL 16.4 (Debian 16.4-1.pgdg110+2) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 10.2.1-6) 10.2.1 20210110, 64-bit
benchi-postgres  | 2025-03-18 16:08:08.162 UTC [1] LOG:  listening on IPv4 address "0.0.0.0", port 5432
benchi-postgres  | 2025-03-18 16:08:08.162 UTC [1] LOG:  listening on IPv6 address "::", port 5432
benchi-postgres  | 2025-03-18 16:08:08.163 UTC [1] LOG:  listening on Unix socket "/var/run/postgresql/.s.PGSQL.5432"
benchi-postgres  | 2025-03-18 16:08:08.166 UTC [66] LOG:  database system was shut down at 2025-03-18 16:08:08 UTC
benchi-postgres  | 2025-03-18 16:08:08.168 UTC [1] LOG:  database system is ready to accept connections
benchi-kafka     | ===> Running preflight checks ... 
benchi-kafka     | ===> Check if /var/lib/kafka/data is writable ...
benchi-kafka     | ===> Running in KRaft mode, skipping Zookeeper health check...
benchi-kafka     | ===> Using provided cluster id MkU3OEVBNTcwNTJENDM2Qk ...
benchi-kafka     | ===> Launching ... 
benchi-kafka     | ===> Launching kafka ... 
benchi-kafka     | [2025-03-18 16:08:09,982] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
benchi-kafka     | [2025-03-18 16:08:10,251] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
benchi-kafka     | [2025-03-18 16:08:10,252] INFO RemoteLogManagerConfig values: 
benchi-kafka     | 	log.local.retention.bytes = -2
benchi-kafka     | 	log.local.retention.ms = -2
benchi-kafka     | 	remote.fetch.max.wait.ms = 500
benchi-kafka     | 	remote.log.index.file.cache.total.size.bytes = 1073741824
benchi-kafka     | 	remote.log.manager.copier.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.copy.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.copy.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.expiration.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.fetch.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.fetch.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.task.interval.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.max.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.ms = 500
benchi-kafka     | 	remote.log.manager.task.retry.jitter = 0.2
benchi-kafka     | 	remote.log.manager.thread.pool.size = 10
benchi-kafka     | 	remote.log.metadata.custom.metadata.max.bytes = 128
benchi-kafka     | 	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
benchi-kafka     | 	remote.log.metadata.manager.class.path = null
benchi-kafka     | 	remote.log.metadata.manager.impl.prefix = rlmm.config.
benchi-kafka     | 	remote.log.metadata.manager.listener.name = null
benchi-kafka     | 	remote.log.reader.max.pending.tasks = 100
benchi-kafka     | 	remote.log.reader.threads = 10
benchi-kafka     | 	remote.log.storage.manager.class.name = null
benchi-kafka     | 	remote.log.storage.manager.class.path = null
benchi-kafka     | 	remote.log.storage.manager.impl.prefix = rsm.config.
benchi-kafka     | 	remote.log.storage.system.enable = false
benchi-kafka     |  (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
benchi-kafka     | [2025-03-18 16:08:10,330] INFO RemoteLogManagerConfig values: 
benchi-kafka     | 	log.local.retention.bytes = -2
benchi-kafka     | 	log.local.retention.ms = -2
benchi-kafka     | 	remote.fetch.max.wait.ms = 500
benchi-kafka     | 	remote.log.index.file.cache.total.size.bytes = 1073741824
benchi-kafka     | 	remote.log.manager.copier.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.copy.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.copy.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.expiration.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.fetch.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.fetch.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.task.interval.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.max.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.ms = 500
benchi-kafka     | 	remote.log.manager.task.retry.jitter = 0.2
benchi-kafka     | 	remote.log.manager.thread.pool.size = 10
benchi-kafka     | 	remote.log.metadata.custom.metadata.max.bytes = 128
benchi-kafka     | 	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
benchi-kafka     | 	remote.log.metadata.manager.class.path = null
benchi-kafka     | 	remote.log.metadata.manager.impl.prefix = rlmm.config.
benchi-kafka     | 	remote.log.metadata.manager.listener.name = null
benchi-kafka     | 	remote.log.reader.max.pending.tasks = 100
benchi-kafka     | 	remote.log.reader.threads = 10
benchi-kafka     | 	remote.log.storage.manager.class.name = null
benchi-kafka     | 	remote.log.storage.manager.class.path = null
benchi-kafka     | 	remote.log.storage.manager.impl.prefix = rsm.config.
benchi-kafka     | 	remote.log.storage.system.enable = false
benchi-kafka     |  (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
benchi-kafka     | [2025-03-18 16:08:10,334] INFO RemoteLogManagerConfig values: 
benchi-kafka     | 	log.local.retention.bytes = -2
benchi-kafka     | 	log.local.retention.ms = -2
benchi-kafka     | 	remote.fetch.max.wait.ms = 500
benchi-kafka     | 	remote.log.index.file.cache.total.size.bytes = 1073741824
benchi-kafka     | 	remote.log.manager.copier.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.copy.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.copy.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.expiration.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.fetch.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.fetch.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.task.interval.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.max.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.ms = 500
benchi-kafka     | 	remote.log.manager.task.retry.jitter = 0.2
benchi-kafka     | 	remote.log.manager.thread.pool.size = 10
benchi-kafka     | 	remote.log.metadata.custom.metadata.max.bytes = 128
benchi-kafka     | 	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
benchi-kafka     | 	remote.log.metadata.manager.class.path = null
benchi-kafka     | 	remote.log.metadata.manager.impl.prefix = rlmm.config.
benchi-kafka     | 	remote.log.metadata.manager.listener.name = null
benchi-kafka     | 	remote.log.reader.max.pending.tasks = 100
benchi-kafka     | 	remote.log.reader.threads = 10
benchi-kafka     | 	remote.log.storage.manager.class.name = null
benchi-kafka     | 	remote.log.storage.manager.class.path = null
benchi-kafka     | 	remote.log.storage.manager.impl.prefix = rsm.config.
benchi-kafka     | 	remote.log.storage.system.enable = false
benchi-kafka     |  (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
benchi-kafka     | [2025-03-18 16:08:10,348] INFO RemoteLogManagerConfig values: 
benchi-kafka     | 	log.local.retention.bytes = -2
benchi-kafka     | 	log.local.retention.ms = -2
benchi-kafka     | 	remote.fetch.max.wait.ms = 500
benchi-kafka     | 	remote.log.index.file.cache.total.size.bytes = 1073741824
benchi-kafka     | 	remote.log.manager.copier.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.copy.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.copy.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.expiration.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.fetch.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.fetch.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.task.interval.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.max.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.ms = 500
benchi-kafka     | 	remote.log.manager.task.retry.jitter = 0.2
benchi-kafka     | 	remote.log.manager.thread.pool.size = 10
benchi-kafka     | 	remote.log.metadata.custom.metadata.max.bytes = 128
benchi-kafka     | 	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
benchi-kafka     | 	remote.log.metadata.manager.class.path = null
benchi-kafka     | 	remote.log.metadata.manager.impl.prefix = rlmm.config.
benchi-kafka     | 	remote.log.metadata.manager.listener.name = null
benchi-kafka     | 	remote.log.reader.max.pending.tasks = 100
benchi-kafka     | 	remote.log.reader.threads = 10
benchi-kafka     | 	remote.log.storage.manager.class.name = null
benchi-kafka     | 	remote.log.storage.manager.class.path = null
benchi-kafka     | 	remote.log.storage.manager.impl.prefix = rsm.config.
benchi-kafka     | 	remote.log.storage.system.enable = false
benchi-kafka     |  (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
benchi-kafka     | [2025-03-18 16:08:10,354] INFO KafkaConfig values: 
benchi-kafka     | 	advertised.listeners = PLAINTEXT://broker:29092,PLAINTEXT_HOST://localhost:9092
benchi-kafka     | 	alter.config.policy.class.name = null
benchi-kafka     | 	alter.log.dirs.replication.quota.window.num = 11
benchi-kafka     | 	alter.log.dirs.replication.quota.window.size.seconds = 1
benchi-kafka     | 	authorizer.class.name = 
benchi-kafka     | 	auto.create.topics.enable = true
benchi-kafka     | 	auto.include.jmx.reporter = true
benchi-kafka     | 	auto.leader.rebalance.enable = true
benchi-kafka     | 	background.threads = 10
benchi-kafka     | 	broker.heartbeat.interval.ms = 2000
benchi-kafka     | 	broker.id = 1
benchi-kafka     | 	broker.id.generation.enable = true
benchi-kafka     | 	broker.interceptor.class = class org.apache.kafka.server.interceptor.DefaultBrokerInterceptor
benchi-kafka     | 	broker.rack = null
benchi-kafka     | 	broker.session.timeout.ms = 9000
benchi-kafka     | 	broker.session.uuid = iNf5MSGJTFOqkdMXHRqjNg
benchi-kafka     | 	client.quota.callback.class = null
benchi-kafka     | 	client.quota.max.throttle.time.ms = 5000
benchi-kafka     | 	compression.gzip.level = -1
benchi-kafka     | 	compression.lz4.level = 9
benchi-kafka     | 	compression.type = producer
benchi-kafka     | 	compression.zstd.level = 3
benchi-kafka     | 	confluent.alter.broker.health.max.demoted.brokers = 2147483647
benchi-kafka     | 	confluent.alter.broker.health.max.demoted.brokers.percentage = 0
benchi-kafka     | 	confluent.ansible.managed = false
benchi-kafka     | 	confluent.api.visibility = DEFAULT
benchi-kafka     | 	confluent.append.record.interceptor.classes = []
benchi-kafka     | 	confluent.apply.create.topic.policy.to.create.partitions = false
benchi-kafka     | 	confluent.authorizer.authority.name = 
benchi-kafka     | 	confluent.automatic.alter.broker.health.retry.backoff.ms = 2000
benchi-kafka     | 	confluent.backpressure.disk.enable = false
benchi-kafka     | 	confluent.backpressure.disk.free.threshold.bytes = 21474836480
benchi-kafka     | 	confluent.backpressure.disk.produce.bytes.per.second = 131072
benchi-kafka     | 	confluent.backpressure.disk.threshold.recovery.factor = 1.5
benchi-kafka     | 	confluent.backpressure.request.min.broker.limit = 200
benchi-kafka     | 	confluent.backpressure.request.queue.size.percentile = p95
benchi-kafka     | 	confluent.backpressure.types = null
benchi-kafka     | 	confluent.balancer.api.state.topic = _confluent_balancer_api_state
benchi-kafka     | 	confluent.balancer.broker.addition.elapsed.time.ms.completion.threshold = 57600000
benchi-kafka     | 	confluent.balancer.broker.addition.mean.cpu.percent.completion.threshold = 0.5
benchi-kafka     | 	confluent.balancer.capacity.threshold.upper.limit = 0.95
benchi-kafka     | 	confluent.balancer.cell.load.upper.bound = 0.7
benchi-kafka     | 	confluent.balancer.cell.overload.detection.interval.ms = 3600000
benchi-kafka     | 	confluent.balancer.cell.overload.duration.ms = 86400000
benchi-kafka     | 	confluent.balancer.class = io.confluent.databalancer.SbcDataBalanceManager
benchi-kafka     | 	confluent.balancer.consumer.out.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.balancer.cpu.balance.threshold = 1.1
benchi-kafka     | 	confluent.balancer.cpu.low.utilization.threshold = 0.2
benchi-kafka     | 	confluent.balancer.cpu.utilization.detector.duration.ms = 600000
benchi-kafka     | 	confluent.balancer.cpu.utilization.detector.enabled = false
benchi-kafka     | 	confluent.balancer.cpu.utilization.detector.overutilization.threshold = 80.0
benchi-kafka     | 	confluent.balancer.cpu.utilization.detector.underutilization.threshold = 50.0
benchi-kafka     | 	confluent.balancer.disk.max.load = 0.85
benchi-kafka     | 	confluent.balancer.disk.min.free.space.gb = 0
benchi-kafka     | 	confluent.balancer.disk.min.free.space.lower.limit.gb = 0
benchi-kafka     | 	confluent.balancer.disk.utilization.detector.duration.ms = 600000
benchi-kafka     | 	confluent.balancer.disk.utilization.detector.enabled = false
benchi-kafka     | 	confluent.balancer.disk.utilization.detector.overutilization.threshold = 80.0
benchi-kafka     | 	confluent.balancer.disk.utilization.detector.reserved.capacity = 150000.0
benchi-kafka     | 	confluent.balancer.disk.utilization.detector.underutilization.threshold = 35.0
benchi-kafka     | 	confluent.balancer.enable = true
benchi-kafka     | 	confluent.balancer.exclude.topic.names = []
benchi-kafka     | 	confluent.balancer.exclude.topic.prefixes = []
benchi-kafka     | 	confluent.balancer.goal.violation.delay.on.new.brokers.ms = 1800000
benchi-kafka     | 	confluent.balancer.goal.violation.distribution.threshold.multiplier = 1.1
benchi-kafka     | 	confluent.balancer.heal.broker.failure.threshold.ms = 3600000
benchi-kafka     | 	confluent.balancer.heal.uneven.load.trigger = EMPTY_BROKER
benchi-kafka     | 	confluent.balancer.incremental.balancing.cpu.top.proposal.tracking.enabled = true
benchi-kafka     | 	confluent.balancer.incremental.balancing.cpu.top.proposal.tracking.num.proposals = 15
benchi-kafka     | 	confluent.balancer.incremental.balancing.enabled = false
benchi-kafka     | 	confluent.balancer.incremental.balancing.goals = []
benchi-kafka     | 	confluent.balancer.incremental.balancing.lower.bound = 0.02
benchi-kafka     | 	confluent.balancer.incremental.balancing.min.valid.windows = 5
benchi-kafka     | 	confluent.balancer.incremental.balancing.step.ratio = 0.2
benchi-kafka     | 	confluent.balancer.inter.cell.balancing.enabled = false
benchi-kafka     | 	confluent.balancer.max.capacity.balancing.delta.percentage = 0.0
benchi-kafka     | 	confluent.balancer.max.replicas = 2147483647
benchi-kafka     | 	confluent.balancer.network.in.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.balancer.network.out.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.balancer.plan.computation.retry.timeout.ms = 3600000
benchi-kafka     | 	confluent.balancer.producer.in.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.balancer.rebalancing.goals = []
benchi-kafka     | 	confluent.balancer.replication.in.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.balancer.resource.utilization.detector.interval.ms = 60000
benchi-kafka     | 	confluent.balancer.sbc.metrics.parser.enabled = false
benchi-kafka     | 	confluent.balancer.self.healing.maximum.rounds = 1
benchi-kafka     | 	confluent.balancer.task.history.retention.days = 30
benchi-kafka     | 	confluent.balancer.tenant.maximum.movements = 0
benchi-kafka     | 	confluent.balancer.tenant.suspension.ms = 86400000
benchi-kafka     | 	confluent.balancer.throttle.bytes.per.second = 10485760
benchi-kafka     | 	confluent.balancer.topic.partition.maximum.movements = 5
benchi-kafka     | 	confluent.balancer.topic.partition.movement.expiration.ms = 10800000
benchi-kafka     | 	confluent.balancer.topic.partition.suspension.ms = 18000000
benchi-kafka     | 	confluent.balancer.topic.replication.factor = 1
benchi-kafka     | 	confluent.balancer.triggering.goals = []
benchi-kafka     | 	confluent.balancer.v2.addition.enabled = false
benchi-kafka     | 	confluent.basic.auth.credentials.source = null
benchi-kafka     | 	confluent.basic.auth.user.info = null
benchi-kafka     | 	confluent.bearer.auth.cache.expiry.buffer.seconds = 300
benchi-kafka     | 	confluent.bearer.auth.client.id = null
benchi-kafka     | 	confluent.bearer.auth.client.secret = null
benchi-kafka     | 	confluent.bearer.auth.credentials.source = null
benchi-kafka     | 	confluent.bearer.auth.identity.pool.id = null
benchi-kafka     | 	confluent.bearer.auth.issuer.endpoint.url = null
benchi-kafka     | 	confluent.bearer.auth.logical.cluster = null
benchi-kafka     | 	confluent.bearer.auth.scope = null
benchi-kafka     | 	confluent.bearer.auth.scope.claim.name = scope
benchi-kafka     | 	confluent.bearer.auth.sub.claim.name = sub
benchi-kafka     | 	confluent.bearer.auth.token = null
benchi-kafka     | 	confluent.broker.health.manager.enabled = true
benchi-kafka     | 	confluent.broker.health.manager.engine.request.handler.threads.stuck.criteria = AllThreadsStuck
benchi-kafka     | 	confluent.broker.health.manager.hard.kill.duration.ms = 60000
benchi-kafka     | 	confluent.broker.health.manager.mitigation.enabled = false
benchi-kafka     | 	confluent.broker.health.manager.num.samples.before.broker.suspect = 30
benchi-kafka     | 	confluent.broker.health.manager.num.samples.before.broker.unhealthy = 180
benchi-kafka     | 	confluent.broker.health.manager.percentage.unhealthy.samples.before.broker.suspect = 90
benchi-kafka     | 	confluent.broker.health.manager.percentage.unhealthy.samples.before.broker.unhealthy = 70
benchi-kafka     | 	confluent.broker.health.manager.sample.duration.ms = 1000
benchi-kafka     | 	confluent.broker.health.manager.storage.background.threads.stuck.criteria = AnyThreadStuck
benchi-kafka     | 	confluent.broker.health.manager.storage.network.threads.stuck.criteria = AnyThreadStuck
benchi-kafka     | 	confluent.broker.health.manager.storage.request.handler.threads.stuck.criteria = AnyThreadStuck
benchi-kafka     | 	confluent.broker.limit.consumer.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.broker.limit.producer.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.broker.load.advertised.limit.load = 0.8
benchi-kafka     | 	confluent.broker.load.average.service.request.time.ms = 0.1
benchi-kafka     | 	confluent.broker.load.delay.metric.start.ms = 180000
benchi-kafka     | 	confluent.broker.load.enabled = false
benchi-kafka     | 	confluent.broker.load.num.samples = 60
benchi-kafka     | 	confluent.broker.load.tenant.metric.enable = false
benchi-kafka     | 	confluent.broker.load.update.metric.tags.interval.ms = 60000
benchi-kafka     | 	confluent.broker.load.window.size.ms = 60000
benchi-kafka     | 	confluent.broker.load.workload.coefficient = 20.0
benchi-kafka     | 	confluent.broker.registration.delay.ms = 0
benchi-kafka     | 	confluent.catalog.collector.destination.topic = telemetry.events.data_catalog_source
benchi-kafka     | 	confluent.catalog.collector.enable = false
benchi-kafka     | 	confluent.catalog.collector.full.configs.enable = false
benchi-kafka     | 	confluent.catalog.collector.max.bytes.per.snapshot = 850000
benchi-kafka     | 	confluent.catalog.collector.max.topics.process = 500
benchi-kafka     | 	confluent.catalog.collector.max.zookeeper.request.per.sec = 100
benchi-kafka     | 	confluent.catalog.collector.snapshot.init.delay.sec = 60
benchi-kafka     | 	confluent.catalog.collector.snapshot.interval.sec = 300
benchi-kafka     | 	confluent.ccloud.host.suffixes = .confluent.cloud,.cpdev.cloud,.confluentgov.com,.confluentgov-internal.com
benchi-kafka     | 	confluent.ccloud.intranet.host.suffixes = .intranet.stag.cpdev.cloud,.intranet.stag.cpdev-untrusted.cloud,.intranet.devel.cpdev.cloud,.intranet.devel.cpdev-untrusted.cloud,.intranet.confluent.cloud,.intranet.confluent-untrusted.cloud
benchi-kafka     | 	confluent.cdc.api.keys.topic = 
benchi-kafka     | 	confluent.cdc.api.keys.topic.load.timeout.ms = 600000
benchi-kafka     | 	confluent.cdc.client.quotas.enable = false
benchi-kafka     | 	confluent.cdc.client.quotas.topic.name = 
benchi-kafka     | 	confluent.cdc.lkc.metadata.topic = 
benchi-kafka     | 	confluent.cdc.user.metadata.enable = false
benchi-kafka     | 	confluent.cdc.user.metadata.topic = _confluent-user_metadata
benchi-kafka     | 	confluent.cell.metrics.refresh.period.ms = 60000
benchi-kafka     | 	confluent.cells.default.size = 15
benchi-kafka     | 	confluent.cells.enable = false
benchi-kafka     | 	confluent.cells.implicit.creation.enable = false
benchi-kafka     | 	confluent.cells.load.refresher.enable = true
benchi-kafka     | 	confluent.cells.max.size = 15
benchi-kafka     | 	confluent.cells.min.size = 6
benchi-kafka     | 	confluent.checksum.enabled.files = [none]
benchi-kafka     | 	confluent.clm.enabled = false
benchi-kafka     | 	confluent.clm.frequency.in.hours = 6
benchi-kafka     | 	confluent.clm.list.object.thread_pool.size = 1
benchi-kafka     | 	confluent.clm.max.backup.days = 3
benchi-kafka     | 	confluent.clm.min.delay.in.minutes = 30
benchi-kafka     | 	confluent.clm.thread.pool.size = 2
benchi-kafka     | 	confluent.clm.topic.retention.days.to.backup.days = 0:0,3:3
benchi-kafka     | 	confluent.close.connections.on.credential.delete = false
benchi-kafka     | 	confluent.cluster.link.admin.max.in.flight.requests = 1000
benchi-kafka     | 	confluent.cluster.link.admin.request.batch.size = 1
benchi-kafka     | 	confluent.cluster.link.allow.config.providers = true
benchi-kafka     | 	confluent.cluster.link.allow.legacy.message.format = false
benchi-kafka     | 	confluent.cluster.link.allow.truncation.below.hwm = true
benchi-kafka     | 	confluent.cluster.link.availability.check.mode = ALL
benchi-kafka     | 	confluent.cluster.link.background.thread.affinity = LINK
benchi-kafka     | 	confluent.cluster.link.clients.max.idle.ms = 3153600000000
benchi-kafka     | 	confluent.cluster.link.enable = true
benchi-kafka     | 	confluent.cluster.link.enable.local.admin = false
benchi-kafka     | 	confluent.cluster.link.enable.metrics.reduction = false
benchi-kafka     | 	confluent.cluster.link.fetch.response.min.bytes = 1
benchi-kafka     | 	confluent.cluster.link.fetch.response.total.bytes = 2147483647
benchi-kafka     | 	confluent.cluster.link.fetcher.auto.tune.enable = false
benchi-kafka     | 	confluent.cluster.link.fetcher.thread.pool.mode = ENDPOINT
benchi-kafka     | 	confluent.cluster.link.insync.fetch.response.min.bytes = 1
benchi-kafka     | 	confluent.cluster.link.insync.fetch.response.total.bytes = 2147483647
benchi-kafka     | 	confluent.cluster.link.intranet.connectivity.enable = false
benchi-kafka     | 	confluent.cluster.link.io.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.cluster.link.local.reverse.connection.listener.map = null
benchi-kafka     | 	confluent.cluster.link.max.client.connections = 2147483647
benchi-kafka     | 	confluent.cluster.link.metadata.topic.create.retry.delay.ms = 1000
benchi-kafka     | 	confluent.cluster.link.metadata.topic.enable = false
benchi-kafka     | 	confluent.cluster.link.metadata.topic.min.isr = 2
benchi-kafka     | 	confluent.cluster.link.metadata.topic.partitions = 50
benchi-kafka     | 	confluent.cluster.link.metadata.topic.replication.factor = 1
benchi-kafka     | 	confluent.cluster.link.mirror.transition.batch.size = 10
benchi-kafka     | 	confluent.cluster.link.num.background.threads = 1
benchi-kafka     | 	confluent.cluster.link.periodic.task.batch.size = 2147483647
benchi-kafka     | 	confluent.cluster.link.periodic.task.min.interval.ms = 1000
benchi-kafka     | 	confluent.cluster.link.replica.fetch.connections.mode = combined
benchi-kafka     | 	confluent.cluster.link.replication.quota.mode = CLUSTER_LINK_ONLY
benchi-kafka     | 	confluent.cluster.link.replication.quota.mode.per.tenant.overrides = 
benchi-kafka     | 	confluent.cluster.link.replication.quota.window.num = 11
benchi-kafka     | 	confluent.cluster.link.replication.quota.window.size.seconds = 2
benchi-kafka     | 	confluent.cluster.link.request.quota.capacity = 400
benchi-kafka     | 	confluent.cluster.link.request.quota.request.percentage.multiplier = 1.0
benchi-kafka     | 	confluent.cluster.link.tenant.replication.quota.enable = false
benchi-kafka     | 	confluent.cluster.link.tenant.request.quota.enable = false
benchi-kafka     | 	confluent.cluster.metadata.snapshot.tier.delete.enable = false
benchi-kafka     | 	confluent.cluster.metadata.snapshot.tier.delete.maintain.min.snapshots = 3
benchi-kafka     | 	confluent.cluster.metadata.snapshot.tier.delete.retention.ms = 604800000
benchi-kafka     | 	confluent.cluster.metadata.snapshot.tier.upload.enable = false
benchi-kafka     | 	confluent.compacted.topic.prefer.tier.fetch.ms = -1
benchi-kafka     | 	confluent.connection.invalid.request.delay.enable = false
benchi-kafka     | 	confluent.connections.idle.expiry.manager.ignore.idleness.requests = []
benchi-kafka     | 	confluent.consumer.fetch.partition.pruning.enable = true
benchi-kafka     | 	confluent.consumer.lag.emitter.enabled = false
benchi-kafka     | 	confluent.consumer.lag.emitter.interval.ms = 60000
benchi-kafka     | 	confluent.dataflow.policy.watch.monitor.ms = 300000
benchi-kafka     | 	confluent.defer.isr.shrink.enable = false
benchi-kafka     | 	confluent.describe.topic.partitions.enabled = false
benchi-kafka     | 	confluent.disk.io.manager.enable = false
benchi-kafka     | 	confluent.disk.throughput.headroom = 10485760
benchi-kafka     | 	confluent.disk.throughput.limit = 10485760000
benchi-kafka     | 	confluent.disk.throughput.quota.tier.archive = 1048576000
benchi-kafka     | 	confluent.disk.throughput.quota.tier.archive.throttled = 104857600
benchi-kafka     | 	confluent.durability.audit.batch.flush.frequency.ms = 900000
benchi-kafka     | 	confluent.durability.audit.checks = PeriodicalAudit,ChecksumAudit
benchi-kafka     | 	confluent.durability.audit.enable = false
benchi-kafka     | 	confluent.durability.audit.idempotent.producer = false
benchi-kafka     | 	confluent.durability.audit.initial.job.delay.ms = 900000
benchi-kafka     | 	confluent.durability.audit.io.bytes.per.sec = 10485760
benchi-kafka     | 	confluent.durability.audit.log.ignored.event.types = 
benchi-kafka     | 	confluent.durability.audit.reporting.batch.ms = 1800000
benchi-kafka     | 	confluent.durability.audit.tier.compaction.audit.duration.ms = 14400000
benchi-kafka     | 	confluent.durability.events.allowed = OffsetChangeType,EpochChangeType,IsrExpandType,DeleteRecordsType,RetentionChangeType,StartOffsetChangeType,DeletePartitionType,HealthCheckType
benchi-kafka     | 	confluent.durability.topic.partition.count = 50
benchi-kafka     | 	confluent.durability.topic.replication.factor = 1
benchi-kafka     | 	confluent.e2e_checksum.protection.enabled = false
benchi-kafka     | 	confluent.e2e_checksum.protection.files = [none]
benchi-kafka     | 	confluent.e2e_checksum.protection.store.entry.ttl.ms = 2592000000
benchi-kafka     | 	confluent.elastic.cku.enabled = false
benchi-kafka     | 	confluent.elastic.cku.scaletozero.enabled = false
benchi-kafka     | 	confluent.eligible.controllers = []
benchi-kafka     | 	confluent.enable.stray.partition.deletion = false
benchi-kafka     | 	confluent.encryption.key.manager.rotation.interval.ms = 31536000000
benchi-kafka     | 	confluent.fetch.from.follower.require.leader.epoch.enable = false
benchi-kafka     | 	confluent.fetch.partition.pruning.enable = true
benchi-kafka     | 	confluent.floor.connection.rate.per.ip = -1.0
benchi-kafka     | 	confluent.floor.connection.rate.per.tenant = -1.0
benchi-kafka     | 	confluent.group.coordinator.offsets.batching.enable = false
benchi-kafka     | 	confluent.group.coordinator.offsets.writer.threads = 2
benchi-kafka     | 	confluent.group.metadata.load.threads = 32
benchi-kafka     | 	confluent.heap.tenured.notify.bytes = 0
benchi-kafka     | 	confluent.heap.tenured.notify.enabled = false
benchi-kafka     | 	confluent.hot.partition.ratio = 0.8
benchi-kafka     | 	confluent.http.server.start.timeout.ms = 60000
benchi-kafka     | 	confluent.http.server.stop.timeout.ms = 30000
benchi-kafka     | 	confluent.internal.metrics.enable = false
benchi-kafka     | 	confluent.internal.rest.server.bind.port = null
benchi-kafka     | 	confluent.internal.tenant.scoped.listener.name = INTERNAL_TENANT_SCOPED
benchi-kafka     | 	confluent.leader.epoch.checkpoint.checksum.enabled = false
benchi-kafka     | 	confluent.log.cleaner.timestamp.validation.enable = true
benchi-kafka     | 	confluent.log.placement.constraints = 
benchi-kafka     | 	confluent.max.connection.creation.rate.per.ip = 1.7976931348623157E308
benchi-kafka     | 	confluent.max.connection.creation.rate.per.tenant = 1.7976931348623157E308
benchi-kafka     | 	confluent.max.connection.rate.per.ip = -1.0
benchi-kafka     | 	confluent.max.connection.rate.per.tenant = -1.0
benchi-kafka     | 	confluent.max.connection.throttle.ms = null
benchi-kafka     | 	confluent.max.segment.ms = 9223372036854775807
benchi-kafka     | 	confluent.metadata.active.encryptor = null
benchi-kafka     | 	confluent.metadata.controlled.shutdown.partition.slice.delay.ms = 100
benchi-kafka     | 	confluent.metadata.encryptor.classes = null
benchi-kafka     | 	confluent.metadata.encryptor.required = false
benchi-kafka     | 	confluent.metadata.encryptor.secret.file = null
benchi-kafka     | 	confluent.metadata.encryptor.secrets = null
benchi-kafka     | 	confluent.metadata.jvm.warmup.ms = 60000
benchi-kafka     | 	confluent.metadata.leader.balance.slice.delay.ms = 100
benchi-kafka     | 	confluent.metadata.max.controlled.shutdown.partition.changes.per.slice = 1000
benchi-kafka     | 	confluent.metadata.max.leader.balance.changes.per.slice = 1000
benchi-kafka     | 	confluent.metadata.server.cluster.registry.clusters = []
benchi-kafka     | 	confluent.metadata.throttle.pre.check.for.non.wildcard.requests.enable = false
benchi-kafka     | 	confluent.metadata.throttle.pre.check.for.wildcard.requests.enable = false
benchi-kafka     | 	confluent.metrics.reporter.bootstrap.servers = localhost:9092
benchi-kafka     | 	confluent.min.acks = 0
benchi-kafka     | 	confluent.min.connection.throttle.ms = 0
benchi-kafka     | 	confluent.min.segment.ms = 1
benchi-kafka     | 	confluent.missing.id.cache.ttl.sec = 60
benchi-kafka     | 	confluent.missing.id.query.range = 20000
benchi-kafka     | 	confluent.missing.schema.cache.ttl.sec = 60
benchi-kafka     | 	confluent.mtls.enable = false
benchi-kafka     | 	confluent.mtls.listener.name = EXTERNAL
benchi-kafka     | 	confluent.mtls.sasl.authenticator.request.max.bytes = 104857600
benchi-kafka     | 	confluent.mtls.truststore.manager.class.name = null
benchi-kafka     | 	confluent.multitenant.authorizer.enable.acl.state = false
benchi-kafka     | 	confluent.multitenant.interceptor.balancer.apis.enabled = false
benchi-kafka     | 	confluent.multitenant.interceptor.collect.client.apiversions.max.per.tenant = 1000
benchi-kafka     | 	confluent.multitenant.interceptor.collect.client.apiversions.metric = false
benchi-kafka     | 	confluent.multitenant.listener.hostname.cluster.prefix.enable = false
benchi-kafka     | 	confluent.multitenant.listener.hostname.subdomain.suffix.enable = false
benchi-kafka     | 	confluent.multitenant.listener.names = null
benchi-kafka     | 	confluent.multitenant.parse.lkc.id.enable = false
benchi-kafka     | 	confluent.multitenant.parse.sni.host.name.enable = false
benchi-kafka     | 	confluent.network.health.manager.enabled = false
benchi-kafka     | 	confluent.network.health.manager.external.listener.name = EXTERNAL
benchi-kafka     | 	confluent.network.health.manager.externalconnectivitystartup.enabled = false
benchi-kafka     | 	confluent.network.health.manager.min.healthy.network.samples = 3
benchi-kafka     | 	confluent.network.health.manager.min.percentage.healthy.network.samples = 3
benchi-kafka     | 	confluent.network.health.manager.mitigation.enabled = false
benchi-kafka     | 	confluent.network.health.manager.network.sample.window.size = 120
benchi-kafka     | 	confluent.network.health.manager.sample.duration.ms = 1000
benchi-kafka     | 	confluent.oauth.flat.networking.verification.enable = false
benchi-kafka     | 	confluent.offsets.log.cleaner.delete.retention.ms = 86400000
benchi-kafka     | 	confluent.offsets.log.cleaner.max.compaction.lag.ms = 9223372036854775807
benchi-kafka     | 	confluent.offsets.log.cleaner.min.cleanable.dirty.ratio = 0.5
benchi-kafka     | 	confluent.offsets.topic.placement.constraints = 
benchi-kafka     | 	confluent.omit.network.processor.metric.tag = false
benchi-kafka     | 	confluent.operator.managed = false
benchi-kafka     | 	confluent.password.encoder.old.secret.ttl.ms = 9223372036854775807
benchi-kafka     | 	confluent.plugins.cluster.link.policy.max.destination.links.per.tenant = 10
benchi-kafka     | 	confluent.plugins.cluster.link.policy.max.source.links.per.tenant = 10
benchi-kafka     | 	confluent.plugins.topic.policy.max.partitions.per.cluster = 2147483647
benchi-kafka     | 	confluent.plugins.topic.policy.max.partitions.per.tenant = 512
benchi-kafka     | 	confluent.plugins.topic.policy.max.replicas.per.broker = 2147483647
benchi-kafka     | 	confluent.plugins.topic.policy.max.topics.per.cluster = 2147483647
benchi-kafka     | 	confluent.prefer.tier.fetch.ms = -1
benchi-kafka     | 	confluent.produce.throttle.pre.check.enable = false
benchi-kafka     | 	confluent.producer.id.cache.broker.hard.limit = -1
benchi-kafka     | 	confluent.producer.id.cache.eviction.minimal.expiration.ms = 900000
benchi-kafka     | 	confluent.producer.id.cache.extra.eviction.percentage = 0
benchi-kafka     | 	confluent.producer.id.cache.limit = 2147483647
benchi-kafka     | 	confluent.producer.id.cache.partition.hard.limit = -1
benchi-kafka     | 	confluent.producer.id.cache.tenant.hard.limit = -1
benchi-kafka     | 	confluent.producer.id.quota.manager.enable = false
benchi-kafka     | 	confluent.producer.id.quota.window.num = 11
benchi-kafka     | 	confluent.producer.id.quota.window.size.seconds = 1
benchi-kafka     | 	confluent.producer.id.throttle.enable = false
benchi-kafka     | 	confluent.producer.id.throttle.enable.threshold.percentage = 100
benchi-kafka     | 	confluent.proxy.mode.local.default = false
benchi-kafka     | 	confluent.proxy.protocol.fallback.enabled = false
benchi-kafka     | 	confluent.proxy.protocol.version = NONE
benchi-kafka     | 	confluent.quota.computing.usage.adjustment = 0.5
benchi-kafka     | 	confluent.quota.dynamic.enable = false
benchi-kafka     | 	confluent.quota.dynamic.publishing.interval.ms = 60000
benchi-kafka     | 	confluent.quota.dynamic.reporting.interval.ms = 30000
benchi-kafka     | 	confluent.quota.dynamic.reporting.min.usage = 102400
benchi-kafka     | 	confluent.quota.tenant.broker.max.consumer.rate = 13107200
benchi-kafka     | 	confluent.quota.tenant.broker.max.producer.rate = 13107200
benchi-kafka     | 	confluent.quota.tenant.default.controller.mutation.rate = 2.147483647E9
benchi-kafka     | 	confluent.quota.tenant.default.producer.id.rate = 2.147483647E9
benchi-kafka     | 	confluent.quota.tenant.fetch.multiplier = 1.0
benchi-kafka     | 	confluent.quota.tenant.follower.broker.min.consumer.rate = 10485760
benchi-kafka     | 	confluent.quota.tenant.follower.broker.min.producer.rate = 10485760
benchi-kafka     | 	confluent.quota.tenant.produce.multiplier = 1.0
benchi-kafka     | 	confluent.quota.tenant.user.quotas.enable = false
benchi-kafka     | 	confluent.regional.metadata.client.class = null
benchi-kafka     | 	confluent.regional.resource.manager.client.scheduler.threads = 2
benchi-kafka     | 	confluent.regional.resource.manager.endpoint = null
benchi-kafka     | 	confluent.regional.resource.manager.watch.endpoint = null
benchi-kafka     | 	confluent.replica.fetch.backoff.max.ms = 1000
benchi-kafka     | 	confluent.replica.fetch.connections.mode = combined
benchi-kafka     | 	confluent.replication.mode = PULL
benchi-kafka     | 	confluent.replication.push.feature.enable = false
benchi-kafka     | 	confluent.reporters.telemetry.auto.enable = true
benchi-kafka     | 	confluent.request.log.filter.class = class org.apache.kafka.common.requests.SamplingRequestLogFilter
benchi-kafka     | 	confluent.request.pipelining.enable = true
benchi-kafka     | 	confluent.request.pipelining.max.in.flight.requests.per.connection = 5
benchi-kafka     | 	confluent.require.calling.resource.identity = false
benchi-kafka     | 	confluent.require.compatible.keystore.updates = true
benchi-kafka     | 	confluent.require.confluent.issuer = false
benchi-kafka     | 	confluent.roll.check.interval.ms = 300000
benchi-kafka     | 	confluent.schema.registry.max.cache.size = 10000
benchi-kafka     | 	confluent.schema.registry.max.retries = 1
benchi-kafka     | 	confluent.schema.registry.retries.wait.ms = 0
benchi-kafka     | 	confluent.schema.registry.url = null
benchi-kafka     | 	confluent.schema.validator.interceptor.class = io.confluent.kafka.schemaregistry.validator.RecordSchemaValidator
benchi-kafka     | 	confluent.schema.validator.multitenant.enable = false
benchi-kafka     | 	confluent.schema.validator.samples.per.min = 0
benchi-kafka     | 	confluent.security.bc.approved.mode.enable = false
benchi-kafka     | 	confluent.security.event.logger.authentication.enable = false
benchi-kafka     | 	confluent.security.event.logger.authentication.event.rate.limit = -1
benchi-kafka     | 	confluent.security.event.logger.authorization.event.rate.limit = -1
benchi-kafka     | 	confluent.security.event.logger.detailed.audit.logs.filter.class = class org.apache.kafka.common.requests.DetailedRequestAuditLogFilter
benchi-kafka     | 	confluent.security.event.logger.enable = true
benchi-kafka     | 	confluent.security.event.logger.kafka.request.rate.limit = -1
benchi-kafka     | 	confluent.security.event.logger.physical.cluster.id = 
benchi-kafka     | 	confluent.security.event.router.config = 
benchi-kafka     | 	confluent.security.revoked.certificate.ids = 
benchi-kafka     | 	confluent.segment.eager.roll.enable = false
benchi-kafka     | 	confluent.segment.speculative.prefetch.enable = false
benchi-kafka     | 	confluent.spiffe.id.principal.extraction.rules = 
benchi-kafka     | 	confluent.ssl.key.password = null
benchi-kafka     | 	confluent.ssl.keystore.location = null
benchi-kafka     | 	confluent.ssl.keystore.password = null
benchi-kafka     | 	confluent.ssl.keystore.type = null
benchi-kafka     | 	confluent.ssl.protocol = null
benchi-kafka     | 	confluent.ssl.truststore.location = null
benchi-kafka     | 	confluent.ssl.truststore.password = null
benchi-kafka     | 	confluent.ssl.truststore.type = null
benchi-kafka     | 	confluent.step.connection.rate.per.ip = -1.0
benchi-kafka     | 	confluent.step.connection.rate.per.tenant = -1.0
benchi-kafka     | 	confluent.storage.probe.period.ms = -1
benchi-kafka     | 	confluent.storage.probe.slow.write.threshold.ms = 5000
benchi-kafka     | 	confluent.stray.log.delete.delay.ms = 604800000
benchi-kafka     | 	confluent.stray.log.max.deletions.per.run = 72
benchi-kafka     | 	confluent.subdomain.prefix = null
benchi-kafka     | 	confluent.subdomain.separator.map = null
benchi-kafka     | 	confluent.subdomain.separator.variable = %sep
benchi-kafka     | 	confluent.system.time.roll.enable = false
benchi-kafka     | 	confluent.telemetry.enabled = false
benchi-kafka     | 	confluent.telemetry.external.client.metrics.push.enabled = false
benchi-kafka     | 	confluent.tenant.latency.metric.enabled = false
benchi-kafka     | 	confluent.tier.archiver.num.threads = 2
benchi-kafka     | 	confluent.tier.azure.block.blob.auto.abort.threshold.bytes = 500000
benchi-kafka     | 	confluent.tier.azure.block.blob.container = null
benchi-kafka     | 	confluent.tier.azure.block.blob.cred.file.path = null
benchi-kafka     | 	confluent.tier.azure.block.blob.endpoint = null
benchi-kafka     | 	confluent.tier.azure.block.blob.prefix = 
benchi-kafka     | 	confluent.tier.backend = 
benchi-kafka     | 	confluent.tier.bucket.probe.period.ms = -1
benchi-kafka     | 	confluent.tier.cleaner.compact.min.efficiency = 0.5
benchi-kafka     | 	confluent.tier.cleaner.compact.segment.min.bytes = 20971520
benchi-kafka     | 	confluent.tier.cleaner.dedupe.buffer.size = 134217728
benchi-kafka     | 	confluent.tier.cleaner.dual.compaction = false
benchi-kafka     | 	confluent.tier.cleaner.dual.compaction.validation.max.bytes = 1073741824
benchi-kafka     | 	confluent.tier.cleaner.dual.compaction.validation.percent = 0
benchi-kafka     | 	confluent.tier.cleaner.enable = false
benchi-kafka     | 	confluent.tier.cleaner.excluded.topics = [^_confluent.*]
benchi-kafka     | 	confluent.tier.cleaner.feature.enable = false
benchi-kafka     | 	confluent.tier.cleaner.io.buffer.load.factor = 0.9
benchi-kafka     | 	confluent.tier.cleaner.io.buffer.size = 10485760
benchi-kafka     | 	confluent.tier.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
benchi-kafka     | 	confluent.tier.cleaner.min.cleanable.ratio = 0.75
benchi-kafka     | 	confluent.tier.cleaner.num.threads = 2
benchi-kafka     | 	confluent.tier.enable = false
benchi-kafka     | 	confluent.tier.feature = false
benchi-kafka     | 	confluent.tier.fenced.segment.delete.delay.ms = 600000
benchi-kafka     | 	confluent.tier.fetcher.async.enable = false
benchi-kafka     | 	confluent.tier.fetcher.async.timestamp.offset.parallelism = 1
benchi-kafka     | 	confluent.tier.fetcher.fetch.based.on.segment_and_metadata_layout.field = false
benchi-kafka     | 	confluent.tier.fetcher.memorypool.bytes = 0
benchi-kafka     | 	confluent.tier.fetcher.num.threads = 4
benchi-kafka     | 	confluent.tier.fetcher.offset.cache.expiration.ms = 1800000
benchi-kafka     | 	confluent.tier.fetcher.offset.cache.period.ms = 60000
benchi-kafka     | 	confluent.tier.fetcher.offset.cache.size = 200000
benchi-kafka     | 	confluent.tier.gcs.bucket = null
benchi-kafka     | 	confluent.tier.gcs.cred.file.path = null
benchi-kafka     | 	confluent.tier.gcs.prefix = 
benchi-kafka     | 	confluent.tier.gcs.region = null
benchi-kafka     | 	confluent.tier.gcs.sse.customer.encryption.key = null
benchi-kafka     | 	confluent.tier.gcs.write.chunk.size = 0
benchi-kafka     | 	confluent.tier.local.hotset.bytes = -1
benchi-kafka     | 	confluent.tier.local.hotset.ms = 86400000
benchi-kafka     | 	confluent.tier.max.partition.fetch.bytes.override = 0
benchi-kafka     | 	confluent.tier.metadata.bootstrap.servers = null
benchi-kafka     | 	confluent.tier.metadata.max.poll.ms = 100
benchi-kafka     | 	confluent.tier.metadata.namespace = null
benchi-kafka     | 	confluent.tier.metadata.num.partitions = 50
benchi-kafka     | 	confluent.tier.metadata.replication.factor = 1
benchi-kafka     | 	confluent.tier.metadata.request.timeout.ms = 30000
benchi-kafka     | 	confluent.tier.metadata.snapshots.enable = false
benchi-kafka     | 	confluent.tier.metadata.snapshots.interval.ms = 86400000
benchi-kafka     | 	confluent.tier.metadata.snapshots.retention.days = 7
benchi-kafka     | 	confluent.tier.metadata.snapshots.threads = 2
benchi-kafka     | 	confluent.tier.object.fetcher.num.threads = 1
benchi-kafka     | 	confluent.tier.partition.state.cleanup.delay.ms = 2592000000
benchi-kafka     | 	confluent.tier.partition.state.cleanup.enable = false
benchi-kafka     | 	confluent.tier.partition.state.cleanup.interval.ms = 86400000
benchi-kafka     | 	confluent.tier.partition.state.commit.interval.ms = 15000
benchi-kafka     | 	confluent.tier.s3.assumerole.arn = null
benchi-kafka     | 	confluent.tier.s3.auto.abort.threshold.bytes = 500000
benchi-kafka     | 	confluent.tier.s3.aws.endpoint.override = null
benchi-kafka     | 	confluent.tier.s3.aws.signer.override = null
benchi-kafka     | 	confluent.tier.s3.bucket = null
benchi-kafka     | 	confluent.tier.s3.cred.file.path = null
benchi-kafka     | 	confluent.tier.s3.force.path.style.access = false
benchi-kafka     | 	confluent.tier.s3.prefix = 
benchi-kafka     | 	confluent.tier.s3.region = null
benchi-kafka     | 	confluent.tier.s3.security.providers = null
benchi-kafka     | 	confluent.tier.s3.sse.algorithm = AES256
benchi-kafka     | 	confluent.tier.s3.sse.customer.encryption.key = null
benchi-kafka     | 	confluent.tier.s3.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	confluent.tier.s3.ssl.key.password = null
benchi-kafka     | 	confluent.tier.s3.ssl.keystore.location = null
benchi-kafka     | 	confluent.tier.s3.ssl.keystore.password = null
benchi-kafka     | 	confluent.tier.s3.ssl.keystore.type = null
benchi-kafka     | 	confluent.tier.s3.ssl.protocol = TLSv1.3
benchi-kafka     | 	confluent.tier.s3.ssl.provider = null
benchi-kafka     | 	confluent.tier.s3.ssl.truststore.location = null
benchi-kafka     | 	confluent.tier.s3.ssl.truststore.password = null
benchi-kafka     | 	confluent.tier.s3.ssl.truststore.type = null
benchi-kafka     | 	confluent.tier.s3.user.agent.prefix = APN/1.0 Confluent/1.0 TieredStorageS3/1.0
benchi-kafka     | 	confluent.tier.segment.hotset.roll.min.bytes = 104857600
benchi-kafka     | 	confluent.tier.segment.metadata.layout.put.mode = LegacyMultiObject
benchi-kafka     | 	confluent.tier.topic.data.loss.validation.fencing.enable = false
benchi-kafka     | 	confluent.tier.topic.delete.backoff.ms = 21600000
benchi-kafka     | 	confluent.tier.topic.delete.check.interval.ms = 300000
benchi-kafka     | 	confluent.tier.topic.delete.max.inprogress.partitions = 100
benchi-kafka     | 	confluent.tier.topic.head.data.loss.validation.enable = true
benchi-kafka     | 	confluent.tier.topic.head.data.loss.validation.max.timeout.ms = 900000
benchi-kafka     | 	confluent.tier.topic.materialization.from.snapshot.enable = false
benchi-kafka     | 	confluent.tier.topic.producer.enable.idempotence = true
benchi-kafka     | 	confluent.tier.topic.snapshots.enable = false
benchi-kafka     | 	confluent.tier.topic.snapshots.interval.ms = 300000
benchi-kafka     | 	confluent.tier.topic.snapshots.max.records = 100000
benchi-kafka     | 	confluent.tier.topic.snapshots.retention.hours = 168
benchi-kafka     | 	confluent.topic.partition.default.placement = 2
benchi-kafka     | 	confluent.topic.policy.use.computed.assignments = false
benchi-kafka     | 	confluent.topic.replica.assignor.builder.class = 
benchi-kafka     | 	confluent.track.api.key.per.ip = false
benchi-kafka     | 	confluent.track.per.ip.max.size = 100000
benchi-kafka     | 	confluent.track.tenant.id.per.ip = false
benchi-kafka     | 	confluent.traffic.cdc.network.id.routes.enable = false
benchi-kafka     | 	confluent.traffic.cdc.network.id.routes.listener.names = EXTERNAL_BACKCHANNEL
benchi-kafka     | 	confluent.traffic.cdc.network.id.routes.periodic.start.task.ms = 300000
benchi-kafka     | 	confluent.traffic.cdc.network.id.routes.topic.name = _confluent-network_id_routes
benchi-kafka     | 	confluent.traffic.network.id = 
benchi-kafka     | 	confluent.transaction.2pc.timeout.ms = -1
benchi-kafka     | 	confluent.transaction.logging.verbosity = 0
benchi-kafka     | 	confluent.transaction.state.log.placement.constraints = 
benchi-kafka     | 	confluent.valid.broker.rack.set = null
benchi-kafka     | 	confluent.verify.group.subscription.prefix = false
benchi-kafka     | 	confluent.virtual.topic.creation.enabled = false
benchi-kafka     | 	confluent.zookeeper.metadata.migration.controller.check.disable = false
benchi-kafka     | 	confluent.zookeeper.metadata.migration.trigger.file.path = null
benchi-kafka     | 	connection.failed.authentication.delay.ms = 100
benchi-kafka     | 	connection.min.expire.interval.ms = 250
benchi-kafka     | 	connections.max.age.ms = 3153600000000
benchi-kafka     | 	connections.max.idle.ms = 600000
benchi-kafka     | 	connections.max.reauth.ms = 0
benchi-kafka     | 	control.plane.listener.name = null
benchi-kafka     | 	controlled.shutdown.enable = true
benchi-kafka     | 	controlled.shutdown.max.retries = 3
benchi-kafka     | 	controlled.shutdown.retry.backoff.ms = 5000
benchi-kafka     | 	controller.listener.names = CONTROLLER
benchi-kafka     | 	controller.quorum.append.linger.ms = 25
benchi-kafka     | 	controller.quorum.bootstrap.servers = []
benchi-kafka     | 	controller.quorum.election.backoff.max.ms = 1000
benchi-kafka     | 	controller.quorum.election.timeout.ms = 1000
benchi-kafka     | 	controller.quorum.fetch.timeout.ms = 2000
benchi-kafka     | 	controller.quorum.request.timeout.ms = 2000
benchi-kafka     | 	controller.quorum.retry.backoff.ms = 20
benchi-kafka     | 	controller.quorum.voters = [1@broker:29093]
benchi-kafka     | 	controller.quota.window.num = 11
benchi-kafka     | 	controller.quota.window.size.seconds = 1
benchi-kafka     | 	controller.socket.timeout.ms = 30000
benchi-kafka     | 	create.cluster.link.policy.class.name = null
benchi-kafka     | 	create.topic.policy.class.name = null
benchi-kafka     | 	default.replication.factor = 1
benchi-kafka     | 	delegation.token.expiry.check.interval.ms = 3600000
benchi-kafka     | 	delegation.token.expiry.time.ms = 86400000
benchi-kafka     | 	delegation.token.master.key = null
benchi-kafka     | 	delegation.token.max.lifetime.ms = 604800000
benchi-kafka     | 	delegation.token.secret.key = null
benchi-kafka     | 	delete.records.purgatory.purge.interval.requests = 1
benchi-kafka     | 	delete.topic.enable = true
benchi-kafka     | 	early.start.listeners = null
benchi-kafka     | 	eligible.leader.replicas.enable = false
benchi-kafka     | 	enable.fips = false
benchi-kafka     | 	fetch.max.bytes = 57671680
benchi-kafka     | 	fetch.purgatory.purge.interval.requests = 1000
benchi-kafka     | 	floor.max.connection.creation.rate = null
benchi-kafka     | 	follower.replication.throttled.rate = 9223372036854775807
benchi-kafka     | 	follower.replication.throttled.replicas = none
benchi-kafka     | 	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
benchi-kafka     | 	group.consumer.heartbeat.interval.ms = 5000
benchi-kafka     | 	group.consumer.max.heartbeat.interval.ms = 15000
benchi-kafka     | 	group.consumer.max.session.timeout.ms = 60000
benchi-kafka     | 	group.consumer.max.size = 2147483647
benchi-kafka     | 	group.consumer.migration.policy = disabled
benchi-kafka     | 	group.consumer.min.heartbeat.interval.ms = 5000
benchi-kafka     | 	group.consumer.min.session.timeout.ms = 45000
benchi-kafka     | 	group.consumer.session.timeout.ms = 45000
benchi-kafka     | 	group.coordinator.append.linger.ms = 10
benchi-kafka     | 	group.coordinator.new.enable = false
benchi-kafka     | 	group.coordinator.rebalance.protocols = [classic]
benchi-kafka     | 	group.coordinator.threads = 1
benchi-kafka     | 	group.initial.rebalance.delay.ms = 0
benchi-kafka     | 	group.max.session.timeout.ms = 1800000
benchi-kafka     | 	group.max.size = 2147483647
benchi-kafka     | 	group.min.session.timeout.ms = 6000
benchi-kafka     | 	initial.broker.registration.timeout.ms = 60000
benchi-kafka     | 	inter.broker.listener.name = PLAINTEXT
benchi-kafka     | 	inter.broker.protocol.version = 3.8-IV0A
benchi-kafka     | 	k2.stack.builder.class.name = null
benchi-kafka     | 	k2.startup.timeout.ms = 60000
benchi-kafka     | 	k2.topic.metadata.max.total.partition.count = 20000
benchi-kafka     | 	k2.topic.metadata.refresh.ms = 10000
benchi-kafka     | 	kafka.metrics.polling.interval.secs = 10
benchi-kafka     | 	kafka.metrics.reporters = []
benchi-kafka     | 	leader.imbalance.check.interval.seconds = 300
benchi-kafka     | 	leader.imbalance.per.broker.percentage = 10
benchi-kafka     | 	leader.replication.throttled.rate = 9223372036854775807
benchi-kafka     | 	leader.replication.throttled.replicas = none
benchi-kafka     | 	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
benchi-kafka     | 	listeners = PLAINTEXT://broker:29092,CONTROLLER://broker:29093,PLAINTEXT_HOST://0.0.0.0:9092
benchi-kafka     | 	log.cleaner.backoff.ms = 15000
benchi-kafka     | 	log.cleaner.dedupe.buffer.size = 134217728
benchi-kafka     | 	log.cleaner.delete.retention.ms = 86400000
benchi-kafka     | 	log.cleaner.enable = true
benchi-kafka     | 	log.cleaner.hash.algorithm = MD5
benchi-kafka     | 	log.cleaner.io.buffer.load.factor = 0.9
benchi-kafka     | 	log.cleaner.io.buffer.size = 524288
benchi-kafka     | 	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
benchi-kafka     | 	log.cleaner.max.compaction.lag.ms = 9223372036854775807
benchi-kafka     | 	log.cleaner.min.cleanable.ratio = 0.5
benchi-kafka     | 	log.cleaner.min.compaction.lag.ms = 0
benchi-kafka     | 	log.cleaner.threads = 1
benchi-kafka     | 	log.cleanup.policy = [delete]
benchi-kafka     | 	log.deletion.max.segments.per.run = 2147483647
benchi-kafka     | 	log.deletion.throttler.disk.free.headroom.bytes = 21474836480
benchi-kafka     | 	log.dir = /tmp/kafka-logs
benchi-kafka     | 	log.dir.failure.timeout.ms = 30000
benchi-kafka     | 	log.dirs = /tmp/kraft-combined-logs
benchi-kafka     | 	log.flush.interval.messages = 9223372036854775807
benchi-kafka     | 	log.flush.interval.ms = null
benchi-kafka     | 	log.flush.offset.checkpoint.interval.ms = 60000
benchi-kafka     | 	log.flush.scheduler.interval.ms = 9223372036854775807
benchi-kafka     | 	log.flush.start.offset.checkpoint.interval.ms = 60000
benchi-kafka     | 	log.index.interval.bytes = 4096
benchi-kafka     | 	log.index.size.max.bytes = 10485760
benchi-kafka     | 	log.initial.task.delay.ms = 30000
benchi-kafka     | 	log.local.retention.bytes = -2
benchi-kafka     | 	log.local.retention.ms = -2
benchi-kafka     | 	log.message.downconversion.enable = true
benchi-kafka     | 	log.message.format.version = 3.0-IV1
benchi-kafka     | 	log.message.timestamp.after.max.ms = 9223372036854775807
benchi-kafka     | 	log.message.timestamp.before.max.ms = 9223372036854775807
benchi-kafka     | 	log.message.timestamp.difference.max.ms = 9223372036854775807
benchi-kafka     | 	log.message.timestamp.type = CreateTime
benchi-kafka     | 	log.preallocate = false
benchi-kafka     | 	log.retention.bytes = -1
benchi-kafka     | 	log.retention.check.interval.ms = 300000
benchi-kafka     | 	log.retention.hours = 168
benchi-kafka     | 	log.retention.minutes = null
benchi-kafka     | 	log.retention.ms = null
benchi-kafka     | 	log.roll.hours = 168
benchi-kafka     | 	log.roll.jitter.hours = 0
benchi-kafka     | 	log.roll.jitter.ms = null
benchi-kafka     | 	log.roll.ms = null
benchi-kafka     | 	log.segment.bytes = 1073741824
benchi-kafka     | 	log.segment.delete.delay.ms = 60000
benchi-kafka     | 	max.connection.creation.rate = 1.7976931348623157E308
benchi-kafka     | 	max.connection.creation.rate.per.ip.enable.threshold = 0.0
benchi-kafka     | 	max.connection.creation.rate.per.tenant.enable.threshold = 0.0
benchi-kafka     | 	max.connections = 2147483647
benchi-kafka     | 	max.connections.per.ip = 2147483647
benchi-kafka     | 	max.connections.per.ip.overrides = 
benchi-kafka     | 	max.connections.per.tenant = 0
benchi-kafka     | 	max.connections.protected.listeners = []
benchi-kafka     | 	max.connections.reap.amount = 0
benchi-kafka     | 	max.incremental.fetch.session.cache.slots = 1000
benchi-kafka     | 	max.request.partition.size.limit = 2000
benchi-kafka     | 	message.max.bytes = 1048588
benchi-kafka     | 	metadata.log.dir = null
benchi-kafka     | 	metadata.log.max.record.bytes.between.snapshots = 20971520
benchi-kafka     | 	metadata.log.max.snapshot.interval.ms = 3600000
benchi-kafka     | 	metadata.log.segment.bytes = 1073741824
benchi-kafka     | 	metadata.log.segment.min.bytes = 8388608
benchi-kafka     | 	metadata.log.segment.ms = 604800000
benchi-kafka     | 	metadata.max.idle.interval.ms = 500
benchi-kafka     | 	metadata.max.retention.bytes = 104857600
benchi-kafka     | 	metadata.max.retention.ms = 604800000
benchi-kafka     | 	metric.reporters = [io.confluent.metrics.reporter.ConfluentMetricsReporter]
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.recording.level = INFO
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	min.insync.replicas = 1
benchi-kafka     | 	multitenant.authorizer.support.resource.ids = false
benchi-kafka     | 	multitenant.metadata.class = null
benchi-kafka     | 	multitenant.metadata.dir = null
benchi-kafka     | 	multitenant.metadata.reload.delay.ms = 120000
benchi-kafka     | 	multitenant.metadata.ssl.certs.path = null
benchi-kafka     | 	multitenant.tenant.delete.batch.size = 10
benchi-kafka     | 	multitenant.tenant.delete.check.ms = 120000
benchi-kafka     | 	multitenant.tenant.delete.delay = 604800000
benchi-kafka     | 	node.id = 1
benchi-kafka     | 	num.io.threads = 8
benchi-kafka     | 	num.network.threads = 3
benchi-kafka     | 	num.partitions = 1
benchi-kafka     | 	num.recovery.threads.per.data.dir = 1
benchi-kafka     | 	num.replica.alter.log.dirs.threads = null
benchi-kafka     | 	num.replica.fetchers = 1
benchi-kafka     | 	offset.metadata.max.bytes = 4096
benchi-kafka     | 	offsets.commit.required.acks = -1
benchi-kafka     | 	offsets.commit.timeout.ms = 5000
benchi-kafka     | 	offsets.load.buffer.size = 5242880
benchi-kafka     | 	offsets.retention.check.interval.ms = 600000
benchi-kafka     | 	offsets.retention.minutes = 10080
benchi-kafka     | 	offsets.topic.compression.codec = 0
benchi-kafka     | 	offsets.topic.num.partitions = 50
benchi-kafka     | 	offsets.topic.replication.factor = 1
benchi-kafka     | 	offsets.topic.segment.bytes = 104857600
benchi-kafka     | 	otel.exporter.otlp.custom.endpoint = default
benchi-kafka     | 	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
benchi-kafka     | 	password.encoder.iterations = 4096
benchi-kafka     | 	password.encoder.key.length = 128
benchi-kafka     | 	password.encoder.keyfactory.algorithm = null
benchi-kafka     | 	password.encoder.old.secret = null
benchi-kafka     | 	password.encoder.secret = null
benchi-kafka     | 	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
benchi-kafka     | 	process.roles = [broker, controller]
benchi-kafka     | 	producer.id.expiration.check.interval.ms = 600000
benchi-kafka     | 	producer.id.expiration.ms = 86400000
benchi-kafka     | 	producer.purgatory.purge.interval.requests = 1000
benchi-kafka     | 	queued.max.request.bytes = -1
benchi-kafka     | 	queued.max.requests = 500
benchi-kafka     | 	quota.window.num = 11
benchi-kafka     | 	quota.window.size.seconds = 1
benchi-kafka     | 	quotas.consumption.expiration.time.ms = 600000
benchi-kafka     | 	quotas.expiration.interval.ms = 3600000
benchi-kafka     | 	quotas.expiration.time.ms = 604800000
benchi-kafka     | 	quotas.lazy.evaluation.threshold = 0.5
benchi-kafka     | 	quotas.topic.append.timeout.ms = 5000
benchi-kafka     | 	quotas.topic.compression.codec = 3
benchi-kafka     | 	quotas.topic.load.buffer.size = 5242880
benchi-kafka     | 	quotas.topic.num.partitions = 50
benchi-kafka     | 	quotas.topic.placement.constraints = 
benchi-kafka     | 	quotas.topic.replication.factor = 3
benchi-kafka     | 	quotas.topic.segment.bytes = 104857600
benchi-kafka     | 	remote.fetch.max.wait.ms = 500
benchi-kafka     | 	remote.log.index.file.cache.total.size.bytes = 1073741824
benchi-kafka     | 	remote.log.manager.copier.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.copy.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.copy.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.expiration.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.fetch.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.fetch.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.task.interval.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.max.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.ms = 500
benchi-kafka     | 	remote.log.manager.task.retry.jitter = 0.2
benchi-kafka     | 	remote.log.manager.thread.pool.size = 10
benchi-kafka     | 	remote.log.metadata.custom.metadata.max.bytes = 128
benchi-kafka     | 	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
benchi-kafka     | 	remote.log.metadata.manager.class.path = null
benchi-kafka     | 	remote.log.metadata.manager.impl.prefix = rlmm.config.
benchi-kafka     | 	remote.log.metadata.manager.listener.name = null
benchi-kafka     | 	remote.log.reader.max.pending.tasks = 100
benchi-kafka     | 	remote.log.reader.threads = 10
benchi-kafka     | 	remote.log.storage.manager.class.name = null
benchi-kafka     | 	remote.log.storage.manager.class.path = null
benchi-kafka     | 	remote.log.storage.manager.impl.prefix = rsm.config.
benchi-kafka     | 	remote.log.storage.system.enable = false
benchi-kafka     | 	replica.fetch.backoff.ms = 1000
benchi-kafka     | 	replica.fetch.max.bytes = 1048576
benchi-kafka     | 	replica.fetch.min.bytes = 1
benchi-kafka     | 	replica.fetch.response.max.bytes = 10485760
benchi-kafka     | 	replica.fetch.wait.max.ms = 500
benchi-kafka     | 	replica.high.watermark.checkpoint.interval.ms = 5000
benchi-kafka     | 	replica.lag.time.max.ms = 30000
benchi-kafka     | 	replica.selector.class = null
benchi-kafka     | 	replica.socket.receive.buffer.bytes = 65536
benchi-kafka     | 	replica.socket.timeout.ms = 30000
benchi-kafka     | 	replication.quota.window.num = 11
benchi-kafka     | 	replication.quota.window.size.seconds = 1
benchi-kafka     | 	request.timeout.ms = 30000
benchi-kafka     | 	reserved.broker.max.id = 1000
benchi-kafka     | 	sasl.client.callback.handler.class = null
benchi-kafka     | 	sasl.enabled.mechanisms = [GSSAPI]
benchi-kafka     | 	sasl.jaas.config = null
benchi-kafka     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
benchi-kafka     | 	sasl.kerberos.min.time.before.relogin = 60000
benchi-kafka     | 	sasl.kerberos.principal.to.local.rules = [DEFAULT]
benchi-kafka     | 	sasl.kerberos.service.name = null
benchi-kafka     | 	sasl.kerberos.ticket.renew.jitter = 0.05
benchi-kafka     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
benchi-kafka     | 	sasl.login.callback.handler.class = null
benchi-kafka     | 	sasl.login.class = null
benchi-kafka     | 	sasl.login.connect.timeout.ms = null
benchi-kafka     | 	sasl.login.read.timeout.ms = null
benchi-kafka     | 	sasl.login.refresh.buffer.seconds = 300
benchi-kafka     | 	sasl.login.refresh.min.period.seconds = 60
benchi-kafka     | 	sasl.login.refresh.window.factor = 0.8
benchi-kafka     | 	sasl.login.refresh.window.jitter = 0.05
benchi-kafka     | 	sasl.login.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.login.retry.backoff.ms = 100
benchi-kafka     | 	sasl.mechanism.controller.protocol = GSSAPI
benchi-kafka     | 	sasl.mechanism.inter.broker.protocol = GSSAPI
benchi-kafka     | 	sasl.oauthbearer.clientassertion.audience = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.expiration = 5
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.jti.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.nbf.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.issuer = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.location = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key.passphrase = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.subject = null
benchi-kafka     | 	sasl.oauthbearer.clock.skew.seconds = 30
benchi-kafka     | 	sasl.oauthbearer.expected.audience = null
benchi-kafka     | 	sasl.oauthbearer.expected.issuer = null
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.url = null
benchi-kafka     | 	sasl.oauthbearer.scope.claim.name = scope
benchi-kafka     | 	sasl.oauthbearer.sub.claim.name = sub
benchi-kafka     | 	sasl.oauthbearer.token.endpoint.url = null
benchi-kafka     | 	sasl.server.authn.async.enable = false
benchi-kafka     | 	sasl.server.authn.async.max.threads = 1
benchi-kafka     | 	sasl.server.authn.async.timeout.ms = 30000
benchi-kafka     | 	sasl.server.callback.handler.class = null
benchi-kafka     | 	sasl.server.max.receive.size = 524288
benchi-kafka     | 	security.inter.broker.protocol = PLAINTEXT
benchi-kafka     | 	security.providers = null
benchi-kafka     | 	server.max.startup.time.ms = 9223372036854775807
benchi-kafka     | 	socket.connection.setup.timeout.max.ms = 30000
benchi-kafka     | 	socket.connection.setup.timeout.ms = 10000
benchi-kafka     | 	socket.listen.backlog.size = 50
benchi-kafka     | 	socket.receive.buffer.bytes = 102400
benchi-kafka     | 	socket.request.max.bytes = 104857600
benchi-kafka     | 	socket.send.buffer.bytes = 102400
benchi-kafka     | 	ssl.allow.dn.changes = false
benchi-kafka     | 	ssl.allow.san.changes = false
benchi-kafka     | 	ssl.cipher.suites = []
benchi-kafka     | 	ssl.client.auth = none
benchi-kafka     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	ssl.engine.factory.class = null
benchi-kafka     | 	ssl.key.password = null
benchi-kafka     | 	ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	ssl.keystore.certificate.chain = null
benchi-kafka     | 	ssl.keystore.key = null
benchi-kafka     | 	ssl.keystore.location = null
benchi-kafka     | 	ssl.keystore.password = null
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.principal.mapping.rules = DEFAULT
benchi-kafka     | 	ssl.protocol = TLSv1.3
benchi-kafka     | 	ssl.provider = null
benchi-kafka     | 	ssl.secure.random.implementation = null
benchi-kafka     | 	ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	ssl.truststore.certificates = null
benchi-kafka     | 	ssl.truststore.location = null
benchi-kafka     | 	ssl.truststore.password = null
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     | 	telemetry.max.bytes = 1048576
benchi-kafka     | 	throughput.quota.window.num = 11
benchi-kafka     | 	token.impersonation.validation = true
benchi-kafka     | 	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
benchi-kafka     | 	transaction.max.timeout.ms = 900000
benchi-kafka     | 	transaction.metadata.load.threads = 32
benchi-kafka     | 	transaction.partition.verification.enable = true
benchi-kafka     | 	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
benchi-kafka     | 	transaction.state.log.load.buffer.size = 5242880
benchi-kafka     | 	transaction.state.log.min.isr = 1
benchi-kafka     | 	transaction.state.log.num.partitions = 50
benchi-kafka     | 	transaction.state.log.replication.factor = 1
benchi-kafka     | 	transaction.state.log.segment.bytes = 104857600
benchi-kafka     | 	transactional.id.expiration.ms = 604800000
benchi-kafka     | 	unclean.leader.election.enable = false
benchi-kafka     | 	unstable.api.versions.enable = false
benchi-kafka     | 	unstable.feature.versions.enable = false
benchi-kafka     | 	zookeeper.acl.change.notification.expiration.ms = 900000
benchi-kafka     | 	zookeeper.clientCnxnSocket = null
benchi-kafka     | 	zookeeper.connect = 
benchi-kafka     | 	zookeeper.connection.timeout.ms = null
benchi-kafka     | 	zookeeper.max.in.flight.requests = 10
benchi-kafka     | 	zookeeper.metadata.migration.enable = false
benchi-kafka     | 	zookeeper.metadata.migration.min.batch.size = 200
benchi-kafka     | 	zookeeper.session.timeout.ms = 18000
benchi-kafka     | 	zookeeper.set.acl = false
benchi-kafka     | 	zookeeper.ssl.cipher.suites = null
benchi-kafka     | 	zookeeper.ssl.client.enable = false
benchi-kafka     | 	zookeeper.ssl.crl.enable = false
benchi-kafka     | 	zookeeper.ssl.enabled.protocols = null
benchi-kafka     | 	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
benchi-kafka     | 	zookeeper.ssl.keystore.location = null
benchi-kafka     | 	zookeeper.ssl.keystore.password = null
benchi-kafka     | 	zookeeper.ssl.keystore.type = null
benchi-kafka     | 	zookeeper.ssl.ocsp.enable = false
benchi-kafka     | 	zookeeper.ssl.protocol = TLSv1.2
benchi-kafka     | 	zookeeper.ssl.truststore.location = null
benchi-kafka     | 	zookeeper.ssl.truststore.password = null
benchi-kafka     | 	zookeeper.ssl.truststore.type = null
benchi-kafka     |  (kafka.server.KafkaConfig)
benchi-kafka     | [2025-03-18 16:08:10,357] INFO RemoteLogManagerConfig values: 
benchi-kafka     | 	log.local.retention.bytes = -2
benchi-kafka     | 	log.local.retention.ms = -2
benchi-kafka     | 	remote.fetch.max.wait.ms = 500
benchi-kafka     | 	remote.log.index.file.cache.total.size.bytes = 1073741824
benchi-kafka     | 	remote.log.manager.copier.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.copy.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.copy.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.expiration.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.fetch.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.fetch.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.task.interval.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.max.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.ms = 500
benchi-kafka     | 	remote.log.manager.task.retry.jitter = 0.2
benchi-kafka     | 	remote.log.manager.thread.pool.size = 10
benchi-kafka     | 	remote.log.metadata.custom.metadata.max.bytes = 128
benchi-kafka     | 	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
benchi-kafka     | 	remote.log.metadata.manager.class.path = null
benchi-kafka     | 	remote.log.metadata.manager.impl.prefix = rlmm.config.
benchi-kafka     | 	remote.log.metadata.manager.listener.name = null
benchi-kafka     | 	remote.log.reader.max.pending.tasks = 100
benchi-kafka     | 	remote.log.reader.threads = 10
benchi-kafka     | 	remote.log.storage.manager.class.name = null
benchi-kafka     | 	remote.log.storage.manager.class.path = null
benchi-kafka     | 	remote.log.storage.manager.impl.prefix = rsm.config.
benchi-kafka     | 	remote.log.storage.system.enable = false
benchi-kafka     |  (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
benchi-kafka     | [2025-03-18 16:08:10,367] INFO RemoteLogManagerConfig values: 
benchi-kafka     | 	log.local.retention.bytes = -2
benchi-kafka     | 	log.local.retention.ms = -2
benchi-kafka     | 	remote.fetch.max.wait.ms = 500
benchi-kafka     | 	remote.log.index.file.cache.total.size.bytes = 1073741824
benchi-kafka     | 	remote.log.manager.copier.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.copy.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.copy.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.expiration.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.fetch.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.fetch.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.task.interval.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.max.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.ms = 500
benchi-kafka     | 	remote.log.manager.task.retry.jitter = 0.2
benchi-kafka     | 	remote.log.manager.thread.pool.size = 10
benchi-kafka     | 	remote.log.metadata.custom.metadata.max.bytes = 128
benchi-kafka     | 	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
benchi-kafka     | 	remote.log.metadata.manager.class.path = null
benchi-kafka     | 	remote.log.metadata.manager.impl.prefix = rlmm.config.
benchi-kafka     | 	remote.log.metadata.manager.listener.name = null
benchi-kafka     | 	remote.log.reader.max.pending.tasks = 100
benchi-kafka     | 	remote.log.reader.threads = 10
benchi-kafka     | 	remote.log.storage.manager.class.name = null
benchi-kafka     | 	remote.log.storage.manager.class.path = null
benchi-kafka     | 	remote.log.storage.manager.impl.prefix = rsm.config.
benchi-kafka     | 	remote.log.storage.system.enable = false
benchi-kafka     |  (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
benchi-kafka     | [2025-03-18 16:08:10,370] INFO KafkaConfig values: 
benchi-kafka     | 	advertised.listeners = PLAINTEXT://broker:29092,PLAINTEXT_HOST://localhost:9092
benchi-kafka     | 	alter.config.policy.class.name = null
benchi-kafka     | 	alter.log.dirs.replication.quota.window.num = 11
benchi-kafka     | 	alter.log.dirs.replication.quota.window.size.seconds = 1
benchi-kafka     | 	authorizer.class.name = 
benchi-kafka     | 	auto.create.topics.enable = true
benchi-kafka     | 	auto.include.jmx.reporter = true
benchi-kafka     | 	auto.leader.rebalance.enable = true
benchi-kafka     | 	background.threads = 10
benchi-kafka     | 	broker.heartbeat.interval.ms = 2000
benchi-kafka     | 	broker.id = 1
benchi-kafka     | 	broker.id.generation.enable = true
benchi-kafka     | 	broker.interceptor.class = class org.apache.kafka.server.interceptor.DefaultBrokerInterceptor
benchi-kafka     | 	broker.rack = null
benchi-kafka     | 	broker.session.timeout.ms = 9000
benchi-kafka     | 	broker.session.uuid = iNf5MSGJTFOqkdMXHRqjNg
benchi-kafka     | 	client.quota.callback.class = null
benchi-kafka     | 	client.quota.max.throttle.time.ms = 5000
benchi-kafka     | 	compression.gzip.level = -1
benchi-kafka     | 	compression.lz4.level = 9
benchi-kafka     | 	compression.type = producer
benchi-kafka     | 	compression.zstd.level = 3
benchi-kafka     | 	confluent.alter.broker.health.max.demoted.brokers = 2147483647
benchi-kafka     | 	confluent.alter.broker.health.max.demoted.brokers.percentage = 0
benchi-kafka     | 	confluent.ansible.managed = false
benchi-kafka     | 	confluent.api.visibility = DEFAULT
benchi-kafka     | 	confluent.append.record.interceptor.classes = []
benchi-kafka     | 	confluent.apply.create.topic.policy.to.create.partitions = false
benchi-kafka     | 	confluent.authorizer.authority.name = 
benchi-kafka     | 	confluent.automatic.alter.broker.health.retry.backoff.ms = 2000
benchi-kafka     | 	confluent.backpressure.disk.enable = false
benchi-kafka     | 	confluent.backpressure.disk.free.threshold.bytes = 21474836480
benchi-kafka     | 	confluent.backpressure.disk.produce.bytes.per.second = 131072
benchi-kafka     | 	confluent.backpressure.disk.threshold.recovery.factor = 1.5
benchi-kafka     | 	confluent.backpressure.request.min.broker.limit = 200
benchi-kafka     | 	confluent.backpressure.request.queue.size.percentile = p95
benchi-kafka     | 	confluent.backpressure.types = null
benchi-kafka     | 	confluent.balancer.api.state.topic = _confluent_balancer_api_state
benchi-kafka     | 	confluent.balancer.broker.addition.elapsed.time.ms.completion.threshold = 57600000
benchi-kafka     | 	confluent.balancer.broker.addition.mean.cpu.percent.completion.threshold = 0.5
benchi-kafka     | 	confluent.balancer.capacity.threshold.upper.limit = 0.95
benchi-kafka     | 	confluent.balancer.cell.load.upper.bound = 0.7
benchi-kafka     | 	confluent.balancer.cell.overload.detection.interval.ms = 3600000
benchi-kafka     | 	confluent.balancer.cell.overload.duration.ms = 86400000
benchi-kafka     | 	confluent.balancer.class = io.confluent.databalancer.SbcDataBalanceManager
benchi-kafka     | 	confluent.balancer.consumer.out.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.balancer.cpu.balance.threshold = 1.1
benchi-kafka     | 	confluent.balancer.cpu.low.utilization.threshold = 0.2
benchi-kafka     | 	confluent.balancer.cpu.utilization.detector.duration.ms = 600000
benchi-kafka     | 	confluent.balancer.cpu.utilization.detector.enabled = false
benchi-kafka     | 	confluent.balancer.cpu.utilization.detector.overutilization.threshold = 80.0
benchi-kafka     | 	confluent.balancer.cpu.utilization.detector.underutilization.threshold = 50.0
benchi-kafka     | 	confluent.balancer.disk.max.load = 0.85
benchi-kafka     | 	confluent.balancer.disk.min.free.space.gb = 0
benchi-kafka     | 	confluent.balancer.disk.min.free.space.lower.limit.gb = 0
benchi-kafka     | 	confluent.balancer.disk.utilization.detector.duration.ms = 600000
benchi-kafka     | 	confluent.balancer.disk.utilization.detector.enabled = false
benchi-kafka     | 	confluent.balancer.disk.utilization.detector.overutilization.threshold = 80.0
benchi-kafka     | 	confluent.balancer.disk.utilization.detector.reserved.capacity = 150000.0
benchi-kafka     | 	confluent.balancer.disk.utilization.detector.underutilization.threshold = 35.0
benchi-kafka     | 	confluent.balancer.enable = true
benchi-kafka     | 	confluent.balancer.exclude.topic.names = []
benchi-kafka     | 	confluent.balancer.exclude.topic.prefixes = []
benchi-kafka     | 	confluent.balancer.goal.violation.delay.on.new.brokers.ms = 1800000
benchi-kafka     | 	confluent.balancer.goal.violation.distribution.threshold.multiplier = 1.1
benchi-kafka     | 	confluent.balancer.heal.broker.failure.threshold.ms = 3600000
benchi-kafka     | 	confluent.balancer.heal.uneven.load.trigger = EMPTY_BROKER
benchi-kafka     | 	confluent.balancer.incremental.balancing.cpu.top.proposal.tracking.enabled = true
benchi-kafka     | 	confluent.balancer.incremental.balancing.cpu.top.proposal.tracking.num.proposals = 15
benchi-kafka     | 	confluent.balancer.incremental.balancing.enabled = false
benchi-kafka     | 	confluent.balancer.incremental.balancing.goals = []
benchi-kafka     | 	confluent.balancer.incremental.balancing.lower.bound = 0.02
benchi-kafka     | 	confluent.balancer.incremental.balancing.min.valid.windows = 5
benchi-kafka     | 	confluent.balancer.incremental.balancing.step.ratio = 0.2
benchi-kafka     | 	confluent.balancer.inter.cell.balancing.enabled = false
benchi-kafka     | 	confluent.balancer.max.capacity.balancing.delta.percentage = 0.0
benchi-kafka     | 	confluent.balancer.max.replicas = 2147483647
benchi-kafka     | 	confluent.balancer.network.in.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.balancer.network.out.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.balancer.plan.computation.retry.timeout.ms = 3600000
benchi-kafka     | 	confluent.balancer.producer.in.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.balancer.rebalancing.goals = []
benchi-kafka     | 	confluent.balancer.replication.in.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.balancer.resource.utilization.detector.interval.ms = 60000
benchi-kafka     | 	confluent.balancer.sbc.metrics.parser.enabled = false
benchi-kafka     | 	confluent.balancer.self.healing.maximum.rounds = 1
benchi-kafka     | 	confluent.balancer.task.history.retention.days = 30
benchi-kafka     | 	confluent.balancer.tenant.maximum.movements = 0
benchi-kafka     | 	confluent.balancer.tenant.suspension.ms = 86400000
benchi-kafka     | 	confluent.balancer.throttle.bytes.per.second = 10485760
benchi-kafka     | 	confluent.balancer.topic.partition.maximum.movements = 5
benchi-kafka     | 	confluent.balancer.topic.partition.movement.expiration.ms = 10800000
benchi-kafka     | 	confluent.balancer.topic.partition.suspension.ms = 18000000
benchi-kafka     | 	confluent.balancer.topic.replication.factor = 1
benchi-kafka     | 	confluent.balancer.triggering.goals = []
benchi-kafka     | 	confluent.balancer.v2.addition.enabled = false
benchi-kafka     | 	confluent.basic.auth.credentials.source = null
benchi-kafka     | 	confluent.basic.auth.user.info = null
benchi-kafka     | 	confluent.bearer.auth.cache.expiry.buffer.seconds = 300
benchi-kafka     | 	confluent.bearer.auth.client.id = null
benchi-kafka     | 	confluent.bearer.auth.client.secret = null
benchi-kafka     | 	confluent.bearer.auth.credentials.source = null
benchi-kafka     | 	confluent.bearer.auth.identity.pool.id = null
benchi-kafka     | 	confluent.bearer.auth.issuer.endpoint.url = null
benchi-kafka     | 	confluent.bearer.auth.logical.cluster = null
benchi-kafka     | 	confluent.bearer.auth.scope = null
benchi-kafka     | 	confluent.bearer.auth.scope.claim.name = scope
benchi-kafka     | 	confluent.bearer.auth.sub.claim.name = sub
benchi-kafka     | 	confluent.bearer.auth.token = null
benchi-kafka     | 	confluent.broker.health.manager.enabled = true
benchi-kafka     | 	confluent.broker.health.manager.engine.request.handler.threads.stuck.criteria = AllThreadsStuck
benchi-kafka     | 	confluent.broker.health.manager.hard.kill.duration.ms = 60000
benchi-kafka     | 	confluent.broker.health.manager.mitigation.enabled = false
benchi-kafka     | 	confluent.broker.health.manager.num.samples.before.broker.suspect = 30
benchi-kafka     | 	confluent.broker.health.manager.num.samples.before.broker.unhealthy = 180
benchi-kafka     | 	confluent.broker.health.manager.percentage.unhealthy.samples.before.broker.suspect = 90
benchi-kafka     | 	confluent.broker.health.manager.percentage.unhealthy.samples.before.broker.unhealthy = 70
benchi-kafka     | 	confluent.broker.health.manager.sample.duration.ms = 1000
benchi-kafka     | 	confluent.broker.health.manager.storage.background.threads.stuck.criteria = AnyThreadStuck
benchi-kafka     | 	confluent.broker.health.manager.storage.network.threads.stuck.criteria = AnyThreadStuck
benchi-kafka     | 	confluent.broker.health.manager.storage.request.handler.threads.stuck.criteria = AnyThreadStuck
benchi-kafka     | 	confluent.broker.limit.consumer.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.broker.limit.producer.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.broker.load.advertised.limit.load = 0.8
benchi-kafka     | 	confluent.broker.load.average.service.request.time.ms = 0.1
benchi-kafka     | 	confluent.broker.load.delay.metric.start.ms = 180000
benchi-kafka     | 	confluent.broker.load.enabled = false
benchi-kafka     | 	confluent.broker.load.num.samples = 60
benchi-kafka     | 	confluent.broker.load.tenant.metric.enable = false
benchi-kafka     | 	confluent.broker.load.update.metric.tags.interval.ms = 60000
benchi-kafka     | 	confluent.broker.load.window.size.ms = 60000
benchi-kafka     | 	confluent.broker.load.workload.coefficient = 20.0
benchi-kafka     | 	confluent.broker.registration.delay.ms = 0
benchi-kafka     | 	confluent.catalog.collector.destination.topic = telemetry.events.data_catalog_source
benchi-kafka     | 	confluent.catalog.collector.enable = false
benchi-kafka     | 	confluent.catalog.collector.full.configs.enable = false
benchi-kafka     | 	confluent.catalog.collector.max.bytes.per.snapshot = 850000
benchi-kafka     | 	confluent.catalog.collector.max.topics.process = 500
benchi-kafka     | 	confluent.catalog.collector.max.zookeeper.request.per.sec = 100
benchi-kafka     | 	confluent.catalog.collector.snapshot.init.delay.sec = 60
benchi-kafka     | 	confluent.catalog.collector.snapshot.interval.sec = 300
benchi-kafka     | 	confluent.ccloud.host.suffixes = .confluent.cloud,.cpdev.cloud,.confluentgov.com,.confluentgov-internal.com
benchi-kafka     | 	confluent.ccloud.intranet.host.suffixes = .intranet.stag.cpdev.cloud,.intranet.stag.cpdev-untrusted.cloud,.intranet.devel.cpdev.cloud,.intranet.devel.cpdev-untrusted.cloud,.intranet.confluent.cloud,.intranet.confluent-untrusted.cloud
benchi-kafka     | 	confluent.cdc.api.keys.topic = 
benchi-kafka     | 	confluent.cdc.api.keys.topic.load.timeout.ms = 600000
benchi-kafka     | 	confluent.cdc.client.quotas.enable = false
benchi-kafka     | 	confluent.cdc.client.quotas.topic.name = 
benchi-kafka     | 	confluent.cdc.lkc.metadata.topic = 
benchi-kafka     | 	confluent.cdc.user.metadata.enable = false
benchi-kafka     | 	confluent.cdc.user.metadata.topic = _confluent-user_metadata
benchi-kafka     | 	confluent.cell.metrics.refresh.period.ms = 60000
benchi-kafka     | 	confluent.cells.default.size = 15
benchi-kafka     | 	confluent.cells.enable = false
benchi-kafka     | 	confluent.cells.implicit.creation.enable = false
benchi-kafka     | 	confluent.cells.load.refresher.enable = true
benchi-kafka     | 	confluent.cells.max.size = 15
benchi-kafka     | 	confluent.cells.min.size = 6
benchi-kafka     | 	confluent.checksum.enabled.files = [none]
benchi-kafka     | 	confluent.clm.enabled = false
benchi-kafka     | 	confluent.clm.frequency.in.hours = 6
benchi-kafka     | 	confluent.clm.list.object.thread_pool.size = 1
benchi-kafka     | 	confluent.clm.max.backup.days = 3
benchi-kafka     | 	confluent.clm.min.delay.in.minutes = 30
benchi-kafka     | 	confluent.clm.thread.pool.size = 2
benchi-kafka     | 	confluent.clm.topic.retention.days.to.backup.days = 0:0,3:3
benchi-kafka     | 	confluent.close.connections.on.credential.delete = false
benchi-kafka     | 	confluent.cluster.link.admin.max.in.flight.requests = 1000
benchi-kafka     | 	confluent.cluster.link.admin.request.batch.size = 1
benchi-kafka     | 	confluent.cluster.link.allow.config.providers = true
benchi-kafka     | 	confluent.cluster.link.allow.legacy.message.format = false
benchi-kafka     | 	confluent.cluster.link.allow.truncation.below.hwm = true
benchi-kafka     | 	confluent.cluster.link.availability.check.mode = ALL
benchi-kafka     | 	confluent.cluster.link.background.thread.affinity = LINK
benchi-kafka     | 	confluent.cluster.link.clients.max.idle.ms = 3153600000000
benchi-kafka     | 	confluent.cluster.link.enable = true
benchi-kafka     | 	confluent.cluster.link.enable.local.admin = false
benchi-kafka     | 	confluent.cluster.link.enable.metrics.reduction = false
benchi-kafka     | 	confluent.cluster.link.fetch.response.min.bytes = 1
benchi-kafka     | 	confluent.cluster.link.fetch.response.total.bytes = 2147483647
benchi-kafka     | 	confluent.cluster.link.fetcher.auto.tune.enable = false
benchi-kafka     | 	confluent.cluster.link.fetcher.thread.pool.mode = ENDPOINT
benchi-kafka     | 	confluent.cluster.link.insync.fetch.response.min.bytes = 1
benchi-kafka     | 	confluent.cluster.link.insync.fetch.response.total.bytes = 2147483647
benchi-kafka     | 	confluent.cluster.link.intranet.connectivity.enable = false
benchi-kafka     | 	confluent.cluster.link.io.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.cluster.link.local.reverse.connection.listener.map = null
benchi-kafka     | 	confluent.cluster.link.max.client.connections = 2147483647
benchi-kafka     | 	confluent.cluster.link.metadata.topic.create.retry.delay.ms = 1000
benchi-kafka     | 	confluent.cluster.link.metadata.topic.enable = false
benchi-kafka     | 	confluent.cluster.link.metadata.topic.min.isr = 2
benchi-kafka     | 	confluent.cluster.link.metadata.topic.partitions = 50
benchi-kafka     | 	confluent.cluster.link.metadata.topic.replication.factor = 1
benchi-kafka     | 	confluent.cluster.link.mirror.transition.batch.size = 10
benchi-kafka     | 	confluent.cluster.link.num.background.threads = 1
benchi-kafka     | 	confluent.cluster.link.periodic.task.batch.size = 2147483647
benchi-kafka     | 	confluent.cluster.link.periodic.task.min.interval.ms = 1000
benchi-kafka     | 	confluent.cluster.link.replica.fetch.connections.mode = combined
benchi-kafka     | 	confluent.cluster.link.replication.quota.mode = CLUSTER_LINK_ONLY
benchi-kafka     | 	confluent.cluster.link.replication.quota.mode.per.tenant.overrides = 
benchi-kafka     | 	confluent.cluster.link.replication.quota.window.num = 11
benchi-kafka     | 	confluent.cluster.link.replication.quota.window.size.seconds = 2
benchi-kafka     | 	confluent.cluster.link.request.quota.capacity = 400
benchi-kafka     | 	confluent.cluster.link.request.quota.request.percentage.multiplier = 1.0
benchi-kafka     | 	confluent.cluster.link.tenant.replication.quota.enable = false
benchi-kafka     | 	confluent.cluster.link.tenant.request.quota.enable = false
benchi-kafka     | 	confluent.cluster.metadata.snapshot.tier.delete.enable = false
benchi-kafka     | 	confluent.cluster.metadata.snapshot.tier.delete.maintain.min.snapshots = 3
benchi-kafka     | 	confluent.cluster.metadata.snapshot.tier.delete.retention.ms = 604800000
benchi-kafka     | 	confluent.cluster.metadata.snapshot.tier.upload.enable = false
benchi-kafka     | 	confluent.compacted.topic.prefer.tier.fetch.ms = -1
benchi-kafka     | 	confluent.connection.invalid.request.delay.enable = false
benchi-kafka     | 	confluent.connections.idle.expiry.manager.ignore.idleness.requests = []
benchi-kafka     | 	confluent.consumer.fetch.partition.pruning.enable = true
benchi-kafka     | 	confluent.consumer.lag.emitter.enabled = false
benchi-kafka     | 	confluent.consumer.lag.emitter.interval.ms = 60000
benchi-kafka     | 	confluent.dataflow.policy.watch.monitor.ms = 300000
benchi-kafka     | 	confluent.defer.isr.shrink.enable = false
benchi-kafka     | 	confluent.describe.topic.partitions.enabled = false
benchi-kafka     | 	confluent.disk.io.manager.enable = false
benchi-kafka     | 	confluent.disk.throughput.headroom = 10485760
benchi-kafka     | 	confluent.disk.throughput.limit = 10485760000
benchi-kafka     | 	confluent.disk.throughput.quota.tier.archive = 1048576000
benchi-kafka     | 	confluent.disk.throughput.quota.tier.archive.throttled = 104857600
benchi-kafka     | 	confluent.durability.audit.batch.flush.frequency.ms = 900000
benchi-kafka     | 	confluent.durability.audit.checks = PeriodicalAudit,ChecksumAudit
benchi-kafka     | 	confluent.durability.audit.enable = false
benchi-kafka     | 	confluent.durability.audit.idempotent.producer = false
benchi-kafka     | 	confluent.durability.audit.initial.job.delay.ms = 900000
benchi-kafka     | 	confluent.durability.audit.io.bytes.per.sec = 10485760
benchi-kafka     | 	confluent.durability.audit.log.ignored.event.types = 
benchi-kafka     | 	confluent.durability.audit.reporting.batch.ms = 1800000
benchi-kafka     | 	confluent.durability.audit.tier.compaction.audit.duration.ms = 14400000
benchi-kafka     | 	confluent.durability.events.allowed = OffsetChangeType,EpochChangeType,IsrExpandType,DeleteRecordsType,RetentionChangeType,StartOffsetChangeType,DeletePartitionType,HealthCheckType
benchi-kafka     | 	confluent.durability.topic.partition.count = 50
benchi-kafka     | 	confluent.durability.topic.replication.factor = 1
benchi-kafka     | 	confluent.e2e_checksum.protection.enabled = false
benchi-kafka     | 	confluent.e2e_checksum.protection.files = [none]
benchi-kafka     | 	confluent.e2e_checksum.protection.store.entry.ttl.ms = 2592000000
benchi-kafka     | 	confluent.elastic.cku.enabled = false
benchi-kafka     | 	confluent.elastic.cku.scaletozero.enabled = false
benchi-kafka     | 	confluent.eligible.controllers = []
benchi-kafka     | 	confluent.enable.stray.partition.deletion = false
benchi-kafka     | 	confluent.encryption.key.manager.rotation.interval.ms = 31536000000
benchi-kafka     | 	confluent.fetch.from.follower.require.leader.epoch.enable = false
benchi-kafka     | 	confluent.fetch.partition.pruning.enable = true
benchi-kafka     | 	confluent.floor.connection.rate.per.ip = -1.0
benchi-kafka     | 	confluent.floor.connection.rate.per.tenant = -1.0
benchi-kafka     | 	confluent.group.coordinator.offsets.batching.enable = false
benchi-kafka     | 	confluent.group.coordinator.offsets.writer.threads = 2
benchi-kafka     | 	confluent.group.metadata.load.threads = 32
benchi-kafka     | 	confluent.heap.tenured.notify.bytes = 0
benchi-kafka     | 	confluent.heap.tenured.notify.enabled = false
benchi-kafka     | 	confluent.hot.partition.ratio = 0.8
benchi-kafka     | 	confluent.http.server.start.timeout.ms = 60000
benchi-kafka     | 	confluent.http.server.stop.timeout.ms = 30000
benchi-kafka     | 	confluent.internal.metrics.enable = false
benchi-kafka     | 	confluent.internal.rest.server.bind.port = null
benchi-kafka     | 	confluent.internal.tenant.scoped.listener.name = INTERNAL_TENANT_SCOPED
benchi-kafka     | 	confluent.leader.epoch.checkpoint.checksum.enabled = false
benchi-kafka     | 	confluent.log.cleaner.timestamp.validation.enable = true
benchi-kafka     | 	confluent.log.placement.constraints = 
benchi-kafka     | 	confluent.max.connection.creation.rate.per.ip = 1.7976931348623157E308
benchi-kafka     | 	confluent.max.connection.creation.rate.per.tenant = 1.7976931348623157E308
benchi-kafka     | 	confluent.max.connection.rate.per.ip = -1.0
benchi-kafka     | 	confluent.max.connection.rate.per.tenant = -1.0
benchi-kafka     | 	confluent.max.connection.throttle.ms = null
benchi-kafka     | 	confluent.max.segment.ms = 9223372036854775807
benchi-kafka     | 	confluent.metadata.active.encryptor = null
benchi-kafka     | 	confluent.metadata.controlled.shutdown.partition.slice.delay.ms = 100
benchi-kafka     | 	confluent.metadata.encryptor.classes = null
benchi-kafka     | 	confluent.metadata.encryptor.required = false
benchi-kafka     | 	confluent.metadata.encryptor.secret.file = null
benchi-kafka     | 	confluent.metadata.encryptor.secrets = null
benchi-kafka     | 	confluent.metadata.jvm.warmup.ms = 60000
benchi-kafka     | 	confluent.metadata.leader.balance.slice.delay.ms = 100
benchi-kafka     | 	confluent.metadata.max.controlled.shutdown.partition.changes.per.slice = 1000
benchi-kafka     | 	confluent.metadata.max.leader.balance.changes.per.slice = 1000
benchi-kafka     | 	confluent.metadata.server.cluster.registry.clusters = []
benchi-kafka     | 	confluent.metadata.throttle.pre.check.for.non.wildcard.requests.enable = false
benchi-kafka     | 	confluent.metadata.throttle.pre.check.for.wildcard.requests.enable = false
benchi-kafka     | 	confluent.metrics.reporter.bootstrap.servers = localhost:9092
benchi-kafka     | 	confluent.min.acks = 0
benchi-kafka     | 	confluent.min.connection.throttle.ms = 0
benchi-kafka     | 	confluent.min.segment.ms = 1
benchi-kafka     | 	confluent.missing.id.cache.ttl.sec = 60
benchi-kafka     | 	confluent.missing.id.query.range = 20000
benchi-kafka     | 	confluent.missing.schema.cache.ttl.sec = 60
benchi-kafka     | 	confluent.mtls.enable = false
benchi-kafka     | 	confluent.mtls.listener.name = EXTERNAL
benchi-kafka     | 	confluent.mtls.sasl.authenticator.request.max.bytes = 104857600
benchi-kafka     | 	confluent.mtls.truststore.manager.class.name = null
benchi-kafka     | 	confluent.multitenant.authorizer.enable.acl.state = false
benchi-kafka     | 	confluent.multitenant.interceptor.balancer.apis.enabled = false
benchi-kafka     | 	confluent.multitenant.interceptor.collect.client.apiversions.max.per.tenant = 1000
benchi-kafka     | 	confluent.multitenant.interceptor.collect.client.apiversions.metric = false
benchi-kafka     | 	confluent.multitenant.listener.hostname.cluster.prefix.enable = false
benchi-kafka     | 	confluent.multitenant.listener.hostname.subdomain.suffix.enable = false
benchi-kafka     | 	confluent.multitenant.listener.names = null
benchi-kafka     | 	confluent.multitenant.parse.lkc.id.enable = false
benchi-kafka     | 	confluent.multitenant.parse.sni.host.name.enable = false
benchi-kafka     | 	confluent.network.health.manager.enabled = false
benchi-kafka     | 	confluent.network.health.manager.external.listener.name = EXTERNAL
benchi-kafka     | 	confluent.network.health.manager.externalconnectivitystartup.enabled = false
benchi-kafka     | 	confluent.network.health.manager.min.healthy.network.samples = 3
benchi-kafka     | 	confluent.network.health.manager.min.percentage.healthy.network.samples = 3
benchi-kafka     | 	confluent.network.health.manager.mitigation.enabled = false
benchi-kafka     | 	confluent.network.health.manager.network.sample.window.size = 120
benchi-kafka     | 	confluent.network.health.manager.sample.duration.ms = 1000
benchi-kafka     | 	confluent.oauth.flat.networking.verification.enable = false
benchi-kafka     | 	confluent.offsets.log.cleaner.delete.retention.ms = 86400000
benchi-kafka     | 	confluent.offsets.log.cleaner.max.compaction.lag.ms = 9223372036854775807
benchi-kafka     | 	confluent.offsets.log.cleaner.min.cleanable.dirty.ratio = 0.5
benchi-kafka     | 	confluent.offsets.topic.placement.constraints = 
benchi-kafka     | 	confluent.omit.network.processor.metric.tag = false
benchi-kafka     | 	confluent.operator.managed = false
benchi-kafka     | 	confluent.password.encoder.old.secret.ttl.ms = 9223372036854775807
benchi-kafka     | 	confluent.plugins.cluster.link.policy.max.destination.links.per.tenant = 10
benchi-kafka     | 	confluent.plugins.cluster.link.policy.max.source.links.per.tenant = 10
benchi-kafka     | 	confluent.plugins.topic.policy.max.partitions.per.cluster = 2147483647
benchi-kafka     | 	confluent.plugins.topic.policy.max.partitions.per.tenant = 512
benchi-kafka     | 	confluent.plugins.topic.policy.max.replicas.per.broker = 2147483647
benchi-kafka     | 	confluent.plugins.topic.policy.max.topics.per.cluster = 2147483647
benchi-kafka     | 	confluent.prefer.tier.fetch.ms = -1
benchi-kafka     | 	confluent.produce.throttle.pre.check.enable = false
benchi-kafka     | 	confluent.producer.id.cache.broker.hard.limit = -1
benchi-kafka     | 	confluent.producer.id.cache.eviction.minimal.expiration.ms = 900000
benchi-kafka     | 	confluent.producer.id.cache.extra.eviction.percentage = 0
benchi-kafka     | 	confluent.producer.id.cache.limit = 2147483647
benchi-kafka     | 	confluent.producer.id.cache.partition.hard.limit = -1
benchi-kafka     | 	confluent.producer.id.cache.tenant.hard.limit = -1
benchi-kafka     | 	confluent.producer.id.quota.manager.enable = false
benchi-kafka     | 	confluent.producer.id.quota.window.num = 11
benchi-kafka     | 	confluent.producer.id.quota.window.size.seconds = 1
benchi-kafka     | 	confluent.producer.id.throttle.enable = false
benchi-kafka     | 	confluent.producer.id.throttle.enable.threshold.percentage = 100
benchi-kafka     | 	confluent.proxy.mode.local.default = false
benchi-kafka     | 	confluent.proxy.protocol.fallback.enabled = false
benchi-kafka     | 	confluent.proxy.protocol.version = NONE
benchi-kafka     | 	confluent.quota.computing.usage.adjustment = 0.5
benchi-kafka     | 	confluent.quota.dynamic.enable = false
benchi-kafka     | 	confluent.quota.dynamic.publishing.interval.ms = 60000
benchi-kafka     | 	confluent.quota.dynamic.reporting.interval.ms = 30000
benchi-kafka     | 	confluent.quota.dynamic.reporting.min.usage = 102400
benchi-kafka     | 	confluent.quota.tenant.broker.max.consumer.rate = 13107200
benchi-kafka     | 	confluent.quota.tenant.broker.max.producer.rate = 13107200
benchi-kafka     | 	confluent.quota.tenant.default.controller.mutation.rate = 2.147483647E9
benchi-kafka     | 	confluent.quota.tenant.default.producer.id.rate = 2.147483647E9
benchi-kafka     | 	confluent.quota.tenant.fetch.multiplier = 1.0
benchi-kafka     | 	confluent.quota.tenant.follower.broker.min.consumer.rate = 10485760
benchi-kafka     | 	confluent.quota.tenant.follower.broker.min.producer.rate = 10485760
benchi-kafka     | 	confluent.quota.tenant.produce.multiplier = 1.0
benchi-kafka     | 	confluent.quota.tenant.user.quotas.enable = false
benchi-kafka     | 	confluent.regional.metadata.client.class = null
benchi-kafka     | 	confluent.regional.resource.manager.client.scheduler.threads = 2
benchi-kafka     | 	confluent.regional.resource.manager.endpoint = null
benchi-kafka     | 	confluent.regional.resource.manager.watch.endpoint = null
benchi-kafka     | 	confluent.replica.fetch.backoff.max.ms = 1000
benchi-kafka     | 	confluent.replica.fetch.connections.mode = combined
benchi-kafka     | 	confluent.replication.mode = PULL
benchi-kafka     | 	confluent.replication.push.feature.enable = false
benchi-kafka     | 	confluent.reporters.telemetry.auto.enable = true
benchi-kafka     | 	confluent.request.log.filter.class = class org.apache.kafka.common.requests.SamplingRequestLogFilter
benchi-kafka     | 	confluent.request.pipelining.enable = true
benchi-kafka     | 	confluent.request.pipelining.max.in.flight.requests.per.connection = 5
benchi-kafka     | 	confluent.require.calling.resource.identity = false
benchi-kafka     | 	confluent.require.compatible.keystore.updates = true
benchi-kafka     | 	confluent.require.confluent.issuer = false
benchi-kafka     | 	confluent.roll.check.interval.ms = 300000
benchi-kafka     | 	confluent.schema.registry.max.cache.size = 10000
benchi-kafka     | 	confluent.schema.registry.max.retries = 1
benchi-kafka     | 	confluent.schema.registry.retries.wait.ms = 0
benchi-kafka     | 	confluent.schema.registry.url = null
benchi-kafka     | 	confluent.schema.validator.interceptor.class = io.confluent.kafka.schemaregistry.validator.RecordSchemaValidator
benchi-kafka     | 	confluent.schema.validator.multitenant.enable = false
benchi-kafka     | 	confluent.schema.validator.samples.per.min = 0
benchi-kafka     | 	confluent.security.bc.approved.mode.enable = false
benchi-kafka     | 	confluent.security.event.logger.authentication.enable = false
benchi-kafka     | 	confluent.security.event.logger.authentication.event.rate.limit = -1
benchi-kafka     | 	confluent.security.event.logger.authorization.event.rate.limit = -1
benchi-kafka     | 	confluent.security.event.logger.detailed.audit.logs.filter.class = class org.apache.kafka.common.requests.DetailedRequestAuditLogFilter
benchi-kafka     | 	confluent.security.event.logger.enable = true
benchi-kafka     | 	confluent.security.event.logger.kafka.request.rate.limit = -1
benchi-kafka     | 	confluent.security.event.logger.physical.cluster.id = 
benchi-kafka     | 	confluent.security.event.router.config = 
benchi-kafka     | 	confluent.security.revoked.certificate.ids = 
benchi-kafka     | 	confluent.segment.eager.roll.enable = false
benchi-kafka     | 	confluent.segment.speculative.prefetch.enable = false
benchi-kafka     | 	confluent.spiffe.id.principal.extraction.rules = 
benchi-kafka     | 	confluent.ssl.key.password = null
benchi-kafka     | 	confluent.ssl.keystore.location = null
benchi-kafka     | 	confluent.ssl.keystore.password = null
benchi-kafka     | 	confluent.ssl.keystore.type = null
benchi-kafka     | 	confluent.ssl.protocol = null
benchi-kafka     | 	confluent.ssl.truststore.location = null
benchi-kafka     | 	confluent.ssl.truststore.password = null
benchi-kafka     | 	confluent.ssl.truststore.type = null
benchi-kafka     | 	confluent.step.connection.rate.per.ip = -1.0
benchi-kafka     | 	confluent.step.connection.rate.per.tenant = -1.0
benchi-kafka     | 	confluent.storage.probe.period.ms = -1
benchi-kafka     | 	confluent.storage.probe.slow.write.threshold.ms = 5000
benchi-kafka     | 	confluent.stray.log.delete.delay.ms = 604800000
benchi-kafka     | 	confluent.stray.log.max.deletions.per.run = 72
benchi-kafka     | 	confluent.subdomain.prefix = null
benchi-kafka     | 	confluent.subdomain.separator.map = null
benchi-kafka     | 	confluent.subdomain.separator.variable = %sep
benchi-kafka     | 	confluent.system.time.roll.enable = false
benchi-kafka     | 	confluent.telemetry.enabled = false
benchi-kafka     | 	confluent.telemetry.external.client.metrics.push.enabled = false
benchi-kafka     | 	confluent.tenant.latency.metric.enabled = false
benchi-kafka     | 	confluent.tier.archiver.num.threads = 2
benchi-kafka     | 	confluent.tier.azure.block.blob.auto.abort.threshold.bytes = 500000
benchi-kafka     | 	confluent.tier.azure.block.blob.container = null
benchi-kafka     | 	confluent.tier.azure.block.blob.cred.file.path = null
benchi-kafka     | 	confluent.tier.azure.block.blob.endpoint = null
benchi-kafka     | 	confluent.tier.azure.block.blob.prefix = 
benchi-kafka     | 	confluent.tier.backend = 
benchi-kafka     | 	confluent.tier.bucket.probe.period.ms = -1
benchi-kafka     | 	confluent.tier.cleaner.compact.min.efficiency = 0.5
benchi-kafka     | 	confluent.tier.cleaner.compact.segment.min.bytes = 20971520
benchi-kafka     | 	confluent.tier.cleaner.dedupe.buffer.size = 134217728
benchi-kafka     | 	confluent.tier.cleaner.dual.compaction = false
benchi-kafka     | 	confluent.tier.cleaner.dual.compaction.validation.max.bytes = 1073741824
benchi-kafka     | 	confluent.tier.cleaner.dual.compaction.validation.percent = 0
benchi-kafka     | 	confluent.tier.cleaner.enable = false
benchi-kafka     | 	confluent.tier.cleaner.excluded.topics = [^_confluent.*]
benchi-kafka     | 	confluent.tier.cleaner.feature.enable = false
benchi-kafka     | 	confluent.tier.cleaner.io.buffer.load.factor = 0.9
benchi-kafka     | 	confluent.tier.cleaner.io.buffer.size = 10485760
benchi-kafka     | 	confluent.tier.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
benchi-kafka     | 	confluent.tier.cleaner.min.cleanable.ratio = 0.75
benchi-kafka     | 	confluent.tier.cleaner.num.threads = 2
benchi-kafka     | 	confluent.tier.enable = false
benchi-kafka     | 	confluent.tier.feature = false
benchi-kafka     | 	confluent.tier.fenced.segment.delete.delay.ms = 600000
benchi-kafka     | 	confluent.tier.fetcher.async.enable = false
benchi-kafka     | 	confluent.tier.fetcher.async.timestamp.offset.parallelism = 1
benchi-kafka     | 	confluent.tier.fetcher.fetch.based.on.segment_and_metadata_layout.field = false
benchi-kafka     | 	confluent.tier.fetcher.memorypool.bytes = 0
benchi-kafka     | 	confluent.tier.fetcher.num.threads = 4
benchi-kafka     | 	confluent.tier.fetcher.offset.cache.expiration.ms = 1800000
benchi-kafka     | 	confluent.tier.fetcher.offset.cache.period.ms = 60000
benchi-kafka     | 	confluent.tier.fetcher.offset.cache.size = 200000
benchi-kafka     | 	confluent.tier.gcs.bucket = null
benchi-kafka     | 	confluent.tier.gcs.cred.file.path = null
benchi-kafka     | 	confluent.tier.gcs.prefix = 
benchi-kafka     | 	confluent.tier.gcs.region = null
benchi-kafka     | 	confluent.tier.gcs.sse.customer.encryption.key = null
benchi-kafka     | 	confluent.tier.gcs.write.chunk.size = 0
benchi-kafka     | 	confluent.tier.local.hotset.bytes = -1
benchi-kafka     | 	confluent.tier.local.hotset.ms = 86400000
benchi-kafka     | 	confluent.tier.max.partition.fetch.bytes.override = 0
benchi-kafka     | 	confluent.tier.metadata.bootstrap.servers = null
benchi-kafka     | 	confluent.tier.metadata.max.poll.ms = 100
benchi-kafka     | 	confluent.tier.metadata.namespace = null
benchi-kafka     | 	confluent.tier.metadata.num.partitions = 50
benchi-kafka     | 	confluent.tier.metadata.replication.factor = 1
benchi-kafka     | 	confluent.tier.metadata.request.timeout.ms = 30000
benchi-kafka     | 	confluent.tier.metadata.snapshots.enable = false
benchi-kafka     | 	confluent.tier.metadata.snapshots.interval.ms = 86400000
benchi-kafka     | 	confluent.tier.metadata.snapshots.retention.days = 7
benchi-kafka     | 	confluent.tier.metadata.snapshots.threads = 2
benchi-kafka     | 	confluent.tier.object.fetcher.num.threads = 1
benchi-kafka     | 	confluent.tier.partition.state.cleanup.delay.ms = 2592000000
benchi-kafka     | 	confluent.tier.partition.state.cleanup.enable = false
benchi-kafka     | 	confluent.tier.partition.state.cleanup.interval.ms = 86400000
benchi-kafka     | 	confluent.tier.partition.state.commit.interval.ms = 15000
benchi-kafka     | 	confluent.tier.s3.assumerole.arn = null
benchi-kafka     | 	confluent.tier.s3.auto.abort.threshold.bytes = 500000
benchi-kafka     | 	confluent.tier.s3.aws.endpoint.override = null
benchi-kafka     | 	confluent.tier.s3.aws.signer.override = null
benchi-kafka     | 	confluent.tier.s3.bucket = null
benchi-kafka     | 	confluent.tier.s3.cred.file.path = null
benchi-kafka     | 	confluent.tier.s3.force.path.style.access = false
benchi-kafka     | 	confluent.tier.s3.prefix = 
benchi-kafka     | 	confluent.tier.s3.region = null
benchi-kafka     | 	confluent.tier.s3.security.providers = null
benchi-kafka     | 	confluent.tier.s3.sse.algorithm = AES256
benchi-kafka     | 	confluent.tier.s3.sse.customer.encryption.key = null
benchi-kafka     | 	confluent.tier.s3.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	confluent.tier.s3.ssl.key.password = null
benchi-kafka     | 	confluent.tier.s3.ssl.keystore.location = null
benchi-kafka     | 	confluent.tier.s3.ssl.keystore.password = null
benchi-kafka     | 	confluent.tier.s3.ssl.keystore.type = null
benchi-kafka     | 	confluent.tier.s3.ssl.protocol = TLSv1.3
benchi-kafka     | 	confluent.tier.s3.ssl.provider = null
benchi-kafka     | 	confluent.tier.s3.ssl.truststore.location = null
benchi-kafka     | 	confluent.tier.s3.ssl.truststore.password = null
benchi-kafka     | 	confluent.tier.s3.ssl.truststore.type = null
benchi-kafka     | 	confluent.tier.s3.user.agent.prefix = APN/1.0 Confluent/1.0 TieredStorageS3/1.0
benchi-kafka     | 	confluent.tier.segment.hotset.roll.min.bytes = 104857600
benchi-kafka     | 	confluent.tier.segment.metadata.layout.put.mode = LegacyMultiObject
benchi-kafka     | 	confluent.tier.topic.data.loss.validation.fencing.enable = false
benchi-kafka     | 	confluent.tier.topic.delete.backoff.ms = 21600000
benchi-kafka     | 	confluent.tier.topic.delete.check.interval.ms = 300000
benchi-kafka     | 	confluent.tier.topic.delete.max.inprogress.partitions = 100
benchi-kafka     | 	confluent.tier.topic.head.data.loss.validation.enable = true
benchi-kafka     | 	confluent.tier.topic.head.data.loss.validation.max.timeout.ms = 900000
benchi-kafka     | 	confluent.tier.topic.materialization.from.snapshot.enable = false
benchi-kafka     | 	confluent.tier.topic.producer.enable.idempotence = true
benchi-kafka     | 	confluent.tier.topic.snapshots.enable = false
benchi-kafka     | 	confluent.tier.topic.snapshots.interval.ms = 300000
benchi-kafka     | 	confluent.tier.topic.snapshots.max.records = 100000
benchi-kafka     | 	confluent.tier.topic.snapshots.retention.hours = 168
benchi-kafka     | 	confluent.topic.partition.default.placement = 2
benchi-kafka     | 	confluent.topic.policy.use.computed.assignments = false
benchi-kafka     | 	confluent.topic.replica.assignor.builder.class = 
benchi-kafka     | 	confluent.track.api.key.per.ip = false
benchi-kafka     | 	confluent.track.per.ip.max.size = 100000
benchi-kafka     | 	confluent.track.tenant.id.per.ip = false
benchi-kafka     | 	confluent.traffic.cdc.network.id.routes.enable = false
benchi-kafka     | 	confluent.traffic.cdc.network.id.routes.listener.names = EXTERNAL_BACKCHANNEL
benchi-kafka     | 	confluent.traffic.cdc.network.id.routes.periodic.start.task.ms = 300000
benchi-kafka     | 	confluent.traffic.cdc.network.id.routes.topic.name = _confluent-network_id_routes
benchi-kafka     | 	confluent.traffic.network.id = 
benchi-kafka     | 	confluent.transaction.2pc.timeout.ms = -1
benchi-kafka     | 	confluent.transaction.logging.verbosity = 0
benchi-kafka     | 	confluent.transaction.state.log.placement.constraints = 
benchi-kafka     | 	confluent.valid.broker.rack.set = null
benchi-kafka     | 	confluent.verify.group.subscription.prefix = false
benchi-kafka     | 	confluent.virtual.topic.creation.enabled = false
benchi-kafka     | 	confluent.zookeeper.metadata.migration.controller.check.disable = false
benchi-kafka     | 	confluent.zookeeper.metadata.migration.trigger.file.path = null
benchi-kafka     | 	connection.failed.authentication.delay.ms = 100
benchi-kafka     | 	connection.min.expire.interval.ms = 250
benchi-kafka     | 	connections.max.age.ms = 3153600000000
benchi-kafka     | 	connections.max.idle.ms = 600000
benchi-kafka     | 	connections.max.reauth.ms = 0
benchi-kafka     | 	control.plane.listener.name = null
benchi-kafka     | 	controlled.shutdown.enable = true
benchi-kafka     | 	controlled.shutdown.max.retries = 3
benchi-kafka     | 	controlled.shutdown.retry.backoff.ms = 5000
benchi-kafka     | 	controller.listener.names = CONTROLLER
benchi-kafka     | 	controller.quorum.append.linger.ms = 25
benchi-kafka     | 	controller.quorum.bootstrap.servers = []
benchi-kafka     | 	controller.quorum.election.backoff.max.ms = 1000
benchi-kafka     | 	controller.quorum.election.timeout.ms = 1000
benchi-kafka     | 	controller.quorum.fetch.timeout.ms = 2000
benchi-kafka     | 	controller.quorum.request.timeout.ms = 2000
benchi-kafka     | 	controller.quorum.retry.backoff.ms = 20
benchi-kafka     | 	controller.quorum.voters = [1@broker:29093]
benchi-kafka     | 	controller.quota.window.num = 11
benchi-kafka     | 	controller.quota.window.size.seconds = 1
benchi-kafka     | 	controller.socket.timeout.ms = 30000
benchi-kafka     | 	create.cluster.link.policy.class.name = null
benchi-kafka     | 	create.topic.policy.class.name = null
benchi-kafka     | 	default.replication.factor = 1
benchi-kafka     | 	delegation.token.expiry.check.interval.ms = 3600000
benchi-kafka     | 	delegation.token.expiry.time.ms = 86400000
benchi-kafka     | 	delegation.token.master.key = null
benchi-kafka     | 	delegation.token.max.lifetime.ms = 604800000
benchi-kafka     | 	delegation.token.secret.key = null
benchi-kafka     | 	delete.records.purgatory.purge.interval.requests = 1
benchi-kafka     | 	delete.topic.enable = true
benchi-kafka     | 	early.start.listeners = null
benchi-kafka     | 	eligible.leader.replicas.enable = false
benchi-kafka     | 	enable.fips = false
benchi-kafka     | 	fetch.max.bytes = 57671680
benchi-kafka     | 	fetch.purgatory.purge.interval.requests = 1000
benchi-kafka     | 	floor.max.connection.creation.rate = null
benchi-kafka     | 	follower.replication.throttled.rate = 9223372036854775807
benchi-kafka     | 	follower.replication.throttled.replicas = none
benchi-kafka     | 	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
benchi-kafka     | 	group.consumer.heartbeat.interval.ms = 5000
benchi-kafka     | 	group.consumer.max.heartbeat.interval.ms = 15000
benchi-kafka     | 	group.consumer.max.session.timeout.ms = 60000
benchi-kafka     | 	group.consumer.max.size = 2147483647
benchi-kafka     | 	group.consumer.migration.policy = disabled
benchi-kafka     | 	group.consumer.min.heartbeat.interval.ms = 5000
benchi-kafka     | 	group.consumer.min.session.timeout.ms = 45000
benchi-kafka     | 	group.consumer.session.timeout.ms = 45000
benchi-kafka     | 	group.coordinator.append.linger.ms = 10
benchi-kafka     | 	group.coordinator.new.enable = false
benchi-kafka     | 	group.coordinator.rebalance.protocols = [classic]
benchi-kafka     | 	group.coordinator.threads = 1
benchi-kafka     | 	group.initial.rebalance.delay.ms = 0
benchi-kafka     | 	group.max.session.timeout.ms = 1800000
benchi-kafka     | 	group.max.size = 2147483647
benchi-kafka     | 	group.min.session.timeout.ms = 6000
benchi-kafka     | 	initial.broker.registration.timeout.ms = 60000
benchi-kafka     | 	inter.broker.listener.name = PLAINTEXT
benchi-kafka     | 	inter.broker.protocol.version = 3.8-IV0A
benchi-kafka     | 	k2.stack.builder.class.name = null
benchi-kafka     | 	k2.startup.timeout.ms = 60000
benchi-kafka     | 	k2.topic.metadata.max.total.partition.count = 20000
benchi-kafka     | 	k2.topic.metadata.refresh.ms = 10000
benchi-kafka     | 	kafka.metrics.polling.interval.secs = 10
benchi-kafka     | 	kafka.metrics.reporters = []
benchi-kafka     | 	leader.imbalance.check.interval.seconds = 300
benchi-kafka     | 	leader.imbalance.per.broker.percentage = 10
benchi-kafka     | 	leader.replication.throttled.rate = 9223372036854775807
benchi-kafka     | 	leader.replication.throttled.replicas = none
benchi-kafka     | 	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
benchi-kafka     | 	listeners = PLAINTEXT://broker:29092,CONTROLLER://broker:29093,PLAINTEXT_HOST://0.0.0.0:9092
benchi-kafka     | 	log.cleaner.backoff.ms = 15000
benchi-kafka     | 	log.cleaner.dedupe.buffer.size = 134217728
benchi-kafka     | 	log.cleaner.delete.retention.ms = 86400000
benchi-kafka     | 	log.cleaner.enable = true
benchi-kafka     | 	log.cleaner.hash.algorithm = MD5
benchi-kafka     | 	log.cleaner.io.buffer.load.factor = 0.9
benchi-kafka     | 	log.cleaner.io.buffer.size = 524288
benchi-kafka     | 	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
benchi-kafka     | 	log.cleaner.max.compaction.lag.ms = 9223372036854775807
benchi-kafka     | 	log.cleaner.min.cleanable.ratio = 0.5
benchi-kafka     | 	log.cleaner.min.compaction.lag.ms = 0
benchi-kafka     | 	log.cleaner.threads = 1
benchi-kafka     | 	log.cleanup.policy = [delete]
benchi-kafka     | 	log.deletion.max.segments.per.run = 2147483647
benchi-kafka     | 	log.deletion.throttler.disk.free.headroom.bytes = 21474836480
benchi-kafka     | 	log.dir = /tmp/kafka-logs
benchi-kafka     | 	log.dir.failure.timeout.ms = 30000
benchi-kafka     | 	log.dirs = /tmp/kraft-combined-logs
benchi-kafka     | 	log.flush.interval.messages = 9223372036854775807
benchi-kafka     | 	log.flush.interval.ms = null
benchi-kafka     | 	log.flush.offset.checkpoint.interval.ms = 60000
benchi-kafka     | 	log.flush.scheduler.interval.ms = 9223372036854775807
benchi-kafka     | 	log.flush.start.offset.checkpoint.interval.ms = 60000
benchi-kafka     | 	log.index.interval.bytes = 4096
benchi-kafka     | 	log.index.size.max.bytes = 10485760
benchi-kafka     | 	log.initial.task.delay.ms = 30000
benchi-kafka     | 	log.local.retention.bytes = -2
benchi-kafka     | 	log.local.retention.ms = -2
benchi-kafka     | 	log.message.downconversion.enable = true
benchi-kafka     | 	log.message.format.version = 3.0-IV1
benchi-kafka     | 	log.message.timestamp.after.max.ms = 9223372036854775807
benchi-kafka     | 	log.message.timestamp.before.max.ms = 9223372036854775807
benchi-kafka     | 	log.message.timestamp.difference.max.ms = 9223372036854775807
benchi-kafka     | 	log.message.timestamp.type = CreateTime
benchi-kafka     | 	log.preallocate = false
benchi-kafka     | 	log.retention.bytes = -1
benchi-kafka     | 	log.retention.check.interval.ms = 300000
benchi-kafka     | 	log.retention.hours = 168
benchi-kafka     | 	log.retention.minutes = null
benchi-kafka     | 	log.retention.ms = null
benchi-kafka     | 	log.roll.hours = 168
benchi-kafka     | 	log.roll.jitter.hours = 0
benchi-kafka     | 	log.roll.jitter.ms = null
benchi-kafka     | 	log.roll.ms = null
benchi-kafka     | 	log.segment.bytes = 1073741824
benchi-kafka     | 	log.segment.delete.delay.ms = 60000
benchi-kafka     | 	max.connection.creation.rate = 1.7976931348623157E308
benchi-kafka     | 	max.connection.creation.rate.per.ip.enable.threshold = 0.0
benchi-kafka     | 	max.connection.creation.rate.per.tenant.enable.threshold = 0.0
benchi-kafka     | 	max.connections = 2147483647
benchi-kafka     | 	max.connections.per.ip = 2147483647
benchi-kafka     | 	max.connections.per.ip.overrides = 
benchi-kafka     | 	max.connections.per.tenant = 0
benchi-kafka     | 	max.connections.protected.listeners = []
benchi-kafka     | 	max.connections.reap.amount = 0
benchi-kafka     | 	max.incremental.fetch.session.cache.slots = 1000
benchi-kafka     | 	max.request.partition.size.limit = 2000
benchi-kafka     | 	message.max.bytes = 1048588
benchi-kafka     | 	metadata.log.dir = null
benchi-kafka     | 	metadata.log.max.record.bytes.between.snapshots = 20971520
benchi-kafka     | 	metadata.log.max.snapshot.interval.ms = 3600000
benchi-kafka     | 	metadata.log.segment.bytes = 1073741824
benchi-kafka     | 	metadata.log.segment.min.bytes = 8388608
benchi-kafka     | 	metadata.log.segment.ms = 604800000
benchi-kafka     | 	metadata.max.idle.interval.ms = 500
benchi-kafka     | 	metadata.max.retention.bytes = 104857600
benchi-kafka     | 	metadata.max.retention.ms = 604800000
benchi-kafka     | 	metric.reporters = [io.confluent.metrics.reporter.ConfluentMetricsReporter]
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.recording.level = INFO
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	min.insync.replicas = 1
benchi-kafka     | 	multitenant.authorizer.support.resource.ids = false
benchi-kafka     | 	multitenant.metadata.class = null
benchi-kafka     | 	multitenant.metadata.dir = null
benchi-kafka     | 	multitenant.metadata.reload.delay.ms = 120000
benchi-kafka     | 	multitenant.metadata.ssl.certs.path = null
benchi-kafka     | 	multitenant.tenant.delete.batch.size = 10
benchi-kafka     | 	multitenant.tenant.delete.check.ms = 120000
benchi-kafka     | 	multitenant.tenant.delete.delay = 604800000
benchi-kafka     | 	node.id = 1
benchi-kafka     | 	num.io.threads = 8
benchi-kafka     | 	num.network.threads = 3
benchi-kafka     | 	num.partitions = 1
benchi-kafka     | 	num.recovery.threads.per.data.dir = 1
benchi-kafka     | 	num.replica.alter.log.dirs.threads = null
benchi-kafka     | 	num.replica.fetchers = 1
benchi-kafka     | 	offset.metadata.max.bytes = 4096
benchi-kafka     | 	offsets.commit.required.acks = -1
benchi-kafka     | 	offsets.commit.timeout.ms = 5000
benchi-kafka     | 	offsets.load.buffer.size = 5242880
benchi-kafka     | 	offsets.retention.check.interval.ms = 600000
benchi-kafka     | 	offsets.retention.minutes = 10080
benchi-kafka     | 	offsets.topic.compression.codec = 0
benchi-kafka     | 	offsets.topic.num.partitions = 50
benchi-kafka     | 	offsets.topic.replication.factor = 1
benchi-kafka     | 	offsets.topic.segment.bytes = 104857600
benchi-kafka     | 	otel.exporter.otlp.custom.endpoint = default
benchi-kafka     | 	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
benchi-kafka     | 	password.encoder.iterations = 4096
benchi-kafka     | 	password.encoder.key.length = 128
benchi-kafka     | 	password.encoder.keyfactory.algorithm = null
benchi-kafka     | 	password.encoder.old.secret = null
benchi-kafka     | 	password.encoder.secret = null
benchi-kafka     | 	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
benchi-kafka     | 	process.roles = [broker, controller]
benchi-kafka     | 	producer.id.expiration.check.interval.ms = 600000
benchi-kafka     | 	producer.id.expiration.ms = 86400000
benchi-kafka     | 	producer.purgatory.purge.interval.requests = 1000
benchi-kafka     | 	queued.max.request.bytes = -1
benchi-kafka     | 	queued.max.requests = 500
benchi-kafka     | 	quota.window.num = 11
benchi-kafka     | 	quota.window.size.seconds = 1
benchi-kafka     | 	quotas.consumption.expiration.time.ms = 600000
benchi-kafka     | 	quotas.expiration.interval.ms = 3600000
benchi-kafka     | 	quotas.expiration.time.ms = 604800000
benchi-kafka     | 	quotas.lazy.evaluation.threshold = 0.5
benchi-kafka     | 	quotas.topic.append.timeout.ms = 5000
benchi-kafka     | 	quotas.topic.compression.codec = 3
benchi-kafka     | 	quotas.topic.load.buffer.size = 5242880
benchi-kafka     | 	quotas.topic.num.partitions = 50
benchi-kafka     | 	quotas.topic.placement.constraints = 
benchi-kafka     | 	quotas.topic.replication.factor = 3
benchi-kafka     | 	quotas.topic.segment.bytes = 104857600
benchi-kafka     | 	remote.fetch.max.wait.ms = 500
benchi-kafka     | 	remote.log.index.file.cache.total.size.bytes = 1073741824
benchi-kafka     | 	remote.log.manager.copier.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.copy.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.copy.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.expiration.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.fetch.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.fetch.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.task.interval.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.max.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.ms = 500
benchi-kafka     | 	remote.log.manager.task.retry.jitter = 0.2
benchi-kafka     | 	remote.log.manager.thread.pool.size = 10
benchi-kafka     | 	remote.log.metadata.custom.metadata.max.bytes = 128
benchi-kafka     | 	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
benchi-kafka     | 	remote.log.metadata.manager.class.path = null
benchi-kafka     | 	remote.log.metadata.manager.impl.prefix = rlmm.config.
benchi-kafka     | 	remote.log.metadata.manager.listener.name = null
benchi-kafka     | 	remote.log.reader.max.pending.tasks = 100
benchi-kafka     | 	remote.log.reader.threads = 10
benchi-kafka     | 	remote.log.storage.manager.class.name = null
benchi-kafka     | 	remote.log.storage.manager.class.path = null
benchi-kafka     | 	remote.log.storage.manager.impl.prefix = rsm.config.
benchi-kafka     | 	remote.log.storage.system.enable = false
benchi-kafka     | 	replica.fetch.backoff.ms = 1000
benchi-kafka     | 	replica.fetch.max.bytes = 1048576
benchi-kafka     | 	replica.fetch.min.bytes = 1
benchi-kafka     | 	replica.fetch.response.max.bytes = 10485760
benchi-kafka     | 	replica.fetch.wait.max.ms = 500
benchi-kafka     | 	replica.high.watermark.checkpoint.interval.ms = 5000
benchi-kafka     | 	replica.lag.time.max.ms = 30000
benchi-kafka     | 	replica.selector.class = null
benchi-kafka     | 	replica.socket.receive.buffer.bytes = 65536
benchi-kafka     | 	replica.socket.timeout.ms = 30000
benchi-kafka     | 	replication.quota.window.num = 11
benchi-kafka     | 	replication.quota.window.size.seconds = 1
benchi-kafka     | 	request.timeout.ms = 30000
benchi-kafka     | 	reserved.broker.max.id = 1000
benchi-kafka     | 	sasl.client.callback.handler.class = null
benchi-kafka     | 	sasl.enabled.mechanisms = [GSSAPI]
benchi-kafka     | 	sasl.jaas.config = null
benchi-kafka     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
benchi-kafka     | 	sasl.kerberos.min.time.before.relogin = 60000
benchi-kafka     | 	sasl.kerberos.principal.to.local.rules = [DEFAULT]
benchi-kafka     | 	sasl.kerberos.service.name = null
benchi-kafka     | 	sasl.kerberos.ticket.renew.jitter = 0.05
benchi-kafka     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
benchi-kafka     | 	sasl.login.callback.handler.class = null
benchi-kafka     | 	sasl.login.class = null
benchi-kafka     | 	sasl.login.connect.timeout.ms = null
benchi-kafka     | 	sasl.login.read.timeout.ms = null
benchi-kafka     | 	sasl.login.refresh.buffer.seconds = 300
benchi-kafka     | 	sasl.login.refresh.min.period.seconds = 60
benchi-kafka     | 	sasl.login.refresh.window.factor = 0.8
benchi-kafka     | 	sasl.login.refresh.window.jitter = 0.05
benchi-kafka     | 	sasl.login.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.login.retry.backoff.ms = 100
benchi-kafka     | 	sasl.mechanism.controller.protocol = GSSAPI
benchi-kafka     | 	sasl.mechanism.inter.broker.protocol = GSSAPI
benchi-kafka     | 	sasl.oauthbearer.clientassertion.audience = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.expiration = 5
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.jti.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.nbf.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.issuer = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.location = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key.passphrase = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.subject = null
benchi-kafka     | 	sasl.oauthbearer.clock.skew.seconds = 30
benchi-kafka     | 	sasl.oauthbearer.expected.audience = null
benchi-kafka     | 	sasl.oauthbearer.expected.issuer = null
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.url = null
benchi-kafka     | 	sasl.oauthbearer.scope.claim.name = scope
benchi-kafka     | 	sasl.oauthbearer.sub.claim.name = sub
benchi-kafka     | 	sasl.oauthbearer.token.endpoint.url = null
benchi-kafka     | 	sasl.server.authn.async.enable = false
benchi-kafka     | 	sasl.server.authn.async.max.threads = 1
benchi-kafka     | 	sasl.server.authn.async.timeout.ms = 30000
benchi-kafka     | 	sasl.server.callback.handler.class = null
benchi-kafka     | 	sasl.server.max.receive.size = 524288
benchi-kafka     | 	security.inter.broker.protocol = PLAINTEXT
benchi-kafka     | 	security.providers = null
benchi-kafka     | 	server.max.startup.time.ms = 9223372036854775807
benchi-kafka     | 	socket.connection.setup.timeout.max.ms = 30000
benchi-kafka     | 	socket.connection.setup.timeout.ms = 10000
benchi-kafka     | 	socket.listen.backlog.size = 50
benchi-kafka     | 	socket.receive.buffer.bytes = 102400
benchi-kafka     | 	socket.request.max.bytes = 104857600
benchi-kafka     | 	socket.send.buffer.bytes = 102400
benchi-kafka     | 	ssl.allow.dn.changes = false
benchi-kafka     | 	ssl.allow.san.changes = false
benchi-kafka     | 	ssl.cipher.suites = []
benchi-kafka     | 	ssl.client.auth = none
benchi-kafka     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	ssl.engine.factory.class = null
benchi-kafka     | 	ssl.key.password = null
benchi-kafka     | 	ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	ssl.keystore.certificate.chain = null
benchi-kafka     | 	ssl.keystore.key = null
benchi-kafka     | 	ssl.keystore.location = null
benchi-kafka     | 	ssl.keystore.password = null
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.principal.mapping.rules = DEFAULT
benchi-kafka     | 	ssl.protocol = TLSv1.3
benchi-kafka     | 	ssl.provider = null
benchi-kafka     | 	ssl.secure.random.implementation = null
benchi-kafka     | 	ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	ssl.truststore.certificates = null
benchi-kafka     | 	ssl.truststore.location = null
benchi-kafka     | 	ssl.truststore.password = null
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     | 	telemetry.max.bytes = 1048576
benchi-kafka     | 	throughput.quota.window.num = 11
benchi-kafka     | 	token.impersonation.validation = true
benchi-kafka     | 	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
benchi-kafka     | 	transaction.max.timeout.ms = 900000
benchi-kafka     | 	transaction.metadata.load.threads = 32
benchi-kafka     | 	transaction.partition.verification.enable = true
benchi-kafka     | 	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
benchi-kafka     | 	transaction.state.log.load.buffer.size = 5242880
benchi-kafka     | 	transaction.state.log.min.isr = 1
benchi-kafka     | 	transaction.state.log.num.partitions = 50
benchi-kafka     | 	transaction.state.log.replication.factor = 1
benchi-kafka     | 	transaction.state.log.segment.bytes = 104857600
benchi-kafka     | 	transactional.id.expiration.ms = 604800000
benchi-kafka     | 	unclean.leader.election.enable = false
benchi-kafka     | 	unstable.api.versions.enable = false
benchi-kafka     | 	unstable.feature.versions.enable = false
benchi-kafka     | 	zookeeper.acl.change.notification.expiration.ms = 900000
benchi-kafka     | 	zookeeper.clientCnxnSocket = null
benchi-kafka     | 	zookeeper.connect = 
benchi-kafka     | 	zookeeper.connection.timeout.ms = null
benchi-kafka     | 	zookeeper.max.in.flight.requests = 10
benchi-kafka     | 	zookeeper.metadata.migration.enable = false
benchi-kafka     | 	zookeeper.metadata.migration.min.batch.size = 200
benchi-kafka     | 	zookeeper.session.timeout.ms = 18000
benchi-kafka     | 	zookeeper.set.acl = false
benchi-kafka     | 	zookeeper.ssl.cipher.suites = null
benchi-kafka     | 	zookeeper.ssl.client.enable = false
benchi-kafka     | 	zookeeper.ssl.crl.enable = false
benchi-kafka     | 	zookeeper.ssl.enabled.protocols = null
benchi-kafka     | 	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
benchi-kafka     | 	zookeeper.ssl.keystore.location = null
benchi-kafka     | 	zookeeper.ssl.keystore.password = null
benchi-kafka     | 	zookeeper.ssl.keystore.type = null
benchi-kafka     | 	zookeeper.ssl.ocsp.enable = false
benchi-kafka     | 	zookeeper.ssl.protocol = TLSv1.2
benchi-kafka     | 	zookeeper.ssl.truststore.location = null
benchi-kafka     | 	zookeeper.ssl.truststore.password = null
benchi-kafka     | 	zookeeper.ssl.truststore.type = null
benchi-kafka     |  (kafka.server.KafkaConfig)
benchi-kafka     | [2025-03-18 16:08:10,371] INFO RemoteLogManagerConfig values: 
benchi-kafka     | 	log.local.retention.bytes = -2
benchi-kafka     | 	log.local.retention.ms = -2
benchi-kafka     | 	remote.fetch.max.wait.ms = 500
benchi-kafka     | 	remote.log.index.file.cache.total.size.bytes = 1073741824
benchi-kafka     | 	remote.log.manager.copier.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.copy.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.copy.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.expiration.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.fetch.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.fetch.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.task.interval.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.max.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.ms = 500
benchi-kafka     | 	remote.log.manager.task.retry.jitter = 0.2
benchi-kafka     | 	remote.log.manager.thread.pool.size = 10
benchi-kafka     | 	remote.log.metadata.custom.metadata.max.bytes = 128
benchi-kafka     | 	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
benchi-kafka     | 	remote.log.metadata.manager.class.path = null
benchi-kafka     | 	remote.log.metadata.manager.impl.prefix = rlmm.config.
benchi-kafka     | 	remote.log.metadata.manager.listener.name = null
benchi-kafka     | 	remote.log.reader.max.pending.tasks = 100
benchi-kafka     | 	remote.log.reader.threads = 10
benchi-kafka     | 	remote.log.storage.manager.class.name = null
benchi-kafka     | 	remote.log.storage.manager.class.path = null
benchi-kafka     | 	remote.log.storage.manager.impl.prefix = rsm.config.
benchi-kafka     | 	remote.log.storage.system.enable = false
benchi-kafka     |  (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
benchi-kafka     | [2025-03-18 16:08:10,383] INFO Registering metric ActiveBalancerCount (io.confluent.databalancer.KafkaDataBalanceManager)
benchi-kafka     | [2025-03-18 16:08:10,396] INFO Instantiating ClusterBalanceManager with an instance of io.confluent.databalancer.SbcDataBalanceManager (ClusterBalanceManager)
benchi-kafka     | [2025-03-18 16:08:10,397] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)
benchi-kafka     | [2025-03-18 16:08:10,398] INFO [ControllerServer id=1] Starting controller (kafka.server.ControllerServer)
benchi-kafka     | [2025-03-18 16:08:10,400] INFO RemoteLogManagerConfig values: 
benchi-kafka     | 	log.local.retention.bytes = -2
benchi-kafka     | 	log.local.retention.ms = -2
benchi-kafka     | 	remote.fetch.max.wait.ms = 500
benchi-kafka     | 	remote.log.index.file.cache.total.size.bytes = 1073741824
benchi-kafka     | 	remote.log.manager.copier.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.copy.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.copy.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.expiration.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.fetch.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.fetch.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.task.interval.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.max.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.ms = 500
benchi-kafka     | 	remote.log.manager.task.retry.jitter = 0.2
benchi-kafka     | 	remote.log.manager.thread.pool.size = 10
benchi-kafka     | 	remote.log.metadata.custom.metadata.max.bytes = 128
benchi-kafka     | 	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
benchi-kafka     | 	remote.log.metadata.manager.class.path = null
benchi-kafka     | 	remote.log.metadata.manager.impl.prefix = rlmm.config.
benchi-kafka     | 	remote.log.metadata.manager.listener.name = null
benchi-kafka     | 	remote.log.reader.max.pending.tasks = 100
benchi-kafka     | 	remote.log.reader.threads = 10
benchi-kafka     | 	remote.log.storage.manager.class.name = null
benchi-kafka     | 	remote.log.storage.manager.class.path = null
benchi-kafka     | 	remote.log.storage.manager.impl.prefix = rsm.config.
benchi-kafka     | 	remote.log.storage.system.enable = false
benchi-kafka     |  (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
benchi-kafka     | [2025-03-18 16:08:10,401] INFO [ControllerServer id=1] FIPS mode enabled: false (kafka.server.ControllerServer)
benchi-kafka     | [2025-03-18 16:08:10,405] INFO AuditLogConfig values: 
benchi-kafka     | 	confluent.security.event.logger.authentication.enable = false
benchi-kafka     | 	confluent.security.event.logger.authentication.event.rate.limit = -1
benchi-kafka     | 	confluent.security.event.logger.authorization.event.rate.limit = -1
benchi-kafka     | 	confluent.security.event.logger.cloudevent.codec = structured
benchi-kafka     | 	confluent.security.event.logger.enable = true
benchi-kafka     | 	confluent.security.event.logger.exporter.class = class io.confluent.security.audit.telemetry.exporter.NonBlockingKafkaExporter
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.create = true
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.partitions = 12
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.replicas = 3
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.retention.bytes = -1
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.retention.ms = 2592000000
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.roll.ms = 14400000
benchi-kafka     | 	confluent.security.event.logger.kafka.request.rate.limit = -1
benchi-kafka     | 	confluent.security.event.logger.physical.cluster.id = 
benchi-kafka     | 	confluent.security.event.router.cache.entries = 10000
benchi-kafka     | 	confluent.security.event.router.config = 
benchi-kafka     | 	confluent.security.event.router.named.config.enabled = false
benchi-kafka     |  (io.confluent.security.audit.AuditLogConfig)
benchi-kafka     | [2025-03-18 16:08:10,405] INFO CrnAuthorityConfig values: 
benchi-kafka     | 	confluent.authorizer.authority.cache.entries = 10000
benchi-kafka     | 	confluent.authorizer.authority.name = 
benchi-kafka     | 	confluent.metadata.server.api.flavor = CP
benchi-kafka     |  (io.confluent.crn.CrnAuthorityConfig)
benchi-kafka     | [2025-03-18 16:08:10,407] INFO NamedConfigEnabled: false (io.confluent.security.audit.provider.ConfluentAuditLogProvider)
benchi-kafka     | [2025-03-18 16:08:10,407] INFO AuditLogConfig values: 
benchi-kafka     | 	confluent.security.event.logger.authentication.enable = false
benchi-kafka     | 	confluent.security.event.logger.authentication.event.rate.limit = -1
benchi-kafka     | 	confluent.security.event.logger.authorization.event.rate.limit = -1
benchi-kafka     | 	confluent.security.event.logger.cloudevent.codec = structured
benchi-kafka     | 	confluent.security.event.logger.enable = true
benchi-kafka     | 	confluent.security.event.logger.exporter.class = class io.confluent.security.audit.telemetry.exporter.NonBlockingKafkaExporter
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.create = true
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.partitions = 12
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.replicas = 3
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.retention.bytes = -1
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.retention.ms = 2592000000
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.roll.ms = 14400000
benchi-kafka     | 	confluent.security.event.logger.kafka.request.rate.limit = -1
benchi-kafka     | 	confluent.security.event.logger.physical.cluster.id = 
benchi-kafka     | 	confluent.security.event.router.cache.entries = 10000
benchi-kafka     | 	confluent.security.event.router.config = 
benchi-kafka     | 	confluent.security.event.router.named.config.enabled = false
benchi-kafka     |  (io.confluent.security.audit.AuditLogConfig)
benchi-kafka     | [2025-03-18 16:08:10,415] INFO CrnAuthorityConfig values: 
benchi-kafka     | 	confluent.authorizer.authority.cache.entries = 10000
benchi-kafka     | 	confluent.authorizer.authority.name = 
benchi-kafka     | 	confluent.metadata.server.api.flavor = CP
benchi-kafka     |  (io.confluent.crn.CrnAuthorityConfig)
benchi-kafka     | [2025-03-18 16:08:10,415] INFO MultiTenantAuditLogConfig values: 
benchi-kafka     | 	confluent.security.event.logger.client.ip.enable = false
benchi-kafka     | 	confluent.security.event.logger.multitenant.enable = false
benchi-kafka     |  (io.confluent.kafka.multitenant.authorizer.MultiTenantAuditLogConfig)
benchi-kafka     | [2025-03-18 16:08:10,415] INFO AuditLogConfig values: 
benchi-kafka     | 	confluent.security.event.logger.authentication.enable = false
benchi-kafka     | 	confluent.security.event.logger.authentication.event.rate.limit = -1
benchi-kafka     | 	confluent.security.event.logger.authorization.event.rate.limit = -1
benchi-kafka     | 	confluent.security.event.logger.cloudevent.codec = structured
benchi-kafka     | 	confluent.security.event.logger.enable = true
benchi-kafka     | 	confluent.security.event.logger.exporter.class = class io.confluent.security.audit.telemetry.exporter.NonBlockingKafkaExporter
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.create = true
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.partitions = 12
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.replicas = 3
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.retention.bytes = -1
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.retention.ms = 2592000000
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.roll.ms = 14400000
benchi-kafka     | 	confluent.security.event.logger.kafka.request.rate.limit = -1
benchi-kafka     | 	confluent.security.event.logger.physical.cluster.id = 
benchi-kafka     | 	confluent.security.event.router.cache.entries = 10000
benchi-kafka     | 	confluent.security.event.router.config = 
benchi-kafka     | 	confluent.security.event.router.named.config.enabled = false
benchi-kafka     |  (io.confluent.security.audit.AuditLogConfig)
benchi-kafka     | [2025-03-18 16:08:10,415] INFO Audit Log rate limiter reconfigured: Authn: -1, Authz: -1, Kafka request: -1 (io.confluent.security.audit.provider.AuditLogRateLimiter)
benchi-kafka     | [2025-03-18 16:08:10,495] INFO [SocketServer listenerType=CONTROLLER, nodeId=1] Creating data-plane acceptor and processors for endpoint : ListenerName(CONTROLLER) (kafka.network.SocketServer)
benchi-kafka     | [2025-03-18 16:08:10,501] INFO Quota CONTROLLER-per-ip-connection-rate configured - (max: -1.0, floor: -1.0, adjustment: -1.0) (kafka.network.ListenerIpAutoTuningQuota)
benchi-kafka     | [2025-03-18 16:08:10,501] INFO Quota CONTROLLER-per-tenant-connection-rate configured - (max: -1.0, floor: -1.0, adjustment: -1.0) (kafka.network.ListenerTenantAutoTuningQuota)
benchi-kafka     | [2025-03-18 16:08:10,502] INFO Quota CONTROLLER-connection-rate configured - (max: 1.7976931348623157E308, floor: 1.7976931348623157E308, adjustment: 5.0) (kafka.network.ListenerAutoTuningQuota)
benchi-kafka     | [2025-03-18 16:08:10,502] INFO Updated connection-tokens max connection creation rate to 1.7976931348623157E308 (kafka.network.ConnectionQuotas)
benchi-kafka     | [2025-03-18 16:08:10,510] INFO Config values: 
benchi-kafka     | 	confluent.security.event.logger.detailed.audit.logs.disabled.apis = 
benchi-kafka     | 	confluent.security.event.logger.enable.detailed.audit.logs = false
benchi-kafka     | 	confluent.security.event.logger.enable.produce.consume.audit.logs = false
benchi-kafka     |  (org.apache.kafka.common.requests.DetailedRequestAuditLogFilter$Config)
benchi-kafka     | [2025-03-18 16:08:10,538] INFO Config values: 
benchi-kafka     | 	confluent.security.event.logger.detailed.audit.logs.disabled.apis = 
benchi-kafka     | 	confluent.security.event.logger.enable.detailed.audit.logs = false
benchi-kafka     | 	confluent.security.event.logger.enable.produce.consume.audit.logs = false
benchi-kafka     |  (org.apache.kafka.common.requests.DetailedRequestAuditLogFilter$Config)
benchi-kafka     | [2025-03-18 16:08:10,539] INFO Config values: 
benchi-kafka     | 	confluent.security.event.logger.detailed.audit.logs.disabled.apis = 
benchi-kafka     | 	confluent.security.event.logger.enable.detailed.audit.logs = false
benchi-kafka     | 	confluent.security.event.logger.enable.produce.consume.audit.logs = false
benchi-kafka     |  (org.apache.kafka.common.requests.DetailedRequestAuditLogFilter$Config)
benchi-kafka     | [2025-03-18 16:08:10,541] INFO [SocketServer listenerType=CONTROLLER, nodeId=1] Created data-plane acceptor and processors for endpoint : ListenerName(CONTROLLER) (kafka.network.SocketServer)
benchi-kafka     | [2025-03-18 16:08:10,544] INFO [SharedServer id=1] Using broker:29092 as bootstrap.servers for inter broker client config. (kafka.server.SharedServer)
benchi-kafka     | [2025-03-18 16:08:10,549] INFO [SharedServer id=1] Starting SharedServer (kafka.server.SharedServer)
benchi-kafka     | [2025-03-18 16:08:10,550] INFO RemoteLogManagerConfig values: 
benchi-kafka     | 	log.local.retention.bytes = -2
benchi-kafka     | 	log.local.retention.ms = -2
benchi-kafka     | 	remote.fetch.max.wait.ms = 500
benchi-kafka     | 	remote.log.index.file.cache.total.size.bytes = 1073741824
benchi-kafka     | 	remote.log.manager.copier.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.copy.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.copy.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.expiration.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.fetch.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.fetch.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.task.interval.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.max.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.ms = 500
benchi-kafka     | 	remote.log.manager.task.retry.jitter = 0.2
benchi-kafka     | 	remote.log.manager.thread.pool.size = 10
benchi-kafka     | 	remote.log.metadata.custom.metadata.max.bytes = 128
benchi-kafka     | 	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
benchi-kafka     | 	remote.log.metadata.manager.class.path = null
benchi-kafka     | 	remote.log.metadata.manager.impl.prefix = rlmm.config.
benchi-kafka     | 	remote.log.metadata.manager.listener.name = null
benchi-kafka     | 	remote.log.reader.max.pending.tasks = 100
benchi-kafka     | 	remote.log.reader.threads = 10
benchi-kafka     | 	remote.log.storage.manager.class.name = null
benchi-kafka     | 	remote.log.storage.manager.class.path = null
benchi-kafka     | 	remote.log.storage.manager.impl.prefix = rsm.config.
benchi-kafka     | 	remote.log.storage.system.enable = false
benchi-kafka     |  (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
benchi-kafka     | [2025-03-18 16:08:10,555] INFO RemoteLogManagerConfig values: 
benchi-kafka     | 	log.local.retention.bytes = -2
benchi-kafka     | 	log.local.retention.ms = -2
benchi-kafka     | 	remote.fetch.max.wait.ms = 500
benchi-kafka     | 	remote.log.index.file.cache.total.size.bytes = 1073741824
benchi-kafka     | 	remote.log.manager.copier.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.copy.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.copy.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.expiration.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.fetch.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.fetch.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.task.interval.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.max.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.ms = 500
benchi-kafka     | 	remote.log.manager.task.retry.jitter = 0.2
benchi-kafka     | 	remote.log.manager.thread.pool.size = 10
benchi-kafka     | 	remote.log.metadata.custom.metadata.max.bytes = 128
benchi-kafka     | 	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
benchi-kafka     | 	remote.log.metadata.manager.class.path = null
benchi-kafka     | 	remote.log.metadata.manager.impl.prefix = rlmm.config.
benchi-kafka     | 	remote.log.metadata.manager.listener.name = null
benchi-kafka     | 	remote.log.reader.max.pending.tasks = 100
benchi-kafka     | 	remote.log.reader.threads = 10
benchi-kafka     | 	remote.log.storage.manager.class.name = null
benchi-kafka     | 	remote.log.storage.manager.class.path = null
benchi-kafka     | 	remote.log.storage.manager.impl.prefix = rsm.config.
benchi-kafka     | 	remote.log.storage.system.enable = false
benchi-kafka     |  (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
benchi-kafka     | [2025-03-18 16:08:10,599] INFO [MergedLog partition=__cluster_metadata-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:08:10,599] INFO [MergedLog partition=__cluster_metadata-0, dir=/tmp/kraft-combined-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:08:10,599] INFO [MergedLog partition=__cluster_metadata-0, dir=/tmp/kraft-combined-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:08:10,603] INFO Initialized snapshots with IDs SortedSet() from /tmp/kraft-combined-logs/__cluster_metadata-0 (kafka.raft.KafkaMetadataLog$)
benchi-kafka     | [2025-03-18 16:08:10,607] INFO [raft-expiration-reaper]: Starting (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:08:10,611] INFO [RaftManager id=1] Reading KRaft snapshot and log as part of the initialization (org.apache.kafka.raft.KafkaRaftClient)
benchi-kafka     | [2025-03-18 16:08:10,612] INFO [RaftManager id=1] Starting request manager with static voters: [broker:29093 (id: 1 rack: null)] (org.apache.kafka.raft.KafkaRaftClient)
benchi-kafka     | [2025-03-18 16:08:10,624] INFO [RaftManager id=1] Completed transition to Unattached(epoch=0, voters=[1], electionTimeoutMs=1094) from null (org.apache.kafka.raft.QuorumState)
benchi-kafka     | [2025-03-18 16:08:10,627] INFO [RaftManager id=1] Completed transition to CandidateState(localId=1, localDirectoryId=Y-adVyLUb4IllEQbE2t_Kg,epoch=1, retries=1, voteStates={1=GRANTED}, highWatermark=Optional.empty, electionTimeoutMs=1389) from Unattached(epoch=0, voters=[1], electionTimeoutMs=1094) (org.apache.kafka.raft.QuorumState)
benchi-kafka     | [2025-03-18 16:08:10,629] INFO [RaftManager id=1] Completed transition to Leader(localId=1, epoch=1, epochStartOffset=0, highWatermark=Optional.empty, voterStates={1=ReplicaState(nodeId=1, endOffset=Optional.empty, lastFetchTimestamp=-1, lastCaughtUpTimestamp=-1, hasAcknowledgedLeader=true)}) from CandidateState(localId=1, localDirectoryId=Y-adVyLUb4IllEQbE2t_Kg,epoch=1, retries=1, voteStates={1=GRANTED}, highWatermark=Optional.empty, electionTimeoutMs=1389) (org.apache.kafka.raft.QuorumState)
benchi-kafka     | [2025-03-18 16:08:10,634] INFO [kafka-1-raft-outbound-request-thread]: Starting (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
benchi-kafka     | [2025-03-18 16:08:10,634] INFO [kafka-1-raft-io-thread]: Starting (org.apache.kafka.raft.KafkaRaftClientDriver)
benchi-kafka     | [2025-03-18 16:08:10,643] INFO [RaftManager id=1] High watermark set to LogOffsetMetadata(offset=1, metadata=Optional[(segmentBaseOffset=0,relativePositionInSegment=91)]) for the first time for epoch 1 based on indexOfHw 0 and voters [ReplicaState(nodeId=1, endOffset=Optional[LogOffsetMetadata(offset=1, metadata=Optional[(segmentBaseOffset=0,relativePositionInSegment=91)])], lastFetchTimestamp=-1, lastCaughtUpTimestamp=-1, hasAcknowledgedLeader=true)] (org.apache.kafka.raft.LeaderState)
benchi-kafka     | [2025-03-18 16:08:10,645] INFO [MetadataLoader id=1] initializeNewPublishers: The loader is still catching up because we have loaded up to offset -1, but the high water mark is 1 (org.apache.kafka.image.loader.MetadataLoader)
benchi-kafka     | [2025-03-18 16:08:10,646] INFO [ControllerServer id=1] Waiting for controller quorum voters future (kafka.server.ControllerServer)
benchi-kafka     | [2025-03-18 16:08:10,646] INFO [ControllerServer id=1] Finished waiting for controller quorum voters future (kafka.server.ControllerServer)
benchi-kafka     | [2025-03-18 16:08:10,651] INFO [RaftManager id=1] Registered the listener org.apache.kafka.image.loader.MetadataLoader@1486552696 (org.apache.kafka.raft.KafkaRaftClient)
benchi-kafka     | [2025-03-18 16:08:10,653] INFO [MetadataLoader id=1] maybePublishMetadata(LOG_DELTA): The loader is still catching up because we have not loaded a controller record as of offset 0 and high water mark is 1 (org.apache.kafka.image.loader.MetadataLoader)
benchi-kafka     | [2025-03-18 16:08:10,667] INFO [ControllerServer id=1] Creating new QuorumController with clusterId MkU3OEVBNTcwNTJENDM2Qk. (org.apache.kafka.controller.QuorumController)
benchi-kafka     | [2025-03-18 16:08:10,667] INFO [RaftManager id=1] Registered the listener org.apache.kafka.controller.QuorumController$QuorumMetaLogListener@1924627918 (org.apache.kafka.raft.KafkaRaftClient)
benchi-kafka     | [2025-03-18 16:08:10,668] INFO [ControllerServer id=1] Becoming the active controller at epoch 1, next write offset 1. (org.apache.kafka.controller.QuorumController)
benchi-kafka     | [2025-03-18 16:08:10,670] WARN [ControllerServer id=1] Performing controller activation. The metadata log appears to be empty. Appending 1 bootstrap record(s) in metadata transaction at metadata.version 3.8-IV0A from bootstrap source 'the binary bootstrap metadata file: /tmp/kraft-combined-logs/bootstrap.checkpoint'. Setting the ZK migration state to NONE since this is a de-novo KRaft cluster. (org.apache.kafka.controller.QuorumController)
benchi-kafka     | [2025-03-18 16:08:10,671] INFO [ControllerServer id=1] Replayed BeginTransactionRecord(name='Bootstrap records') at offset 1. (org.apache.kafka.controller.OffsetControlManager)
benchi-kafka     | [2025-03-18 16:08:10,671] INFO [ControllerServer id=1] Replayed a Confluent FeatureLevelRecord setting metadata version to 3.8-IV0A (org.apache.kafka.controller.FeatureControlManager)
benchi-kafka     | [2025-03-18 16:08:10,671] INFO [ControllerServer id=1] Replayed EndTransactionRecord() at offset 4. (org.apache.kafka.controller.OffsetControlManager)
benchi-kafka     | [2025-03-18 16:08:10,673] INFO Empty logDirs received! Disk based throttling won't be activated! (org.apache.kafka.server.config.DiskUsageBasedThrottlingConfig)
benchi-kafka     | [2025-03-18 16:08:10,673] INFO Empty logDirs received! Disk based throttling won't be activated! (org.apache.kafka.server.config.DiskUsageBasedThrottlingConfig)
benchi-kafka     | [2025-03-18 16:08:10,676] INFO [controller-1-ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:08:10,678] INFO [controller-1-ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:08:10,678] INFO Empty logDirs received! Disk based throttling won't be activated! (org.apache.kafka.server.config.DiskUsageBasedThrottlingConfig)
benchi-kafka     | [2025-03-18 16:08:10,679] INFO [controller-1-ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:08:10,679] INFO Client Quota Max Throttle Time is updated from 5000 to 1000 (kafka.server.ClientRequestQuotaManager)
benchi-kafka     | [2025-03-18 16:08:10,683] INFO Empty logDirs received! Disk based throttling won't be activated! (org.apache.kafka.server.config.DiskUsageBasedThrottlingConfig)
benchi-kafka     | [2025-03-18 16:08:10,684] INFO Client Quota Max Throttle Time is updated from 5000 to 1000 (kafka.server.ClusterLinkRequestQuotaManager)
benchi-kafka     | [2025-03-18 16:08:10,684] INFO [controller-1-ThrottledChannelReaper-ClusterLinkRequest]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:08:10,686] INFO [controller-1-ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:08:10,694] INFO [ExpirationReaper-1-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:08:10,695] INFO Config values: 
benchi-kafka     | 	confluent.request.log.api.samples.per.min = 
benchi-kafka     | 	confluent.request.log.enable.admin.apis = true
benchi-kafka     | 	confluent.request.log.enable.slowlog = false
benchi-kafka     | 	confluent.request.log.samples.per.min = 0
benchi-kafka     | 	confluent.request.slowlog.threshold.override = -1.0
benchi-kafka     | 	confluent.request.slowlog.threshold.p99.min = -1.0
benchi-kafka     |  (org.apache.kafka.common.requests.SamplingRequestLogFilter$Config)
benchi-kafka     | [2025-03-18 16:08:10,696] INFO Config values: 
benchi-kafka     | 	confluent.request.log.api.samples.per.min = 
benchi-kafka     | 	confluent.request.log.enable.admin.apis = true
benchi-kafka     | 	confluent.request.log.enable.slowlog = false
benchi-kafka     | 	confluent.request.log.samples.per.min = 0
benchi-kafka     | 	confluent.request.slowlog.threshold.override = -1.0
benchi-kafka     | 	confluent.request.slowlog.threshold.p99.min = -1.0
benchi-kafka     |  (org.apache.kafka.common.requests.SamplingRequestLogFilter$Config)
benchi-kafka     | [2025-03-18 16:08:10,696] INFO Config values: 
benchi-kafka     | 	confluent.request.log.api.samples.per.min = 
benchi-kafka     | 	confluent.request.log.enable.admin.apis = true
benchi-kafka     | 	confluent.request.log.enable.slowlog = false
benchi-kafka     | 	confluent.request.log.samples.per.min = 0
benchi-kafka     | 	confluent.request.slowlog.threshold.override = -1.0
benchi-kafka     | 	confluent.request.slowlog.threshold.p99.min = -1.0
benchi-kafka     |  (org.apache.kafka.common.requests.SamplingRequestLogFilter$Config)
benchi-kafka     | [2025-03-18 16:08:10,696] INFO Config values: 
benchi-kafka     | 	confluent.request.log.api.samples.per.min = 
benchi-kafka     | 	confluent.request.log.enable.admin.apis = true
benchi-kafka     | 	confluent.request.log.enable.slowlog = false
benchi-kafka     | 	confluent.request.log.samples.per.min = 0
benchi-kafka     | 	confluent.request.slowlog.threshold.override = -1.0
benchi-kafka     | 	confluent.request.slowlog.threshold.p99.min = -1.0
benchi-kafka     |  (org.apache.kafka.common.requests.SamplingRequestLogFilter$Config)
benchi-kafka     | [2025-03-18 16:08:10,697] INFO Config values: 
benchi-kafka     | 	confluent.request.log.api.samples.per.min = 
benchi-kafka     | 	confluent.request.log.enable.admin.apis = true
benchi-kafka     | 	confluent.request.log.enable.slowlog = false
benchi-kafka     | 	confluent.request.log.samples.per.min = 0
benchi-kafka     | 	confluent.request.slowlog.threshold.override = -1.0
benchi-kafka     | 	confluent.request.slowlog.threshold.p99.min = -1.0
benchi-kafka     |  (org.apache.kafka.common.requests.SamplingRequestLogFilter$Config)
benchi-kafka     | [2025-03-18 16:08:10,697] INFO Config values: 
benchi-kafka     | 	confluent.request.log.api.samples.per.min = 
benchi-kafka     | 	confluent.request.log.enable.admin.apis = true
benchi-kafka     | 	confluent.request.log.enable.slowlog = false
benchi-kafka     | 	confluent.request.log.samples.per.min = 0
benchi-kafka     | 	confluent.request.slowlog.threshold.override = -1.0
benchi-kafka     | 	confluent.request.slowlog.threshold.p99.min = -1.0
benchi-kafka     |  (org.apache.kafka.common.requests.SamplingRequestLogFilter$Config)
benchi-kafka     | [2025-03-18 16:08:10,697] INFO Config values: 
benchi-kafka     | 	confluent.request.log.api.samples.per.min = 
benchi-kafka     | 	confluent.request.log.enable.admin.apis = true
benchi-kafka     | 	confluent.request.log.enable.slowlog = false
benchi-kafka     | 	confluent.request.log.samples.per.min = 0
benchi-kafka     | 	confluent.request.slowlog.threshold.override = -1.0
benchi-kafka     | 	confluent.request.slowlog.threshold.p99.min = -1.0
benchi-kafka     |  (org.apache.kafka.common.requests.SamplingRequestLogFilter$Config)
benchi-kafka     | [2025-03-18 16:08:10,698] INFO Config values: 
benchi-kafka     | 	confluent.request.log.api.samples.per.min = 
benchi-kafka     | 	confluent.request.log.enable.admin.apis = true
benchi-kafka     | 	confluent.request.log.enable.slowlog = false
benchi-kafka     | 	confluent.request.log.samples.per.min = 0
benchi-kafka     | 	confluent.request.slowlog.threshold.override = -1.0
benchi-kafka     | 	confluent.request.slowlog.threshold.p99.min = -1.0
benchi-kafka     |  (org.apache.kafka.common.requests.SamplingRequestLogFilter$Config)
benchi-kafka     | [2025-03-18 16:08:10,702] INFO [MetadataLoader id=1] maybePublishMetadata(LOG_DELTA): The loader finished catching up to the current high water mark of 5 (org.apache.kafka.image.loader.MetadataLoader)
benchi-kafka     | [2025-03-18 16:08:10,703] INFO [ControllerServer id=1] Self-Balancing Kafka is enabled and will be installed as a metadata publisher. (kafka.server.ControllerServer)
benchi-kafka     | [2025-03-18 16:08:10,704] INFO [ControllerServer id=1] Waiting for the controller metadata publishers to be installed (kafka.server.ControllerServer)
benchi-kafka     | [2025-03-18 16:08:10,705] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing SnapshotGenerator with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)
benchi-kafka     | [2025-03-18 16:08:10,705] INFO [ControllerServer id=1] Finished waiting for the controller metadata publishers to be installed (kafka.server.ControllerServer)
benchi-kafka     | [2025-03-18 16:08:10,705] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing KRaftMetadataCachePublisher with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)
benchi-kafka     | [2025-03-18 16:08:10,705] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing FeaturesPublisher with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)
benchi-kafka     | [2025-03-18 16:08:10,705] INFO [SocketServer listenerType=CONTROLLER, nodeId=1] Enabling request processing. (kafka.network.SocketServer)
benchi-kafka     | [2025-03-18 16:08:10,705] INFO [ControllerServer id=1] Loaded new metadata Features(metadataVersion=3.8-IV0A, finalizedFeatures={confluent.metadata.version=120}, finalizedFeaturesEpoch=4). (org.apache.kafka.metadata.publisher.FeaturesPublisher)
benchi-kafka     | [2025-03-18 16:08:10,705] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing ControllerRegistrationsPublisher with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)
benchi-kafka     | [2025-03-18 16:08:10,705] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing ControllerRegistrationManager with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)
benchi-kafka     | [2025-03-18 16:08:10,706] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing DynamicConfigPublisher controller id=1 with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)
benchi-kafka     | [2025-03-18 16:08:10,706] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing DynamicClientQuotaPublisher controller id=1 with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)
benchi-kafka     | [2025-03-18 16:08:10,706] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing ScramPublisher controller id=1 with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)
benchi-kafka     | [2025-03-18 16:08:10,706] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing DelegationTokenPublisher controller id=1 with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)
benchi-kafka     | [2025-03-18 16:08:10,707] INFO Awaiting socket connections on broker:29093. (kafka.network.DataPlaneAcceptor)
benchi-kafka     | [2025-03-18 16:08:10,707] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing ControllerMetadataMetricsPublisher with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)
benchi-kafka     | [2025-03-18 16:08:10,707] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing ConfluentControllerMetricsPublisher with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)
benchi-kafka     | [2025-03-18 16:08:10,707] INFO [ConfluentControllerMetricsChanges id=1] Finished reloading all Confluent controller metrics in 0 ms. (org.apache.kafka.controller.metrics.ConfluentControllerMetricsChanges)
benchi-kafka     | [2025-03-18 16:08:10,707] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing CellControllerMetadataMetricsPublisher with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)
benchi-kafka     | [2025-03-18 16:08:10,708] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing SbcDataBalanceManager with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)
benchi-kafka     | [2025-03-18 16:08:10,708] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing AclPublisher controller id=1 with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)
benchi-kafka     | [2025-03-18 16:08:10,710] INFO Balancer received a new Replica Exclusions Image (image: , delta: BrokerReplicaExclusionsDelta{newExclusions=[], removedExclusions=[]}) (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:08:10,710] INFO Handling event SbcAlteredExclusionsEvent-4 (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:08:10,710] INFO SBC Event SbcMetadataUpdateEvent-1 generated 1 more events to enqueue in the following order - [SbcConfigUpdateEvent-3]. Enqueuing... (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:08:10,710] INFO Handling event SbcLeaderUpdateEvent-2 (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:08:10,710] INFO This balancer node is now the metadata quorum leader. Activating kafkadatabalance manager without alive broker snapshot. (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:08:10,710] INFO Handling event SbcConfigUpdateEvent-3 (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:08:10,711] INFO Cluster metadata containing at least one unfenced broker not yet available, SBC config processing delayed. (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:08:10,711] INFO Handling event SbcKraftStartupEvent-5 (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:08:10,711] INFO Cluster metadata containing at least one unfenced broker not yet available, SBC startup delayed. (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:08:10,712] INFO [controller-1-to-controller-registration-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
benchi-kafka     | [2025-03-18 16:08:10,712] INFO [ControllerServer id=1] Waiting for all of the authorizer futures to be completed (kafka.server.ControllerServer)
benchi-kafka     | [2025-03-18 16:08:10,712] INFO [ControllerServer id=1] Finished waiting for all of the authorizer futures to be completed (kafka.server.ControllerServer)
benchi-kafka     | [2025-03-18 16:08:10,712] INFO [ControllerServer id=1] Waiting for all of the SocketServer Acceptors to be started (kafka.server.ControllerServer)
benchi-kafka     | [2025-03-18 16:08:10,712] INFO [ControllerRegistrationManager id=1 incarnation=7_mCSDvPTJS1-SXDmw9VyQ] initialized channel manager. (kafka.server.ControllerRegistrationManager)
benchi-kafka     | [2025-03-18 16:08:10,712] INFO [controller-1-to-controller-registration-channel-manager]: Recorded new KRaft controller, from now on will use node broker:29093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
benchi-kafka     | [2025-03-18 16:08:10,712] INFO [ControllerServer id=1] Finished waiting for all of the SocketServer Acceptors to be started (kafka.server.ControllerServer)
benchi-kafka     | [2025-03-18 16:08:10,712] INFO [ControllerServer id=1] Waiting for userDeletionHandler futures to be completed (kafka.server.ControllerServer)
benchi-kafka     | [2025-03-18 16:08:10,712] INFO [ControllerServer id=1] Finished waiting for userDeletionHandler futures to be completed (kafka.server.ControllerServer)
benchi-kafka     | [2025-03-18 16:08:10,713] INFO [ControllerRegistrationManager id=1 incarnation=7_mCSDvPTJS1-SXDmw9VyQ] sendControllerRegistration: attempting to send ControllerRegistrationRequestData(controllerId=1, incarnationId=7_mCSDvPTJS1-SXDmw9VyQ, zkMigrationReady=false, listeners=[Listener(name='CONTROLLER', host='broker', port=29093, securityProtocol=0)], features=[Feature(name='confluent.metadata.version', minSupportedVersion=1, maxSupportedVersion=120), Feature(name='metadata.version', minSupportedVersion=1, maxSupportedVersion=20)], metadataEncryptors=[]) (kafka.server.ControllerRegistrationManager)
benchi-kafka     | [2025-03-18 16:08:10,713] INFO [BrokerServer id=1] Transition from SHUTDOWN to STARTING (kafka.server.BrokerServer)
benchi-kafka     | [2025-03-18 16:08:10,713] INFO [BrokerServer id=1] Starting broker (kafka.server.BrokerServer)
benchi-kafka     | [2025-03-18 16:08:10,714] INFO RemoteLogManagerConfig values: 
benchi-kafka     | 	log.local.retention.bytes = -2
benchi-kafka     | 	log.local.retention.ms = -2
benchi-kafka     | 	remote.fetch.max.wait.ms = 500
benchi-kafka     | 	remote.log.index.file.cache.total.size.bytes = 1073741824
benchi-kafka     | 	remote.log.manager.copier.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.copy.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.copy.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.expiration.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.fetch.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.fetch.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.task.interval.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.max.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.ms = 500
benchi-kafka     | 	remote.log.manager.task.retry.jitter = 0.2
benchi-kafka     | 	remote.log.manager.thread.pool.size = 10
benchi-kafka     | 	remote.log.metadata.custom.metadata.max.bytes = 128
benchi-kafka     | 	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
benchi-kafka     | 	remote.log.metadata.manager.class.path = null
benchi-kafka     | 	remote.log.metadata.manager.impl.prefix = rlmm.config.
benchi-kafka     | 	remote.log.metadata.manager.listener.name = null
benchi-kafka     | 	remote.log.reader.max.pending.tasks = 100
benchi-kafka     | 	remote.log.reader.threads = 10
benchi-kafka     | 	remote.log.storage.manager.class.name = null
benchi-kafka     | 	remote.log.storage.manager.class.path = null
benchi-kafka     | 	remote.log.storage.manager.impl.prefix = rsm.config.
benchi-kafka     | 	remote.log.storage.system.enable = false
benchi-kafka     |  (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
benchi-kafka     | [2025-03-18 16:08:10,721] INFO [BrokerServer id=1] FIPS mode enabled: false (kafka.server.BrokerServer)
benchi-kafka     | [2025-03-18 16:08:10,723] INFO Empty logDirs received! Disk based throttling won't be activated! (org.apache.kafka.server.config.DiskUsageBasedThrottlingConfig)
benchi-kafka     | [2025-03-18 16:08:10,723] INFO [broker-1-ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:08:10,723] INFO Empty logDirs received! Disk based throttling won't be activated! (org.apache.kafka.server.config.DiskUsageBasedThrottlingConfig)
benchi-kafka     | [2025-03-18 16:08:10,723] INFO [broker-1-ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:08:10,724] INFO Client Quota Max Throttle Time is updated from 5000 to 1000 (kafka.server.ClientRequestQuotaManager)
benchi-kafka     | [2025-03-18 16:08:10,724] INFO [broker-1-ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:08:10,724] INFO Empty logDirs received! Disk based throttling won't be activated! (org.apache.kafka.server.config.DiskUsageBasedThrottlingConfig)
benchi-kafka     | [2025-03-18 16:08:10,724] INFO Client Quota Max Throttle Time is updated from 5000 to 1000 (kafka.server.ClusterLinkRequestQuotaManager)
benchi-kafka     | [2025-03-18 16:08:10,724] INFO [broker-1-ThrottledChannelReaper-ClusterLinkRequest]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:08:10,725] INFO [broker-1-ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:08:10,734] INFO Skip DiskIOManager init: confluent.disk.io.manager.enable = false (kafka.server.resource.DiskIOManager)
benchi-kafka     | [2025-03-18 16:08:10,735] INFO AuditLogConfig values: 
benchi-kafka     | 	confluent.security.event.logger.authentication.enable = false
benchi-kafka     | 	confluent.security.event.logger.authentication.event.rate.limit = -1
benchi-kafka     | 	confluent.security.event.logger.authorization.event.rate.limit = -1
benchi-kafka     | 	confluent.security.event.logger.cloudevent.codec = structured
benchi-kafka     | 	confluent.security.event.logger.enable = true
benchi-kafka     | 	confluent.security.event.logger.exporter.class = class io.confluent.security.audit.telemetry.exporter.NonBlockingKafkaExporter
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.create = true
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.partitions = 12
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.replicas = 3
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.retention.bytes = -1
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.retention.ms = 2592000000
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.roll.ms = 14400000
benchi-kafka     | 	confluent.security.event.logger.kafka.request.rate.limit = -1
benchi-kafka     | 	confluent.security.event.logger.physical.cluster.id = 
benchi-kafka     | 	confluent.security.event.router.cache.entries = 10000
benchi-kafka     | 	confluent.security.event.router.config = 
benchi-kafka     | 	confluent.security.event.router.named.config.enabled = false
benchi-kafka     |  (io.confluent.security.audit.AuditLogConfig)
benchi-kafka     | [2025-03-18 16:08:10,735] INFO CrnAuthorityConfig values: 
benchi-kafka     | 	confluent.authorizer.authority.cache.entries = 10000
benchi-kafka     | 	confluent.authorizer.authority.name = 
benchi-kafka     | 	confluent.metadata.server.api.flavor = CP
benchi-kafka     |  (io.confluent.crn.CrnAuthorityConfig)
benchi-kafka     | [2025-03-18 16:08:10,735] INFO NamedConfigEnabled: false (io.confluent.security.audit.provider.ConfluentAuditLogProvider)
benchi-kafka     | [2025-03-18 16:08:10,735] INFO AuditLogConfig values: 
benchi-kafka     | 	confluent.security.event.logger.authentication.enable = false
benchi-kafka     | 	confluent.security.event.logger.authentication.event.rate.limit = -1
benchi-kafka     | 	confluent.security.event.logger.authorization.event.rate.limit = -1
benchi-kafka     | 	confluent.security.event.logger.cloudevent.codec = structured
benchi-kafka     | 	confluent.security.event.logger.enable = true
benchi-kafka     | 	confluent.security.event.logger.exporter.class = class io.confluent.security.audit.telemetry.exporter.NonBlockingKafkaExporter
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.create = true
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.partitions = 12
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.replicas = 3
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.retention.bytes = -1
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.retention.ms = 2592000000
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.roll.ms = 14400000
benchi-kafka     | 	confluent.security.event.logger.kafka.request.rate.limit = -1
benchi-kafka     | 	confluent.security.event.logger.physical.cluster.id = 
benchi-kafka     | 	confluent.security.event.router.cache.entries = 10000
benchi-kafka     | 	confluent.security.event.router.config = 
benchi-kafka     | 	confluent.security.event.router.named.config.enabled = false
benchi-kafka     |  (io.confluent.security.audit.AuditLogConfig)
benchi-kafka     | [2025-03-18 16:08:10,736] INFO CrnAuthorityConfig values: 
benchi-kafka     | 	confluent.authorizer.authority.cache.entries = 10000
benchi-kafka     | 	confluent.authorizer.authority.name = 
benchi-kafka     | 	confluent.metadata.server.api.flavor = CP
benchi-kafka     |  (io.confluent.crn.CrnAuthorityConfig)
benchi-kafka     | [2025-03-18 16:08:10,736] INFO MultiTenantAuditLogConfig values: 
benchi-kafka     | 	confluent.security.event.logger.client.ip.enable = false
benchi-kafka     | 	confluent.security.event.logger.multitenant.enable = false
benchi-kafka     |  (io.confluent.kafka.multitenant.authorizer.MultiTenantAuditLogConfig)
benchi-kafka     | [2025-03-18 16:08:10,736] INFO AuditLogConfig values: 
benchi-kafka     | 	confluent.security.event.logger.authentication.enable = false
benchi-kafka     | 	confluent.security.event.logger.authentication.event.rate.limit = -1
benchi-kafka     | 	confluent.security.event.logger.authorization.event.rate.limit = -1
benchi-kafka     | 	confluent.security.event.logger.cloudevent.codec = structured
benchi-kafka     | 	confluent.security.event.logger.enable = true
benchi-kafka     | 	confluent.security.event.logger.exporter.class = class io.confluent.security.audit.telemetry.exporter.NonBlockingKafkaExporter
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.create = true
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.partitions = 12
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.replicas = 3
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.retention.bytes = -1
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.retention.ms = 2592000000
benchi-kafka     | 	confluent.security.event.logger.exporter.kafka.topic.roll.ms = 14400000
benchi-kafka     | 	confluent.security.event.logger.kafka.request.rate.limit = -1
benchi-kafka     | 	confluent.security.event.logger.physical.cluster.id = 
benchi-kafka     | 	confluent.security.event.router.cache.entries = 10000
benchi-kafka     | 	confluent.security.event.router.config = 
benchi-kafka     | 	confluent.security.event.router.named.config.enabled = false
benchi-kafka     |  (io.confluent.security.audit.AuditLogConfig)
benchi-kafka     | [2025-03-18 16:08:10,736] INFO Audit Log rate limiter reconfigured: Authn: -1, Authz: -1, Kafka request: -1 (io.confluent.security.audit.provider.AuditLogRateLimiter)
benchi-kafka     | [2025-03-18 16:08:10,738] INFO [BrokerServer id=1] Waiting for controller quorum voters future (kafka.server.BrokerServer)
benchi-kafka     | [2025-03-18 16:08:10,738] INFO [BrokerServer id=1] Finished waiting for controller quorum voters future (kafka.server.BrokerServer)
benchi-kafka     | [2025-03-18 16:08:10,742] INFO [broker-1-to-controller-forwarding-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
benchi-kafka     | [2025-03-18 16:08:10,742] INFO [broker-1-to-controller-forwarding-channel-manager]: Recorded new KRaft controller, from now on will use node broker:29093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
benchi-kafka     | [2025-03-18 16:08:10,743] INFO [client-metrics-reaper]: Starting (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
benchi-kafka     | [2025-03-18 16:08:10,747] INFO [ControllerServer id=1] Node 1 identified 0 potential metadata log encryptor rotation candidates: [] (org.apache.kafka.controller.ClusterControlManager)
benchi-kafka     | [2025-03-18 16:08:10,747] INFO [ControllerServer id=1] Potential metadata log encryptor rotation candidates that are existing in all controllers: [] (org.apache.kafka.controller.ClusterControlManager)
benchi-kafka     | [2025-03-18 16:08:10,748] INFO [ControllerServer id=1] Potential metadata log encryptor rotation candidates that are existing in all brokers: [] (org.apache.kafka.controller.ClusterControlManager)
benchi-kafka     | [2025-03-18 16:08:10,749] INFO [ControllerServer id=1] Replayed RegisterControllerRecord contaning ControllerRegistration(id=1, incarnationId=7_mCSDvPTJS1-SXDmw9VyQ, zkMigrationReady=false, listeners=[Endpoint(listenerName='CONTROLLER', securityProtocol=PLAINTEXT, host='broker', port=29093)], supportedFeatures={confluent.metadata.version: 1-120, metadata.version: 1-20}, metadataEncryptors=[]). (org.apache.kafka.controller.ClusterControlManager)
benchi-kafka     | [2025-03-18 16:08:10,766] INFO [ExpirationReaper-1-ClusterLink]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:08:10,776] INFO [ControllerRegistrationManager id=1 incarnation=7_mCSDvPTJS1-SXDmw9VyQ] Our registration has been persisted to the metadata log. (kafka.server.ControllerRegistrationManager)
benchi-kafka     | [2025-03-18 16:08:10,776] INFO [ControllerRegistrationManager id=1 incarnation=7_mCSDvPTJS1-SXDmw9VyQ] RegistrationResponseHandler: controller acknowledged ControllerRegistrationRequest. (kafka.server.ControllerRegistrationManager)
benchi-kafka     | [2025-03-18 16:08:10,778] INFO [SocketServer listenerType=BROKER, nodeId=1] Creating data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)
benchi-kafka     | [2025-03-18 16:08:10,779] INFO Quota PLAINTEXT-per-ip-connection-rate configured - (max: -1.0, floor: -1.0, adjustment: -1.0) (kafka.network.ListenerIpAutoTuningQuota)
benchi-kafka     | [2025-03-18 16:08:10,779] INFO Quota PLAINTEXT-per-tenant-connection-rate configured - (max: -1.0, floor: -1.0, adjustment: -1.0) (kafka.network.ListenerTenantAutoTuningQuota)
benchi-kafka     | [2025-03-18 16:08:10,779] INFO Quota PLAINTEXT-connection-rate configured - (max: 1.7976931348623157E308, floor: 1.7976931348623157E308, adjustment: 5.0) (kafka.network.ListenerAutoTuningQuota)
benchi-kafka     | [2025-03-18 16:08:10,779] INFO Updated connection-tokens max connection creation rate to 1.7976931348623157E308 (kafka.network.ConnectionQuotas)
benchi-kafka     | [2025-03-18 16:08:10,779] INFO Config values: 
benchi-kafka     | 	confluent.security.event.logger.detailed.audit.logs.disabled.apis = 
benchi-kafka     | 	confluent.security.event.logger.enable.detailed.audit.logs = false
benchi-kafka     | 	confluent.security.event.logger.enable.produce.consume.audit.logs = false
benchi-kafka     |  (org.apache.kafka.common.requests.DetailedRequestAuditLogFilter$Config)
benchi-kafka     | [2025-03-18 16:08:10,781] INFO Config values: 
benchi-kafka     | 	confluent.security.event.logger.detailed.audit.logs.disabled.apis = 
benchi-kafka     | 	confluent.security.event.logger.enable.detailed.audit.logs = false
benchi-kafka     | 	confluent.security.event.logger.enable.produce.consume.audit.logs = false
benchi-kafka     |  (org.apache.kafka.common.requests.DetailedRequestAuditLogFilter$Config)
benchi-kafka     | [2025-03-18 16:08:10,782] INFO Config values: 
benchi-kafka     | 	confluent.security.event.logger.detailed.audit.logs.disabled.apis = 
benchi-kafka     | 	confluent.security.event.logger.enable.detailed.audit.logs = false
benchi-kafka     | 	confluent.security.event.logger.enable.produce.consume.audit.logs = false
benchi-kafka     |  (org.apache.kafka.common.requests.DetailedRequestAuditLogFilter$Config)
benchi-kafka     | [2025-03-18 16:08:10,782] INFO [SocketServer listenerType=BROKER, nodeId=1] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)
benchi-kafka     | [2025-03-18 16:08:10,782] INFO [SocketServer listenerType=BROKER, nodeId=1] Creating data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT_HOST) (kafka.network.SocketServer)
benchi-kafka     | [2025-03-18 16:08:10,782] INFO Quota PLAINTEXT_HOST-per-ip-connection-rate configured - (max: -1.0, floor: -1.0, adjustment: -1.0) (kafka.network.ListenerIpAutoTuningQuota)
benchi-kafka     | [2025-03-18 16:08:10,782] INFO Quota PLAINTEXT_HOST-per-tenant-connection-rate configured - (max: -1.0, floor: -1.0, adjustment: -1.0) (kafka.network.ListenerTenantAutoTuningQuota)
benchi-kafka     | [2025-03-18 16:08:10,783] INFO Quota PLAINTEXT_HOST-connection-rate configured - (max: 1.7976931348623157E308, floor: 1.7976931348623157E308, adjustment: 5.0) (kafka.network.ListenerAutoTuningQuota)
benchi-kafka     | [2025-03-18 16:08:10,783] INFO Updated connection-tokens max connection creation rate to 1.7976931348623157E308 (kafka.network.ConnectionQuotas)
benchi-kafka     | [2025-03-18 16:08:10,783] INFO Config values: 
benchi-kafka     | 	confluent.security.event.logger.detailed.audit.logs.disabled.apis = 
benchi-kafka     | 	confluent.security.event.logger.enable.detailed.audit.logs = false
benchi-kafka     | 	confluent.security.event.logger.enable.produce.consume.audit.logs = false
benchi-kafka     |  (org.apache.kafka.common.requests.DetailedRequestAuditLogFilter$Config)
benchi-kafka     | [2025-03-18 16:08:10,784] INFO Config values: 
benchi-kafka     | 	confluent.security.event.logger.detailed.audit.logs.disabled.apis = 
benchi-kafka     | 	confluent.security.event.logger.enable.detailed.audit.logs = false
benchi-kafka     | 	confluent.security.event.logger.enable.produce.consume.audit.logs = false
benchi-kafka     |  (org.apache.kafka.common.requests.DetailedRequestAuditLogFilter$Config)
benchi-kafka     | [2025-03-18 16:08:10,785] INFO Config values: 
benchi-kafka     | 	confluent.security.event.logger.detailed.audit.logs.disabled.apis = 
benchi-kafka     | 	confluent.security.event.logger.enable.detailed.audit.logs = false
benchi-kafka     | 	confluent.security.event.logger.enable.produce.consume.audit.logs = false
benchi-kafka     |  (org.apache.kafka.common.requests.DetailedRequestAuditLogFilter$Config)
benchi-kafka     | [2025-03-18 16:08:10,786] INFO [SocketServer listenerType=BROKER, nodeId=1] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT_HOST) (kafka.network.SocketServer)
benchi-kafka     | [2025-03-18 16:08:10,788] INFO [broker-1-to-controller-alter-partition-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
benchi-kafka     | [2025-03-18 16:08:10,788] INFO [broker-1-to-controller-alter-partition-channel-manager]: Recorded new KRaft controller, from now on will use node broker:29093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
benchi-kafka     | [2025-03-18 16:08:10,791] INFO [broker-1-to-controller-directory-assignments-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
benchi-kafka     | [2025-03-18 16:08:10,791] INFO [broker-1-to-controller-directory-assignments-channel-manager]: Recorded new KRaft controller, from now on will use node broker:29093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
benchi-kafka     | [2025-03-18 16:08:10,799] INFO [ExpirationReaper-1-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:08:10,800] INFO [ExpirationReaper-1-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:08:10,800] INFO [ExpirationReaper-1-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:08:10,801] INFO [ExpirationReaper-1-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:08:10,801] INFO [ExpirationReaper-1-ListOffsets]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:08:10,806] INFO ReplicationConfig values: 
benchi-kafka     | 	confluent.replication.linger.ms = 0
benchi-kafka     | 	confluent.replication.max.in.flight.requests = 1
benchi-kafka     | 	confluent.replication.max.memory.buffer.bytes = 209715200
benchi-kafka     | 	confluent.replication.max.replica.pushers = 4
benchi-kafka     | 	confluent.replication.max.wait.ms = 500
benchi-kafka     | 	confluent.replication.mode = PULL
benchi-kafka     | 	confluent.replication.num.pushers.per.broker = 1
benchi-kafka     | 	confluent.replication.push.internal.topics.enable = false
benchi-kafka     | 	confluent.replication.request.max.bytes = 52428800
benchi-kafka     | 	confluent.replication.request.max.partition.bytes = 52428800
benchi-kafka     | 	confluent.replication.request.timeout.ms = 5000
benchi-kafka     | 	confluent.replication.retry.timeout.ms = 10000
benchi-kafka     | 	confluent.replication.socket.send.buffer.bytes = 1048576
benchi-kafka     |  (io.confluent.kafka.replication.push.ReplicationConfig)
benchi-kafka     | [2025-03-18 16:08:10,811] INFO [BrokerHealthManager]: Starting (kafka.availability.BrokerHealthManager)
benchi-kafka     | [2025-03-18 16:08:10,813] INFO [ExpirationReaper-1-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:08:10,813] INFO [ExpirationReaper-1-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:08:10,829] INFO Unable to read the broker epoch in /tmp/kraft-combined-logs. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:08:10,830] INFO [broker-1-to-controller-heartbeat-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
benchi-kafka     | [2025-03-18 16:08:10,830] INFO [broker-1-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node broker:29093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
benchi-kafka     | [2025-03-18 16:08:10,830] INFO [SharedServer id=1] Using broker:29092 as bootstrap.servers for inter broker client config. (kafka.server.SharedServer)
benchi-kafka     | [2025-03-18 16:08:10,830] INFO [SharedServer id=1] Using broker:29092 as bootstrap.servers for inter broker client config. (kafka.server.SharedServer)
benchi-kafka     | [2025-03-18 16:08:10,831] INFO [BrokerLifecycleManager id=1] Incarnation UGktM7t7TSeLoK58NcaSAw of broker 1 in cluster MkU3OEVBNTcwNTJENDM2Qk is now STARTING. (kafka.server.BrokerLifecycleManager)
benchi-kafka     | [2025-03-18 16:08:10,835] INFO [ControllerServer id=1] No previous registration found for broker 1. New incarnation ID is UGktM7t7TSeLoK58NcaSAw.  Generated 0 record(s) to clean up previous incarnations. New broker epoch is 6. (org.apache.kafka.controller.ClusterControlManager)
benchi-kafka     | [2025-03-18 16:08:10,835] INFO [ControllerServer id=1] Node 1 identified 0 potential metadata log encryptor rotation candidates: [] (org.apache.kafka.controller.ClusterControlManager)
benchi-kafka     | [2025-03-18 16:08:10,835] INFO [ControllerServer id=1] Potential metadata log encryptor rotation candidates that are existing in all controllers: [] (org.apache.kafka.controller.ClusterControlManager)
benchi-kafka     | [2025-03-18 16:08:10,835] INFO [ControllerServer id=1] Potential metadata log encryptor rotation candidates that are existing in all brokers: [] (org.apache.kafka.controller.ClusterControlManager)
benchi-kafka     | [2025-03-18 16:08:10,836] INFO [ControllerServer id=1] Replayed initial RegisterBrokerRecord for broker 1: RegisterBrokerRecord(brokerId=1, isMigratingZkBroker=false, incarnationId=UGktM7t7TSeLoK58NcaSAw, brokerEpoch=6, endPoints=[BrokerEndpoint(name='PLAINTEXT', host='broker', port=29092, securityProtocol=0), BrokerEndpoint(name='PLAINTEXT_HOST', host='localhost', port=9092, securityProtocol=0)], features=[BrokerFeature(name='confluent.metadata.version', minSupportedVersion=1, maxSupportedVersion=120), BrokerFeature(name='metadata.version', minSupportedVersion=1, maxSupportedVersion=20)], rack=null, fenced=true, inControlledShutdown=false, degradedComponents=[], metadataEncryptors=[], logDirs=[Y-adVyLUb4IllEQbE2t_Kg]) (org.apache.kafka.controller.ClusterControlManager)
benchi-kafka     | [2025-03-18 16:08:10,845] INFO [ExpirationReaper-1-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:08:10,847] INFO Config values: 
benchi-kafka     | 	confluent.request.log.api.samples.per.min = 
benchi-kafka     | 	confluent.request.log.enable.admin.apis = true
benchi-kafka     | 	confluent.request.log.enable.slowlog = false
benchi-kafka     | 	confluent.request.log.samples.per.min = 0
benchi-kafka     | 	confluent.request.slowlog.threshold.override = -1.0
benchi-kafka     | 	confluent.request.slowlog.threshold.p99.min = -1.0
benchi-kafka     |  (org.apache.kafka.common.requests.SamplingRequestLogFilter$Config)
benchi-kafka     | [2025-03-18 16:08:10,847] INFO Config values: 
benchi-kafka     | 	confluent.request.log.api.samples.per.min = 
benchi-kafka     | 	confluent.request.log.enable.admin.apis = true
benchi-kafka     | 	confluent.request.log.enable.slowlog = false
benchi-kafka     | 	confluent.request.log.samples.per.min = 0
benchi-kafka     | 	confluent.request.slowlog.threshold.override = -1.0
benchi-kafka     | 	confluent.request.slowlog.threshold.p99.min = -1.0
benchi-kafka     |  (org.apache.kafka.common.requests.SamplingRequestLogFilter$Config)
benchi-kafka     | [2025-03-18 16:08:10,847] INFO Config values: 
benchi-kafka     | 	confluent.request.log.api.samples.per.min = 
benchi-kafka     | 	confluent.request.log.enable.admin.apis = true
benchi-kafka     | 	confluent.request.log.enable.slowlog = false
benchi-kafka     | 	confluent.request.log.samples.per.min = 0
benchi-kafka     | 	confluent.request.slowlog.threshold.override = -1.0
benchi-kafka     | 	confluent.request.slowlog.threshold.p99.min = -1.0
benchi-kafka     |  (org.apache.kafka.common.requests.SamplingRequestLogFilter$Config)
benchi-kafka     | [2025-03-18 16:08:10,847] INFO Config values: 
benchi-kafka     | 	confluent.request.log.api.samples.per.min = 
benchi-kafka     | 	confluent.request.log.enable.admin.apis = true
benchi-kafka     | 	confluent.request.log.enable.slowlog = false
benchi-kafka     | 	confluent.request.log.samples.per.min = 0
benchi-kafka     | 	confluent.request.slowlog.threshold.override = -1.0
benchi-kafka     | 	confluent.request.slowlog.threshold.p99.min = -1.0
benchi-kafka     |  (org.apache.kafka.common.requests.SamplingRequestLogFilter$Config)
benchi-kafka     | [2025-03-18 16:08:10,848] INFO Config values: 
benchi-kafka     | 	confluent.request.log.api.samples.per.min = 
benchi-kafka     | 	confluent.request.log.enable.admin.apis = true
benchi-kafka     | 	confluent.request.log.enable.slowlog = false
benchi-kafka     | 	confluent.request.log.samples.per.min = 0
benchi-kafka     | 	confluent.request.slowlog.threshold.override = -1.0
benchi-kafka     | 	confluent.request.slowlog.threshold.p99.min = -1.0
benchi-kafka     |  (org.apache.kafka.common.requests.SamplingRequestLogFilter$Config)
benchi-kafka     | [2025-03-18 16:08:10,848] INFO Config values: 
benchi-kafka     | 	confluent.request.log.api.samples.per.min = 
benchi-kafka     | 	confluent.request.log.enable.admin.apis = true
benchi-kafka     | 	confluent.request.log.enable.slowlog = false
benchi-kafka     | 	confluent.request.log.samples.per.min = 0
benchi-kafka     | 	confluent.request.slowlog.threshold.override = -1.0
benchi-kafka     | 	confluent.request.slowlog.threshold.p99.min = -1.0
benchi-kafka     |  (org.apache.kafka.common.requests.SamplingRequestLogFilter$Config)
benchi-kafka     | [2025-03-18 16:08:10,848] INFO Config values: 
benchi-kafka     | 	confluent.request.log.api.samples.per.min = 
benchi-kafka     | 	confluent.request.log.enable.admin.apis = true
benchi-kafka     | 	confluent.request.log.enable.slowlog = false
benchi-kafka     | 	confluent.request.log.samples.per.min = 0
benchi-kafka     | 	confluent.request.slowlog.threshold.override = -1.0
benchi-kafka     | 	confluent.request.slowlog.threshold.p99.min = -1.0
benchi-kafka     |  (org.apache.kafka.common.requests.SamplingRequestLogFilter$Config)
benchi-kafka     | [2025-03-18 16:08:10,848] INFO Config values: 
benchi-kafka     | 	confluent.request.log.api.samples.per.min = 
benchi-kafka     | 	confluent.request.log.enable.admin.apis = true
benchi-kafka     | 	confluent.request.log.enable.slowlog = false
benchi-kafka     | 	confluent.request.log.samples.per.min = 0
benchi-kafka     | 	confluent.request.slowlog.threshold.override = -1.0
benchi-kafka     | 	confluent.request.slowlog.threshold.p99.min = -1.0
benchi-kafka     |  (org.apache.kafka.common.requests.SamplingRequestLogFilter$Config)
benchi-kafka     | [2025-03-18 16:08:10,849] INFO [BrokerServer id=1] Waiting for broker metadata to catch up (kafka.server.BrokerServer)
benchi-kafka     | [2025-03-18 16:08:10,862] INFO Handling event SbcConfigUpdateEvent-3 (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:08:10,863] INFO Cluster metadata containing at least one unfenced broker not yet available, SBC config processing delayed. (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:08:10,863] INFO Handling event SbcKraftStartupEvent-5 (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:08:10,863] INFO Cluster metadata containing at least one unfenced broker not yet available, SBC startup delayed. (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:08:10,863] INFO [BrokerLifecycleManager id=1] Successfully registered broker 1 with broker epoch 6 (kafka.server.BrokerLifecycleManager)
benchi-kafka     | [2025-03-18 16:08:10,865] INFO [BrokerLifecycleManager id=1] The broker has caught up. Transitioning from STARTING to RECOVERY. (kafka.server.BrokerLifecycleManager)
benchi-kafka     | [2025-03-18 16:08:10,866] INFO [BrokerServer id=1] Finished waiting for broker metadata to catch up (kafka.server.BrokerServer)
benchi-kafka     | [2025-03-18 16:08:10,879] INFO [BrokerLifecycleManager id=1] The broker is in RECOVERY. (kafka.server.BrokerLifecycleManager)
benchi-kafka     | [2025-03-18 16:08:10,890] INFO ConfluentMetricsReporterConfig values: 
benchi-kafka     | 	confluent.metrics.reporter.bootstrap.servers = localhost:9092
benchi-kafka     | 	confluent.metrics.reporter.include = .*MaxLag.*|kafka.log:type=Log,name=Size.*|.*name=(ActiveControllerCount|BytesInPerSec|BytesOutPerSec|CaughtUpReplicasCount|FailedFetchRequestsPerSec|FailedProduceRequestsPerSec|InSyncReplicasCount|LeaderCount|LeaderElectionRateAndTimeMs|LocalTimeMs|LogEndOffset|LogStartOffset|NetworkProcessorAvgIdlePercent|NumLogSegments|NumPartitionsInError|OfflinePartitionsCount|ObserverReplicasCount|PartitionCount|RemoteTimeMs|ReplicasCount|RequestHandlerAvgIdlePercent|RequestQueueSize|RequestQueueTimeMs|RequestsPerSec|ResponseQueueSize|ResponseQueueTimeMs|ResponseSendTimeMs|Size|TierSize|TotalFetchRequestsPerSec|TotalLag|TotalProduceRequestsPerSec|TotalSize|TotalTimeMs|UncleanLeaderElectionsPerSec|UnderReplicated|UnderReplicatedPartitions|UnderMinIsrPartitionCount|ZooKeeperDisconnectsPerSec|ZooKeeperExpiresPerSec|TieredPartitionsUndergoingUncleanLeaderRecoveryCount|NonTieredPartitionsUndergoingUncleanLeaderRecoveryCount|TierTopicPartitionsUndergoingUncleanLeaderRecoveryCount|ContiguousUnhealthySamples|ContiguousSamplesEngineThreadGroupsStuck|ContiguousSamplesStorageThreadGroupsStuck|ContiguousSamplesNoStorageThreadMakingProgress).*|.*(BytesFetchedRate).*
benchi-kafka     | 	confluent.metrics.reporter.publish.ms = 15000
benchi-kafka     | 	confluent.metrics.reporter.topic = _confluent-metrics
benchi-kafka     | 	confluent.metrics.reporter.topic.create = true
benchi-kafka     | 	confluent.metrics.reporter.topic.max.message.bytes = 10485760
benchi-kafka     | 	confluent.metrics.reporter.topic.partitions = 12
benchi-kafka     | 	confluent.metrics.reporter.topic.replicas = 3
benchi-kafka     | 	confluent.metrics.reporter.topic.retention.bytes = -1
benchi-kafka     | 	confluent.metrics.reporter.topic.retention.ms = 259200000
benchi-kafka     | 	confluent.metrics.reporter.topic.roll.ms = 14400000
benchi-kafka     | 	confluent.metrics.reporter.volume.metrics.refresh.ms = 15000
benchi-kafka     | 	confluent.metrics.reporter.whitelist = null
benchi-kafka     |  (io.confluent.metrics.reporter.ConfluentMetricsReporterConfig)
benchi-kafka     | [2025-03-18 16:08:10,904] INFO ProducerConfig values: 
benchi-kafka     | 	acks = -1
benchi-kafka     | 	auto.include.jmx.reporter = true
benchi-kafka     | 	batch.size = 16384
benchi-kafka     | 	bootstrap.servers = [localhost:9092]
benchi-kafka     | 	buffer.memory = 33554432
benchi-kafka     | 	client.dns.lookup = use_all_dns_ips
benchi-kafka     | 	client.id = confluent-metrics-reporter
benchi-kafka     | 	compression.gzip.level = -1
benchi-kafka     | 	compression.lz4.level = 9
benchi-kafka     | 	compression.type = zstd
benchi-kafka     | 	compression.zstd.level = 3
benchi-kafka     | 	confluent.lkc.id = null
benchi-kafka     | 	confluent.proxy.protocol.client.address = null
benchi-kafka     | 	confluent.proxy.protocol.client.mode = PROXY
benchi-kafka     | 	confluent.proxy.protocol.client.port = null
benchi-kafka     | 	confluent.proxy.protocol.client.version = NONE
benchi-kafka     | 	connections.max.idle.ms = 540000
benchi-kafka     | 	delivery.timeout.ms = 120000
benchi-kafka     | 	enable.idempotence = false
benchi-kafka     | 	enable.metrics.push = true
benchi-kafka     | 	interceptor.classes = []
benchi-kafka     | 	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
benchi-kafka     | 	linger.ms = 500
benchi-kafka     | 	max.block.ms = 60000
benchi-kafka     | 	max.in.flight.requests.per.connection = 1
benchi-kafka     | 	max.request.size = 10485760
benchi-kafka     | 	metadata.max.age.ms = 300000
benchi-kafka     | 	metadata.max.idle.ms = 300000
benchi-kafka     | 	metadata.recovery.strategy = none
benchi-kafka     | 	metric.reporters = []
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.recording.level = INFO
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	partitioner.adaptive.partitioning.enable = true
benchi-kafka     | 	partitioner.availability.timeout.ms = 0
benchi-kafka     | 	partitioner.class = null
benchi-kafka     | 	partitioner.ignore.keys = false
benchi-kafka     | 	receive.buffer.bytes = 32768
benchi-kafka     | 	reconnect.backoff.max.ms = 1000
benchi-kafka     | 	reconnect.backoff.ms = 50
benchi-kafka     | 	request.timeout.ms = 30000
benchi-kafka     | 	retries = 2147483647
benchi-kafka     | 	retry.backoff.max.ms = 1000
benchi-kafka     | 	retry.backoff.ms = 500
benchi-kafka     | 	sasl.client.callback.handler.class = null
benchi-kafka     | 	sasl.jaas.config = null
benchi-kafka     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
benchi-kafka     | 	sasl.kerberos.min.time.before.relogin = 60000
benchi-kafka     | 	sasl.kerberos.service.name = null
benchi-kafka     | 	sasl.kerberos.ticket.renew.jitter = 0.05
benchi-kafka     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
benchi-kafka     | 	sasl.login.callback.handler.class = null
benchi-kafka     | 	sasl.login.class = null
benchi-kafka     | 	sasl.login.connect.timeout.ms = null
benchi-kafka     | 	sasl.login.read.timeout.ms = null
benchi-kafka     | 	sasl.login.refresh.buffer.seconds = 300
benchi-kafka     | 	sasl.login.refresh.min.period.seconds = 60
benchi-kafka     | 	sasl.login.refresh.window.factor = 0.8
benchi-kafka     | 	sasl.login.refresh.window.jitter = 0.05
benchi-kafka     | 	sasl.login.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.login.retry.backoff.ms = 100
benchi-kafka     | 	sasl.mechanism = GSSAPI
benchi-kafka     | 	sasl.oauthbearer.clientassertion.audience = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.expiration = 5
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.jti.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.nbf.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.issuer = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.location = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key.passphrase = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.subject = null
benchi-kafka     | 	sasl.oauthbearer.clock.skew.seconds = 30
benchi-kafka     | 	sasl.oauthbearer.expected.audience = null
benchi-kafka     | 	sasl.oauthbearer.expected.issuer = null
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.url = null
benchi-kafka     | 	sasl.oauthbearer.scope.claim.name = scope
benchi-kafka     | 	sasl.oauthbearer.sub.claim.name = sub
benchi-kafka     | 	sasl.oauthbearer.token.endpoint.url = null
benchi-kafka     | 	security.protocol = PLAINTEXT
benchi-kafka     | 	security.providers = null
benchi-kafka     | 	send.buffer.bytes = 131072
benchi-kafka     | 	socket.connection.setup.timeout.max.ms = 30000
benchi-kafka     | 	socket.connection.setup.timeout.ms = 10000
benchi-kafka     | 	ssl.cipher.suites = null
benchi-kafka     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	ssl.engine.factory.class = null
benchi-kafka     | 	ssl.key.password = null
benchi-kafka     | 	ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	ssl.keystore.certificate.chain = null
benchi-kafka     | 	ssl.keystore.key = null
benchi-kafka     | 	ssl.keystore.location = null
benchi-kafka     | 	ssl.keystore.password = null
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.protocol = TLSv1.3
benchi-kafka     | 	ssl.provider = null
benchi-kafka     | 	ssl.secure.random.implementation = null
benchi-kafka     | 	ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	ssl.truststore.certificates = null
benchi-kafka     | 	ssl.truststore.location = null
benchi-kafka     | 	ssl.truststore.password = null
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     | 	transaction.timeout.ms = 60000
benchi-kafka     | 	transactional.id = null
benchi-kafka     | 	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
benchi-kafka     |  (org.apache.kafka.clients.producer.ProducerConfig)
benchi-kafka     | [2025-03-18 16:08:10,912] INFO initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector)
benchi-kafka     | [2025-03-18 16:08:10,921] INFO Kafka version: 7.8.1-ce (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:08:10,921] INFO Kafka commitId: eefb51c6f2f1b888b8b03296353338afbac8e213 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:08:10,921] INFO Kafka startTimeMs: 1742314090921 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:08:10,922] INFO [Producer clientId=confluent-metrics-reporter] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:08:10,922] WARN [Producer clientId=confluent-metrics-reporter] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:08:10,923] WARN [Producer clientId=confluent-metrics-reporter] Bootstrap broker localhost:9092 (id: -1 rack: null) disconnected (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:08:10,924] INFO EventEmitterConfig values: 
benchi-kafka     |  (io.confluent.shaded.io.confluent.telemetry.events.EventEmitterConfig)
benchi-kafka     | [2025-03-18 16:08:10,931] INFO Linux CPU collector enabled: true (io.confluent.telemetry.ConfluentTelemetryConfig)
benchi-kafka     | [2025-03-18 16:08:10,931] INFO Using cpu metric: io\.confluent\.kafka\.server/server/linux_system_cpu_utilization_1m (io.confluent.telemetry.ConfluentTelemetryConfig)
benchi-kafka     | [2025-03-18 16:08:10,937] INFO Configuring named client _confluentClient for exporter _confluent (io.confluent.telemetry.exporter.http.HttpExporterConfig)
benchi-kafka     | [2025-03-18 16:08:10,937] WARN no telemetry exporters are enabled (io.confluent.telemetry.ConfluentTelemetryConfig)
benchi-kafka     | [2025-03-18 16:08:10,944] WARN Ignoring redefinition of existing telemetry label kafka.version (io.confluent.shaded.io.confluent.telemetry.ResourceBuilderFacade)
benchi-kafka     | [2025-03-18 16:08:10,949] INFO ConfluentTelemetryConfig values: 
benchi-kafka     | 	confluent.telemetry.api.key = null
benchi-kafka     | 	confluent.telemetry.api.secret = null
benchi-kafka     | 	confluent.telemetry.cluster.id = null
benchi-kafka     | 	confluent.telemetry.debug.enabled = false
benchi-kafka     | 	confluent.telemetry.enabled = false
benchi-kafka     | 	confluent.telemetry.events.collector.include = .*transaction.remove.expired.transaction.cleanup.interval.ms.*|.*transaction.state.log.load.buffer.size.*|.*transaction.state.log.min.isr.*|.*transaction.state.log.num.partitions.*|.*transaction.state.log.replication.factor.*|.*transaction.state.log.segment.bytes.*|.*transactional.id.expiration.ms.*|.*controlled.shutdown.enable.*|.*controlled.shutdown.max.retries.*|.*controlled.shutdown.retry.backoff.ms.*|.*fetch.max.bytes.*|.*fetch.purgatory.purge.interval.requests.*|.*group.initial.rebalance.delay.ms.*|.*group.max.session.timeout.ms.*|.*group.max.size.*|.*group.min.session.timeout.ms.*|.*log.cleaner.backoff.ms.*|.*log.cleaner.dedupe.buffer.size.*|.*log.cleaner.delete.retention.ms.*|.*log.cleaner.enable.*|.*log.cleaner.io.buffer.load.factor.*|.*log.cleaner.io.buffer.size.*|.*log.cleaner.io.max.bytes.per.second.*|.*log.cleaner.max.compaction.lag.ms.*|.*log.cleaner.min.cleanable.ratio.*|.*log.cleaner.min.compaction.lag.ms.*|.*log.cleaner.threads.*|.*log.cleanup.policy.*|.*log.index.interval.bytes.*|.*log.index.size.max.bytes.*|.*log.message.downconversion.enable.*|.*log.message.format.version.*|.*log.message.timestamp.difference.max.ms.*|.*log.message.timestamp.type.*|.*max.connection.creation.rate.*|.*max.connections.*|.*max.connections.per.ip.*|.*max.incremental.fetch.session.cache.slots.*|.*replica.fetch.backoff.ms.*|.*replica.fetch.max.bytes.*|.*replica.fetch.min.bytes.*|.*replica.fetch.response.max.bytes.*|.*replica.fetch.wait.max.ms.*|.*replica.high.watermark.checkpoint.interval.ms.*|.*replica.lag.time.max.ms.*|.*replica.selector.class.*|.*replica.socket.receive.buffer.bytes.*|.*replica.socket.timeout.ms.*|.*alter.config.policy.class.name.*|.*alter.log.dirs.replication.quota.window.num.*|.*alter.log.dirs.replication.quota.window.size.seconds.*|.*metrics.num.samples.*|.*metrics.recording.level.*|.*metrics.sample.window.ms.*|.*quota.window.num.*|.*quota.window.size.seconds.*|.*replication.quota.window.num.*|.*replication.quota.window.size.seconds.*|.*confluent.balancer.enable.*|.*confluent.balancer.throttle.bytes.per.second.*|.*confluent.balancer.heal.uneven.load.trigger.*|.*confluent.balancer.heal.broker.failure.threshold.ms.*|.*confluent.balancer.disk.max.load.*|.*confluent.balancer.exclude.topic.prefixes.*|.*confluent.balancer.exclude.topic.names.*|.*confluent.tier.local.hotset.bytes.*|.*confluent.tier.local.hotset.ms.*|.*confluent.tier.archiver.num.threads.*|.*confluent.tier.backend.*|.*confluent.tier.enable.*|.*confluent.tier.feature.*|.*confluent.tier.fetcher.num.threads.*|.*confluent.tier.max.partition.fetch.bytes.override.*|.*confluent.tier.metadata.replication.factor.*|.*confluent.operator.managed.*|.*confluent.ansible.managed.*|.*confluent.license.*
benchi-kafka     | 	confluent.telemetry.events.enable = true
benchi-kafka     | 	confluent.telemetry.metrics.collector.include = .*io.confluent.telemetry/.*.*|.*io.confluent.system/.*(process_cpu_load|max_file_descriptor_count|open_file_descriptor_count|system_cpu_load|system_load_average|free_physical_memory_size|total_physical_memory_size|disk_total_bytes|disk_usable_bytes|jvm/mem|jvm/gc).*|.*io.confluent.kafka.server/.*(confluent_audit/audit_log_fallback_rate_per_minute|confluent_audit/audit_log_rate_per_minute|confluent_authorizer/authorization_request_rate_per_minute|confluent_authorizer/authorization_allowed_rate_per_minute|confluent_authorizer/authorization_denied_rate_per_minute|confluent_auth_store/rbac_role_bindings_count|confluent_auth_store/rbac_access_rules_count|confluent_auth_store/acl_access_rules_count).*|.*io.confluent.kafka.server/.*(acl_authorizer/zookeeper_disconnects/total/delta|acl_authorizer/zookeeper_expires/total/delta|broker_failure/zookeeper_disconnects/total/delta|broker_failure/zookeeper_expires/total/delta|broker_topic/bytes_in/total/delta|broker_topic/bytes_out/total/delta|broker_topic/failed_produce_requests/total/delta|broker_topic/failed_fetch_requests/total/delta|broker_topic/produce_message_conversions/total/delta|broker_topic/fetch_message_conversions/total/delta|cluster_link/active_link_count|cluster_link/consumer_offset_committed_rate|cluster_link/consumer_offset_committed_total|cluster_link/fetch_throttle_time_avg|cluster_link/fetch_throttle_time_max|cluster_link/link_count|cluster_link/linked_leader_epoch_change_rate|cluster_link/linked_leader_epoch_change_total|cluster_link/linked_topic_partition_addition_rate|cluster_link/linked_topic_partition_addition_total|cluster_link/mirror_partition_count|cluster_link/mirror_topic_byte_total|cluster_link/mirror_topic_count|cluster_link/mirror_topic_lag|cluster_link/topic_config_update_rate|cluster_link/topic_config_update_total|cluster_link_fetcher/connection_count|cluster_link_fetcher/failed_reauthentication_rate|cluster_link_fetcher/failed_reauthentication_total|cluster_link_fetcher/incoming_byte_rate|cluster_link_fetcher/incoming_byte_total|cluster_link_fetcher/outgoing_byte_rate|cluster_link_fetcher/outgoing_byte_total|cluster_link_fetcher/reauthentication_latency_avg|cluster_link_fetcher_manager/max_lag|controller/active_controller_count|controller/leader_election_rate_and_time_ms|controller/offline_partitions_count|controller/partition_availability|controller/preferred_replica_imbalance_count|controller/tenant_partition_availability|controller/global_under_min_isr_partition_count|controller/unclean_leader_elections/total|controller_channel/connection_close_rate|controller_channel/connection_close_total|controller_channel/connection_count|controller_channel/connection_creation_rate|controller_channel/connection_creation_total|controller_channel/request_size_avg|controller_channel/request_size_max|controller_channel_manager/queue_size|controller_channel_manager/total_queue_size|controller_event_manager/event_queue_size|delayed_operation_purgatory/purgatory_size|executor/zookeeper_disconnects/total/delta|executor/zookeeper_expires/total/delta|fetch/queue_size|fetcher/bytes_per_sec|fetcher_lag/consumer_lag|group_coordinator/partition_load_time_max|log_cleaner_manager/achieved_cleaning_ratio/time/delta|log_cleaner_manager/achieved_cleaning_ratio/total/delta|log_cleaner_manager/compacted_partition_bytes|log_cleaner_manager/max_dirty_percent|log_cleaner_manager/time_since_last_run_ms|log_cleaner_manager/uncleanable_bytes|log_cleaner_manager/uncleanable_partitions_count|replica_alter_log_dirs_manager/max_lag|replica_fetcher/request_size_avg|replica_fetcher/request_size_max|replica_fetcher_manager/max_lag|replica_manager/blocked_on_mirror_source_partition_count|replica_manager/isr_shrinks|replica_manager/leader_count|replica_manager/partition_count|replica_manager/under_min_isr_mirror_partition_count|replica_manager/under_min_isr_partition_count|replica_manager/under_replicated_mirror_partitions|replica_manager/under_replicated_partitions|request/errors/total/delta|request/local_time_ms/time/delta|request/local_time_ms/total/delta|request/queue_size|request/remote_time_ms/time/delta|request/remote_time_ms/total/delta|request/request_queue_time_ms/time/delta|request/request_queue_time_ms/total/delta|request/requests|request/response_queue_time_ms/time/delta|request/response_queue_time_ms/total/delta|request/response_send_time_ms/time/delta|request/response_send_time_ms/total/delta|request/total_time_ms/time/delta|request/total_time_ms/total/delta|request_channel/request_queue_size|request_channel/response_queue_size|request_handler_pool/request_handler_avg_idle_percent|session_expire_listener/zookeeper_disconnects/total/delta|session_expire_listener/zookeeper_expires/total/delta|socket_server/connections|socket_server/successful_authentication_total/delta|socket_server/failed_authentication_total/delta|socket_server/network_processor_avg_idle_percent|socket_server/request_size_avg|socket_server/request_size_max|tenant/consumer_lag_offsets).*|.*org\.apache\.kafka\.(producer\.connection\.creation\.rate|producer\.node\.request\.latency\.avg|producer\.node\.request\.latency\.max|producer\.produce\.throttle\.time\.avg|producer\.produce\.throttle\.time\.max|producer\.record\.queue\.time\.avg|producer\.record\.queue\.time\.max|producer\.connection\.creation\.total|consumer\.connection\.creation\.rate|consumer\.connection\.creation\.total|consumer\.node\.request\.latency\.avg|consumer\.node\.request\.latency\.max|consumer\.poll\.idle\.ratio\.avg|consumer\.coordinator\.commit\.latency\.avg|consumer\.coordinator\.commit\.latency\.max|consumer\.coordinator\.assigned\.partitions|consumer\.coordinator\.rebalance\.latency\.avg|consumer\.coordinator\.rebalance\.latency\.max|consumer\.coordinator\.rebalance\.latency\.total|consumer\.fetch\.manager\.fetch\.latency\.avg|consumer\.fetch\.manager\.fetch\.latency\.max).*
benchi-kafka     | 	confluent.telemetry.metrics.collector.interval.ms = 60000
benchi-kafka     | 	confluent.telemetry.metrics.collector.slo.enabled = false
benchi-kafka     | 	confluent.telemetry.proxy.password = null
benchi-kafka     | 	confluent.telemetry.proxy.url = null
benchi-kafka     | 	confluent.telemetry.proxy.username = null
benchi-kafka     |  (io.confluent.telemetry.ConfluentTelemetryConfig)
benchi-kafka     | [2025-03-18 16:08:10,950] INFO VolumeMetricsCollectorConfig values: 
benchi-kafka     | 	confluent.telemetry.metrics.collector.volume.update.ms = 15000
benchi-kafka     |  (io.confluent.telemetry.collector.VolumeMetricsCollector$VolumeMetricsCollectorConfig)
benchi-kafka     | [2025-03-18 16:08:10,950] INFO HttpClientConfig values: 
benchi-kafka     | 	api.key = null
benchi-kafka     | 	api.secret = null
benchi-kafka     | 	buffer.batch.duration.max.ms = null
benchi-kafka     | 	buffer.batch.items.max = null
benchi-kafka     | 	buffer.inflight.submissions.max = null
benchi-kafka     | 	buffer.pending.batches.max = null
benchi-kafka     | 	client.attempts.max = null
benchi-kafka     | 	client.base.url = https://collector.telemetry.confluent.cloud
benchi-kafka     | 	client.compression = null
benchi-kafka     | 	client.connect.timeout.ms = null
benchi-kafka     | 	client.metrics.path.override = /v1/metrics
benchi-kafka     | 	client.request.timeout.ms = null
benchi-kafka     | 	client.retry.delay.seconds = null
benchi-kafka     | 	proxy.password = null
benchi-kafka     | 	proxy.url = null
benchi-kafka     | 	proxy.username = null
benchi-kafka     | 	type = httpTelemetryClient
benchi-kafka     |  (io.confluent.telemetry.exporter.http.HttpClientConfig)
benchi-kafka     | [2025-03-18 16:08:10,950] INFO HttpExporterConfig values: 
benchi-kafka     | 	api.key = null
benchi-kafka     | 	api.secret = null
benchi-kafka     | 	buffer.batch.duration.max.ms = null
benchi-kafka     | 	buffer.batch.items.max = null
benchi-kafka     | 	buffer.inflight.submissions.max = null
benchi-kafka     | 	buffer.pending.batches.max = null
benchi-kafka     | 	client = _confluentClient
benchi-kafka     | 	client.attempts.max = null
benchi-kafka     | 	client.base.url = null
benchi-kafka     | 	client.compression = null
benchi-kafka     | 	client.connect.timeout.ms = null
benchi-kafka     | 	client.metrics.path.override = /v1/metrics
benchi-kafka     | 	client.request.timeout.ms = null
benchi-kafka     | 	client.retry.delay.seconds = null
benchi-kafka     | 	enabled = false
benchi-kafka     | 	events.enabled = true
benchi-kafka     | 	metrics.enabled = true
benchi-kafka     | 	metrics.include = null
benchi-kafka     | 	proxy.password = null
benchi-kafka     | 	proxy.url = null
benchi-kafka     | 	proxy.username = null
benchi-kafka     | 	remote.configurable = false
benchi-kafka     | 	type = http
benchi-kafka     |  (io.confluent.telemetry.exporter.http.HttpExporterConfig)
benchi-kafka     | [2025-03-18 16:08:10,950] INFO Configuring named client _confluentClient for exporter _confluent (io.confluent.telemetry.exporter.http.HttpExporterConfig)
benchi-kafka     | [2025-03-18 16:08:10,952] INFO KafkaExporterConfig values: 
benchi-kafka     | 	client = 
benchi-kafka     | 	enabled = true
benchi-kafka     | 	events.enabled = true
benchi-kafka     | 	metrics.enabled = true
benchi-kafka     | 	metrics.include = (io\.confluent\.kafka\.server/broker_load/broker_load_percent|io\.confluent\.kafka\.server/broker_topic/bytes_in/rate/1_min|io\.confluent\.kafka\.server/broker_topic/bytes_out/rate/1_min|io\.confluent\.kafka\.server/broker_topic/fetch_from_follower_bytes_out/rate/1_min|io\.confluent\.kafka\.server/broker_topic/messages_in/rate/1_min|io\.confluent\.kafka\.server/broker_topic/replication_bytes_in/rate/1_min|io\.confluent\.kafka\.server/broker_topic/replication_bytes_out/rate/1_min|io\.confluent\.kafka\.server/broker_topic/total_fetch_requests/rate/1_min|io\.confluent\.kafka\.server/broker_topic/total_follower_fetch_requests/rate/1_min|io\.confluent\.kafka\.server/broker_topic/mirror_bytes_in/rate/1_min|io\.confluent\.kafka\.server/broker_topic/total_fetch_from_follower_requests/rate/1_min|io\.confluent\.kafka\.server/broker_topic/total_produce_requests/rate/1_min|io\.confluent\.kafka\.server/log/size|io\.confluent\.kafka\.server/request/requests/rate/1_min|io\.confluent\.kafka\.server/request_handler_pool/request_handler_avg_idle_percent/rate/1_min|io\.confluent\.kafka\.server/server/linux_system_cpu_utilization_1m|io\.confluent\.system/volume/disk_total_bytes)
benchi-kafka     | 	producer.bootstrap.servers = broker:29092
benchi-kafka     | 	remote.configurable = false
benchi-kafka     | 	topic.create = true
benchi-kafka     | 	topic.max.message.bytes = 10485760
benchi-kafka     | 	topic.name = _confluent-telemetry-metrics
benchi-kafka     | 	topic.partitions = 12
benchi-kafka     | 	topic.replicas = 1
benchi-kafka     | 	topic.retention.bytes = -1
benchi-kafka     | 	topic.retention.ms = 259200000
benchi-kafka     | 	topic.roll.ms = 14400000
benchi-kafka     | 	type = kafka
benchi-kafka     |  (io.confluent.telemetry.exporter.kafka.KafkaExporterConfig)
benchi-kafka     | [2025-03-18 16:08:10,952] INFO PollingRemoteConfigurationConfig values: 
benchi-kafka     | 	enabled = true
benchi-kafka     | 	refresh.interval.ms = 60000
benchi-kafka     |  (io.confluent.shaded.io.confluent.telemetry.config.remote.polling.PollingRemoteConfigurationConfig)
benchi-kafka     | [2025-03-18 16:08:10,952] INFO Initializing the event logger (io.confluent.telemetry.reporter.TelemetryReporter)
benchi-kafka     | [2025-03-18 16:08:10,953] INFO EventLoggerConfig values: 
benchi-kafka     | 	event.logger.cloudevent.codec = structured
benchi-kafka     | 	event.logger.exporter.class = class io.confluent.shaded.io.confluent.telemetry.events.exporter.http.EventHttpExporter
benchi-kafka     |  (io.confluent.shaded.io.confluent.telemetry.events.EventLoggerConfig)
benchi-kafka     | [2025-03-18 16:08:10,955] INFO HttpExporterConfig values: 
benchi-kafka     | 	api.key = null
benchi-kafka     | 	api.secret = null
benchi-kafka     | 	buffer.batch.duration.max.ms = null
benchi-kafka     | 	buffer.batch.items.max = null
benchi-kafka     | 	buffer.inflight.submissions.max = null
benchi-kafka     | 	buffer.pending.batches.max = null
benchi-kafka     | 	client.attempts.max = null
benchi-kafka     | 	client.base.url = https://collector.telemetry.confluent.cloud
benchi-kafka     | 	client.compression = null
benchi-kafka     | 	client.connect.timeout.ms = null
benchi-kafka     | 	client.request.timeout.ms = null
benchi-kafka     | 	client.retry.delay.seconds = null
benchi-kafka     | 	enabled = true
benchi-kafka     | 	events.enabled = true
benchi-kafka     | 	metrics.enabled = true
benchi-kafka     | 	proxy.password = null
benchi-kafka     | 	proxy.url = null
benchi-kafka     | 	proxy.username = null
benchi-kafka     | 	type = http
benchi-kafka     |  (io.confluent.shaded.io.confluent.telemetry.events.exporter.http.HttpExporterConfig)
benchi-kafka     | [2025-03-18 16:08:10,974] INFO [Producer clientId=confluent-metrics-reporter] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:08:10,974] WARN [Producer clientId=confluent-metrics-reporter] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:08:10,974] WARN [Producer clientId=confluent-metrics-reporter] Bootstrap broker localhost:9092 (id: -1 rack: null) disconnected (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:08:11,077] INFO [Producer clientId=confluent-metrics-reporter] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:08:11,077] WARN [Producer clientId=confluent-metrics-reporter] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:08:11,077] WARN [Producer clientId=confluent-metrics-reporter] Bootstrap broker localhost:9092 (id: -1 rack: null) disconnected (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:08:11,163] INFO Creating kafka exporter named '_local' (io.confluent.telemetry.reporter.TelemetryReporter)
benchi-kafka     | [2025-03-18 16:08:11,166] INFO Kafka Exporter _local getting producer client  (io.confluent.telemetry.exporter.kafka.KafkaExporter)
benchi-kafka     | [2025-03-18 16:08:11,166] INFO Creating new non-static producer client (io.confluent.telemetry.exporter.kafka.KafkaClientFactory)
benchi-kafka     | [2025-03-18 16:08:11,166] INFO ProducerConfig values: 
benchi-kafka     | 	acks = -1
benchi-kafka     | 	auto.include.jmx.reporter = true
benchi-kafka     | 	batch.size = 16384
benchi-kafka     | 	bootstrap.servers = [broker:29092]
benchi-kafka     | 	buffer.memory = 33554432
benchi-kafka     | 	client.dns.lookup = use_all_dns_ips
benchi-kafka     | 	client.id = confluent-telemetry-reporter-local-producer
benchi-kafka     | 	compression.gzip.level = -1
benchi-kafka     | 	compression.lz4.level = 9
benchi-kafka     | 	compression.type = zstd
benchi-kafka     | 	compression.zstd.level = 3
benchi-kafka     | 	confluent.lkc.id = null
benchi-kafka     | 	confluent.proxy.protocol.client.address = null
benchi-kafka     | 	confluent.proxy.protocol.client.mode = PROXY
benchi-kafka     | 	confluent.proxy.protocol.client.port = null
benchi-kafka     | 	confluent.proxy.protocol.client.version = NONE
benchi-kafka     | 	connections.max.idle.ms = 540000
benchi-kafka     | 	delivery.timeout.ms = 120000
benchi-kafka     | 	enable.idempotence = false
benchi-kafka     | 	enable.metrics.push = true
benchi-kafka     | 	interceptor.classes = []
benchi-kafka     | 	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
benchi-kafka     | 	linger.ms = 500
benchi-kafka     | 	max.block.ms = 60000
benchi-kafka     | 	max.in.flight.requests.per.connection = 1
benchi-kafka     | 	max.request.size = 10485760
benchi-kafka     | 	metadata.max.age.ms = 300000
benchi-kafka     | 	metadata.max.idle.ms = 300000
benchi-kafka     | 	metadata.recovery.strategy = none
benchi-kafka     | 	metric.reporters = []
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.recording.level = INFO
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	partitioner.adaptive.partitioning.enable = true
benchi-kafka     | 	partitioner.availability.timeout.ms = 0
benchi-kafka     | 	partitioner.class = class io.confluent.shaded.io.confluent.telemetry.events.exporter.kafka.RandomBrokerPartitionSubsetPartitioner
benchi-kafka     | 	partitioner.ignore.keys = false
benchi-kafka     | 	receive.buffer.bytes = 32768
benchi-kafka     | 	reconnect.backoff.max.ms = 1000
benchi-kafka     | 	reconnect.backoff.ms = 50
benchi-kafka     | 	request.timeout.ms = 30000
benchi-kafka     | 	retries = 2147483647
benchi-kafka     | 	retry.backoff.max.ms = 1000
benchi-kafka     | 	retry.backoff.ms = 500
benchi-kafka     | 	sasl.client.callback.handler.class = null
benchi-kafka     | 	sasl.jaas.config = null
benchi-kafka     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
benchi-kafka     | 	sasl.kerberos.min.time.before.relogin = 60000
benchi-kafka     | 	sasl.kerberos.service.name = null
benchi-kafka     | 	sasl.kerberos.ticket.renew.jitter = 0.05
benchi-kafka     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
benchi-kafka     | 	sasl.login.callback.handler.class = null
benchi-kafka     | 	sasl.login.class = null
benchi-kafka     | 	sasl.login.connect.timeout.ms = null
benchi-kafka     | 	sasl.login.read.timeout.ms = null
benchi-kafka     | 	sasl.login.refresh.buffer.seconds = 300
benchi-kafka     | 	sasl.login.refresh.min.period.seconds = 60
benchi-kafka     | 	sasl.login.refresh.window.factor = 0.8
benchi-kafka     | 	sasl.login.refresh.window.jitter = 0.05
benchi-kafka     | 	sasl.login.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.login.retry.backoff.ms = 100
benchi-kafka     | 	sasl.mechanism = GSSAPI
benchi-kafka     | 	sasl.oauthbearer.clientassertion.audience = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.expiration = 5
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.jti.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.nbf.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.issuer = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.location = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key.passphrase = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.subject = null
benchi-kafka     | 	sasl.oauthbearer.clock.skew.seconds = 30
benchi-kafka     | 	sasl.oauthbearer.expected.audience = null
benchi-kafka     | 	sasl.oauthbearer.expected.issuer = null
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.url = null
benchi-kafka     | 	sasl.oauthbearer.scope.claim.name = scope
benchi-kafka     | 	sasl.oauthbearer.sub.claim.name = sub
benchi-kafka     | 	sasl.oauthbearer.token.endpoint.url = null
benchi-kafka     | 	security.protocol = PLAINTEXT
benchi-kafka     | 	security.providers = null
benchi-kafka     | 	send.buffer.bytes = 131072
benchi-kafka     | 	socket.connection.setup.timeout.max.ms = 30000
benchi-kafka     | 	socket.connection.setup.timeout.ms = 10000
benchi-kafka     | 	ssl.cipher.suites = null
benchi-kafka     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	ssl.engine.factory.class = null
benchi-kafka     | 	ssl.key.password = null
benchi-kafka     | 	ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	ssl.keystore.certificate.chain = null
benchi-kafka     | 	ssl.keystore.key = null
benchi-kafka     | 	ssl.keystore.location = null
benchi-kafka     | 	ssl.keystore.password = null
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.protocol = TLSv1.3
benchi-kafka     | 	ssl.provider = null
benchi-kafka     | 	ssl.secure.random.implementation = null
benchi-kafka     | 	ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	ssl.truststore.certificates = null
benchi-kafka     | 	ssl.truststore.location = null
benchi-kafka     | 	ssl.truststore.password = null
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     | 	transaction.timeout.ms = 60000
benchi-kafka     | 	transactional.id = null
benchi-kafka     | 	value.serializer = class io.confluent.telemetry.serde.OpenTelemetryMetricsSerde
benchi-kafka     |  (org.apache.kafka.clients.producer.ProducerConfig)
benchi-kafka     | [2025-03-18 16:08:11,166] INFO initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector)
benchi-kafka     | [2025-03-18 16:08:11,169] INFO These configurations '[confluent.metrics.reporter.bootstrap.servers]' were supplied but are not used yet. (org.apache.kafka.clients.producer.ProducerConfig)
benchi-kafka     | [2025-03-18 16:08:11,169] INFO Kafka version: 7.8.1-ce (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:08:11,169] INFO Kafka commitId: eefb51c6f2f1b888b8b03296353338afbac8e213 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:08:11,169] INFO Kafka startTimeMs: 1742314091169 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:08:11,169] INFO [Producer clientId=confluent-telemetry-reporter-local-producer] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:08:11,170] WARN [Producer clientId=confluent-telemetry-reporter-local-producer] Connection to node -1 (broker/172.20.0.2:29092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:08:11,170] WARN [Producer clientId=confluent-telemetry-reporter-local-producer] Bootstrap broker broker:29092 (id: -1 rack: null) disconnected (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:08:11,200] INFO Handling event SbcConfigUpdateEvent-3 (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:08:11,200] INFO Cluster metadata containing at least one unfenced broker not yet available, SBC config processing delayed. (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:08:11,200] INFO Handling event SbcKraftStartupEvent-5 (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:08:11,200] INFO Cluster metadata containing at least one unfenced broker not yet available, SBC startup delayed. (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:08:11,221] INFO [Producer clientId=confluent-telemetry-reporter-local-producer] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:08:11,221] WARN [Producer clientId=confluent-telemetry-reporter-local-producer] Connection to node -1 (broker/172.20.0.2:29092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:08:11,221] WARN [Producer clientId=confluent-telemetry-reporter-local-producer] Bootstrap broker broker:29092 (id: -1 rack: null) disconnected (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:08:11,310] INFO Starting Confluent telemetry reporter with an interval of 60000 ms) (io.confluent.telemetry.reporter.TelemetryReporter)
benchi-kafka     | [2025-03-18 16:08:11,323] INFO [Producer clientId=confluent-telemetry-reporter-local-producer] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:08:11,323] WARN [Producer clientId=confluent-telemetry-reporter-local-producer] Connection to node -1 (broker/172.20.0.2:29092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:08:11,323] WARN [Producer clientId=confluent-telemetry-reporter-local-producer] Bootstrap broker broker:29092 (id: -1 rack: null) disconnected (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:08:11,332] INFO [Producer clientId=confluent-metrics-reporter] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:08:11,332] WARN [Producer clientId=confluent-metrics-reporter] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:08:11,332] WARN [Producer clientId=confluent-metrics-reporter] Bootstrap broker localhost:9092 (id: -1 rack: null) disconnected (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:08:11,336] INFO Starting Confluent metrics reporter for cluster id MkU3OEVBNTcwNTJENDM2Qk with an interval of 15000 ms (io.confluent.metrics.reporter.ConfluentMetricsReporter)
benchi-kafka     | [2025-03-18 16:08:11,344] INFO [BrokerServer id=1] Waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
benchi-kafka     | [2025-03-18 16:08:11,344] INFO [BrokerServer id=1] Finished waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
benchi-kafka     | [2025-03-18 16:08:11,344] INFO [BrokerServer id=1] Waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
benchi-kafka     | [2025-03-18 16:08:11,344] INFO [BrokerServer id=1] Finished waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
benchi-kafka     | [2025-03-18 16:08:11,344] INFO [BrokerServer id=1] Waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
benchi-kafka     | [2025-03-18 16:08:11,345] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing MetadataVersionPublisher(id=1) with a snapshot at offset 7 (org.apache.kafka.image.loader.MetadataLoader)
benchi-kafka     | [2025-03-18 16:08:11,345] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing BrokerMetadataPublisher with a snapshot at offset 7 (org.apache.kafka.image.loader.MetadataLoader)
benchi-kafka     | [2025-03-18 16:08:11,345] INFO [BrokerMetadataPublisher id=1] Publishing initial metadata at offset OffsetAndEpoch(offset=7, epoch=1) with metadata.version 3.8-IV0A. (kafka.server.metadata.BrokerMetadataPublisher)
benchi-kafka     | [2025-03-18 16:08:11,345] INFO Loading logs from log dirs ArrayBuffer(/tmp/kraft-combined-logs) (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:08:11,347] INFO No logs found to be loaded in /tmp/kraft-combined-logs (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:08:11,348] INFO Loaded 0 logs in 2ms (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:08:11,348] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:08:11,350] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:08:11,350] INFO Starting log roller with a period of 300000 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:08:11,357] INFO Starting the log cleaner (kafka.log.LogCleaner)
benchi-kafka     | [2025-03-18 16:08:11,364] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner$CleanerThread)
benchi-kafka     | [2025-03-18 16:08:11,365] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
benchi-kafka     | [2025-03-18 16:08:11,366] INFO [AddPartitionsToTxnSenderThread-1]: Starting (kafka.server.AddPartitionsToTxnManager)
benchi-kafka     | [2025-03-18 16:08:11,373] INFO AdminClientConfig values: 
benchi-kafka     | 	auto.include.jmx.reporter = true
benchi-kafka     | 	bootstrap.controllers = []
benchi-kafka     | 	bootstrap.servers = [broker:29092]
benchi-kafka     | 	client.dns.lookup = use_all_dns_ips
benchi-kafka     | 	client.id = cluster-link--local-admin-1
benchi-kafka     | 	confluent.lkc.id = null
benchi-kafka     | 	confluent.metrics.reporter.bootstrap.servers = localhost:9092
benchi-kafka     | 	confluent.proxy.protocol.client.address = null
benchi-kafka     | 	confluent.proxy.protocol.client.mode = PROXY
benchi-kafka     | 	confluent.proxy.protocol.client.port = null
benchi-kafka     | 	confluent.proxy.protocol.client.version = NONE
benchi-kafka     | 	connections.max.idle.ms = 300000
benchi-kafka     | 	default.api.timeout.ms = 60000
benchi-kafka     | 	enable.metrics.push = true
benchi-kafka     | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
benchi-kafka     | 	metadata.max.age.ms = 300000
benchi-kafka     | 	metadata.recovery.strategy = none
benchi-kafka     | 	metric.reporters = []
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.recording.level = INFO
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	receive.buffer.bytes = 65536
benchi-kafka     | 	reconnect.backoff.max.ms = 1000
benchi-kafka     | 	reconnect.backoff.ms = 50
benchi-kafka     | 	request.timeout.ms = 30000
benchi-kafka     | 	retries = 2147483647
benchi-kafka     | 	retry.backoff.max.ms = 1000
benchi-kafka     | 	retry.backoff.ms = 100
benchi-kafka     | 	sasl.client.callback.handler.class = null
benchi-kafka     | 	sasl.jaas.config = null
benchi-kafka     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
benchi-kafka     | 	sasl.kerberos.min.time.before.relogin = 60000
benchi-kafka     | 	sasl.kerberos.service.name = null
benchi-kafka     | 	sasl.kerberos.ticket.renew.jitter = 0.05
benchi-kafka     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
benchi-kafka     | 	sasl.login.callback.handler.class = null
benchi-kafka     | 	sasl.login.class = null
benchi-kafka     | 	sasl.login.connect.timeout.ms = null
benchi-kafka     | 	sasl.login.read.timeout.ms = null
benchi-kafka     | 	sasl.login.refresh.buffer.seconds = 300
benchi-kafka     | 	sasl.login.refresh.min.period.seconds = 60
benchi-kafka     | 	sasl.login.refresh.window.factor = 0.8
benchi-kafka     | 	sasl.login.refresh.window.jitter = 0.05
benchi-kafka     | 	sasl.login.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.login.retry.backoff.ms = 100
benchi-kafka     | 	sasl.mechanism = GSSAPI
benchi-kafka     | 	sasl.oauthbearer.clientassertion.audience = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.expiration = 5
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.jti.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.nbf.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.issuer = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.location = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key.passphrase = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.subject = null
benchi-kafka     | 	sasl.oauthbearer.clock.skew.seconds = 30
benchi-kafka     | 	sasl.oauthbearer.expected.audience = null
benchi-kafka     | 	sasl.oauthbearer.expected.issuer = null
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.url = null
benchi-kafka     | 	sasl.oauthbearer.scope.claim.name = scope
benchi-kafka     | 	sasl.oauthbearer.sub.claim.name = sub
benchi-kafka     | 	sasl.oauthbearer.token.endpoint.url = null
benchi-kafka     | 	security.protocol = PLAINTEXT
benchi-kafka     | 	security.providers = null
benchi-kafka     | 	send.buffer.bytes = 131072
benchi-kafka     | 	socket.connection.setup.timeout.max.ms = 30000
benchi-kafka     | 	socket.connection.setup.timeout.ms = 10000
benchi-kafka     | 	ssl.cipher.suites = null
benchi-kafka     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	ssl.engine.factory.class = null
benchi-kafka     | 	ssl.key.password = null
benchi-kafka     | 	ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	ssl.keystore.certificate.chain = null
benchi-kafka     | 	ssl.keystore.key = null
benchi-kafka     | 	ssl.keystore.location = null
benchi-kafka     | 	ssl.keystore.password = null
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.protocol = TLSv1.3
benchi-kafka     | 	ssl.provider = null
benchi-kafka     | 	ssl.secure.random.implementation = null
benchi-kafka     | 	ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	ssl.truststore.certificates = null
benchi-kafka     | 	ssl.truststore.location = null
benchi-kafka     | 	ssl.truststore.password = null
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     |  (org.apache.kafka.clients.admin.AdminClientConfig)
benchi-kafka     | [2025-03-18 16:08:11,385] INFO These configurations '[confluent.metrics.topic.replicas, confluent.metrics.reporter.bootstrap.servers, jmx.hostname, cluster.link.metadata.topic.replication.factor, jmx.port, confluent.license.topic.replication.factor]' were supplied but are not used yet. (org.apache.kafka.clients.admin.AdminClientConfig)
benchi-kafka     | [2025-03-18 16:08:11,385] INFO Kafka version: 7.8.1-ce (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:08:11,385] INFO Kafka commitId: eefb51c6f2f1b888b8b03296353338afbac8e213 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:08:11,385] INFO Kafka startTimeMs: 1742314091385 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:08:11,386] INFO [ClusterLinkManager-broker-1] ClusterLinkManager has started up. (kafka.server.link.ClusterLinkManager)
benchi-kafka     | [2025-03-18 16:08:11,387] INFO [GroupCoordinator 1]: Starting up. (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:08:11,387] INFO [AdminClient clientId=cluster-link--local-admin-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:08:11,387] WARN [AdminClient clientId=cluster-link--local-admin-1] Connection to node -1 (broker/172.20.0.2:29092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:08:11,387] INFO [GroupCoordinator 1]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:08:11,387] INFO [TransactionCoordinator id=1] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
benchi-kafka     | [2025-03-18 16:08:11,388] INFO [TxnMarkerSenderThread-1]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
benchi-kafka     | [2025-03-18 16:08:11,388] INFO [TransactionCoordinator id=1] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
benchi-kafka     | [2025-03-18 16:08:11,392] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing BrokerRegistrationTracker(id=1) with a snapshot at offset 7 (org.apache.kafka.image.loader.MetadataLoader)
benchi-kafka     | [2025-03-18 16:08:11,392] INFO [BrokerServer id=1] Finished waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
benchi-kafka     | [2025-03-18 16:08:11,392] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing ClusterLinkCoordinatorListener with a snapshot at offset 7 (org.apache.kafka.image.loader.MetadataLoader)
benchi-kafka     | [2025-03-18 16:08:11,393] INFO KafkaConfig values: 
benchi-kafka     | 	advertised.listeners = PLAINTEXT://broker:29092,PLAINTEXT_HOST://localhost:9092
benchi-kafka     | 	alter.config.policy.class.name = null
benchi-kafka     | 	alter.log.dirs.replication.quota.window.num = 11
benchi-kafka     | 	alter.log.dirs.replication.quota.window.size.seconds = 1
benchi-kafka     | 	authorizer.class.name = 
benchi-kafka     | 	auto.create.topics.enable = true
benchi-kafka     | 	auto.include.jmx.reporter = true
benchi-kafka     | 	auto.leader.rebalance.enable = true
benchi-kafka     | 	background.threads = 10
benchi-kafka     | 	broker.heartbeat.interval.ms = 2000
benchi-kafka     | 	broker.id = 1
benchi-kafka     | 	broker.id.generation.enable = true
benchi-kafka     | 	broker.interceptor.class = class org.apache.kafka.server.interceptor.DefaultBrokerInterceptor
benchi-kafka     | 	broker.rack = null
benchi-kafka     | 	broker.session.timeout.ms = 9000
benchi-kafka     | 	broker.session.uuid = iNf5MSGJTFOqkdMXHRqjNg
benchi-kafka     | 	client.quota.callback.class = null
benchi-kafka     | 	client.quota.max.throttle.time.ms = 5000
benchi-kafka     | 	compression.gzip.level = -1
benchi-kafka     | 	compression.lz4.level = 9
benchi-kafka     | 	compression.type = producer
benchi-kafka     | 	compression.zstd.level = 3
benchi-kafka     | 	confluent.alter.broker.health.max.demoted.brokers = 2147483647
benchi-kafka     | 	confluent.alter.broker.health.max.demoted.brokers.percentage = 0
benchi-kafka     | 	confluent.ansible.managed = false
benchi-kafka     | 	confluent.api.visibility = DEFAULT
benchi-kafka     | 	confluent.append.record.interceptor.classes = []
benchi-kafka     | 	confluent.apply.create.topic.policy.to.create.partitions = false
benchi-kafka     | 	confluent.authorizer.authority.name = 
benchi-kafka     | 	confluent.automatic.alter.broker.health.retry.backoff.ms = 2000
benchi-kafka     | 	confluent.backpressure.disk.enable = false
benchi-kafka     | 	confluent.backpressure.disk.free.threshold.bytes = 21474836480
benchi-kafka     | 	confluent.backpressure.disk.produce.bytes.per.second = 131072
benchi-kafka     | 	confluent.backpressure.disk.threshold.recovery.factor = 1.5
benchi-kafka     | 	confluent.backpressure.request.min.broker.limit = 200
benchi-kafka     | 	confluent.backpressure.request.queue.size.percentile = p95
benchi-kafka     | 	confluent.backpressure.types = null
benchi-kafka     | 	confluent.balancer.api.state.topic = _confluent_balancer_api_state
benchi-kafka     | 	confluent.balancer.broker.addition.elapsed.time.ms.completion.threshold = 57600000
benchi-kafka     | 	confluent.balancer.broker.addition.mean.cpu.percent.completion.threshold = 0.5
benchi-kafka     | 	confluent.balancer.capacity.threshold.upper.limit = 0.95
benchi-kafka     | 	confluent.balancer.cell.load.upper.bound = 0.7
benchi-kafka     | 	confluent.balancer.cell.overload.detection.interval.ms = 3600000
benchi-kafka     | 	confluent.balancer.cell.overload.duration.ms = 86400000
benchi-kafka     | 	confluent.balancer.class = io.confluent.databalancer.SbcDataBalanceManager
benchi-kafka     | 	confluent.balancer.consumer.out.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.balancer.cpu.balance.threshold = 1.1
benchi-kafka     | 	confluent.balancer.cpu.low.utilization.threshold = 0.2
benchi-kafka     | 	confluent.balancer.cpu.utilization.detector.duration.ms = 600000
benchi-kafka     | 	confluent.balancer.cpu.utilization.detector.enabled = false
benchi-kafka     | 	confluent.balancer.cpu.utilization.detector.overutilization.threshold = 80.0
benchi-kafka     | 	confluent.balancer.cpu.utilization.detector.underutilization.threshold = 50.0
benchi-kafka     | 	confluent.balancer.disk.max.load = 0.85
benchi-kafka     | 	confluent.balancer.disk.min.free.space.gb = 0
benchi-kafka     | 	confluent.balancer.disk.min.free.space.lower.limit.gb = 0
benchi-kafka     | 	confluent.balancer.disk.utilization.detector.duration.ms = 600000
benchi-kafka     | 	confluent.balancer.disk.utilization.detector.enabled = false
benchi-kafka     | 	confluent.balancer.disk.utilization.detector.overutilization.threshold = 80.0
benchi-kafka     | 	confluent.balancer.disk.utilization.detector.reserved.capacity = 150000.0
benchi-kafka     | 	confluent.balancer.disk.utilization.detector.underutilization.threshold = 35.0
benchi-kafka     | 	confluent.balancer.enable = true
benchi-kafka     | 	confluent.balancer.exclude.topic.names = []
benchi-kafka     | 	confluent.balancer.exclude.topic.prefixes = []
benchi-kafka     | 	confluent.balancer.goal.violation.delay.on.new.brokers.ms = 1800000
benchi-kafka     | 	confluent.balancer.goal.violation.distribution.threshold.multiplier = 1.1
benchi-kafka     | 	confluent.balancer.heal.broker.failure.threshold.ms = 3600000
benchi-kafka     | 	confluent.balancer.heal.uneven.load.trigger = EMPTY_BROKER
benchi-kafka     | 	confluent.balancer.incremental.balancing.cpu.top.proposal.tracking.enabled = true
benchi-kafka     | 	confluent.balancer.incremental.balancing.cpu.top.proposal.tracking.num.proposals = 15
benchi-kafka     | 	confluent.balancer.incremental.balancing.enabled = false
benchi-kafka     | 	confluent.balancer.incremental.balancing.goals = []
benchi-kafka     | 	confluent.balancer.incremental.balancing.lower.bound = 0.02
benchi-kafka     | 	confluent.balancer.incremental.balancing.min.valid.windows = 5
benchi-kafka     | 	confluent.balancer.incremental.balancing.step.ratio = 0.2
benchi-kafka     | 	confluent.balancer.inter.cell.balancing.enabled = false
benchi-kafka     | 	confluent.balancer.max.capacity.balancing.delta.percentage = 0.0
benchi-kafka     | 	confluent.balancer.max.replicas = 2147483647
benchi-kafka     | 	confluent.balancer.network.in.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.balancer.network.out.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.balancer.plan.computation.retry.timeout.ms = 3600000
benchi-kafka     | 	confluent.balancer.producer.in.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.balancer.rebalancing.goals = []
benchi-kafka     | 	confluent.balancer.replication.in.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.balancer.resource.utilization.detector.interval.ms = 60000
benchi-kafka     | 	confluent.balancer.sbc.metrics.parser.enabled = false
benchi-kafka     | 	confluent.balancer.self.healing.maximum.rounds = 1
benchi-kafka     | 	confluent.balancer.task.history.retention.days = 30
benchi-kafka     | 	confluent.balancer.tenant.maximum.movements = 0
benchi-kafka     | 	confluent.balancer.tenant.suspension.ms = 86400000
benchi-kafka     | 	confluent.balancer.throttle.bytes.per.second = 10485760
benchi-kafka     | 	confluent.balancer.topic.partition.maximum.movements = 5
benchi-kafka     | 	confluent.balancer.topic.partition.movement.expiration.ms = 10800000
benchi-kafka     | 	confluent.balancer.topic.partition.suspension.ms = 18000000
benchi-kafka     | 	confluent.balancer.topic.replication.factor = 1
benchi-kafka     | 	confluent.balancer.triggering.goals = []
benchi-kafka     | 	confluent.balancer.v2.addition.enabled = false
benchi-kafka     | 	confluent.basic.auth.credentials.source = null
benchi-kafka     | 	confluent.basic.auth.user.info = null
benchi-kafka     | 	confluent.bearer.auth.cache.expiry.buffer.seconds = 300
benchi-kafka     | 	confluent.bearer.auth.client.id = null
benchi-kafka     | 	confluent.bearer.auth.client.secret = null
benchi-kafka     | 	confluent.bearer.auth.credentials.source = null
benchi-kafka     | 	confluent.bearer.auth.identity.pool.id = null
benchi-kafka     | 	confluent.bearer.auth.issuer.endpoint.url = null
benchi-kafka     | 	confluent.bearer.auth.logical.cluster = null
benchi-kafka     | 	confluent.bearer.auth.scope = null
benchi-kafka     | 	confluent.bearer.auth.scope.claim.name = scope
benchi-kafka     | 	confluent.bearer.auth.sub.claim.name = sub
benchi-kafka     | 	confluent.bearer.auth.token = null
benchi-kafka     | 	confluent.broker.health.manager.enabled = true
benchi-kafka     | 	confluent.broker.health.manager.engine.request.handler.threads.stuck.criteria = AllThreadsStuck
benchi-kafka     | 	confluent.broker.health.manager.hard.kill.duration.ms = 60000
benchi-kafka     | 	confluent.broker.health.manager.mitigation.enabled = false
benchi-kafka     | 	confluent.broker.health.manager.num.samples.before.broker.suspect = 30
benchi-kafka     | 	confluent.broker.health.manager.num.samples.before.broker.unhealthy = 180
benchi-kafka     | 	confluent.broker.health.manager.percentage.unhealthy.samples.before.broker.suspect = 90
benchi-kafka     | 	confluent.broker.health.manager.percentage.unhealthy.samples.before.broker.unhealthy = 70
benchi-kafka     | 	confluent.broker.health.manager.sample.duration.ms = 1000
benchi-kafka     | 	confluent.broker.health.manager.storage.background.threads.stuck.criteria = AnyThreadStuck
benchi-kafka     | 	confluent.broker.health.manager.storage.network.threads.stuck.criteria = AnyThreadStuck
benchi-kafka     | 	confluent.broker.health.manager.storage.request.handler.threads.stuck.criteria = AnyThreadStuck
benchi-kafka     | 	confluent.broker.limit.consumer.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.broker.limit.producer.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.broker.load.advertised.limit.load = 0.8
benchi-kafka     | 	confluent.broker.load.average.service.request.time.ms = 0.1
benchi-kafka     | 	confluent.broker.load.delay.metric.start.ms = 180000
benchi-kafka     | 	confluent.broker.load.enabled = false
benchi-kafka     | 	confluent.broker.load.num.samples = 60
benchi-kafka     | 	confluent.broker.load.tenant.metric.enable = false
benchi-kafka     | 	confluent.broker.load.update.metric.tags.interval.ms = 60000
benchi-kafka     | 	confluent.broker.load.window.size.ms = 60000
benchi-kafka     | 	confluent.broker.load.workload.coefficient = 20.0
benchi-kafka     | 	confluent.broker.registration.delay.ms = 0
benchi-kafka     | 	confluent.catalog.collector.destination.topic = telemetry.events.data_catalog_source
benchi-kafka     | 	confluent.catalog.collector.enable = false
benchi-kafka     | 	confluent.catalog.collector.full.configs.enable = false
benchi-kafka     | 	confluent.catalog.collector.max.bytes.per.snapshot = 850000
benchi-kafka     | 	confluent.catalog.collector.max.topics.process = 500
benchi-kafka     | 	confluent.catalog.collector.max.zookeeper.request.per.sec = 100
benchi-kafka     | 	confluent.catalog.collector.snapshot.init.delay.sec = 60
benchi-kafka     | 	confluent.catalog.collector.snapshot.interval.sec = 300
benchi-kafka     | 	confluent.ccloud.host.suffixes = .confluent.cloud,.cpdev.cloud,.confluentgov.com,.confluentgov-internal.com
benchi-kafka     | 	confluent.ccloud.intranet.host.suffixes = .intranet.stag.cpdev.cloud,.intranet.stag.cpdev-untrusted.cloud,.intranet.devel.cpdev.cloud,.intranet.devel.cpdev-untrusted.cloud,.intranet.confluent.cloud,.intranet.confluent-untrusted.cloud
benchi-kafka     | 	confluent.cdc.api.keys.topic = 
benchi-kafka     | 	confluent.cdc.api.keys.topic.load.timeout.ms = 600000
benchi-kafka     | 	confluent.cdc.client.quotas.enable = false
benchi-kafka     | 	confluent.cdc.client.quotas.topic.name = 
benchi-kafka     | 	confluent.cdc.lkc.metadata.topic = 
benchi-kafka     | 	confluent.cdc.user.metadata.enable = false
benchi-kafka     | 	confluent.cdc.user.metadata.topic = _confluent-user_metadata
benchi-kafka     | 	confluent.cell.metrics.refresh.period.ms = 60000
benchi-kafka     | 	confluent.cells.default.size = 15
benchi-kafka     | 	confluent.cells.enable = false
benchi-kafka     | 	confluent.cells.implicit.creation.enable = false
benchi-kafka     | 	confluent.cells.load.refresher.enable = true
benchi-kafka     | 	confluent.cells.max.size = 15
benchi-kafka     | 	confluent.cells.min.size = 6
benchi-kafka     | 	confluent.checksum.enabled.files = [none]
benchi-kafka     | 	confluent.clm.enabled = false
benchi-kafka     | 	confluent.clm.frequency.in.hours = 6
benchi-kafka     | 	confluent.clm.list.object.thread_pool.size = 1
benchi-kafka     | 	confluent.clm.max.backup.days = 3
benchi-kafka     | 	confluent.clm.min.delay.in.minutes = 30
benchi-kafka     | 	confluent.clm.thread.pool.size = 2
benchi-kafka     | 	confluent.clm.topic.retention.days.to.backup.days = 0:0,3:3
benchi-kafka     | 	confluent.close.connections.on.credential.delete = false
benchi-kafka     | 	confluent.cluster.link.admin.max.in.flight.requests = 1000
benchi-kafka     | 	confluent.cluster.link.admin.request.batch.size = 1
benchi-kafka     | 	confluent.cluster.link.allow.config.providers = true
benchi-kafka     | 	confluent.cluster.link.allow.legacy.message.format = false
benchi-kafka     | 	confluent.cluster.link.allow.truncation.below.hwm = true
benchi-kafka     | 	confluent.cluster.link.availability.check.mode = ALL
benchi-kafka     | 	confluent.cluster.link.background.thread.affinity = LINK
benchi-kafka     | 	confluent.cluster.link.clients.max.idle.ms = 3153600000000
benchi-kafka     | 	confluent.cluster.link.enable = true
benchi-kafka     | 	confluent.cluster.link.enable.local.admin = false
benchi-kafka     | 	confluent.cluster.link.enable.metrics.reduction = false
benchi-kafka     | 	confluent.cluster.link.fetch.response.min.bytes = 1
benchi-kafka     | 	confluent.cluster.link.fetch.response.total.bytes = 2147483647
benchi-kafka     | 	confluent.cluster.link.fetcher.auto.tune.enable = false
benchi-kafka     | 	confluent.cluster.link.fetcher.thread.pool.mode = ENDPOINT
benchi-kafka     | 	confluent.cluster.link.insync.fetch.response.min.bytes = 1
benchi-kafka     | 	confluent.cluster.link.insync.fetch.response.total.bytes = 2147483647
benchi-kafka     | 	confluent.cluster.link.intranet.connectivity.enable = false
benchi-kafka     | 	confluent.cluster.link.io.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.cluster.link.local.reverse.connection.listener.map = null
benchi-kafka     | 	confluent.cluster.link.max.client.connections = 2147483647
benchi-kafka     | 	confluent.cluster.link.metadata.topic.create.retry.delay.ms = 1000
benchi-kafka     | 	confluent.cluster.link.metadata.topic.enable = false
benchi-kafka     | 	confluent.cluster.link.metadata.topic.min.isr = 2
benchi-kafka     | 	confluent.cluster.link.metadata.topic.partitions = 50
benchi-kafka     | 	confluent.cluster.link.metadata.topic.replication.factor = 1
benchi-kafka     | 	confluent.cluster.link.mirror.transition.batch.size = 10
benchi-kafka     | 	confluent.cluster.link.num.background.threads = 1
benchi-kafka     | 	confluent.cluster.link.periodic.task.batch.size = 2147483647
benchi-kafka     | 	confluent.cluster.link.periodic.task.min.interval.ms = 1000
benchi-kafka     | 	confluent.cluster.link.replica.fetch.connections.mode = combined
benchi-kafka     | 	confluent.cluster.link.replication.quota.mode = CLUSTER_LINK_ONLY
benchi-kafka     | 	confluent.cluster.link.replication.quota.mode.per.tenant.overrides = 
benchi-kafka     | 	confluent.cluster.link.replication.quota.window.num = 11
benchi-kafka     | 	confluent.cluster.link.replication.quota.window.size.seconds = 2
benchi-kafka     | 	confluent.cluster.link.request.quota.capacity = 400
benchi-kafka     | 	confluent.cluster.link.request.quota.request.percentage.multiplier = 1.0
benchi-kafka     | 	confluent.cluster.link.tenant.replication.quota.enable = false
benchi-kafka     | 	confluent.cluster.link.tenant.request.quota.enable = false
benchi-kafka     | 	confluent.cluster.metadata.snapshot.tier.delete.enable = false
benchi-kafka     | 	confluent.cluster.metadata.snapshot.tier.delete.maintain.min.snapshots = 3
benchi-kafka     | 	confluent.cluster.metadata.snapshot.tier.delete.retention.ms = 604800000
benchi-kafka     | 	confluent.cluster.metadata.snapshot.tier.upload.enable = false
benchi-kafka     | 	confluent.compacted.topic.prefer.tier.fetch.ms = -1
benchi-kafka     | 	confluent.connection.invalid.request.delay.enable = false
benchi-kafka     | 	confluent.connections.idle.expiry.manager.ignore.idleness.requests = []
benchi-kafka     | 	confluent.consumer.fetch.partition.pruning.enable = true
benchi-kafka     | 	confluent.consumer.lag.emitter.enabled = false
benchi-kafka     | 	confluent.consumer.lag.emitter.interval.ms = 60000
benchi-kafka     | 	confluent.dataflow.policy.watch.monitor.ms = 300000
benchi-kafka     | 	confluent.defer.isr.shrink.enable = false
benchi-kafka     | 	confluent.describe.topic.partitions.enabled = false
benchi-kafka     | 	confluent.disk.io.manager.enable = false
benchi-kafka     | 	confluent.disk.throughput.headroom = 10485760
benchi-kafka     | 	confluent.disk.throughput.limit = 10485760000
benchi-kafka     | 	confluent.disk.throughput.quota.tier.archive = 1048576000
benchi-kafka     | 	confluent.disk.throughput.quota.tier.archive.throttled = 104857600
benchi-kafka     | 	confluent.durability.audit.batch.flush.frequency.ms = 900000
benchi-kafka     | 	confluent.durability.audit.checks = PeriodicalAudit,ChecksumAudit
benchi-kafka     | 	confluent.durability.audit.enable = false
benchi-kafka     | 	confluent.durability.audit.idempotent.producer = false
benchi-kafka     | 	confluent.durability.audit.initial.job.delay.ms = 900000
benchi-kafka     | 	confluent.durability.audit.io.bytes.per.sec = 10485760
benchi-kafka     | 	confluent.durability.audit.log.ignored.event.types = 
benchi-kafka     | 	confluent.durability.audit.reporting.batch.ms = 1800000
benchi-kafka     | 	confluent.durability.audit.tier.compaction.audit.duration.ms = 14400000
benchi-kafka     | 	confluent.durability.events.allowed = OffsetChangeType,EpochChangeType,IsrExpandType,DeleteRecordsType,RetentionChangeType,StartOffsetChangeType,DeletePartitionType,HealthCheckType
benchi-kafka     | 	confluent.durability.topic.partition.count = 50
benchi-kafka     | 	confluent.durability.topic.replication.factor = 1
benchi-kafka     | 	confluent.e2e_checksum.protection.enabled = false
benchi-kafka     | 	confluent.e2e_checksum.protection.files = [none]
benchi-kafka     | 	confluent.e2e_checksum.protection.store.entry.ttl.ms = 2592000000
benchi-kafka     | 	confluent.elastic.cku.enabled = false
benchi-kafka     | 	confluent.elastic.cku.scaletozero.enabled = false
benchi-kafka     | 	confluent.eligible.controllers = []
benchi-kafka     | 	confluent.enable.stray.partition.deletion = false
benchi-kafka     | 	confluent.encryption.key.manager.rotation.interval.ms = 31536000000
benchi-kafka     | 	confluent.fetch.from.follower.require.leader.epoch.enable = false
benchi-kafka     | 	confluent.fetch.partition.pruning.enable = true
benchi-kafka     | 	confluent.floor.connection.rate.per.ip = -1.0
benchi-kafka     | 	confluent.floor.connection.rate.per.tenant = -1.0
benchi-kafka     | 	confluent.group.coordinator.offsets.batching.enable = false
benchi-kafka     | 	confluent.group.coordinator.offsets.writer.threads = 2
benchi-kafka     | 	confluent.group.metadata.load.threads = 32
benchi-kafka     | 	confluent.heap.tenured.notify.bytes = 0
benchi-kafka     | 	confluent.heap.tenured.notify.enabled = false
benchi-kafka     | 	confluent.hot.partition.ratio = 0.8
benchi-kafka     | 	confluent.http.server.start.timeout.ms = 60000
benchi-kafka     | 	confluent.http.server.stop.timeout.ms = 30000
benchi-kafka     | 	confluent.internal.metrics.enable = false
benchi-kafka     | 	confluent.internal.rest.server.bind.port = null
benchi-kafka     | 	confluent.internal.tenant.scoped.listener.name = INTERNAL_TENANT_SCOPED
benchi-kafka     | 	confluent.leader.epoch.checkpoint.checksum.enabled = false
benchi-kafka     | 	confluent.log.cleaner.timestamp.validation.enable = true
benchi-kafka     | 	confluent.log.placement.constraints = 
benchi-kafka     | 	confluent.max.connection.creation.rate.per.ip = 1.7976931348623157E308
benchi-kafka     | 	confluent.max.connection.creation.rate.per.tenant = 1.7976931348623157E308
benchi-kafka     | 	confluent.max.connection.rate.per.ip = -1.0
benchi-kafka     | 	confluent.max.connection.rate.per.tenant = -1.0
benchi-kafka     | 	confluent.max.connection.throttle.ms = null
benchi-kafka     | 	confluent.max.segment.ms = 9223372036854775807
benchi-kafka     | 	confluent.metadata.active.encryptor = null
benchi-kafka     | 	confluent.metadata.controlled.shutdown.partition.slice.delay.ms = 100
benchi-kafka     | 	confluent.metadata.encryptor.classes = null
benchi-kafka     | 	confluent.metadata.encryptor.required = false
benchi-kafka     | 	confluent.metadata.encryptor.secret.file = null
benchi-kafka     | 	confluent.metadata.encryptor.secrets = null
benchi-kafka     | 	confluent.metadata.jvm.warmup.ms = 60000
benchi-kafka     | 	confluent.metadata.leader.balance.slice.delay.ms = 100
benchi-kafka     | 	confluent.metadata.max.controlled.shutdown.partition.changes.per.slice = 1000
benchi-kafka     | 	confluent.metadata.max.leader.balance.changes.per.slice = 1000
benchi-kafka     | 	confluent.metadata.server.cluster.registry.clusters = []
benchi-kafka     | 	confluent.metadata.throttle.pre.check.for.non.wildcard.requests.enable = false
benchi-kafka     | 	confluent.metadata.throttle.pre.check.for.wildcard.requests.enable = false
benchi-kafka     | 	confluent.metrics.reporter.bootstrap.servers = localhost:9092
benchi-kafka     | 	confluent.min.acks = 0
benchi-kafka     | 	confluent.min.connection.throttle.ms = 0
benchi-kafka     | 	confluent.min.segment.ms = 1
benchi-kafka     | 	confluent.missing.id.cache.ttl.sec = 60
benchi-kafka     | 	confluent.missing.id.query.range = 20000
benchi-kafka     | 	confluent.missing.schema.cache.ttl.sec = 60
benchi-kafka     | 	confluent.mtls.enable = false
benchi-kafka     | 	confluent.mtls.listener.name = EXTERNAL
benchi-kafka     | 	confluent.mtls.sasl.authenticator.request.max.bytes = 104857600
benchi-kafka     | 	confluent.mtls.truststore.manager.class.name = null
benchi-kafka     | 	confluent.multitenant.authorizer.enable.acl.state = false
benchi-kafka     | 	confluent.multitenant.interceptor.balancer.apis.enabled = false
benchi-kafka     | 	confluent.multitenant.interceptor.collect.client.apiversions.max.per.tenant = 1000
benchi-kafka     | 	confluent.multitenant.interceptor.collect.client.apiversions.metric = false
benchi-kafka     | 	confluent.multitenant.listener.hostname.cluster.prefix.enable = false
benchi-kafka     | 	confluent.multitenant.listener.hostname.subdomain.suffix.enable = false
benchi-kafka     | 	confluent.multitenant.listener.names = null
benchi-kafka     | 	confluent.multitenant.parse.lkc.id.enable = false
benchi-kafka     | 	confluent.multitenant.parse.sni.host.name.enable = false
benchi-kafka     | 	confluent.network.health.manager.enabled = false
benchi-kafka     | 	confluent.network.health.manager.external.listener.name = EXTERNAL
benchi-kafka     | 	confluent.network.health.manager.externalconnectivitystartup.enabled = false
benchi-kafka     | 	confluent.network.health.manager.min.healthy.network.samples = 3
benchi-kafka     | 	confluent.network.health.manager.min.percentage.healthy.network.samples = 3
benchi-kafka     | 	confluent.network.health.manager.mitigation.enabled = false
benchi-kafka     | 	confluent.network.health.manager.network.sample.window.size = 120
benchi-kafka     | 	confluent.network.health.manager.sample.duration.ms = 1000
benchi-kafka     | 	confluent.oauth.flat.networking.verification.enable = false
benchi-kafka     | 	confluent.offsets.log.cleaner.delete.retention.ms = 86400000
benchi-kafka     | 	confluent.offsets.log.cleaner.max.compaction.lag.ms = 9223372036854775807
benchi-kafka     | 	confluent.offsets.log.cleaner.min.cleanable.dirty.ratio = 0.5
benchi-kafka     | 	confluent.offsets.topic.placement.constraints = 
benchi-kafka     | 	confluent.omit.network.processor.metric.tag = false
benchi-kafka     | 	confluent.operator.managed = false
benchi-kafka     | 	confluent.password.encoder.old.secret.ttl.ms = 9223372036854775807
benchi-kafka     | 	confluent.plugins.cluster.link.policy.max.destination.links.per.tenant = 10
benchi-kafka     | 	confluent.plugins.cluster.link.policy.max.source.links.per.tenant = 10
benchi-kafka     | 	confluent.plugins.topic.policy.max.partitions.per.cluster = 2147483647
benchi-kafka     | 	confluent.plugins.topic.policy.max.partitions.per.tenant = 512
benchi-kafka     | 	confluent.plugins.topic.policy.max.replicas.per.broker = 2147483647
benchi-kafka     | 	confluent.plugins.topic.policy.max.topics.per.cluster = 2147483647
benchi-kafka     | 	confluent.prefer.tier.fetch.ms = -1
benchi-kafka     | 	confluent.produce.throttle.pre.check.enable = false
benchi-kafka     | 	confluent.producer.id.cache.broker.hard.limit = -1
benchi-kafka     | 	confluent.producer.id.cache.eviction.minimal.expiration.ms = 900000
benchi-kafka     | 	confluent.producer.id.cache.extra.eviction.percentage = 0
benchi-kafka     | 	confluent.producer.id.cache.limit = 2147483647
benchi-kafka     | 	confluent.producer.id.cache.partition.hard.limit = -1
benchi-kafka     | 	confluent.producer.id.cache.tenant.hard.limit = -1
benchi-kafka     | 	confluent.producer.id.quota.manager.enable = false
benchi-kafka     | 	confluent.producer.id.quota.window.num = 11
benchi-kafka     | 	confluent.producer.id.quota.window.size.seconds = 1
benchi-kafka     | 	confluent.producer.id.throttle.enable = false
benchi-kafka     | 	confluent.producer.id.throttle.enable.threshold.percentage = 100
benchi-kafka     | 	confluent.proxy.mode.local.default = false
benchi-kafka     | 	confluent.proxy.protocol.fallback.enabled = false
benchi-kafka     | 	confluent.proxy.protocol.version = NONE
benchi-kafka     | 	confluent.quota.computing.usage.adjustment = 0.5
benchi-kafka     | 	confluent.quota.dynamic.enable = false
benchi-kafka     | 	confluent.quota.dynamic.publishing.interval.ms = 60000
benchi-kafka     | 	confluent.quota.dynamic.reporting.interval.ms = 30000
benchi-kafka     | 	confluent.quota.dynamic.reporting.min.usage = 102400
benchi-kafka     | 	confluent.quota.tenant.broker.max.consumer.rate = 13107200
benchi-kafka     | 	confluent.quota.tenant.broker.max.producer.rate = 13107200
benchi-kafka     | 	confluent.quota.tenant.default.controller.mutation.rate = 2.147483647E9
benchi-kafka     | 	confluent.quota.tenant.default.producer.id.rate = 2.147483647E9
benchi-kafka     | 	confluent.quota.tenant.fetch.multiplier = 1.0
benchi-kafka     | 	confluent.quota.tenant.follower.broker.min.consumer.rate = 10485760
benchi-kafka     | 	confluent.quota.tenant.follower.broker.min.producer.rate = 10485760
benchi-kafka     | 	confluent.quota.tenant.produce.multiplier = 1.0
benchi-kafka     | 	confluent.quota.tenant.user.quotas.enable = false
benchi-kafka     | 	confluent.regional.metadata.client.class = null
benchi-kafka     | 	confluent.regional.resource.manager.client.scheduler.threads = 2
benchi-kafka     | 	confluent.regional.resource.manager.endpoint = null
benchi-kafka     | 	confluent.regional.resource.manager.watch.endpoint = null
benchi-kafka     | 	confluent.replica.fetch.backoff.max.ms = 1000
benchi-kafka     | 	confluent.replica.fetch.connections.mode = combined
benchi-kafka     | 	confluent.replication.mode = PULL
benchi-kafka     | 	confluent.replication.push.feature.enable = false
benchi-kafka     | 	confluent.reporters.telemetry.auto.enable = true
benchi-kafka     | 	confluent.request.log.filter.class = class org.apache.kafka.common.requests.SamplingRequestLogFilter
benchi-kafka     | 	confluent.request.pipelining.enable = true
benchi-kafka     | 	confluent.request.pipelining.max.in.flight.requests.per.connection = 5
benchi-kafka     | 	confluent.require.calling.resource.identity = false
benchi-kafka     | 	confluent.require.compatible.keystore.updates = true
benchi-kafka     | 	confluent.require.confluent.issuer = false
benchi-kafka     | 	confluent.roll.check.interval.ms = 300000
benchi-kafka     | 	confluent.schema.registry.max.cache.size = 10000
benchi-kafka     | 	confluent.schema.registry.max.retries = 1
benchi-kafka     | 	confluent.schema.registry.retries.wait.ms = 0
benchi-kafka     | 	confluent.schema.registry.url = null
benchi-kafka     | 	confluent.schema.validator.interceptor.class = io.confluent.kafka.schemaregistry.validator.RecordSchemaValidator
benchi-kafka     | 	confluent.schema.validator.multitenant.enable = false
benchi-kafka     | 	confluent.schema.validator.samples.per.min = 0
benchi-kafka     | 	confluent.security.bc.approved.mode.enable = false
benchi-kafka     | 	confluent.security.event.logger.authentication.enable = false
benchi-kafka     | 	confluent.security.event.logger.authentication.event.rate.limit = -1
benchi-kafka     | 	confluent.security.event.logger.authorization.event.rate.limit = -1
benchi-kafka     | 	confluent.security.event.logger.detailed.audit.logs.filter.class = class org.apache.kafka.common.requests.DetailedRequestAuditLogFilter
benchi-kafka     | 	confluent.security.event.logger.enable = true
benchi-kafka     | 	confluent.security.event.logger.kafka.request.rate.limit = -1
benchi-kafka     | 	confluent.security.event.logger.physical.cluster.id = 
benchi-kafka     | 	confluent.security.event.router.config = 
benchi-kafka     | 	confluent.security.revoked.certificate.ids = 
benchi-kafka     | 	confluent.segment.eager.roll.enable = false
benchi-kafka     | 	confluent.segment.speculative.prefetch.enable = false
benchi-kafka     | 	confluent.spiffe.id.principal.extraction.rules = 
benchi-kafka     | 	confluent.ssl.key.password = null
benchi-kafka     | 	confluent.ssl.keystore.location = null
benchi-kafka     | 	confluent.ssl.keystore.password = null
benchi-kafka     | 	confluent.ssl.keystore.type = null
benchi-kafka     | 	confluent.ssl.protocol = null
benchi-kafka     | 	confluent.ssl.truststore.location = null
benchi-kafka     | 	confluent.ssl.truststore.password = null
benchi-kafka     | 	confluent.ssl.truststore.type = null
benchi-kafka     | 	confluent.step.connection.rate.per.ip = -1.0
benchi-kafka     | 	confluent.step.connection.rate.per.tenant = -1.0
benchi-kafka     | 	confluent.storage.probe.period.ms = -1
benchi-kafka     | 	confluent.storage.probe.slow.write.threshold.ms = 5000
benchi-kafka     | 	confluent.stray.log.delete.delay.ms = 604800000
benchi-kafka     | 	confluent.stray.log.max.deletions.per.run = 72
benchi-kafka     | 	confluent.subdomain.prefix = null
benchi-kafka     | 	confluent.subdomain.separator.map = null
benchi-kafka     | 	confluent.subdomain.separator.variable = %sep
benchi-kafka     | 	confluent.system.time.roll.enable = false
benchi-kafka     | 	confluent.telemetry.enabled = false
benchi-kafka     | 	confluent.telemetry.external.client.metrics.push.enabled = false
benchi-kafka     | 	confluent.tenant.latency.metric.enabled = false
benchi-kafka     | 	confluent.tier.archiver.num.threads = 2
benchi-kafka     | 	confluent.tier.azure.block.blob.auto.abort.threshold.bytes = 500000
benchi-kafka     | 	confluent.tier.azure.block.blob.container = null
benchi-kafka     | 	confluent.tier.azure.block.blob.cred.file.path = null
benchi-kafka     | 	confluent.tier.azure.block.blob.endpoint = null
benchi-kafka     | 	confluent.tier.azure.block.blob.prefix = 
benchi-kafka     | 	confluent.tier.backend = 
benchi-kafka     | 	confluent.tier.bucket.probe.period.ms = -1
benchi-kafka     | 	confluent.tier.cleaner.compact.min.efficiency = 0.5
benchi-kafka     | 	confluent.tier.cleaner.compact.segment.min.bytes = 20971520
benchi-kafka     | 	confluent.tier.cleaner.dedupe.buffer.size = 134217728
benchi-kafka     | 	confluent.tier.cleaner.dual.compaction = false
benchi-kafka     | 	confluent.tier.cleaner.dual.compaction.validation.max.bytes = 1073741824
benchi-kafka     | 	confluent.tier.cleaner.dual.compaction.validation.percent = 0
benchi-kafka     | 	confluent.tier.cleaner.enable = false
benchi-kafka     | 	confluent.tier.cleaner.excluded.topics = [^_confluent.*]
benchi-kafka     | 	confluent.tier.cleaner.feature.enable = false
benchi-kafka     | 	confluent.tier.cleaner.io.buffer.load.factor = 0.9
benchi-kafka     | 	confluent.tier.cleaner.io.buffer.size = 10485760
benchi-kafka     | 	confluent.tier.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
benchi-kafka     | 	confluent.tier.cleaner.min.cleanable.ratio = 0.75
benchi-kafka     | 	confluent.tier.cleaner.num.threads = 2
benchi-kafka     | 	confluent.tier.enable = false
benchi-kafka     | 	confluent.tier.feature = false
benchi-kafka     | 	confluent.tier.fenced.segment.delete.delay.ms = 600000
benchi-kafka     | 	confluent.tier.fetcher.async.enable = false
benchi-kafka     | 	confluent.tier.fetcher.async.timestamp.offset.parallelism = 1
benchi-kafka     | 	confluent.tier.fetcher.fetch.based.on.segment_and_metadata_layout.field = false
benchi-kafka     | 	confluent.tier.fetcher.memorypool.bytes = 0
benchi-kafka     | 	confluent.tier.fetcher.num.threads = 4
benchi-kafka     | 	confluent.tier.fetcher.offset.cache.expiration.ms = 1800000
benchi-kafka     | 	confluent.tier.fetcher.offset.cache.period.ms = 60000
benchi-kafka     | 	confluent.tier.fetcher.offset.cache.size = 200000
benchi-kafka     | 	confluent.tier.gcs.bucket = null
benchi-kafka     | 	confluent.tier.gcs.cred.file.path = null
benchi-kafka     | 	confluent.tier.gcs.prefix = 
benchi-kafka     | 	confluent.tier.gcs.region = null
benchi-kafka     | 	confluent.tier.gcs.sse.customer.encryption.key = null
benchi-kafka     | 	confluent.tier.gcs.write.chunk.size = 0
benchi-kafka     | 	confluent.tier.local.hotset.bytes = -1
benchi-kafka     | 	confluent.tier.local.hotset.ms = 86400000
benchi-kafka     | 	confluent.tier.max.partition.fetch.bytes.override = 0
benchi-kafka     | 	confluent.tier.metadata.bootstrap.servers = null
benchi-kafka     | 	confluent.tier.metadata.max.poll.ms = 100
benchi-kafka     | 	confluent.tier.metadata.namespace = null
benchi-kafka     | 	confluent.tier.metadata.num.partitions = 50
benchi-kafka     | 	confluent.tier.metadata.replication.factor = 1
benchi-kafka     | 	confluent.tier.metadata.request.timeout.ms = 30000
benchi-kafka     | 	confluent.tier.metadata.snapshots.enable = false
benchi-kafka     | 	confluent.tier.metadata.snapshots.interval.ms = 86400000
benchi-kafka     | 	confluent.tier.metadata.snapshots.retention.days = 7
benchi-kafka     | 	confluent.tier.metadata.snapshots.threads = 2
benchi-kafka     | 	confluent.tier.object.fetcher.num.threads = 1
benchi-kafka     | 	confluent.tier.partition.state.cleanup.delay.ms = 2592000000
benchi-kafka     | 	confluent.tier.partition.state.cleanup.enable = false
benchi-kafka     | 	confluent.tier.partition.state.cleanup.interval.ms = 86400000
benchi-kafka     | 	confluent.tier.partition.state.commit.interval.ms = 15000
benchi-kafka     | 	confluent.tier.s3.assumerole.arn = null
benchi-kafka     | 	confluent.tier.s3.auto.abort.threshold.bytes = 500000
benchi-kafka     | 	confluent.tier.s3.aws.endpoint.override = null
benchi-kafka     | 	confluent.tier.s3.aws.signer.override = null
benchi-kafka     | 	confluent.tier.s3.bucket = null
benchi-kafka     | 	confluent.tier.s3.cred.file.path = null
benchi-kafka     | 	confluent.tier.s3.force.path.style.access = false
benchi-kafka     | 	confluent.tier.s3.prefix = 
benchi-kafka     | 	confluent.tier.s3.region = null
benchi-kafka     | 	confluent.tier.s3.security.providers = null
benchi-kafka     | 	confluent.tier.s3.sse.algorithm = AES256
benchi-kafka     | 	confluent.tier.s3.sse.customer.encryption.key = null
benchi-kafka     | 	confluent.tier.s3.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	confluent.tier.s3.ssl.key.password = null
benchi-kafka     | 	confluent.tier.s3.ssl.keystore.location = null
benchi-kafka     | 	confluent.tier.s3.ssl.keystore.password = null
benchi-kafka     | 	confluent.tier.s3.ssl.keystore.type = null
benchi-kafka     | 	confluent.tier.s3.ssl.protocol = TLSv1.3
benchi-kafka     | 	confluent.tier.s3.ssl.provider = null
benchi-kafka     | 	confluent.tier.s3.ssl.truststore.location = null
benchi-kafka     | 	confluent.tier.s3.ssl.truststore.password = null
benchi-kafka     | 	confluent.tier.s3.ssl.truststore.type = null
benchi-kafka     | 	confluent.tier.s3.user.agent.prefix = APN/1.0 Confluent/1.0 TieredStorageS3/1.0
benchi-kafka     | 	confluent.tier.segment.hotset.roll.min.bytes = 104857600
benchi-kafka     | 	confluent.tier.segment.metadata.layout.put.mode = LegacyMultiObject
benchi-kafka     | 	confluent.tier.topic.data.loss.validation.fencing.enable = false
benchi-kafka     | 	confluent.tier.topic.delete.backoff.ms = 21600000
benchi-kafka     | 	confluent.tier.topic.delete.check.interval.ms = 300000
benchi-kafka     | 	confluent.tier.topic.delete.max.inprogress.partitions = 100
benchi-kafka     | 	confluent.tier.topic.head.data.loss.validation.enable = true
benchi-kafka     | 	confluent.tier.topic.head.data.loss.validation.max.timeout.ms = 900000
benchi-kafka     | 	confluent.tier.topic.materialization.from.snapshot.enable = false
benchi-kafka     | 	confluent.tier.topic.producer.enable.idempotence = true
benchi-kafka     | 	confluent.tier.topic.snapshots.enable = false
benchi-kafka     | 	confluent.tier.topic.snapshots.interval.ms = 300000
benchi-kafka     | 	confluent.tier.topic.snapshots.max.records = 100000
benchi-kafka     | 	confluent.tier.topic.snapshots.retention.hours = 168
benchi-kafka     | 	confluent.topic.partition.default.placement = 2
benchi-kafka     | 	confluent.topic.policy.use.computed.assignments = false
benchi-kafka     | 	confluent.topic.replica.assignor.builder.class = 
benchi-kafka     | 	confluent.track.api.key.per.ip = false
benchi-kafka     | 	confluent.track.per.ip.max.size = 100000
benchi-kafka     | 	confluent.track.tenant.id.per.ip = false
benchi-kafka     | 	confluent.traffic.cdc.network.id.routes.enable = false
benchi-kafka     | 	confluent.traffic.cdc.network.id.routes.listener.names = EXTERNAL_BACKCHANNEL
benchi-kafka     | 	confluent.traffic.cdc.network.id.routes.periodic.start.task.ms = 300000
benchi-kafka     | 	confluent.traffic.cdc.network.id.routes.topic.name = _confluent-network_id_routes
benchi-kafka     | 	confluent.traffic.network.id = 
benchi-kafka     | 	confluent.transaction.2pc.timeout.ms = -1
benchi-kafka     | 	confluent.transaction.logging.verbosity = 0
benchi-kafka     | 	confluent.transaction.state.log.placement.constraints = 
benchi-kafka     | 	confluent.valid.broker.rack.set = null
benchi-kafka     | 	confluent.verify.group.subscription.prefix = false
benchi-kafka     | 	confluent.virtual.topic.creation.enabled = false
benchi-kafka     | 	confluent.zookeeper.metadata.migration.controller.check.disable = false
benchi-kafka     | 	confluent.zookeeper.metadata.migration.trigger.file.path = null
benchi-kafka     | 	connection.failed.authentication.delay.ms = 100
benchi-kafka     | 	connection.min.expire.interval.ms = 250
benchi-kafka     | 	connections.max.age.ms = 3153600000000
benchi-kafka     | 	connections.max.idle.ms = 600000
benchi-kafka     | 	connections.max.reauth.ms = 0
benchi-kafka     | 	control.plane.listener.name = null
benchi-kafka     | 	controlled.shutdown.enable = true
benchi-kafka     | 	controlled.shutdown.max.retries = 3
benchi-kafka     | 	controlled.shutdown.retry.backoff.ms = 5000
benchi-kafka     | 	controller.listener.names = CONTROLLER
benchi-kafka     | 	controller.quorum.append.linger.ms = 25
benchi-kafka     | 	controller.quorum.bootstrap.servers = []
benchi-kafka     | 	controller.quorum.election.backoff.max.ms = 1000
benchi-kafka     | 	controller.quorum.election.timeout.ms = 1000
benchi-kafka     | 	controller.quorum.fetch.timeout.ms = 2000
benchi-kafka     | 	controller.quorum.request.timeout.ms = 2000
benchi-kafka     | 	controller.quorum.retry.backoff.ms = 20
benchi-kafka     | 	controller.quorum.voters = [1@broker:29093]
benchi-kafka     | 	controller.quota.window.num = 11
benchi-kafka     | 	controller.quota.window.size.seconds = 1
benchi-kafka     | 	controller.socket.timeout.ms = 30000
benchi-kafka     | 	create.cluster.link.policy.class.name = null
benchi-kafka     | 	create.topic.policy.class.name = null
benchi-kafka     | 	default.replication.factor = 1
benchi-kafka     | 	delegation.token.expiry.check.interval.ms = 3600000
benchi-kafka     | 	delegation.token.expiry.time.ms = 86400000
benchi-kafka     | 	delegation.token.master.key = null
benchi-kafka     | 	delegation.token.max.lifetime.ms = 604800000
benchi-kafka     | 	delegation.token.secret.key = null
benchi-kafka     | 	delete.records.purgatory.purge.interval.requests = 1
benchi-kafka     | 	delete.topic.enable = true
benchi-kafka     | 	early.start.listeners = null
benchi-kafka     | 	eligible.leader.replicas.enable = false
benchi-kafka     | 	enable.fips = false
benchi-kafka     | 	fetch.max.bytes = 57671680
benchi-kafka     | 	fetch.purgatory.purge.interval.requests = 1000
benchi-kafka     | 	floor.max.connection.creation.rate = null
benchi-kafka     | 	follower.replication.throttled.rate = 9223372036854775807
benchi-kafka     | 	follower.replication.throttled.replicas = none
benchi-kafka     | 	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
benchi-kafka     | 	group.consumer.heartbeat.interval.ms = 5000
benchi-kafka     | 	group.consumer.max.heartbeat.interval.ms = 15000
benchi-kafka     | 	group.consumer.max.session.timeout.ms = 60000
benchi-kafka     | 	group.consumer.max.size = 2147483647
benchi-kafka     | 	group.consumer.migration.policy = disabled
benchi-kafka     | 	group.consumer.min.heartbeat.interval.ms = 5000
benchi-kafka     | 	group.consumer.min.session.timeout.ms = 45000
benchi-kafka     | 	group.consumer.session.timeout.ms = 45000
benchi-kafka     | 	group.coordinator.append.linger.ms = 10
benchi-kafka     | 	group.coordinator.new.enable = false
benchi-kafka     | 	group.coordinator.rebalance.protocols = [classic]
benchi-kafka     | 	group.coordinator.threads = 1
benchi-kafka     | 	group.initial.rebalance.delay.ms = 0
benchi-kafka     | 	group.max.session.timeout.ms = 1800000
benchi-kafka     | 	group.max.size = 2147483647
benchi-kafka     | 	group.min.session.timeout.ms = 6000
benchi-kafka     | 	initial.broker.registration.timeout.ms = 60000
benchi-kafka     | 	inter.broker.listener.name = PLAINTEXT
benchi-kafka     | 	inter.broker.protocol.version = 3.8-IV0A
benchi-kafka     | 	k2.stack.builder.class.name = null
benchi-kafka     | 	k2.startup.timeout.ms = 60000
benchi-kafka     | 	k2.topic.metadata.max.total.partition.count = 20000
benchi-kafka     | 	k2.topic.metadata.refresh.ms = 10000
benchi-kafka     | 	kafka.metrics.polling.interval.secs = 10
benchi-kafka     | 	kafka.metrics.reporters = []
benchi-kafka     | 	leader.imbalance.check.interval.seconds = 300
benchi-kafka     | 	leader.imbalance.per.broker.percentage = 10
benchi-kafka     | 	leader.replication.throttled.rate = 9223372036854775807
benchi-kafka     | 	leader.replication.throttled.replicas = none
benchi-kafka     | 	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
benchi-kafka     | 	listeners = PLAINTEXT://broker:29092,CONTROLLER://broker:29093,PLAINTEXT_HOST://0.0.0.0:9092
benchi-kafka     | 	log.cleaner.backoff.ms = 15000
benchi-kafka     | 	log.cleaner.dedupe.buffer.size = 134217728
benchi-kafka     | 	log.cleaner.delete.retention.ms = 86400000
benchi-kafka     | 	log.cleaner.enable = true
benchi-kafka     | 	log.cleaner.hash.algorithm = MD5
benchi-kafka     | 	log.cleaner.io.buffer.load.factor = 0.9
benchi-kafka     | 	log.cleaner.io.buffer.size = 524288
benchi-kafka     | 	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
benchi-kafka     | 	log.cleaner.max.compaction.lag.ms = 9223372036854775807
benchi-kafka     | 	log.cleaner.min.cleanable.ratio = 0.5
benchi-kafka     | 	log.cleaner.min.compaction.lag.ms = 0
benchi-kafka     | 	log.cleaner.threads = 1
benchi-kafka     | 	log.cleanup.policy = [delete]
benchi-kafka     | 	log.deletion.max.segments.per.run = 2147483647
benchi-kafka     | 	log.deletion.throttler.disk.free.headroom.bytes = 21474836480
benchi-kafka     | 	log.dir = /tmp/kafka-logs
benchi-kafka     | 	log.dir.failure.timeout.ms = 30000
benchi-kafka     | 	log.dirs = /tmp/kraft-combined-logs
benchi-kafka     | 	log.flush.interval.messages = 9223372036854775807
benchi-kafka     | 	log.flush.interval.ms = null
benchi-kafka     | 	log.flush.offset.checkpoint.interval.ms = 60000
benchi-kafka     | 	log.flush.scheduler.interval.ms = 9223372036854775807
benchi-kafka     | 	log.flush.start.offset.checkpoint.interval.ms = 60000
benchi-kafka     | 	log.index.interval.bytes = 4096
benchi-kafka     | 	log.index.size.max.bytes = 10485760
benchi-kafka     | 	log.initial.task.delay.ms = 30000
benchi-kafka     | 	log.local.retention.bytes = -2
benchi-kafka     | 	log.local.retention.ms = -2
benchi-kafka     | 	log.message.downconversion.enable = true
benchi-kafka     | 	log.message.format.version = 3.0-IV1
benchi-kafka     | 	log.message.timestamp.after.max.ms = 9223372036854775807
benchi-kafka     | 	log.message.timestamp.before.max.ms = 9223372036854775807
benchi-kafka     | 	log.message.timestamp.difference.max.ms = 9223372036854775807
benchi-kafka     | 	log.message.timestamp.type = CreateTime
benchi-kafka     | 	log.preallocate = false
benchi-kafka     | 	log.retention.bytes = -1
benchi-kafka     | 	log.retention.check.interval.ms = 300000
benchi-kafka     | 	log.retention.hours = 168
benchi-kafka     | 	log.retention.minutes = null
benchi-kafka     | 	log.retention.ms = null
benchi-kafka     | 	log.roll.hours = 168
benchi-kafka     | 	log.roll.jitter.hours = 0
benchi-kafka     | 	log.roll.jitter.ms = null
benchi-kafka     | 	log.roll.ms = null
benchi-kafka     | 	log.segment.bytes = 1073741824
benchi-kafka     | 	log.segment.delete.delay.ms = 60000
benchi-kafka     | 	max.connection.creation.rate = 1.7976931348623157E308
benchi-kafka     | 	max.connection.creation.rate.per.ip.enable.threshold = 0.0
benchi-kafka     | 	max.connection.creation.rate.per.tenant.enable.threshold = 0.0
benchi-kafka     | 	max.connections = 2147483647
benchi-kafka     | 	max.connections.per.ip = 2147483647
benchi-kafka     | 	max.connections.per.ip.overrides = 
benchi-kafka     | 	max.connections.per.tenant = 0
benchi-kafka     | 	max.connections.protected.listeners = []
benchi-kafka     | 	max.connections.reap.amount = 0
benchi-kafka     | 	max.incremental.fetch.session.cache.slots = 1000
benchi-kafka     | 	max.request.partition.size.limit = 2000
benchi-kafka     | 	message.max.bytes = 1048588
benchi-kafka     | 	metadata.log.dir = null
benchi-kafka     | 	metadata.log.max.record.bytes.between.snapshots = 20971520
benchi-kafka     | 	metadata.log.max.snapshot.interval.ms = 3600000
benchi-kafka     | 	metadata.log.segment.bytes = 1073741824
benchi-kafka     | 	metadata.log.segment.min.bytes = 8388608
benchi-kafka     | 	metadata.log.segment.ms = 604800000
benchi-kafka     | 	metadata.max.idle.interval.ms = 500
benchi-kafka     | 	metadata.max.retention.bytes = 104857600
benchi-kafka     | 	metadata.max.retention.ms = 604800000
benchi-kafka     | 	metric.reporters = [io.confluent.metrics.reporter.ConfluentMetricsReporter]
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.recording.level = INFO
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	min.insync.replicas = 1
benchi-kafka     | 	multitenant.authorizer.support.resource.ids = false
benchi-kafka     | 	multitenant.metadata.class = null
benchi-kafka     | 	multitenant.metadata.dir = null
benchi-kafka     | 	multitenant.metadata.reload.delay.ms = 120000
benchi-kafka     | 	multitenant.metadata.ssl.certs.path = null
benchi-kafka     | 	multitenant.tenant.delete.batch.size = 10
benchi-kafka     | 	multitenant.tenant.delete.check.ms = 120000
benchi-kafka     | 	multitenant.tenant.delete.delay = 604800000
benchi-kafka     | 	node.id = 1
benchi-kafka     | 	num.io.threads = 8
benchi-kafka     | 	num.network.threads = 3
benchi-kafka     | 	num.partitions = 1
benchi-kafka     | 	num.recovery.threads.per.data.dir = 1
benchi-kafka     | 	num.replica.alter.log.dirs.threads = null
benchi-kafka     | 	num.replica.fetchers = 1
benchi-kafka     | 	offset.metadata.max.bytes = 4096
benchi-kafka     | 	offsets.commit.required.acks = -1
benchi-kafka     | 	offsets.commit.timeout.ms = 5000
benchi-kafka     | 	offsets.load.buffer.size = 5242880
benchi-kafka     | 	offsets.retention.check.interval.ms = 600000
benchi-kafka     | 	offsets.retention.minutes = 10080
benchi-kafka     | 	offsets.topic.compression.codec = 0
benchi-kafka     | 	offsets.topic.num.partitions = 50
benchi-kafka     | 	offsets.topic.replication.factor = 1
benchi-kafka     | 	offsets.topic.segment.bytes = 104857600
benchi-kafka     | 	otel.exporter.otlp.custom.endpoint = default
benchi-kafka     | 	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
benchi-kafka     | 	password.encoder.iterations = 4096
benchi-kafka     | 	password.encoder.key.length = 128
benchi-kafka     | 	password.encoder.keyfactory.algorithm = null
benchi-kafka     | 	password.encoder.old.secret = null
benchi-kafka     | 	password.encoder.secret = null
benchi-kafka     | 	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
benchi-kafka     | 	process.roles = [broker, controller]
benchi-kafka     | 	producer.id.expiration.check.interval.ms = 600000
benchi-kafka     | 	producer.id.expiration.ms = 86400000
benchi-kafka     | 	producer.purgatory.purge.interval.requests = 1000
benchi-kafka     | 	queued.max.request.bytes = -1
benchi-kafka     | 	queued.max.requests = 500
benchi-kafka     | 	quota.window.num = 11
benchi-kafka     | 	quota.window.size.seconds = 1
benchi-kafka     | 	quotas.consumption.expiration.time.ms = 600000
benchi-kafka     | 	quotas.expiration.interval.ms = 3600000
benchi-kafka     | 	quotas.expiration.time.ms = 604800000
benchi-kafka     | 	quotas.lazy.evaluation.threshold = 0.5
benchi-kafka     | 	quotas.topic.append.timeout.ms = 5000
benchi-kafka     | 	quotas.topic.compression.codec = 3
benchi-kafka     | 	quotas.topic.load.buffer.size = 5242880
benchi-kafka     | 	quotas.topic.num.partitions = 50
benchi-kafka     | 	quotas.topic.placement.constraints = 
benchi-kafka     | 	quotas.topic.replication.factor = 3
benchi-kafka     | 	quotas.topic.segment.bytes = 104857600
benchi-kafka     | 	remote.fetch.max.wait.ms = 500
benchi-kafka     | 	remote.log.index.file.cache.total.size.bytes = 1073741824
benchi-kafka     | 	remote.log.manager.copier.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.copy.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.copy.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.expiration.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.fetch.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.fetch.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.task.interval.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.max.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.ms = 500
benchi-kafka     | 	remote.log.manager.task.retry.jitter = 0.2
benchi-kafka     | 	remote.log.manager.thread.pool.size = 10
benchi-kafka     | 	remote.log.metadata.custom.metadata.max.bytes = 128
benchi-kafka     | 	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
benchi-kafka     | 	remote.log.metadata.manager.class.path = null
benchi-kafka     | 	remote.log.metadata.manager.impl.prefix = rlmm.config.
benchi-kafka     | 	remote.log.metadata.manager.listener.name = null
benchi-kafka     | 	remote.log.reader.max.pending.tasks = 100
benchi-kafka     | 	remote.log.reader.threads = 10
benchi-kafka     | 	remote.log.storage.manager.class.name = null
benchi-kafka     | 	remote.log.storage.manager.class.path = null
benchi-kafka     | 	remote.log.storage.manager.impl.prefix = rsm.config.
benchi-kafka     | 	remote.log.storage.system.enable = false
benchi-kafka     | 	replica.fetch.backoff.ms = 1000
benchi-kafka     | 	replica.fetch.max.bytes = 1048576
benchi-kafka     | 	replica.fetch.min.bytes = 1
benchi-kafka     | 	replica.fetch.response.max.bytes = 10485760
benchi-kafka     | 	replica.fetch.wait.max.ms = 500
benchi-kafka     | 	replica.high.watermark.checkpoint.interval.ms = 5000
benchi-kafka     | 	replica.lag.time.max.ms = 30000
benchi-kafka     | 	replica.selector.class = null
benchi-kafka     | 	replica.socket.receive.buffer.bytes = 65536
benchi-kafka     | 	replica.socket.timeout.ms = 30000
benchi-kafka     | 	replication.quota.window.num = 11
benchi-kafka     | 	replication.quota.window.size.seconds = 1
benchi-kafka     | 	request.timeout.ms = 30000
benchi-kafka     | 	reserved.broker.max.id = 1000
benchi-kafka     | 	sasl.client.callback.handler.class = null
benchi-kafka     | 	sasl.enabled.mechanisms = [GSSAPI]
benchi-kafka     | 	sasl.jaas.config = null
benchi-kafka     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
benchi-kafka     | 	sasl.kerberos.min.time.before.relogin = 60000
benchi-kafka     | 	sasl.kerberos.principal.to.local.rules = [DEFAULT]
benchi-kafka     | 	sasl.kerberos.service.name = null
benchi-kafka     | 	sasl.kerberos.ticket.renew.jitter = 0.05
benchi-kafka     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
benchi-kafka     | 	sasl.login.callback.handler.class = null
benchi-kafka     | 	sasl.login.class = null
benchi-kafka     | 	sasl.login.connect.timeout.ms = null
benchi-kafka     | 	sasl.login.read.timeout.ms = null
benchi-kafka     | 	sasl.login.refresh.buffer.seconds = 300
benchi-kafka     | 	sasl.login.refresh.min.period.seconds = 60
benchi-kafka     | 	sasl.login.refresh.window.factor = 0.8
benchi-kafka     | 	sasl.login.refresh.window.jitter = 0.05
benchi-kafka     | 	sasl.login.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.login.retry.backoff.ms = 100
benchi-kafka     | 	sasl.mechanism.controller.protocol = GSSAPI
benchi-kafka     | 	sasl.mechanism.inter.broker.protocol = GSSAPI
benchi-kafka     | 	sasl.oauthbearer.clientassertion.audience = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.expiration = 5
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.jti.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.nbf.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.issuer = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.location = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key.passphrase = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.subject = null
benchi-kafka     | 	sasl.oauthbearer.clock.skew.seconds = 30
benchi-kafka     | 	sasl.oauthbearer.expected.audience = null
benchi-kafka     | 	sasl.oauthbearer.expected.issuer = null
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.url = null
benchi-kafka     | 	sasl.oauthbearer.scope.claim.name = scope
benchi-kafka     | 	sasl.oauthbearer.sub.claim.name = sub
benchi-kafka     | 	sasl.oauthbearer.token.endpoint.url = null
benchi-kafka     | 	sasl.server.authn.async.enable = false
benchi-kafka     | 	sasl.server.authn.async.max.threads = 1
benchi-kafka     | 	sasl.server.authn.async.timeout.ms = 30000
benchi-kafka     | 	sasl.server.callback.handler.class = null
benchi-kafka     | 	sasl.server.max.receive.size = 524288
benchi-kafka     | 	security.inter.broker.protocol = PLAINTEXT
benchi-kafka     | 	security.providers = null
benchi-kafka     | 	server.max.startup.time.ms = 9223372036854775807
benchi-kafka     | 	socket.connection.setup.timeout.max.ms = 30000
benchi-kafka     | 	socket.connection.setup.timeout.ms = 10000
benchi-kafka     | 	socket.listen.backlog.size = 50
benchi-kafka     | 	socket.receive.buffer.bytes = 102400
benchi-kafka     | 	socket.request.max.bytes = 104857600
benchi-kafka     | 	socket.send.buffer.bytes = 102400
benchi-kafka     | 	ssl.allow.dn.changes = false
benchi-kafka     | 	ssl.allow.san.changes = false
benchi-kafka     | 	ssl.cipher.suites = []
benchi-kafka     | 	ssl.client.auth = none
benchi-kafka     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	ssl.engine.factory.class = null
benchi-kafka     | 	ssl.key.password = null
benchi-kafka     | 	ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	ssl.keystore.certificate.chain = null
benchi-kafka     | 	ssl.keystore.key = null
benchi-kafka     | 	ssl.keystore.location = null
benchi-kafka     | 	ssl.keystore.password = null
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.principal.mapping.rules = DEFAULT
benchi-kafka     | 	ssl.protocol = TLSv1.3
benchi-kafka     | 	ssl.provider = null
benchi-kafka     | 	ssl.secure.random.implementation = null
benchi-kafka     | 	ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	ssl.truststore.certificates = null
benchi-kafka     | 	ssl.truststore.location = null
benchi-kafka     | 	ssl.truststore.password = null
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     | 	telemetry.max.bytes = 1048576
benchi-kafka     | 	throughput.quota.window.num = 11
benchi-kafka     | 	token.impersonation.validation = true
benchi-kafka     | 	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
benchi-kafka     | 	transaction.max.timeout.ms = 900000
benchi-kafka     | 	transaction.metadata.load.threads = 32
benchi-kafka     | 	transaction.partition.verification.enable = true
benchi-kafka     | 	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
benchi-kafka     | 	transaction.state.log.load.buffer.size = 5242880
benchi-kafka     | 	transaction.state.log.min.isr = 1
benchi-kafka     | 	transaction.state.log.num.partitions = 50
benchi-kafka     | 	transaction.state.log.replication.factor = 1
benchi-kafka     | 	transaction.state.log.segment.bytes = 104857600
benchi-kafka     | 	transactional.id.expiration.ms = 604800000
benchi-kafka     | 	unclean.leader.election.enable = false
benchi-kafka     | 	unstable.api.versions.enable = false
benchi-kafka     | 	unstable.feature.versions.enable = false
benchi-kafka     | 	zookeeper.acl.change.notification.expiration.ms = 900000
benchi-kafka     | 	zookeeper.clientCnxnSocket = null
benchi-kafka     | 	zookeeper.connect = 
benchi-kafka     | 	zookeeper.connection.timeout.ms = null
benchi-kafka     | 	zookeeper.max.in.flight.requests = 10
benchi-kafka     | 	zookeeper.metadata.migration.enable = false
benchi-kafka     | 	zookeeper.metadata.migration.min.batch.size = 200
benchi-kafka     | 	zookeeper.session.timeout.ms = 18000
benchi-kafka     | 	zookeeper.set.acl = false
benchi-kafka     | 	zookeeper.ssl.cipher.suites = null
benchi-kafka     | 	zookeeper.ssl.client.enable = false
benchi-kafka     | 	zookeeper.ssl.crl.enable = false
benchi-kafka     | 	zookeeper.ssl.enabled.protocols = null
benchi-kafka     | 	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
benchi-kafka     | 	zookeeper.ssl.keystore.location = null
benchi-kafka     | 	zookeeper.ssl.keystore.password = null
benchi-kafka     | 	zookeeper.ssl.keystore.type = null
benchi-kafka     | 	zookeeper.ssl.ocsp.enable = false
benchi-kafka     | 	zookeeper.ssl.protocol = TLSv1.2
benchi-kafka     | 	zookeeper.ssl.truststore.location = null
benchi-kafka     | 	zookeeper.ssl.truststore.password = null
benchi-kafka     | 	zookeeper.ssl.truststore.type = null
benchi-kafka     |  (kafka.server.KafkaConfig)
benchi-kafka     | [2025-03-18 16:08:11,393] INFO RemoteLogManagerConfig values: 
benchi-kafka     | 	log.local.retention.bytes = -2
benchi-kafka     | 	log.local.retention.ms = -2
benchi-kafka     | 	remote.fetch.max.wait.ms = 500
benchi-kafka     | 	remote.log.index.file.cache.total.size.bytes = 1073741824
benchi-kafka     | 	remote.log.manager.copier.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.copy.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.copy.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.expiration.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.fetch.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.fetch.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.task.interval.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.max.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.ms = 500
benchi-kafka     | 	remote.log.manager.task.retry.jitter = 0.2
benchi-kafka     | 	remote.log.manager.thread.pool.size = 10
benchi-kafka     | 	remote.log.metadata.custom.metadata.max.bytes = 128
benchi-kafka     | 	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
benchi-kafka     | 	remote.log.metadata.manager.class.path = null
benchi-kafka     | 	remote.log.metadata.manager.impl.prefix = rlmm.config.
benchi-kafka     | 	remote.log.metadata.manager.listener.name = null
benchi-kafka     | 	remote.log.reader.max.pending.tasks = 100
benchi-kafka     | 	remote.log.reader.threads = 10
benchi-kafka     | 	remote.log.storage.manager.class.name = null
benchi-kafka     | 	remote.log.storage.manager.class.path = null
benchi-kafka     | 	remote.log.storage.manager.impl.prefix = rsm.config.
benchi-kafka     | 	remote.log.storage.system.enable = false
benchi-kafka     |  (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
benchi-kafka     | [2025-03-18 16:08:11,396] INFO [BrokerServer id=1] Waiting for the broker to be unfenced (kafka.server.BrokerServer)
benchi-kafka     | [2025-03-18 16:08:11,397] INFO [ControllerServer id=1] The request from broker 1 to unfence has been granted because it has caught up with the offset of its register broker record 6. (org.apache.kafka.controller.BrokerHeartbeatManager)
benchi-kafka     | [2025-03-18 16:08:11,400] INFO [ControllerServer id=1] Replayed BrokerRegistrationChangeRecord modifying the registration for broker 1: BrokerRegistrationChangeRecord(brokerId=1, brokerEpoch=6, fenced=-1, inControlledShutdown=0, degradedComponents=null, logDirs=[]) (org.apache.kafka.controller.ClusterControlManager)
benchi-kafka     | [2025-03-18 16:08:11,427] INFO [BrokerLifecycleManager id=1] The broker has been unfenced. Transitioning from RECOVERY to RUNNING. (kafka.server.BrokerLifecycleManager)
benchi-kafka     | [2025-03-18 16:08:11,427] INFO [BrokerServer id=1] Finished waiting for the broker to be unfenced (kafka.server.BrokerServer)
benchi-kafka     | [2025-03-18 16:08:11,427] INFO SBC Event SbcMetadataUpdateEvent-9 generated 1 more events to enqueue in the following order - [SbcKraftBrokerAdditionEvent-10]. Enqueuing... (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:08:11,427] INFO [SocketServer listenerType=BROKER, nodeId=1] Enabling request processing. (kafka.network.SocketServer)
benchi-kafka     | [2025-03-18 16:08:11,427] INFO Handling event SbcConfigUpdateEvent-3 (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:08:11,427] INFO Awaiting socket connections on broker:29092. (kafka.network.DataPlaneAcceptor)
benchi-kafka     | [2025-03-18 16:08:11,428] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.DataPlaneAcceptor)
benchi-kafka     | [2025-03-18 16:08:11,428] INFO Balancer notified of a config change: ConfigurationsDelta(changes={}) (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:08:11,428] INFO There were 0 change(s) and 0 deletion(s) to balancer configs. Changed Configs: {}, Deleted Configs: [] (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:08:11,428] INFO Handling event SbcKraftStartupEvent-5 (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:08:11,428] INFO Configs metadata not yet available, SBC startup delayed. (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:08:11,428] INFO Handling event SbcKraftBrokerAdditionEvent-10 (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:08:11,428] INFO Topics Image not present, pausing broker addition event of brokers (new brokers: [1]) until it is received. (io.confluent.databalancer.event.SbcKraftBrokerAdditionEvent)
benchi-kafka     | [2025-03-18 16:08:11,429] INFO [BrokerServer id=1] Waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
benchi-kafka     | [2025-03-18 16:08:11,429] INFO [BrokerServer id=1] Finished waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
benchi-kafka     | [2025-03-18 16:08:11,429] INFO [BrokerServer id=1] Waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
benchi-kafka     | [2025-03-18 16:08:11,429] INFO [BrokerServer id=1] Finished waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
benchi-kafka     | [2025-03-18 16:08:11,441] INFO RestConfig values: 
benchi-kafka     | 	access.control.allow.headers = 
benchi-kafka     | 	access.control.allow.methods = 
benchi-kafka     | 	access.control.allow.origin = 
benchi-kafka     | 	access.control.skip.options = true
benchi-kafka     | 	authentication.method = NONE
benchi-kafka     | 	authentication.realm = 
benchi-kafka     | 	authentication.roles = [*]
benchi-kafka     | 	authentication.skip.paths = []
benchi-kafka     | 	compression.enable = true
benchi-kafka     | 	connector.connection.limit = 0
benchi-kafka     | 	csrf.prevention.enable = false
benchi-kafka     | 	csrf.prevention.token.endpoint = /csrf
benchi-kafka     | 	csrf.prevention.token.expiration.minutes = 30
benchi-kafka     | 	csrf.prevention.token.max.entries = 10000
benchi-kafka     | 	debug = false
benchi-kafka     | 	dos.filter.delay.ms = 100
benchi-kafka     | 	dos.filter.enabled = false
benchi-kafka     | 	dos.filter.insert.headers = true
benchi-kafka     | 	dos.filter.ip.whitelist = []
benchi-kafka     | 	dos.filter.managed.attr = false
benchi-kafka     | 	dos.filter.max.idle.tracker.ms = 30000
benchi-kafka     | 	dos.filter.max.requests.ms = 30000
benchi-kafka     | 	dos.filter.max.requests.per.connection.per.sec = 25
benchi-kafka     | 	dos.filter.max.requests.per.sec = 25
benchi-kafka     | 	dos.filter.max.wait.ms = 50
benchi-kafka     | 	dos.filter.throttle.ms = 30000
benchi-kafka     | 	dos.filter.throttled.requests = 5
benchi-kafka     | 	http2.enabled = true
benchi-kafka     | 	idle.timeout.ms = 30000
benchi-kafka     | 	listener.protocol.map = []
benchi-kafka     | 	listeners = []
benchi-kafka     | 	max.request.header.size = 8192
benchi-kafka     | 	max.response.header.size = 8192
benchi-kafka     | 	metric.reporters = []
benchi-kafka     | 	metrics.global.stats.request.tags.enable = false
benchi-kafka     | 	metrics.jmx.prefix = rest-utils
benchi-kafka     | 	metrics.latency.sla.ms = 50
benchi-kafka     | 	metrics.latency.slo.ms = 5
benchi-kafka     | 	metrics.latency.slo.sla.enable = false
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	metrics.tag.map = []
benchi-kafka     | 	network.traffic.rate.limit.backend = guava
benchi-kafka     | 	network.traffic.rate.limit.bytes.per.sec = 20971520
benchi-kafka     | 	network.traffic.rate.limit.enable = false
benchi-kafka     | 	nosniff.prevention.enable = false
benchi-kafka     | 	port = 8080
benchi-kafka     | 	proxy.protocol.enabled = false
benchi-kafka     | 	reject.options.request = false
benchi-kafka     | 	request.logger.name = io.confluent.rest-utils.requests
benchi-kafka     | 	request.queue.capacity = 2147483647
benchi-kafka     | 	request.queue.capacity.growby = 64
benchi-kafka     | 	request.queue.capacity.init = 128
benchi-kafka     | 	resource.extension.classes = []
benchi-kafka     | 	response.http.headers.config = 
benchi-kafka     | 	response.mediatype.default = application/json
benchi-kafka     | 	response.mediatype.preferred = [application/json]
benchi-kafka     | 	rest.servlet.initializor.classes = []
benchi-kafka     | 	server.connection.limit = 0
benchi-kafka     | 	shutdown.graceful.ms = 1000
benchi-kafka     | 	sni.check.enabled = false
benchi-kafka     | 	ssl.cipher.suites = []
benchi-kafka     | 	ssl.client.auth = false
benchi-kafka     | 	ssl.client.authentication = NONE
benchi-kafka     | 	ssl.enabled.protocols = []
benchi-kafka     | 	ssl.endpoint.identification.algorithm = null
benchi-kafka     | 	ssl.key.password = [hidden]
benchi-kafka     | 	ssl.keymanager.algorithm = 
benchi-kafka     | 	ssl.keystore.location = 
benchi-kafka     | 	ssl.keystore.password = [hidden]
benchi-kafka     | 	ssl.keystore.reload = false
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.keystore.watch.location = 
benchi-kafka     | 	ssl.protocol = TLS
benchi-kafka     | 	ssl.provider = 
benchi-kafka     | 	ssl.trustmanager.algorithm = 
benchi-kafka     | 	ssl.truststore.location = 
benchi-kafka     | 	ssl.truststore.password = [hidden]
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     | 	suppress.stack.trace.response = true
benchi-kafka     | 	thread.pool.max = 200
benchi-kafka     | 	thread.pool.min = 8
benchi-kafka     | 	websocket.path.prefix = /ws
benchi-kafka     | 	websocket.servlet.initializor.classes = []
benchi-kafka     |  (io.confluent.rest.RestConfig)
benchi-kafka     | [2025-03-18 16:08:11,445] INFO Logging initialized @1741ms to org.eclipse.jetty.util.log.Slf4jLog (org.eclipse.jetty.util.log)
benchi-kafka     | [2025-03-18 16:08:11,455] INFO Application provider 'MetadataApiApplicationProvider' provided 1 instance(s). (io.confluent.http.server.KafkaHttpApplicationLoader)
benchi-kafka     | [2025-03-18 16:08:11,456] INFO MetadataServerConfig values: 
benchi-kafka     | 	confluent.http.server.listeners = [http://0.0.0.0:8090]
benchi-kafka     | 	confluent.metadata.server.advertised.listeners = null
benchi-kafka     | 	confluent.metadata.server.enable = false
benchi-kafka     | 	confluent.metadata.server.kraft.controller.enabled = false
benchi-kafka     | 	confluent.metadata.server.listeners = null
benchi-kafka     |  (org.apache.kafka.server.http.MetadataServerConfig)
benchi-kafka     | [2025-03-18 16:08:11,456] INFO Application provider 'RbacApplicationProvider' did not provide any instances. (io.confluent.http.server.KafkaHttpApplicationLoader)
benchi-kafka     | [2025-03-18 16:08:11,458] INFO KafkaConfig values: 
benchi-kafka     | 	advertised.listeners = PLAINTEXT://broker:29092,PLAINTEXT_HOST://localhost:9092
benchi-kafka     | 	alter.config.policy.class.name = null
benchi-kafka     | 	alter.log.dirs.replication.quota.window.num = 11
benchi-kafka     | 	alter.log.dirs.replication.quota.window.size.seconds = 1
benchi-kafka     | 	authorizer.class.name = 
benchi-kafka     | 	auto.create.topics.enable = true
benchi-kafka     | 	auto.include.jmx.reporter = true
benchi-kafka     | 	auto.leader.rebalance.enable = true
benchi-kafka     | 	background.threads = 10
benchi-kafka     | 	broker.heartbeat.interval.ms = 2000
benchi-kafka     | 	broker.id = 1
benchi-kafka     | 	broker.id.generation.enable = true
benchi-kafka     | 	broker.interceptor.class = class org.apache.kafka.server.interceptor.DefaultBrokerInterceptor
benchi-kafka     | 	broker.rack = null
benchi-kafka     | 	broker.session.timeout.ms = 9000
benchi-kafka     | 	broker.session.uuid = iNf5MSGJTFOqkdMXHRqjNg
benchi-kafka     | 	client.quota.callback.class = null
benchi-kafka     | 	client.quota.max.throttle.time.ms = 5000
benchi-kafka     | 	compression.gzip.level = -1
benchi-kafka     | 	compression.lz4.level = 9
benchi-kafka     | 	compression.type = producer
benchi-kafka     | 	compression.zstd.level = 3
benchi-kafka     | 	confluent.alter.broker.health.max.demoted.brokers = 2147483647
benchi-kafka     | 	confluent.alter.broker.health.max.demoted.brokers.percentage = 0
benchi-kafka     | 	confluent.ansible.managed = false
benchi-kafka     | 	confluent.api.visibility = DEFAULT
benchi-kafka     | 	confluent.append.record.interceptor.classes = []
benchi-kafka     | 	confluent.apply.create.topic.policy.to.create.partitions = false
benchi-kafka     | 	confluent.authorizer.authority.name = 
benchi-kafka     | 	confluent.automatic.alter.broker.health.retry.backoff.ms = 2000
benchi-kafka     | 	confluent.backpressure.disk.enable = false
benchi-kafka     | 	confluent.backpressure.disk.free.threshold.bytes = 21474836480
benchi-kafka     | 	confluent.backpressure.disk.produce.bytes.per.second = 131072
benchi-kafka     | 	confluent.backpressure.disk.threshold.recovery.factor = 1.5
benchi-kafka     | 	confluent.backpressure.request.min.broker.limit = 200
benchi-kafka     | 	confluent.backpressure.request.queue.size.percentile = p95
benchi-kafka     | 	confluent.backpressure.types = null
benchi-kafka     | 	confluent.balancer.api.state.topic = _confluent_balancer_api_state
benchi-kafka     | 	confluent.balancer.broker.addition.elapsed.time.ms.completion.threshold = 57600000
benchi-kafka     | 	confluent.balancer.broker.addition.mean.cpu.percent.completion.threshold = 0.5
benchi-kafka     | 	confluent.balancer.capacity.threshold.upper.limit = 0.95
benchi-kafka     | 	confluent.balancer.cell.load.upper.bound = 0.7
benchi-kafka     | 	confluent.balancer.cell.overload.detection.interval.ms = 3600000
benchi-kafka     | 	confluent.balancer.cell.overload.duration.ms = 86400000
benchi-kafka     | 	confluent.balancer.class = io.confluent.databalancer.SbcDataBalanceManager
benchi-kafka     | 	confluent.balancer.consumer.out.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.balancer.cpu.balance.threshold = 1.1
benchi-kafka     | 	confluent.balancer.cpu.low.utilization.threshold = 0.2
benchi-kafka     | 	confluent.balancer.cpu.utilization.detector.duration.ms = 600000
benchi-kafka     | 	confluent.balancer.cpu.utilization.detector.enabled = false
benchi-kafka     | 	confluent.balancer.cpu.utilization.detector.overutilization.threshold = 80.0
benchi-kafka     | 	confluent.balancer.cpu.utilization.detector.underutilization.threshold = 50.0
benchi-kafka     | 	confluent.balancer.disk.max.load = 0.85
benchi-kafka     | 	confluent.balancer.disk.min.free.space.gb = 0
benchi-kafka     | 	confluent.balancer.disk.min.free.space.lower.limit.gb = 0
benchi-kafka     | 	confluent.balancer.disk.utilization.detector.duration.ms = 600000
benchi-kafka     | 	confluent.balancer.disk.utilization.detector.enabled = false
benchi-kafka     | 	confluent.balancer.disk.utilization.detector.overutilization.threshold = 80.0
benchi-kafka     | 	confluent.balancer.disk.utilization.detector.reserved.capacity = 150000.0
benchi-kafka     | 	confluent.balancer.disk.utilization.detector.underutilization.threshold = 35.0
benchi-kafka     | 	confluent.balancer.enable = true
benchi-kafka     | 	confluent.balancer.exclude.topic.names = []
benchi-kafka     | 	confluent.balancer.exclude.topic.prefixes = []
benchi-kafka     | 	confluent.balancer.goal.violation.delay.on.new.brokers.ms = 1800000
benchi-kafka     | 	confluent.balancer.goal.violation.distribution.threshold.multiplier = 1.1
benchi-kafka     | 	confluent.balancer.heal.broker.failure.threshold.ms = 3600000
benchi-kafka     | 	confluent.balancer.heal.uneven.load.trigger = EMPTY_BROKER
benchi-kafka     | 	confluent.balancer.incremental.balancing.cpu.top.proposal.tracking.enabled = true
benchi-kafka     | 	confluent.balancer.incremental.balancing.cpu.top.proposal.tracking.num.proposals = 15
benchi-kafka     | 	confluent.balancer.incremental.balancing.enabled = false
benchi-kafka     | 	confluent.balancer.incremental.balancing.goals = []
benchi-kafka     | 	confluent.balancer.incremental.balancing.lower.bound = 0.02
benchi-kafka     | 	confluent.balancer.incremental.balancing.min.valid.windows = 5
benchi-kafka     | 	confluent.balancer.incremental.balancing.step.ratio = 0.2
benchi-kafka     | 	confluent.balancer.inter.cell.balancing.enabled = false
benchi-kafka     | 	confluent.balancer.max.capacity.balancing.delta.percentage = 0.0
benchi-kafka     | 	confluent.balancer.max.replicas = 2147483647
benchi-kafka     | 	confluent.balancer.network.in.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.balancer.network.out.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.balancer.plan.computation.retry.timeout.ms = 3600000
benchi-kafka     | 	confluent.balancer.producer.in.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.balancer.rebalancing.goals = []
benchi-kafka     | 	confluent.balancer.replication.in.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.balancer.resource.utilization.detector.interval.ms = 60000
benchi-kafka     | 	confluent.balancer.sbc.metrics.parser.enabled = false
benchi-kafka     | 	confluent.balancer.self.healing.maximum.rounds = 1
benchi-kafka     | 	confluent.balancer.task.history.retention.days = 30
benchi-kafka     | 	confluent.balancer.tenant.maximum.movements = 0
benchi-kafka     | 	confluent.balancer.tenant.suspension.ms = 86400000
benchi-kafka     | 	confluent.balancer.throttle.bytes.per.second = 10485760
benchi-kafka     | 	confluent.balancer.topic.partition.maximum.movements = 5
benchi-kafka     | 	confluent.balancer.topic.partition.movement.expiration.ms = 10800000
benchi-kafka     | 	confluent.balancer.topic.partition.suspension.ms = 18000000
benchi-kafka     | 	confluent.balancer.topic.replication.factor = 1
benchi-kafka     | 	confluent.balancer.triggering.goals = []
benchi-kafka     | 	confluent.balancer.v2.addition.enabled = false
benchi-kafka     | 	confluent.basic.auth.credentials.source = null
benchi-kafka     | 	confluent.basic.auth.user.info = null
benchi-kafka     | 	confluent.bearer.auth.cache.expiry.buffer.seconds = 300
benchi-kafka     | 	confluent.bearer.auth.client.id = null
benchi-kafka     | 	confluent.bearer.auth.client.secret = null
benchi-kafka     | 	confluent.bearer.auth.credentials.source = null
benchi-kafka     | 	confluent.bearer.auth.identity.pool.id = null
benchi-kafka     | 	confluent.bearer.auth.issuer.endpoint.url = null
benchi-kafka     | 	confluent.bearer.auth.logical.cluster = null
benchi-kafka     | 	confluent.bearer.auth.scope = null
benchi-kafka     | 	confluent.bearer.auth.scope.claim.name = scope
benchi-kafka     | 	confluent.bearer.auth.sub.claim.name = sub
benchi-kafka     | 	confluent.bearer.auth.token = null
benchi-kafka     | 	confluent.broker.health.manager.enabled = true
benchi-kafka     | 	confluent.broker.health.manager.engine.request.handler.threads.stuck.criteria = AllThreadsStuck
benchi-kafka     | 	confluent.broker.health.manager.hard.kill.duration.ms = 60000
benchi-kafka     | 	confluent.broker.health.manager.mitigation.enabled = false
benchi-kafka     | 	confluent.broker.health.manager.num.samples.before.broker.suspect = 30
benchi-kafka     | 	confluent.broker.health.manager.num.samples.before.broker.unhealthy = 180
benchi-kafka     | 	confluent.broker.health.manager.percentage.unhealthy.samples.before.broker.suspect = 90
benchi-kafka     | 	confluent.broker.health.manager.percentage.unhealthy.samples.before.broker.unhealthy = 70
benchi-kafka     | 	confluent.broker.health.manager.sample.duration.ms = 1000
benchi-kafka     | 	confluent.broker.health.manager.storage.background.threads.stuck.criteria = AnyThreadStuck
benchi-kafka     | 	confluent.broker.health.manager.storage.network.threads.stuck.criteria = AnyThreadStuck
benchi-kafka     | 	confluent.broker.health.manager.storage.request.handler.threads.stuck.criteria = AnyThreadStuck
benchi-kafka     | 	confluent.broker.limit.consumer.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.broker.limit.producer.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.broker.load.advertised.limit.load = 0.8
benchi-kafka     | 	confluent.broker.load.average.service.request.time.ms = 0.1
benchi-kafka     | 	confluent.broker.load.delay.metric.start.ms = 180000
benchi-kafka     | 	confluent.broker.load.enabled = false
benchi-kafka     | 	confluent.broker.load.num.samples = 60
benchi-kafka     | 	confluent.broker.load.tenant.metric.enable = false
benchi-kafka     | 	confluent.broker.load.update.metric.tags.interval.ms = 60000
benchi-kafka     | 	confluent.broker.load.window.size.ms = 60000
benchi-kafka     | 	confluent.broker.load.workload.coefficient = 20.0
benchi-kafka     | 	confluent.broker.registration.delay.ms = 0
benchi-kafka     | 	confluent.catalog.collector.destination.topic = telemetry.events.data_catalog_source
benchi-kafka     | 	confluent.catalog.collector.enable = false
benchi-kafka     | 	confluent.catalog.collector.full.configs.enable = false
benchi-kafka     | 	confluent.catalog.collector.max.bytes.per.snapshot = 850000
benchi-kafka     | 	confluent.catalog.collector.max.topics.process = 500
benchi-kafka     | 	confluent.catalog.collector.max.zookeeper.request.per.sec = 100
benchi-kafka     | 	confluent.catalog.collector.snapshot.init.delay.sec = 60
benchi-kafka     | 	confluent.catalog.collector.snapshot.interval.sec = 300
benchi-kafka     | 	confluent.ccloud.host.suffixes = .confluent.cloud,.cpdev.cloud,.confluentgov.com,.confluentgov-internal.com
benchi-kafka     | 	confluent.ccloud.intranet.host.suffixes = .intranet.stag.cpdev.cloud,.intranet.stag.cpdev-untrusted.cloud,.intranet.devel.cpdev.cloud,.intranet.devel.cpdev-untrusted.cloud,.intranet.confluent.cloud,.intranet.confluent-untrusted.cloud
benchi-kafka     | 	confluent.cdc.api.keys.topic = 
benchi-kafka     | 	confluent.cdc.api.keys.topic.load.timeout.ms = 600000
benchi-kafka     | 	confluent.cdc.client.quotas.enable = false
benchi-kafka     | 	confluent.cdc.client.quotas.topic.name = 
benchi-kafka     | 	confluent.cdc.lkc.metadata.topic = 
benchi-kafka     | 	confluent.cdc.user.metadata.enable = false
benchi-kafka     | 	confluent.cdc.user.metadata.topic = _confluent-user_metadata
benchi-kafka     | 	confluent.cell.metrics.refresh.period.ms = 60000
benchi-kafka     | 	confluent.cells.default.size = 15
benchi-kafka     | 	confluent.cells.enable = false
benchi-kafka     | 	confluent.cells.implicit.creation.enable = false
benchi-kafka     | 	confluent.cells.load.refresher.enable = true
benchi-kafka     | 	confluent.cells.max.size = 15
benchi-kafka     | 	confluent.cells.min.size = 6
benchi-kafka     | 	confluent.checksum.enabled.files = [none]
benchi-kafka     | 	confluent.clm.enabled = false
benchi-kafka     | 	confluent.clm.frequency.in.hours = 6
benchi-kafka     | 	confluent.clm.list.object.thread_pool.size = 1
benchi-kafka     | 	confluent.clm.max.backup.days = 3
benchi-kafka     | 	confluent.clm.min.delay.in.minutes = 30
benchi-kafka     | 	confluent.clm.thread.pool.size = 2
benchi-kafka     | 	confluent.clm.topic.retention.days.to.backup.days = 0:0,3:3
benchi-kafka     | 	confluent.close.connections.on.credential.delete = false
benchi-kafka     | 	confluent.cluster.link.admin.max.in.flight.requests = 1000
benchi-kafka     | 	confluent.cluster.link.admin.request.batch.size = 1
benchi-kafka     | 	confluent.cluster.link.allow.config.providers = true
benchi-kafka     | 	confluent.cluster.link.allow.legacy.message.format = false
benchi-kafka     | 	confluent.cluster.link.allow.truncation.below.hwm = true
benchi-kafka     | 	confluent.cluster.link.availability.check.mode = ALL
benchi-kafka     | 	confluent.cluster.link.background.thread.affinity = LINK
benchi-kafka     | 	confluent.cluster.link.clients.max.idle.ms = 3153600000000
benchi-kafka     | 	confluent.cluster.link.enable = true
benchi-kafka     | 	confluent.cluster.link.enable.local.admin = false
benchi-kafka     | 	confluent.cluster.link.enable.metrics.reduction = false
benchi-kafka     | 	confluent.cluster.link.fetch.response.min.bytes = 1
benchi-kafka     | 	confluent.cluster.link.fetch.response.total.bytes = 2147483647
benchi-kafka     | 	confluent.cluster.link.fetcher.auto.tune.enable = false
benchi-kafka     | 	confluent.cluster.link.fetcher.thread.pool.mode = ENDPOINT
benchi-kafka     | 	confluent.cluster.link.insync.fetch.response.min.bytes = 1
benchi-kafka     | 	confluent.cluster.link.insync.fetch.response.total.bytes = 2147483647
benchi-kafka     | 	confluent.cluster.link.intranet.connectivity.enable = false
benchi-kafka     | 	confluent.cluster.link.io.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	confluent.cluster.link.local.reverse.connection.listener.map = null
benchi-kafka     | 	confluent.cluster.link.max.client.connections = 2147483647
benchi-kafka     | 	confluent.cluster.link.metadata.topic.create.retry.delay.ms = 1000
benchi-kafka     | 	confluent.cluster.link.metadata.topic.enable = false
benchi-kafka     | 	confluent.cluster.link.metadata.topic.min.isr = 2
benchi-kafka     | 	confluent.cluster.link.metadata.topic.partitions = 50
benchi-kafka     | 	confluent.cluster.link.metadata.topic.replication.factor = 1
benchi-kafka     | 	confluent.cluster.link.mirror.transition.batch.size = 10
benchi-kafka     | 	confluent.cluster.link.num.background.threads = 1
benchi-kafka     | 	confluent.cluster.link.periodic.task.batch.size = 2147483647
benchi-kafka     | 	confluent.cluster.link.periodic.task.min.interval.ms = 1000
benchi-kafka     | 	confluent.cluster.link.replica.fetch.connections.mode = combined
benchi-kafka     | 	confluent.cluster.link.replication.quota.mode = CLUSTER_LINK_ONLY
benchi-kafka     | 	confluent.cluster.link.replication.quota.mode.per.tenant.overrides = 
benchi-kafka     | 	confluent.cluster.link.replication.quota.window.num = 11
benchi-kafka     | 	confluent.cluster.link.replication.quota.window.size.seconds = 2
benchi-kafka     | 	confluent.cluster.link.request.quota.capacity = 400
benchi-kafka     | 	confluent.cluster.link.request.quota.request.percentage.multiplier = 1.0
benchi-kafka     | 	confluent.cluster.link.tenant.replication.quota.enable = false
benchi-kafka     | 	confluent.cluster.link.tenant.request.quota.enable = false
benchi-kafka     | 	confluent.cluster.metadata.snapshot.tier.delete.enable = false
benchi-kafka     | 	confluent.cluster.metadata.snapshot.tier.delete.maintain.min.snapshots = 3
benchi-kafka     | 	confluent.cluster.metadata.snapshot.tier.delete.retention.ms = 604800000
benchi-kafka     | 	confluent.cluster.metadata.snapshot.tier.upload.enable = false
benchi-kafka     | 	confluent.compacted.topic.prefer.tier.fetch.ms = -1
benchi-kafka     | 	confluent.connection.invalid.request.delay.enable = false
benchi-kafka     | 	confluent.connections.idle.expiry.manager.ignore.idleness.requests = []
benchi-kafka     | 	confluent.consumer.fetch.partition.pruning.enable = true
benchi-kafka     | 	confluent.consumer.lag.emitter.enabled = false
benchi-kafka     | 	confluent.consumer.lag.emitter.interval.ms = 60000
benchi-kafka     | 	confluent.dataflow.policy.watch.monitor.ms = 300000
benchi-kafka     | 	confluent.defer.isr.shrink.enable = false
benchi-kafka     | 	confluent.describe.topic.partitions.enabled = false
benchi-kafka     | 	confluent.disk.io.manager.enable = false
benchi-kafka     | 	confluent.disk.throughput.headroom = 10485760
benchi-kafka     | 	confluent.disk.throughput.limit = 10485760000
benchi-kafka     | 	confluent.disk.throughput.quota.tier.archive = 1048576000
benchi-kafka     | 	confluent.disk.throughput.quota.tier.archive.throttled = 104857600
benchi-kafka     | 	confluent.durability.audit.batch.flush.frequency.ms = 900000
benchi-kafka     | 	confluent.durability.audit.checks = PeriodicalAudit,ChecksumAudit
benchi-kafka     | 	confluent.durability.audit.enable = false
benchi-kafka     | 	confluent.durability.audit.idempotent.producer = false
benchi-kafka     | 	confluent.durability.audit.initial.job.delay.ms = 900000
benchi-kafka     | 	confluent.durability.audit.io.bytes.per.sec = 10485760
benchi-kafka     | 	confluent.durability.audit.log.ignored.event.types = 
benchi-kafka     | 	confluent.durability.audit.reporting.batch.ms = 1800000
benchi-kafka     | 	confluent.durability.audit.tier.compaction.audit.duration.ms = 14400000
benchi-kafka     | 	confluent.durability.events.allowed = OffsetChangeType,EpochChangeType,IsrExpandType,DeleteRecordsType,RetentionChangeType,StartOffsetChangeType,DeletePartitionType,HealthCheckType
benchi-kafka     | 	confluent.durability.topic.partition.count = 50
benchi-kafka     | 	confluent.durability.topic.replication.factor = 1
benchi-kafka     | 	confluent.e2e_checksum.protection.enabled = false
benchi-kafka     | 	confluent.e2e_checksum.protection.files = [none]
benchi-kafka     | 	confluent.e2e_checksum.protection.store.entry.ttl.ms = 2592000000
benchi-kafka     | 	confluent.elastic.cku.enabled = false
benchi-kafka     | 	confluent.elastic.cku.scaletozero.enabled = false
benchi-kafka     | 	confluent.eligible.controllers = []
benchi-kafka     | 	confluent.enable.stray.partition.deletion = false
benchi-kafka     | 	confluent.encryption.key.manager.rotation.interval.ms = 31536000000
benchi-kafka     | 	confluent.fetch.from.follower.require.leader.epoch.enable = false
benchi-kafka     | 	confluent.fetch.partition.pruning.enable = true
benchi-kafka     | 	confluent.floor.connection.rate.per.ip = -1.0
benchi-kafka     | 	confluent.floor.connection.rate.per.tenant = -1.0
benchi-kafka     | 	confluent.group.coordinator.offsets.batching.enable = false
benchi-kafka     | 	confluent.group.coordinator.offsets.writer.threads = 2
benchi-kafka     | 	confluent.group.metadata.load.threads = 32
benchi-kafka     | 	confluent.heap.tenured.notify.bytes = 0
benchi-kafka     | 	confluent.heap.tenured.notify.enabled = false
benchi-kafka     | 	confluent.hot.partition.ratio = 0.8
benchi-kafka     | 	confluent.http.server.start.timeout.ms = 60000
benchi-kafka     | 	confluent.http.server.stop.timeout.ms = 30000
benchi-kafka     | 	confluent.internal.metrics.enable = false
benchi-kafka     | 	confluent.internal.rest.server.bind.port = null
benchi-kafka     | 	confluent.internal.tenant.scoped.listener.name = INTERNAL_TENANT_SCOPED
benchi-kafka     | 	confluent.leader.epoch.checkpoint.checksum.enabled = false
benchi-kafka     | 	confluent.log.cleaner.timestamp.validation.enable = true
benchi-kafka     | 	confluent.log.placement.constraints = 
benchi-kafka     | 	confluent.max.connection.creation.rate.per.ip = 1.7976931348623157E308
benchi-kafka     | 	confluent.max.connection.creation.rate.per.tenant = 1.7976931348623157E308
benchi-kafka     | 	confluent.max.connection.rate.per.ip = -1.0
benchi-kafka     | 	confluent.max.connection.rate.per.tenant = -1.0
benchi-kafka     | 	confluent.max.connection.throttle.ms = null
benchi-kafka     | 	confluent.max.segment.ms = 9223372036854775807
benchi-kafka     | 	confluent.metadata.active.encryptor = null
benchi-kafka     | 	confluent.metadata.controlled.shutdown.partition.slice.delay.ms = 100
benchi-kafka     | 	confluent.metadata.encryptor.classes = null
benchi-kafka     | 	confluent.metadata.encryptor.required = false
benchi-kafka     | 	confluent.metadata.encryptor.secret.file = null
benchi-kafka     | 	confluent.metadata.encryptor.secrets = null
benchi-kafka     | 	confluent.metadata.jvm.warmup.ms = 60000
benchi-kafka     | 	confluent.metadata.leader.balance.slice.delay.ms = 100
benchi-kafka     | 	confluent.metadata.max.controlled.shutdown.partition.changes.per.slice = 1000
benchi-kafka     | 	confluent.metadata.max.leader.balance.changes.per.slice = 1000
benchi-kafka     | 	confluent.metadata.server.cluster.registry.clusters = []
benchi-kafka     | 	confluent.metadata.throttle.pre.check.for.non.wildcard.requests.enable = false
benchi-kafka     | 	confluent.metadata.throttle.pre.check.for.wildcard.requests.enable = false
benchi-kafka     | 	confluent.metrics.reporter.bootstrap.servers = localhost:9092
benchi-kafka     | 	confluent.min.acks = 0
benchi-kafka     | 	confluent.min.connection.throttle.ms = 0
benchi-kafka     | 	confluent.min.segment.ms = 1
benchi-kafka     | 	confluent.missing.id.cache.ttl.sec = 60
benchi-kafka     | 	confluent.missing.id.query.range = 20000
benchi-kafka     | 	confluent.missing.schema.cache.ttl.sec = 60
benchi-kafka     | 	confluent.mtls.enable = false
benchi-kafka     | 	confluent.mtls.listener.name = EXTERNAL
benchi-kafka     | 	confluent.mtls.sasl.authenticator.request.max.bytes = 104857600
benchi-kafka     | 	confluent.mtls.truststore.manager.class.name = null
benchi-kafka     | 	confluent.multitenant.authorizer.enable.acl.state = false
benchi-kafka     | 	confluent.multitenant.interceptor.balancer.apis.enabled = false
benchi-kafka     | 	confluent.multitenant.interceptor.collect.client.apiversions.max.per.tenant = 1000
benchi-kafka     | 	confluent.multitenant.interceptor.collect.client.apiversions.metric = false
benchi-kafka     | 	confluent.multitenant.listener.hostname.cluster.prefix.enable = false
benchi-kafka     | 	confluent.multitenant.listener.hostname.subdomain.suffix.enable = false
benchi-kafka     | 	confluent.multitenant.listener.names = null
benchi-kafka     | 	confluent.multitenant.parse.lkc.id.enable = false
benchi-kafka     | 	confluent.multitenant.parse.sni.host.name.enable = false
benchi-kafka     | 	confluent.network.health.manager.enabled = false
benchi-kafka     | 	confluent.network.health.manager.external.listener.name = EXTERNAL
benchi-kafka     | 	confluent.network.health.manager.externalconnectivitystartup.enabled = false
benchi-kafka     | 	confluent.network.health.manager.min.healthy.network.samples = 3
benchi-kafka     | 	confluent.network.health.manager.min.percentage.healthy.network.samples = 3
benchi-kafka     | 	confluent.network.health.manager.mitigation.enabled = false
benchi-kafka     | 	confluent.network.health.manager.network.sample.window.size = 120
benchi-kafka     | 	confluent.network.health.manager.sample.duration.ms = 1000
benchi-kafka     | 	confluent.oauth.flat.networking.verification.enable = false
benchi-kafka     | 	confluent.offsets.log.cleaner.delete.retention.ms = 86400000
benchi-kafka     | 	confluent.offsets.log.cleaner.max.compaction.lag.ms = 9223372036854775807
benchi-kafka     | 	confluent.offsets.log.cleaner.min.cleanable.dirty.ratio = 0.5
benchi-kafka     | 	confluent.offsets.topic.placement.constraints = 
benchi-kafka     | 	confluent.omit.network.processor.metric.tag = false
benchi-kafka     | 	confluent.operator.managed = false
benchi-kafka     | 	confluent.password.encoder.old.secret.ttl.ms = 9223372036854775807
benchi-kafka     | 	confluent.plugins.cluster.link.policy.max.destination.links.per.tenant = 10
benchi-kafka     | 	confluent.plugins.cluster.link.policy.max.source.links.per.tenant = 10
benchi-kafka     | 	confluent.plugins.topic.policy.max.partitions.per.cluster = 2147483647
benchi-kafka     | 	confluent.plugins.topic.policy.max.partitions.per.tenant = 512
benchi-kafka     | 	confluent.plugins.topic.policy.max.replicas.per.broker = 2147483647
benchi-kafka     | 	confluent.plugins.topic.policy.max.topics.per.cluster = 2147483647
benchi-kafka     | 	confluent.prefer.tier.fetch.ms = -1
benchi-kafka     | 	confluent.produce.throttle.pre.check.enable = false
benchi-kafka     | 	confluent.producer.id.cache.broker.hard.limit = -1
benchi-kafka     | 	confluent.producer.id.cache.eviction.minimal.expiration.ms = 900000
benchi-kafka     | 	confluent.producer.id.cache.extra.eviction.percentage = 0
benchi-kafka     | 	confluent.producer.id.cache.limit = 2147483647
benchi-kafka     | 	confluent.producer.id.cache.partition.hard.limit = -1
benchi-kafka     | 	confluent.producer.id.cache.tenant.hard.limit = -1
benchi-kafka     | 	confluent.producer.id.quota.manager.enable = false
benchi-kafka     | 	confluent.producer.id.quota.window.num = 11
benchi-kafka     | 	confluent.producer.id.quota.window.size.seconds = 1
benchi-kafka     | 	confluent.producer.id.throttle.enable = false
benchi-kafka     | 	confluent.producer.id.throttle.enable.threshold.percentage = 100
benchi-kafka     | 	confluent.proxy.mode.local.default = false
benchi-kafka     | 	confluent.proxy.protocol.fallback.enabled = false
benchi-kafka     | 	confluent.proxy.protocol.version = NONE
benchi-kafka     | 	confluent.quota.computing.usage.adjustment = 0.5
benchi-kafka     | 	confluent.quota.dynamic.enable = false
benchi-kafka     | 	confluent.quota.dynamic.publishing.interval.ms = 60000
benchi-kafka     | 	confluent.quota.dynamic.reporting.interval.ms = 30000
benchi-kafka     | 	confluent.quota.dynamic.reporting.min.usage = 102400
benchi-kafka     | 	confluent.quota.tenant.broker.max.consumer.rate = 13107200
benchi-kafka     | 	confluent.quota.tenant.broker.max.producer.rate = 13107200
benchi-kafka     | 	confluent.quota.tenant.default.controller.mutation.rate = 2.147483647E9
benchi-kafka     | 	confluent.quota.tenant.default.producer.id.rate = 2.147483647E9
benchi-kafka     | 	confluent.quota.tenant.fetch.multiplier = 1.0
benchi-kafka     | 	confluent.quota.tenant.follower.broker.min.consumer.rate = 10485760
benchi-kafka     | 	confluent.quota.tenant.follower.broker.min.producer.rate = 10485760
benchi-kafka     | 	confluent.quota.tenant.produce.multiplier = 1.0
benchi-kafka     | 	confluent.quota.tenant.user.quotas.enable = false
benchi-kafka     | 	confluent.regional.metadata.client.class = null
benchi-kafka     | 	confluent.regional.resource.manager.client.scheduler.threads = 2
benchi-kafka     | 	confluent.regional.resource.manager.endpoint = null
benchi-kafka     | 	confluent.regional.resource.manager.watch.endpoint = null
benchi-kafka     | 	confluent.replica.fetch.backoff.max.ms = 1000
benchi-kafka     | 	confluent.replica.fetch.connections.mode = combined
benchi-kafka     | 	confluent.replication.mode = PULL
benchi-kafka     | 	confluent.replication.push.feature.enable = false
benchi-kafka     | 	confluent.reporters.telemetry.auto.enable = true
benchi-kafka     | 	confluent.request.log.filter.class = class org.apache.kafka.common.requests.SamplingRequestLogFilter
benchi-kafka     | 	confluent.request.pipelining.enable = true
benchi-kafka     | 	confluent.request.pipelining.max.in.flight.requests.per.connection = 5
benchi-kafka     | 	confluent.require.calling.resource.identity = false
benchi-kafka     | 	confluent.require.compatible.keystore.updates = true
benchi-kafka     | 	confluent.require.confluent.issuer = false
benchi-kafka     | 	confluent.roll.check.interval.ms = 300000
benchi-kafka     | 	confluent.schema.registry.max.cache.size = 10000
benchi-kafka     | 	confluent.schema.registry.max.retries = 1
benchi-kafka     | 	confluent.schema.registry.retries.wait.ms = 0
benchi-kafka     | 	confluent.schema.registry.url = null
benchi-kafka     | 	confluent.schema.validator.interceptor.class = io.confluent.kafka.schemaregistry.validator.RecordSchemaValidator
benchi-kafka     | 	confluent.schema.validator.multitenant.enable = false
benchi-kafka     | 	confluent.schema.validator.samples.per.min = 0
benchi-kafka     | 	confluent.security.bc.approved.mode.enable = false
benchi-kafka     | 	confluent.security.event.logger.authentication.enable = false
benchi-kafka     | 	confluent.security.event.logger.authentication.event.rate.limit = -1
benchi-kafka     | 	confluent.security.event.logger.authorization.event.rate.limit = -1
benchi-kafka     | 	confluent.security.event.logger.detailed.audit.logs.filter.class = class org.apache.kafka.common.requests.DetailedRequestAuditLogFilter
benchi-kafka     | 	confluent.security.event.logger.enable = true
benchi-kafka     | 	confluent.security.event.logger.kafka.request.rate.limit = -1
benchi-kafka     | 	confluent.security.event.logger.physical.cluster.id = 
benchi-kafka     | 	confluent.security.event.router.config = 
benchi-kafka     | 	confluent.security.revoked.certificate.ids = 
benchi-kafka     | 	confluent.segment.eager.roll.enable = false
benchi-kafka     | 	confluent.segment.speculative.prefetch.enable = false
benchi-kafka     | 	confluent.spiffe.id.principal.extraction.rules = 
benchi-kafka     | 	confluent.ssl.key.password = null
benchi-kafka     | 	confluent.ssl.keystore.location = null
benchi-kafka     | 	confluent.ssl.keystore.password = null
benchi-kafka     | 	confluent.ssl.keystore.type = null
benchi-kafka     | 	confluent.ssl.protocol = null
benchi-kafka     | 	confluent.ssl.truststore.location = null
benchi-kafka     | 	confluent.ssl.truststore.password = null
benchi-kafka     | 	confluent.ssl.truststore.type = null
benchi-kafka     | 	confluent.step.connection.rate.per.ip = -1.0
benchi-kafka     | 	confluent.step.connection.rate.per.tenant = -1.0
benchi-kafka     | 	confluent.storage.probe.period.ms = -1
benchi-kafka     | 	confluent.storage.probe.slow.write.threshold.ms = 5000
benchi-kafka     | 	confluent.stray.log.delete.delay.ms = 604800000
benchi-kafka     | 	confluent.stray.log.max.deletions.per.run = 72
benchi-kafka     | 	confluent.subdomain.prefix = null
benchi-kafka     | 	confluent.subdomain.separator.map = null
benchi-kafka     | 	confluent.subdomain.separator.variable = %sep
benchi-kafka     | 	confluent.system.time.roll.enable = false
benchi-kafka     | 	confluent.telemetry.enabled = false
benchi-kafka     | 	confluent.telemetry.external.client.metrics.push.enabled = false
benchi-kafka     | 	confluent.tenant.latency.metric.enabled = false
benchi-kafka     | 	confluent.tier.archiver.num.threads = 2
benchi-kafka     | 	confluent.tier.azure.block.blob.auto.abort.threshold.bytes = 500000
benchi-kafka     | 	confluent.tier.azure.block.blob.container = null
benchi-kafka     | 	confluent.tier.azure.block.blob.cred.file.path = null
benchi-kafka     | 	confluent.tier.azure.block.blob.endpoint = null
benchi-kafka     | 	confluent.tier.azure.block.blob.prefix = 
benchi-kafka     | 	confluent.tier.backend = 
benchi-kafka     | 	confluent.tier.bucket.probe.period.ms = -1
benchi-kafka     | 	confluent.tier.cleaner.compact.min.efficiency = 0.5
benchi-kafka     | 	confluent.tier.cleaner.compact.segment.min.bytes = 20971520
benchi-kafka     | 	confluent.tier.cleaner.dedupe.buffer.size = 134217728
benchi-kafka     | 	confluent.tier.cleaner.dual.compaction = false
benchi-kafka     | 	confluent.tier.cleaner.dual.compaction.validation.max.bytes = 1073741824
benchi-kafka     | 	confluent.tier.cleaner.dual.compaction.validation.percent = 0
benchi-kafka     | 	confluent.tier.cleaner.enable = false
benchi-kafka     | 	confluent.tier.cleaner.excluded.topics = [^_confluent.*]
benchi-kafka     | 	confluent.tier.cleaner.feature.enable = false
benchi-kafka     | 	confluent.tier.cleaner.io.buffer.load.factor = 0.9
benchi-kafka     | 	confluent.tier.cleaner.io.buffer.size = 10485760
benchi-kafka     | 	confluent.tier.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
benchi-kafka     | 	confluent.tier.cleaner.min.cleanable.ratio = 0.75
benchi-kafka     | 	confluent.tier.cleaner.num.threads = 2
benchi-kafka     | 	confluent.tier.enable = false
benchi-kafka     | 	confluent.tier.feature = false
benchi-kafka     | 	confluent.tier.fenced.segment.delete.delay.ms = 600000
benchi-kafka     | 	confluent.tier.fetcher.async.enable = false
benchi-kafka     | 	confluent.tier.fetcher.async.timestamp.offset.parallelism = 1
benchi-kafka     | 	confluent.tier.fetcher.fetch.based.on.segment_and_metadata_layout.field = false
benchi-kafka     | 	confluent.tier.fetcher.memorypool.bytes = 0
benchi-kafka     | 	confluent.tier.fetcher.num.threads = 4
benchi-kafka     | 	confluent.tier.fetcher.offset.cache.expiration.ms = 1800000
benchi-kafka     | 	confluent.tier.fetcher.offset.cache.period.ms = 60000
benchi-kafka     | 	confluent.tier.fetcher.offset.cache.size = 200000
benchi-kafka     | 	confluent.tier.gcs.bucket = null
benchi-kafka     | 	confluent.tier.gcs.cred.file.path = null
benchi-kafka     | 	confluent.tier.gcs.prefix = 
benchi-kafka     | 	confluent.tier.gcs.region = null
benchi-kafka     | 	confluent.tier.gcs.sse.customer.encryption.key = null
benchi-kafka     | 	confluent.tier.gcs.write.chunk.size = 0
benchi-kafka     | 	confluent.tier.local.hotset.bytes = -1
benchi-kafka     | 	confluent.tier.local.hotset.ms = 86400000
benchi-kafka     | 	confluent.tier.max.partition.fetch.bytes.override = 0
benchi-kafka     | 	confluent.tier.metadata.bootstrap.servers = null
benchi-kafka     | 	confluent.tier.metadata.max.poll.ms = 100
benchi-kafka     | 	confluent.tier.metadata.namespace = null
benchi-kafka     | 	confluent.tier.metadata.num.partitions = 50
benchi-kafka     | 	confluent.tier.metadata.replication.factor = 1
benchi-kafka     | 	confluent.tier.metadata.request.timeout.ms = 30000
benchi-kafka     | 	confluent.tier.metadata.snapshots.enable = false
benchi-kafka     | 	confluent.tier.metadata.snapshots.interval.ms = 86400000
benchi-kafka     | 	confluent.tier.metadata.snapshots.retention.days = 7
benchi-kafka     | 	confluent.tier.metadata.snapshots.threads = 2
benchi-kafka     | 	confluent.tier.object.fetcher.num.threads = 1
benchi-kafka     | 	confluent.tier.partition.state.cleanup.delay.ms = 2592000000
benchi-kafka     | 	confluent.tier.partition.state.cleanup.enable = false
benchi-kafka     | 	confluent.tier.partition.state.cleanup.interval.ms = 86400000
benchi-kafka     | 	confluent.tier.partition.state.commit.interval.ms = 15000
benchi-kafka     | 	confluent.tier.s3.assumerole.arn = null
benchi-kafka     | 	confluent.tier.s3.auto.abort.threshold.bytes = 500000
benchi-kafka     | 	confluent.tier.s3.aws.endpoint.override = null
benchi-kafka     | 	confluent.tier.s3.aws.signer.override = null
benchi-kafka     | 	confluent.tier.s3.bucket = null
benchi-kafka     | 	confluent.tier.s3.cred.file.path = null
benchi-kafka     | 	confluent.tier.s3.force.path.style.access = false
benchi-kafka     | 	confluent.tier.s3.prefix = 
benchi-kafka     | 	confluent.tier.s3.region = null
benchi-kafka     | 	confluent.tier.s3.security.providers = null
benchi-kafka     | 	confluent.tier.s3.sse.algorithm = AES256
benchi-kafka     | 	confluent.tier.s3.sse.customer.encryption.key = null
benchi-kafka     | 	confluent.tier.s3.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	confluent.tier.s3.ssl.key.password = null
benchi-kafka     | 	confluent.tier.s3.ssl.keystore.location = null
benchi-kafka     | 	confluent.tier.s3.ssl.keystore.password = null
benchi-kafka     | 	confluent.tier.s3.ssl.keystore.type = null
benchi-kafka     | 	confluent.tier.s3.ssl.protocol = TLSv1.3
benchi-kafka     | 	confluent.tier.s3.ssl.provider = null
benchi-kafka     | 	confluent.tier.s3.ssl.truststore.location = null
benchi-kafka     | 	confluent.tier.s3.ssl.truststore.password = null
benchi-kafka     | 	confluent.tier.s3.ssl.truststore.type = null
benchi-kafka     | 	confluent.tier.s3.user.agent.prefix = APN/1.0 Confluent/1.0 TieredStorageS3/1.0
benchi-kafka     | 	confluent.tier.segment.hotset.roll.min.bytes = 104857600
benchi-kafka     | 	confluent.tier.segment.metadata.layout.put.mode = LegacyMultiObject
benchi-kafka     | 	confluent.tier.topic.data.loss.validation.fencing.enable = false
benchi-kafka     | 	confluent.tier.topic.delete.backoff.ms = 21600000
benchi-kafka     | 	confluent.tier.topic.delete.check.interval.ms = 300000
benchi-kafka     | 	confluent.tier.topic.delete.max.inprogress.partitions = 100
benchi-kafka     | 	confluent.tier.topic.head.data.loss.validation.enable = true
benchi-kafka     | 	confluent.tier.topic.head.data.loss.validation.max.timeout.ms = 900000
benchi-kafka     | 	confluent.tier.topic.materialization.from.snapshot.enable = false
benchi-kafka     | 	confluent.tier.topic.producer.enable.idempotence = true
benchi-kafka     | 	confluent.tier.topic.snapshots.enable = false
benchi-kafka     | 	confluent.tier.topic.snapshots.interval.ms = 300000
benchi-kafka     | 	confluent.tier.topic.snapshots.max.records = 100000
benchi-kafka     | 	confluent.tier.topic.snapshots.retention.hours = 168
benchi-kafka     | 	confluent.topic.partition.default.placement = 2
benchi-kafka     | 	confluent.topic.policy.use.computed.assignments = false
benchi-kafka     | 	confluent.topic.replica.assignor.builder.class = 
benchi-kafka     | 	confluent.track.api.key.per.ip = false
benchi-kafka     | 	confluent.track.per.ip.max.size = 100000
benchi-kafka     | 	confluent.track.tenant.id.per.ip = false
benchi-kafka     | 	confluent.traffic.cdc.network.id.routes.enable = false
benchi-kafka     | 	confluent.traffic.cdc.network.id.routes.listener.names = EXTERNAL_BACKCHANNEL
benchi-kafka     | 	confluent.traffic.cdc.network.id.routes.periodic.start.task.ms = 300000
benchi-kafka     | 	confluent.traffic.cdc.network.id.routes.topic.name = _confluent-network_id_routes
benchi-kafka     | 	confluent.traffic.network.id = 
benchi-kafka     | 	confluent.transaction.2pc.timeout.ms = -1
benchi-kafka     | 	confluent.transaction.logging.verbosity = 0
benchi-kafka     | 	confluent.transaction.state.log.placement.constraints = 
benchi-kafka     | 	confluent.valid.broker.rack.set = null
benchi-kafka     | 	confluent.verify.group.subscription.prefix = false
benchi-kafka     | 	confluent.virtual.topic.creation.enabled = false
benchi-kafka     | 	confluent.zookeeper.metadata.migration.controller.check.disable = false
benchi-kafka     | 	confluent.zookeeper.metadata.migration.trigger.file.path = null
benchi-kafka     | 	connection.failed.authentication.delay.ms = 100
benchi-kafka     | 	connection.min.expire.interval.ms = 250
benchi-kafka     | 	connections.max.age.ms = 3153600000000
benchi-kafka     | 	connections.max.idle.ms = 600000
benchi-kafka     | 	connections.max.reauth.ms = 0
benchi-kafka     | 	control.plane.listener.name = null
benchi-kafka     | 	controlled.shutdown.enable = true
benchi-kafka     | 	controlled.shutdown.max.retries = 3
benchi-kafka     | 	controlled.shutdown.retry.backoff.ms = 5000
benchi-kafka     | 	controller.listener.names = CONTROLLER
benchi-kafka     | 	controller.quorum.append.linger.ms = 25
benchi-kafka     | 	controller.quorum.bootstrap.servers = []
benchi-kafka     | 	controller.quorum.election.backoff.max.ms = 1000
benchi-kafka     | 	controller.quorum.election.timeout.ms = 1000
benchi-kafka     | 	controller.quorum.fetch.timeout.ms = 2000
benchi-kafka     | 	controller.quorum.request.timeout.ms = 2000
benchi-kafka     | 	controller.quorum.retry.backoff.ms = 20
benchi-kafka     | 	controller.quorum.voters = [1@broker:29093]
benchi-kafka     | 	controller.quota.window.num = 11
benchi-kafka     | 	controller.quota.window.size.seconds = 1
benchi-kafka     | 	controller.socket.timeout.ms = 30000
benchi-kafka     | 	create.cluster.link.policy.class.name = null
benchi-kafka     | 	create.topic.policy.class.name = null
benchi-kafka     | 	default.replication.factor = 1
benchi-kafka     | 	delegation.token.expiry.check.interval.ms = 3600000
benchi-kafka     | 	delegation.token.expiry.time.ms = 86400000
benchi-kafka     | 	delegation.token.master.key = null
benchi-kafka     | 	delegation.token.max.lifetime.ms = 604800000
benchi-kafka     | 	delegation.token.secret.key = null
benchi-kafka     | 	delete.records.purgatory.purge.interval.requests = 1
benchi-kafka     | 	delete.topic.enable = true
benchi-kafka     | 	early.start.listeners = null
benchi-kafka     | 	eligible.leader.replicas.enable = false
benchi-kafka     | 	enable.fips = false
benchi-kafka     | 	fetch.max.bytes = 57671680
benchi-kafka     | 	fetch.purgatory.purge.interval.requests = 1000
benchi-kafka     | 	floor.max.connection.creation.rate = null
benchi-kafka     | 	follower.replication.throttled.rate = 9223372036854775807
benchi-kafka     | 	follower.replication.throttled.replicas = none
benchi-kafka     | 	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
benchi-kafka     | 	group.consumer.heartbeat.interval.ms = 5000
benchi-kafka     | 	group.consumer.max.heartbeat.interval.ms = 15000
benchi-kafka     | 	group.consumer.max.session.timeout.ms = 60000
benchi-kafka     | 	group.consumer.max.size = 2147483647
benchi-kafka     | 	group.consumer.migration.policy = disabled
benchi-kafka     | 	group.consumer.min.heartbeat.interval.ms = 5000
benchi-kafka     | 	group.consumer.min.session.timeout.ms = 45000
benchi-kafka     | 	group.consumer.session.timeout.ms = 45000
benchi-kafka     | 	group.coordinator.append.linger.ms = 10
benchi-kafka     | 	group.coordinator.new.enable = false
benchi-kafka     | 	group.coordinator.rebalance.protocols = [classic]
benchi-kafka     | 	group.coordinator.threads = 1
benchi-kafka     | 	group.initial.rebalance.delay.ms = 0
benchi-kafka     | 	group.max.session.timeout.ms = 1800000
benchi-kafka     | 	group.max.size = 2147483647
benchi-kafka     | 	group.min.session.timeout.ms = 6000
benchi-kafka     | 	initial.broker.registration.timeout.ms = 60000
benchi-kafka     | 	inter.broker.listener.name = PLAINTEXT
benchi-kafka     | 	inter.broker.protocol.version = 3.8-IV0A
benchi-kafka     | 	k2.stack.builder.class.name = null
benchi-kafka     | 	k2.startup.timeout.ms = 60000
benchi-kafka     | 	k2.topic.metadata.max.total.partition.count = 20000
benchi-kafka     | 	k2.topic.metadata.refresh.ms = 10000
benchi-kafka     | 	kafka.metrics.polling.interval.secs = 10
benchi-kafka     | 	kafka.metrics.reporters = []
benchi-kafka     | 	leader.imbalance.check.interval.seconds = 300
benchi-kafka     | 	leader.imbalance.per.broker.percentage = 10
benchi-kafka     | 	leader.replication.throttled.rate = 9223372036854775807
benchi-kafka     | 	leader.replication.throttled.replicas = none
benchi-kafka     | 	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
benchi-kafka     | 	listeners = PLAINTEXT://broker:29092,CONTROLLER://broker:29093,PLAINTEXT_HOST://0.0.0.0:9092
benchi-kafka     | 	log.cleaner.backoff.ms = 15000
benchi-kafka     | 	log.cleaner.dedupe.buffer.size = 134217728
benchi-kafka     | 	log.cleaner.delete.retention.ms = 86400000
benchi-kafka     | 	log.cleaner.enable = true
benchi-kafka     | 	log.cleaner.hash.algorithm = MD5
benchi-kafka     | 	log.cleaner.io.buffer.load.factor = 0.9
benchi-kafka     | 	log.cleaner.io.buffer.size = 524288
benchi-kafka     | 	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
benchi-kafka     | 	log.cleaner.max.compaction.lag.ms = 9223372036854775807
benchi-kafka     | 	log.cleaner.min.cleanable.ratio = 0.5
benchi-kafka     | 	log.cleaner.min.compaction.lag.ms = 0
benchi-kafka     | 	log.cleaner.threads = 1
benchi-kafka     | 	log.cleanup.policy = [delete]
benchi-kafka     | 	log.deletion.max.segments.per.run = 2147483647
benchi-kafka     | 	log.deletion.throttler.disk.free.headroom.bytes = 21474836480
benchi-kafka     | 	log.dir = /tmp/kafka-logs
benchi-kafka     | 	log.dir.failure.timeout.ms = 30000
benchi-kafka     | 	log.dirs = /tmp/kraft-combined-logs
benchi-kafka     | 	log.flush.interval.messages = 9223372036854775807
benchi-kafka     | 	log.flush.interval.ms = null
benchi-kafka     | 	log.flush.offset.checkpoint.interval.ms = 60000
benchi-kafka     | 	log.flush.scheduler.interval.ms = 9223372036854775807
benchi-kafka     | 	log.flush.start.offset.checkpoint.interval.ms = 60000
benchi-kafka     | 	log.index.interval.bytes = 4096
benchi-kafka     | 	log.index.size.max.bytes = 10485760
benchi-kafka     | 	log.initial.task.delay.ms = 30000
benchi-kafka     | 	log.local.retention.bytes = -2
benchi-kafka     | 	log.local.retention.ms = -2
benchi-kafka     | 	log.message.downconversion.enable = true
benchi-kafka     | 	log.message.format.version = 3.0-IV1
benchi-kafka     | 	log.message.timestamp.after.max.ms = 9223372036854775807
benchi-kafka     | 	log.message.timestamp.before.max.ms = 9223372036854775807
benchi-kafka     | 	log.message.timestamp.difference.max.ms = 9223372036854775807
benchi-kafka     | 	log.message.timestamp.type = CreateTime
benchi-kafka     | 	log.preallocate = false
benchi-kafka     | 	log.retention.bytes = -1
benchi-kafka     | 	log.retention.check.interval.ms = 300000
benchi-kafka     | 	log.retention.hours = 168
benchi-kafka     | 	log.retention.minutes = null
benchi-kafka     | 	log.retention.ms = null
benchi-kafka     | 	log.roll.hours = 168
benchi-kafka     | 	log.roll.jitter.hours = 0
benchi-kafka     | 	log.roll.jitter.ms = null
benchi-kafka     | 	log.roll.ms = null
benchi-kafka     | 	log.segment.bytes = 1073741824
benchi-kafka     | 	log.segment.delete.delay.ms = 60000
benchi-kafka     | 	max.connection.creation.rate = 1.7976931348623157E308
benchi-kafka     | 	max.connection.creation.rate.per.ip.enable.threshold = 0.0
benchi-kafka     | 	max.connection.creation.rate.per.tenant.enable.threshold = 0.0
benchi-kafka     | 	max.connections = 2147483647
benchi-kafka     | 	max.connections.per.ip = 2147483647
benchi-kafka     | 	max.connections.per.ip.overrides = 
benchi-kafka     | 	max.connections.per.tenant = 0
benchi-kafka     | 	max.connections.protected.listeners = []
benchi-kafka     | 	max.connections.reap.amount = 0
benchi-kafka     | 	max.incremental.fetch.session.cache.slots = 1000
benchi-kafka     | 	max.request.partition.size.limit = 2000
benchi-kafka     | 	message.max.bytes = 1048588
benchi-kafka     | 	metadata.log.dir = null
benchi-kafka     | 	metadata.log.max.record.bytes.between.snapshots = 20971520
benchi-kafka     | 	metadata.log.max.snapshot.interval.ms = 3600000
benchi-kafka     | 	metadata.log.segment.bytes = 1073741824
benchi-kafka     | 	metadata.log.segment.min.bytes = 8388608
benchi-kafka     | 	metadata.log.segment.ms = 604800000
benchi-kafka     | 	metadata.max.idle.interval.ms = 500
benchi-kafka     | 	metadata.max.retention.bytes = 104857600
benchi-kafka     | 	metadata.max.retention.ms = 604800000
benchi-kafka     | 	metric.reporters = [io.confluent.metrics.reporter.ConfluentMetricsReporter]
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.recording.level = INFO
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	min.insync.replicas = 1
benchi-kafka     | 	multitenant.authorizer.support.resource.ids = false
benchi-kafka     | 	multitenant.metadata.class = null
benchi-kafka     | 	multitenant.metadata.dir = null
benchi-kafka     | 	multitenant.metadata.reload.delay.ms = 120000
benchi-kafka     | 	multitenant.metadata.ssl.certs.path = null
benchi-kafka     | 	multitenant.tenant.delete.batch.size = 10
benchi-kafka     | 	multitenant.tenant.delete.check.ms = 120000
benchi-kafka     | 	multitenant.tenant.delete.delay = 604800000
benchi-kafka     | 	node.id = 1
benchi-kafka     | 	num.io.threads = 8
benchi-kafka     | 	num.network.threads = 3
benchi-kafka     | 	num.partitions = 1
benchi-kafka     | 	num.recovery.threads.per.data.dir = 1
benchi-kafka     | 	num.replica.alter.log.dirs.threads = null
benchi-kafka     | 	num.replica.fetchers = 1
benchi-kafka     | 	offset.metadata.max.bytes = 4096
benchi-kafka     | 	offsets.commit.required.acks = -1
benchi-kafka     | 	offsets.commit.timeout.ms = 5000
benchi-kafka     | 	offsets.load.buffer.size = 5242880
benchi-kafka     | 	offsets.retention.check.interval.ms = 600000
benchi-kafka     | 	offsets.retention.minutes = 10080
benchi-kafka     | 	offsets.topic.compression.codec = 0
benchi-kafka     | 	offsets.topic.num.partitions = 50
benchi-kafka     | 	offsets.topic.replication.factor = 1
benchi-kafka     | 	offsets.topic.segment.bytes = 104857600
benchi-kafka     | 	otel.exporter.otlp.custom.endpoint = default
benchi-kafka     | 	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
benchi-kafka     | 	password.encoder.iterations = 4096
benchi-kafka     | 	password.encoder.key.length = 128
benchi-kafka     | 	password.encoder.keyfactory.algorithm = null
benchi-kafka     | 	password.encoder.old.secret = null
benchi-kafka     | 	password.encoder.secret = null
benchi-kafka     | 	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
benchi-kafka     | 	process.roles = [broker, controller]
benchi-kafka     | 	producer.id.expiration.check.interval.ms = 600000
benchi-kafka     | 	producer.id.expiration.ms = 86400000
benchi-kafka     | 	producer.purgatory.purge.interval.requests = 1000
benchi-kafka     | 	queued.max.request.bytes = -1
benchi-kafka     | 	queued.max.requests = 500
benchi-kafka     | 	quota.window.num = 11
benchi-kafka     | 	quota.window.size.seconds = 1
benchi-kafka     | 	quotas.consumption.expiration.time.ms = 600000
benchi-kafka     | 	quotas.expiration.interval.ms = 3600000
benchi-kafka     | 	quotas.expiration.time.ms = 604800000
benchi-kafka     | 	quotas.lazy.evaluation.threshold = 0.5
benchi-kafka     | 	quotas.topic.append.timeout.ms = 5000
benchi-kafka     | 	quotas.topic.compression.codec = 3
benchi-kafka     | 	quotas.topic.load.buffer.size = 5242880
benchi-kafka     | 	quotas.topic.num.partitions = 50
benchi-kafka     | 	quotas.topic.placement.constraints = 
benchi-kafka     | 	quotas.topic.replication.factor = 3
benchi-kafka     | 	quotas.topic.segment.bytes = 104857600
benchi-kafka     | 	remote.fetch.max.wait.ms = 500
benchi-kafka     | 	remote.log.index.file.cache.total.size.bytes = 1073741824
benchi-kafka     | 	remote.log.manager.copier.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.copy.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.copy.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.expiration.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.fetch.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.fetch.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.task.interval.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.max.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.ms = 500
benchi-kafka     | 	remote.log.manager.task.retry.jitter = 0.2
benchi-kafka     | 	remote.log.manager.thread.pool.size = 10
benchi-kafka     | 	remote.log.metadata.custom.metadata.max.bytes = 128
benchi-kafka     | 	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
benchi-kafka     | 	remote.log.metadata.manager.class.path = null
benchi-kafka     | 	remote.log.metadata.manager.impl.prefix = rlmm.config.
benchi-kafka     | 	remote.log.metadata.manager.listener.name = null
benchi-kafka     | 	remote.log.reader.max.pending.tasks = 100
benchi-kafka     | 	remote.log.reader.threads = 10
benchi-kafka     | 	remote.log.storage.manager.class.name = null
benchi-kafka     | 	remote.log.storage.manager.class.path = null
benchi-kafka     | 	remote.log.storage.manager.impl.prefix = rsm.config.
benchi-kafka     | 	remote.log.storage.system.enable = false
benchi-kafka     | 	replica.fetch.backoff.ms = 1000
benchi-kafka     | 	replica.fetch.max.bytes = 1048576
benchi-kafka     | 	replica.fetch.min.bytes = 1
benchi-kafka     | 	replica.fetch.response.max.bytes = 10485760
benchi-kafka     | 	replica.fetch.wait.max.ms = 500
benchi-kafka     | 	replica.high.watermark.checkpoint.interval.ms = 5000
benchi-kafka     | 	replica.lag.time.max.ms = 30000
benchi-kafka     | 	replica.selector.class = null
benchi-kafka     | 	replica.socket.receive.buffer.bytes = 65536
benchi-kafka     | 	replica.socket.timeout.ms = 30000
benchi-kafka     | 	replication.quota.window.num = 11
benchi-kafka     | 	replication.quota.window.size.seconds = 1
benchi-kafka     | 	request.timeout.ms = 30000
benchi-kafka     | 	reserved.broker.max.id = 1000
benchi-kafka     | 	sasl.client.callback.handler.class = null
benchi-kafka     | 	sasl.enabled.mechanisms = [GSSAPI]
benchi-kafka     | 	sasl.jaas.config = null
benchi-kafka     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
benchi-kafka     | 	sasl.kerberos.min.time.before.relogin = 60000
benchi-kafka     | 	sasl.kerberos.principal.to.local.rules = [DEFAULT]
benchi-kafka     | 	sasl.kerberos.service.name = null
benchi-kafka     | 	sasl.kerberos.ticket.renew.jitter = 0.05
benchi-kafka     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
benchi-kafka     | 	sasl.login.callback.handler.class = null
benchi-kafka     | 	sasl.login.class = null
benchi-kafka     | 	sasl.login.connect.timeout.ms = null
benchi-kafka     | 	sasl.login.read.timeout.ms = null
benchi-kafka     | 	sasl.login.refresh.buffer.seconds = 300
benchi-kafka     | 	sasl.login.refresh.min.period.seconds = 60
benchi-kafka     | 	sasl.login.refresh.window.factor = 0.8
benchi-kafka     | 	sasl.login.refresh.window.jitter = 0.05
benchi-kafka     | 	sasl.login.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.login.retry.backoff.ms = 100
benchi-kafka     | 	sasl.mechanism.controller.protocol = GSSAPI
benchi-kafka     | 	sasl.mechanism.inter.broker.protocol = GSSAPI
benchi-kafka     | 	sasl.oauthbearer.clientassertion.audience = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.expiration = 5
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.jti.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.nbf.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.issuer = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.location = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key.passphrase = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.subject = null
benchi-kafka     | 	sasl.oauthbearer.clock.skew.seconds = 30
benchi-kafka     | 	sasl.oauthbearer.expected.audience = null
benchi-kafka     | 	sasl.oauthbearer.expected.issuer = null
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.url = null
benchi-kafka     | 	sasl.oauthbearer.scope.claim.name = scope
benchi-kafka     | 	sasl.oauthbearer.sub.claim.name = sub
benchi-kafka     | 	sasl.oauthbearer.token.endpoint.url = null
benchi-kafka     | 	sasl.server.authn.async.enable = false
benchi-kafka     | 	sasl.server.authn.async.max.threads = 1
benchi-kafka     | 	sasl.server.authn.async.timeout.ms = 30000
benchi-kafka     | 	sasl.server.callback.handler.class = null
benchi-kafka     | 	sasl.server.max.receive.size = 524288
benchi-kafka     | 	security.inter.broker.protocol = PLAINTEXT
benchi-kafka     | 	security.providers = null
benchi-kafka     | 	server.max.startup.time.ms = 9223372036854775807
benchi-kafka     | 	socket.connection.setup.timeout.max.ms = 30000
benchi-kafka     | 	socket.connection.setup.timeout.ms = 10000
benchi-kafka     | 	socket.listen.backlog.size = 50
benchi-kafka     | 	socket.receive.buffer.bytes = 102400
benchi-kafka     | 	socket.request.max.bytes = 104857600
benchi-kafka     | 	socket.send.buffer.bytes = 102400
benchi-kafka     | 	ssl.allow.dn.changes = false
benchi-kafka     | 	ssl.allow.san.changes = false
benchi-kafka     | 	ssl.cipher.suites = []
benchi-kafka     | 	ssl.client.auth = none
benchi-kafka     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	ssl.engine.factory.class = null
benchi-kafka     | 	ssl.key.password = null
benchi-kafka     | 	ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	ssl.keystore.certificate.chain = null
benchi-kafka     | 	ssl.keystore.key = null
benchi-kafka     | 	ssl.keystore.location = null
benchi-kafka     | 	ssl.keystore.password = null
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.principal.mapping.rules = DEFAULT
benchi-kafka     | 	ssl.protocol = TLSv1.3
benchi-kafka     | 	ssl.provider = null
benchi-kafka     | 	ssl.secure.random.implementation = null
benchi-kafka     | 	ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	ssl.truststore.certificates = null
benchi-kafka     | 	ssl.truststore.location = null
benchi-kafka     | 	ssl.truststore.password = null
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     | 	telemetry.max.bytes = 1048576
benchi-kafka     | 	throughput.quota.window.num = 11
benchi-kafka     | 	token.impersonation.validation = true
benchi-kafka     | 	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
benchi-kafka     | 	transaction.max.timeout.ms = 900000
benchi-kafka     | 	transaction.metadata.load.threads = 32
benchi-kafka     | 	transaction.partition.verification.enable = true
benchi-kafka     | 	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
benchi-kafka     | 	transaction.state.log.load.buffer.size = 5242880
benchi-kafka     | 	transaction.state.log.min.isr = 1
benchi-kafka     | 	transaction.state.log.num.partitions = 50
benchi-kafka     | 	transaction.state.log.replication.factor = 1
benchi-kafka     | 	transaction.state.log.segment.bytes = 104857600
benchi-kafka     | 	transactional.id.expiration.ms = 604800000
benchi-kafka     | 	unclean.leader.election.enable = false
benchi-kafka     | 	unstable.api.versions.enable = false
benchi-kafka     | 	unstable.feature.versions.enable = false
benchi-kafka     | 	zookeeper.acl.change.notification.expiration.ms = 900000
benchi-kafka     | 	zookeeper.clientCnxnSocket = null
benchi-kafka     | 	zookeeper.connect = 
benchi-kafka     | 	zookeeper.connection.timeout.ms = null
benchi-kafka     | 	zookeeper.max.in.flight.requests = 10
benchi-kafka     | 	zookeeper.metadata.migration.enable = false
benchi-kafka     | 	zookeeper.metadata.migration.min.batch.size = 200
benchi-kafka     | 	zookeeper.session.timeout.ms = 18000
benchi-kafka     | 	zookeeper.set.acl = false
benchi-kafka     | 	zookeeper.ssl.cipher.suites = null
benchi-kafka     | 	zookeeper.ssl.client.enable = false
benchi-kafka     | 	zookeeper.ssl.crl.enable = false
benchi-kafka     | 	zookeeper.ssl.enabled.protocols = null
benchi-kafka     | 	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
benchi-kafka     | 	zookeeper.ssl.keystore.location = null
benchi-kafka     | 	zookeeper.ssl.keystore.password = null
benchi-kafka     | 	zookeeper.ssl.keystore.type = null
benchi-kafka     | 	zookeeper.ssl.ocsp.enable = false
benchi-kafka     | 	zookeeper.ssl.protocol = TLSv1.2
benchi-kafka     | 	zookeeper.ssl.truststore.location = null
benchi-kafka     | 	zookeeper.ssl.truststore.password = null
benchi-kafka     | 	zookeeper.ssl.truststore.type = null
benchi-kafka     |  (kafka.server.KafkaConfig)
benchi-kafka     | [2025-03-18 16:08:11,458] INFO RemoteLogManagerConfig values: 
benchi-kafka     | 	log.local.retention.bytes = -2
benchi-kafka     | 	log.local.retention.ms = -2
benchi-kafka     | 	remote.fetch.max.wait.ms = 500
benchi-kafka     | 	remote.log.index.file.cache.total.size.bytes = 1073741824
benchi-kafka     | 	remote.log.manager.copier.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.copy.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.copy.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.expiration.thread.pool.size = 10
benchi-kafka     | 	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.fetch.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.fetch.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.task.interval.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.max.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.ms = 500
benchi-kafka     | 	remote.log.manager.task.retry.jitter = 0.2
benchi-kafka     | 	remote.log.manager.thread.pool.size = 10
benchi-kafka     | 	remote.log.metadata.custom.metadata.max.bytes = 128
benchi-kafka     | 	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
benchi-kafka     | 	remote.log.metadata.manager.class.path = null
benchi-kafka     | 	remote.log.metadata.manager.impl.prefix = rlmm.config.
benchi-kafka     | 	remote.log.metadata.manager.listener.name = null
benchi-kafka     | 	remote.log.reader.max.pending.tasks = 100
benchi-kafka     | 	remote.log.reader.threads = 10
benchi-kafka     | 	remote.log.storage.manager.class.name = null
benchi-kafka     | 	remote.log.storage.manager.class.path = null
benchi-kafka     | 	remote.log.storage.manager.impl.prefix = rsm.config.
benchi-kafka     | 	remote.log.storage.system.enable = false
benchi-kafka     |  (org.apache.kafka.server.log.remote.storage.RemoteLogManagerConfig)
benchi-kafka     | [2025-03-18 16:08:11,466] INFO Unexpected credentials store injected: null (io.confluent.kafkarest.servlet.KafkaRestApplicationProvider)
benchi-kafka     | [2025-03-18 16:08:11,468] INFO For rest-app with listener null, configuring custom request logging (io.confluent.kafkarest.KafkaRestApplication)
benchi-kafka     | [2025-03-18 16:08:11,468] INFO EventEmitterConfig values: 
benchi-kafka     |  (io.confluent.shaded.io.confluent.telemetry.events.EventEmitterConfig)
benchi-kafka     | [2025-03-18 16:08:11,468] INFO EventEmitterConfig values: 
benchi-kafka     |  (io.confluent.shaded.io.confluent.telemetry.events.EventEmitterConfig)
benchi-kafka     | [2025-03-18 16:08:11,469] INFO ConfluentTelemetryConfig values: 
benchi-kafka     | 	confluent.telemetry.api.key = null
benchi-kafka     | 	confluent.telemetry.api.secret = null
benchi-kafka     | 	confluent.telemetry.cluster.id = null
benchi-kafka     | 	confluent.telemetry.debug.enabled = false
benchi-kafka     | 	confluent.telemetry.enabled = false
benchi-kafka     | 	confluent.telemetry.events.collector.include = .*transaction.remove.expired.transaction.cleanup.interval.ms.*|.*transaction.state.log.load.buffer.size.*|.*transaction.state.log.min.isr.*|.*transaction.state.log.num.partitions.*|.*transaction.state.log.replication.factor.*|.*transaction.state.log.segment.bytes.*|.*transactional.id.expiration.ms.*|.*controlled.shutdown.enable.*|.*controlled.shutdown.max.retries.*|.*controlled.shutdown.retry.backoff.ms.*|.*fetch.max.bytes.*|.*fetch.purgatory.purge.interval.requests.*|.*group.initial.rebalance.delay.ms.*|.*group.max.session.timeout.ms.*|.*group.max.size.*|.*group.min.session.timeout.ms.*|.*log.cleaner.backoff.ms.*|.*log.cleaner.dedupe.buffer.size.*|.*log.cleaner.delete.retention.ms.*|.*log.cleaner.enable.*|.*log.cleaner.io.buffer.load.factor.*|.*log.cleaner.io.buffer.size.*|.*log.cleaner.io.max.bytes.per.second.*|.*log.cleaner.max.compaction.lag.ms.*|.*log.cleaner.min.cleanable.ratio.*|.*log.cleaner.min.compaction.lag.ms.*|.*log.cleaner.threads.*|.*log.cleanup.policy.*|.*log.index.interval.bytes.*|.*log.index.size.max.bytes.*|.*log.message.downconversion.enable.*|.*log.message.format.version.*|.*log.message.timestamp.difference.max.ms.*|.*log.message.timestamp.type.*|.*max.connection.creation.rate.*|.*max.connections.*|.*max.connections.per.ip.*|.*max.incremental.fetch.session.cache.slots.*|.*replica.fetch.backoff.ms.*|.*replica.fetch.max.bytes.*|.*replica.fetch.min.bytes.*|.*replica.fetch.response.max.bytes.*|.*replica.fetch.wait.max.ms.*|.*replica.high.watermark.checkpoint.interval.ms.*|.*replica.lag.time.max.ms.*|.*replica.selector.class.*|.*replica.socket.receive.buffer.bytes.*|.*replica.socket.timeout.ms.*|.*alter.config.policy.class.name.*|.*alter.log.dirs.replication.quota.window.num.*|.*alter.log.dirs.replication.quota.window.size.seconds.*|.*metrics.num.samples.*|.*metrics.recording.level.*|.*metrics.sample.window.ms.*|.*quota.window.num.*|.*quota.window.size.seconds.*|.*replication.quota.window.num.*|.*replication.quota.window.size.seconds.*|.*confluent.balancer.enable.*|.*confluent.balancer.throttle.bytes.per.second.*|.*confluent.balancer.heal.uneven.load.trigger.*|.*confluent.balancer.heal.broker.failure.threshold.ms.*|.*confluent.balancer.disk.max.load.*|.*confluent.balancer.exclude.topic.prefixes.*|.*confluent.balancer.exclude.topic.names.*|.*confluent.tier.local.hotset.bytes.*|.*confluent.tier.local.hotset.ms.*|.*confluent.tier.archiver.num.threads.*|.*confluent.tier.backend.*|.*confluent.tier.enable.*|.*confluent.tier.feature.*|.*confluent.tier.fetcher.num.threads.*|.*confluent.tier.max.partition.fetch.bytes.override.*|.*confluent.tier.metadata.replication.factor.*|.*confluent.operator.managed.*|.*confluent.ansible.managed.*|.*confluent.license.*
benchi-kafka     | 	confluent.telemetry.events.enable = true
benchi-kafka     | 	confluent.telemetry.metrics.collector.include = .*io.confluent.system/.*(process_cpu_load|max_file_descriptor_count|open_file_descriptor_count|system_cpu_load|system_load_average|free_physical_memory_size|total_physical_memory_size|disk_total_bytes|disk_usable_bytes|jvm/mem|jvm/gc).*
benchi-kafka     | 	confluent.telemetry.metrics.collector.interval.ms = 60000
benchi-kafka     | 	confluent.telemetry.metrics.collector.slo.enabled = false
benchi-kafka     | 	confluent.telemetry.proxy.password = null
benchi-kafka     | 	confluent.telemetry.proxy.url = null
benchi-kafka     | 	confluent.telemetry.proxy.username = null
benchi-kafka     |  (io.confluent.telemetry.ConfluentTelemetryConfig)
benchi-kafka     | [2025-03-18 16:08:11,469] INFO VolumeMetricsCollectorConfig values: 
benchi-kafka     | 	confluent.telemetry.metrics.collector.volume.update.ms = 15000
benchi-kafka     |  (io.confluent.telemetry.collector.VolumeMetricsCollector$VolumeMetricsCollectorConfig)
benchi-kafka     | [2025-03-18 16:08:11,469] INFO HttpClientConfig values: 
benchi-kafka     | 	api.key = null
benchi-kafka     | 	api.secret = null
benchi-kafka     | 	buffer.batch.duration.max.ms = null
benchi-kafka     | 	buffer.batch.items.max = null
benchi-kafka     | 	buffer.inflight.submissions.max = null
benchi-kafka     | 	buffer.pending.batches.max = null
benchi-kafka     | 	client.attempts.max = null
benchi-kafka     | 	client.base.url = https://collector.telemetry.confluent.cloud
benchi-kafka     | 	client.compression = null
benchi-kafka     | 	client.connect.timeout.ms = null
benchi-kafka     | 	client.metrics.path.override = /v1/metrics
benchi-kafka     | 	client.request.timeout.ms = null
benchi-kafka     | 	client.retry.delay.seconds = null
benchi-kafka     | 	proxy.password = null
benchi-kafka     | 	proxy.url = null
benchi-kafka     | 	proxy.username = null
benchi-kafka     | 	type = httpTelemetryClient
benchi-kafka     |  (io.confluent.telemetry.exporter.http.HttpClientConfig)
benchi-kafka     | [2025-03-18 16:08:11,469] INFO HttpExporterConfig values: 
benchi-kafka     | 	api.key = null
benchi-kafka     | 	api.secret = null
benchi-kafka     | 	buffer.batch.duration.max.ms = null
benchi-kafka     | 	buffer.batch.items.max = null
benchi-kafka     | 	buffer.inflight.submissions.max = null
benchi-kafka     | 	buffer.pending.batches.max = null
benchi-kafka     | 	client = _confluentClient
benchi-kafka     | 	client.attempts.max = null
benchi-kafka     | 	client.base.url = null
benchi-kafka     | 	client.compression = null
benchi-kafka     | 	client.connect.timeout.ms = null
benchi-kafka     | 	client.metrics.path.override = /v1/metrics
benchi-kafka     | 	client.request.timeout.ms = null
benchi-kafka     | 	client.retry.delay.seconds = null
benchi-kafka     | 	enabled = false
benchi-kafka     | 	events.enabled = true
benchi-kafka     | 	metrics.enabled = true
benchi-kafka     | 	metrics.include = null
benchi-kafka     | 	proxy.password = null
benchi-kafka     | 	proxy.url = null
benchi-kafka     | 	proxy.username = null
benchi-kafka     | 	remote.configurable = false
benchi-kafka     | 	type = http
benchi-kafka     |  (io.confluent.telemetry.exporter.http.HttpExporterConfig)
benchi-kafka     | [2025-03-18 16:08:11,469] INFO Configuring named client _confluentClient for exporter _confluent (io.confluent.telemetry.exporter.http.HttpExporterConfig)
benchi-kafka     | [2025-03-18 16:08:11,469] WARN no telemetry exporters are enabled (io.confluent.telemetry.ConfluentTelemetryConfig)
benchi-kafka     | [2025-03-18 16:08:11,469] INFO PollingRemoteConfigurationConfig values: 
benchi-kafka     | 	enabled = true
benchi-kafka     | 	refresh.interval.ms = 60000
benchi-kafka     |  (io.confluent.shaded.io.confluent.telemetry.config.remote.polling.PollingRemoteConfigurationConfig)
benchi-kafka     | [2025-03-18 16:08:11,469] WARN Ignoring redefinition of existing telemetry label kafka_rest.version (io.confluent.shaded.io.confluent.telemetry.ResourceBuilderFacade)
benchi-kafka     | [2025-03-18 16:08:11,470] INFO ConfluentTelemetryConfig values: 
benchi-kafka     | 	confluent.telemetry.api.key = null
benchi-kafka     | 	confluent.telemetry.api.secret = null
benchi-kafka     | 	confluent.telemetry.cluster.id = null
benchi-kafka     | 	confluent.telemetry.debug.enabled = false
benchi-kafka     | 	confluent.telemetry.enabled = false
benchi-kafka     | 	confluent.telemetry.events.collector.include = .*transaction.remove.expired.transaction.cleanup.interval.ms.*|.*transaction.state.log.load.buffer.size.*|.*transaction.state.log.min.isr.*|.*transaction.state.log.num.partitions.*|.*transaction.state.log.replication.factor.*|.*transaction.state.log.segment.bytes.*|.*transactional.id.expiration.ms.*|.*controlled.shutdown.enable.*|.*controlled.shutdown.max.retries.*|.*controlled.shutdown.retry.backoff.ms.*|.*fetch.max.bytes.*|.*fetch.purgatory.purge.interval.requests.*|.*group.initial.rebalance.delay.ms.*|.*group.max.session.timeout.ms.*|.*group.max.size.*|.*group.min.session.timeout.ms.*|.*log.cleaner.backoff.ms.*|.*log.cleaner.dedupe.buffer.size.*|.*log.cleaner.delete.retention.ms.*|.*log.cleaner.enable.*|.*log.cleaner.io.buffer.load.factor.*|.*log.cleaner.io.buffer.size.*|.*log.cleaner.io.max.bytes.per.second.*|.*log.cleaner.max.compaction.lag.ms.*|.*log.cleaner.min.cleanable.ratio.*|.*log.cleaner.min.compaction.lag.ms.*|.*log.cleaner.threads.*|.*log.cleanup.policy.*|.*log.index.interval.bytes.*|.*log.index.size.max.bytes.*|.*log.message.downconversion.enable.*|.*log.message.format.version.*|.*log.message.timestamp.difference.max.ms.*|.*log.message.timestamp.type.*|.*max.connection.creation.rate.*|.*max.connections.*|.*max.connections.per.ip.*|.*max.incremental.fetch.session.cache.slots.*|.*replica.fetch.backoff.ms.*|.*replica.fetch.max.bytes.*|.*replica.fetch.min.bytes.*|.*replica.fetch.response.max.bytes.*|.*replica.fetch.wait.max.ms.*|.*replica.high.watermark.checkpoint.interval.ms.*|.*replica.lag.time.max.ms.*|.*replica.selector.class.*|.*replica.socket.receive.buffer.bytes.*|.*replica.socket.timeout.ms.*|.*alter.config.policy.class.name.*|.*alter.log.dirs.replication.quota.window.num.*|.*alter.log.dirs.replication.quota.window.size.seconds.*|.*metrics.num.samples.*|.*metrics.recording.level.*|.*metrics.sample.window.ms.*|.*quota.window.num.*|.*quota.window.size.seconds.*|.*replication.quota.window.num.*|.*replication.quota.window.size.seconds.*|.*confluent.balancer.enable.*|.*confluent.balancer.throttle.bytes.per.second.*|.*confluent.balancer.heal.uneven.load.trigger.*|.*confluent.balancer.heal.broker.failure.threshold.ms.*|.*confluent.balancer.disk.max.load.*|.*confluent.balancer.exclude.topic.prefixes.*|.*confluent.balancer.exclude.topic.names.*|.*confluent.tier.local.hotset.bytes.*|.*confluent.tier.local.hotset.ms.*|.*confluent.tier.archiver.num.threads.*|.*confluent.tier.backend.*|.*confluent.tier.enable.*|.*confluent.tier.feature.*|.*confluent.tier.fetcher.num.threads.*|.*confluent.tier.max.partition.fetch.bytes.override.*|.*confluent.tier.metadata.replication.factor.*|.*confluent.operator.managed.*|.*confluent.ansible.managed.*|.*confluent.license.*
benchi-kafka     | 	confluent.telemetry.events.enable = true
benchi-kafka     | 	confluent.telemetry.metrics.collector.include = .*io.confluent.telemetry/.*.*|.*io.confluent.system/.*(process_cpu_load|max_file_descriptor_count|open_file_descriptor_count|system_cpu_load|system_load_average|free_physical_memory_size|total_physical_memory_size|disk_total_bytes|disk_usable_bytes|jvm/mem|jvm/gc).*|.*io.confluent.kafka.rest/.*(connections_active|connections_closed_rate|request_error_rate|request_latency_avg|request_latency_max|request_rate|response_size_avg|response_size_max).*
benchi-kafka     | 	confluent.telemetry.metrics.collector.interval.ms = 60000
benchi-kafka     | 	confluent.telemetry.metrics.collector.slo.enabled = false
benchi-kafka     | 	confluent.telemetry.proxy.password = null
benchi-kafka     | 	confluent.telemetry.proxy.url = null
benchi-kafka     | 	confluent.telemetry.proxy.username = null
benchi-kafka     |  (io.confluent.telemetry.ConfluentTelemetryConfig)
benchi-kafka     | [2025-03-18 16:08:11,470] INFO VolumeMetricsCollectorConfig values: 
benchi-kafka     | 	confluent.telemetry.metrics.collector.volume.update.ms = 15000
benchi-kafka     |  (io.confluent.telemetry.collector.VolumeMetricsCollector$VolumeMetricsCollectorConfig)
benchi-kafka     | [2025-03-18 16:08:11,470] INFO HttpClientConfig values: 
benchi-kafka     | 	api.key = null
benchi-kafka     | 	api.secret = null
benchi-kafka     | 	buffer.batch.duration.max.ms = null
benchi-kafka     | 	buffer.batch.items.max = null
benchi-kafka     | 	buffer.inflight.submissions.max = null
benchi-kafka     | 	buffer.pending.batches.max = null
benchi-kafka     | 	client.attempts.max = null
benchi-kafka     | 	client.base.url = https://collector.telemetry.confluent.cloud
benchi-kafka     | 	client.compression = null
benchi-kafka     | 	client.connect.timeout.ms = null
benchi-kafka     | 	client.metrics.path.override = /v1/metrics
benchi-kafka     | 	client.request.timeout.ms = null
benchi-kafka     | 	client.retry.delay.seconds = null
benchi-kafka     | 	proxy.password = null
benchi-kafka     | 	proxy.url = null
benchi-kafka     | 	proxy.username = null
benchi-kafka     | 	type = httpTelemetryClient
benchi-kafka     |  (io.confluent.telemetry.exporter.http.HttpClientConfig)
benchi-kafka     | [2025-03-18 16:08:11,470] INFO HttpExporterConfig values: 
benchi-kafka     | 	api.key = null
benchi-kafka     | 	api.secret = null
benchi-kafka     | 	buffer.batch.duration.max.ms = null
benchi-kafka     | 	buffer.batch.items.max = null
benchi-kafka     | 	buffer.inflight.submissions.max = null
benchi-kafka     | 	buffer.pending.batches.max = null
benchi-kafka     | 	client = _confluentClient
benchi-kafka     | 	client.attempts.max = null
benchi-kafka     | 	client.base.url = null
benchi-kafka     | 	client.compression = null
benchi-kafka     | 	client.connect.timeout.ms = null
benchi-kafka     | 	client.metrics.path.override = /v1/metrics
benchi-kafka     | 	client.request.timeout.ms = null
benchi-kafka     | 	client.retry.delay.seconds = null
benchi-kafka     | 	enabled = false
benchi-kafka     | 	events.enabled = true
benchi-kafka     | 	metrics.enabled = true
benchi-kafka     | 	metrics.include = null
benchi-kafka     | 	proxy.password = null
benchi-kafka     | 	proxy.url = null
benchi-kafka     | 	proxy.username = null
benchi-kafka     | 	remote.configurable = false
benchi-kafka     | 	type = http
benchi-kafka     |  (io.confluent.telemetry.exporter.http.HttpExporterConfig)
benchi-kafka     | [2025-03-18 16:08:11,470] INFO Configuring named client _confluentClient for exporter _confluent (io.confluent.telemetry.exporter.http.HttpExporterConfig)
benchi-kafka     | [2025-03-18 16:08:11,470] WARN no telemetry exporters are enabled (io.confluent.telemetry.ConfluentTelemetryConfig)
benchi-kafka     | [2025-03-18 16:08:11,470] INFO PollingRemoteConfigurationConfig values: 
benchi-kafka     | 	enabled = true
benchi-kafka     | 	refresh.interval.ms = 60000
benchi-kafka     |  (io.confluent.shaded.io.confluent.telemetry.config.remote.polling.PollingRemoteConfigurationConfig)
benchi-kafka     | [2025-03-18 16:08:11,470] INFO Initializing the event logger (io.confluent.telemetry.reporter.TelemetryReporter)
benchi-kafka     | [2025-03-18 16:08:11,470] INFO EventLoggerConfig values: 
benchi-kafka     | 	event.logger.cloudevent.codec = structured
benchi-kafka     | 	event.logger.exporter.class = class io.confluent.shaded.io.confluent.telemetry.events.exporter.http.EventHttpExporter
benchi-kafka     |  (io.confluent.shaded.io.confluent.telemetry.events.EventLoggerConfig)
benchi-kafka     | [2025-03-18 16:08:11,470] INFO HttpExporterConfig values: 
benchi-kafka     | 	api.key = null
benchi-kafka     | 	api.secret = null
benchi-kafka     | 	buffer.batch.duration.max.ms = null
benchi-kafka     | 	buffer.batch.items.max = null
benchi-kafka     | 	buffer.inflight.submissions.max = null
benchi-kafka     | 	buffer.pending.batches.max = null
benchi-kafka     | 	client.attempts.max = null
benchi-kafka     | 	client.base.url = https://collector.telemetry.confluent.cloud
benchi-kafka     | 	client.compression = null
benchi-kafka     | 	client.connect.timeout.ms = null
benchi-kafka     | 	client.request.timeout.ms = null
benchi-kafka     | 	client.retry.delay.seconds = null
benchi-kafka     | 	enabled = true
benchi-kafka     | 	events.enabled = true
benchi-kafka     | 	metrics.enabled = true
benchi-kafka     | 	proxy.password = null
benchi-kafka     | 	proxy.url = null
benchi-kafka     | 	proxy.username = null
benchi-kafka     | 	type = http
benchi-kafka     |  (io.confluent.shaded.io.confluent.telemetry.events.exporter.http.HttpExporterConfig)
benchi-kafka     | [2025-03-18 16:08:11,474] INFO Starting Confluent telemetry reporter with an interval of 60000 ms) (io.confluent.telemetry.reporter.TelemetryReporter)
benchi-kafka     | [2025-03-18 16:08:11,475] INFO Application provider 'KafkaRestApplicationProvider' provided 1 instance(s). (io.confluent.http.server.KafkaHttpApplicationLoader)
benchi-kafka     | [2025-03-18 16:08:11,477] INFO Initial capacity 128, increased by 64, maximum capacity 2147483647. (io.confluent.rest.ApplicationServer)
benchi-kafka     | [2025-03-18 16:08:11,515] INFO RestConfig values: 
benchi-kafka     | 	access.control.allow.headers = 
benchi-kafka     | 	access.control.allow.methods = 
benchi-kafka     | 	access.control.allow.origin = 
benchi-kafka     | 	access.control.skip.options = true
benchi-kafka     | 	authentication.method = NONE
benchi-kafka     | 	authentication.realm = 
benchi-kafka     | 	authentication.roles = [*]
benchi-kafka     | 	authentication.skip.paths = []
benchi-kafka     | 	compression.enable = true
benchi-kafka     | 	connector.connection.limit = 0
benchi-kafka     | 	csrf.prevention.enable = false
benchi-kafka     | 	csrf.prevention.token.endpoint = /csrf
benchi-kafka     | 	csrf.prevention.token.expiration.minutes = 30
benchi-kafka     | 	csrf.prevention.token.max.entries = 10000
benchi-kafka     | 	debug = false
benchi-kafka     | 	dos.filter.delay.ms = 100
benchi-kafka     | 	dos.filter.enabled = false
benchi-kafka     | 	dos.filter.insert.headers = true
benchi-kafka     | 	dos.filter.ip.whitelist = []
benchi-kafka     | 	dos.filter.managed.attr = false
benchi-kafka     | 	dos.filter.max.idle.tracker.ms = 30000
benchi-kafka     | 	dos.filter.max.requests.ms = 30000
benchi-kafka     | 	dos.filter.max.requests.per.connection.per.sec = 25
benchi-kafka     | 	dos.filter.max.requests.per.sec = 25
benchi-kafka     | 	dos.filter.max.wait.ms = 50
benchi-kafka     | 	dos.filter.throttle.ms = 30000
benchi-kafka     | 	dos.filter.throttled.requests = 5
benchi-kafka     | 	http2.enabled = true
benchi-kafka     | 	idle.timeout.ms = 30000
benchi-kafka     | 	listener.protocol.map = []
benchi-kafka     | 	listeners = []
benchi-kafka     | 	max.request.header.size = 8192
benchi-kafka     | 	max.response.header.size = 8192
benchi-kafka     | 	metric.reporters = []
benchi-kafka     | 	metrics.global.stats.request.tags.enable = false
benchi-kafka     | 	metrics.jmx.prefix = rest-utils
benchi-kafka     | 	metrics.latency.sla.ms = 50
benchi-kafka     | 	metrics.latency.slo.ms = 5
benchi-kafka     | 	metrics.latency.slo.sla.enable = false
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	metrics.tag.map = []
benchi-kafka     | 	network.traffic.rate.limit.backend = guava
benchi-kafka     | 	network.traffic.rate.limit.bytes.per.sec = 20971520
benchi-kafka     | 	network.traffic.rate.limit.enable = false
benchi-kafka     | 	nosniff.prevention.enable = false
benchi-kafka     | 	port = 8080
benchi-kafka     | 	proxy.protocol.enabled = false
benchi-kafka     | 	reject.options.request = false
benchi-kafka     | 	request.logger.name = io.confluent.rest-utils.requests
benchi-kafka     | 	request.queue.capacity = 2147483647
benchi-kafka     | 	request.queue.capacity.growby = 64
benchi-kafka     | 	request.queue.capacity.init = 128
benchi-kafka     | 	resource.extension.classes = []
benchi-kafka     | 	response.http.headers.config = 
benchi-kafka     | 	response.mediatype.default = application/json
benchi-kafka     | 	response.mediatype.preferred = [application/json]
benchi-kafka     | 	rest.servlet.initializor.classes = []
benchi-kafka     | 	server.connection.limit = 0
benchi-kafka     | 	shutdown.graceful.ms = 1000
benchi-kafka     | 	sni.check.enabled = false
benchi-kafka     | 	ssl.cipher.suites = []
benchi-kafka     | 	ssl.client.auth = false
benchi-kafka     | 	ssl.client.authentication = NONE
benchi-kafka     | 	ssl.enabled.protocols = []
benchi-kafka     | 	ssl.endpoint.identification.algorithm = null
benchi-kafka     | 	ssl.key.password = [hidden]
benchi-kafka     | 	ssl.keymanager.algorithm = 
benchi-kafka     | 	ssl.keystore.location = 
benchi-kafka     | 	ssl.keystore.password = [hidden]
benchi-kafka     | 	ssl.keystore.reload = false
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.keystore.watch.location = 
benchi-kafka     | 	ssl.protocol = TLS
benchi-kafka     | 	ssl.provider = 
benchi-kafka     | 	ssl.trustmanager.algorithm = 
benchi-kafka     | 	ssl.truststore.location = 
benchi-kafka     | 	ssl.truststore.password = [hidden]
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     | 	suppress.stack.trace.response = true
benchi-kafka     | 	thread.pool.max = 200
benchi-kafka     | 	thread.pool.min = 8
benchi-kafka     | 	websocket.path.prefix = /ws
benchi-kafka     | 	websocket.servlet.initializor.classes = []
benchi-kafka     |  (io.confluent.rest.RestConfig)
benchi-kafka     | [2025-03-18 16:08:11,515] INFO Adding listener with HTTP/2: NamedURI{uri=http://0.0.0.0:8090, name='null'} (io.confluent.rest.ApplicationServer)
benchi-kafka     | [2025-03-18 16:08:11,520] INFO [ControllerServer id=1] CreateTopics result(s): CreatableTopic(name='_confluent-link-metadata', numPartitions=50, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact'), CreateableTopicConfig(name='min.insync.replicas', value='2')], linkName=null, mirrorTopic=null, sourceTopicId=AAAAAAAAAAAAAAAAAAAAAA, mirrorStartOffsetSpec=-9223372036854775808, mirrorStartOffsets=[], stoppedSequenceNumber=0): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:08:11,520] INFO [ControllerServer id=1] Replayed TopicRecord for topic _confluent-link-metadata with topic ID 9kJAA8KZRoWZtgk1eJTuEA. (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:08:11,521] INFO [ControllerServer id=1] Replayed ConfigRecord for ConfigResource(type=TOPIC, name='_confluent-link-metadata') which set configuration cleanup.policy to compact (org.apache.kafka.controller.ConfigurationControlManager)
benchi-kafka     | [2025-03-18 16:08:11,521] INFO [ControllerServer id=1] Replayed ConfigRecord for ConfigResource(type=TOPIC, name='_confluent-link-metadata') which set configuration min.insync.replicas to 2 (org.apache.kafka.controller.ConfigurationControlManager)
benchi-kafka     | [2025-03-18 16:08:11,521] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-0 with topic ID 9kJAA8KZRoWZtgk1eJTuEA and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:08:11,522] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-1 with topic ID 9kJAA8KZRoWZtgk1eJTuEA and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:08:11,522] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-2 with topic ID 9kJAA8KZRoWZtgk1eJTuEA and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:08:11,522] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-3 with topic ID 9kJAA8KZRoWZtgk1eJTuEA and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:08:11,522] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-4 with topic ID 9kJAA8KZRoWZtgk1eJTuEA and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:08:11,522] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-5 with topic ID 9kJAA8KZRoWZtgk1eJTuEA and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:08:11,522] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-6 with topic ID 9kJAA8KZRoWZtgk1eJTuEA and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:08:11,522] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-7 with topic ID 9kJAA8KZRoWZtgk1eJTuEA and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:08:11,522] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-8 with topic ID 9kJAA8KZRoWZtgk1eJTuEA and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:08:11,522] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-9 with topic ID 9kJAA8KZRoWZtgk1eJTuEA and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:08:11,522] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-10 with topic ID 9kJAA8KZRoWZtgk1eJTuEA and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:08:11,522] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-11 with topic ID 9kJAA8KZRoWZtgk1eJTuEA and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:08:11,522] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-12 with topic ID 9kJAA8KZRoWZtgk1eJTuEA and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:08:11,523] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-13 with topic ID 9kJAA8KZRoWZtgk1eJTuEA and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:08:11,523] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-14 with topic ID 9kJAA8KZRoWZtgk1eJTuEA and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:08:11,523] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-15 with topic ID 9kJAA8KZRoWZtgk1eJTuEA and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:08:11,523] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-16 with topic ID 9kJAA8KZRoWZtgk1eJTuEA and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:08:11,523] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-17 with topic ID 9kJAA8KZRoWZtgk1eJTuEA and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:08:11,523] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-18 with topic ID 9kJAA8KZRoWZtgk1eJTuEA and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:08:11,523] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-19 with topic ID 9kJAA8KZRoWZtgk1eJTuEA and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:08:11,523] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-20 with topic ID 9kJAA8KZRoWZtgk1eJTuEA and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:08:11,523] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-21 with topic ID 9kJAA8KZRoWZtgk1eJTuEA and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:08:11,523] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-22 with topic ID 9kJAA8KZRoWZtgk1eJTuEA and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:08:11,524] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-23 with topic ID 9kJAA8KZRoWZtgk1eJTuEA and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:08:11,524] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-24 with topic ID 9kJAA8KZRoWZtgk1eJTuEA and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:08:11,524] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-25 with topic ID 9kJAA8KZRoWZtgk1eJTuEA and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:08:11,524] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-26 with topic ID 9kJAA8KZRoWZtgk1eJTuEA and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:08:11,524] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-27 with topic ID 9kJAA8KZRoWZtgk1eJTuEA and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:08:11,524] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-28 with topic ID 9kJAA8KZRoWZtgk1eJTuEA and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:08:11,524] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-29 with topic ID 9kJAA8KZRoWZtgk1eJTuEA and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:08:11,524] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-30 with topic ID 9kJAA8KZRoWZtgk1eJTuEA and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:08:11,524] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-31 with topic ID 9kJAA8KZRoWZtgk1eJTuEA and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:08:11,524] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-32 with topic ID 9kJAA8KZRoWZtgk1eJTuEA and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:08:11,524] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-33 with topic ID 9kJAA8KZRoWZtgk1eJTuEA and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:08:11,524] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-34 with topic ID 9kJAA8KZRoWZtgk1eJTuEA and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:08:11,524] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-35 with topic ID 9kJAA8KZRoWZtgk1eJTuEA and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:08:11,524] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-36 with topic ID 9kJAA8KZRoWZtgk1eJTuEA and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:08:11,525] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-37 with topic ID 9kJAA8KZRoWZtgk1eJTuEA and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:08:11,525] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-38 with topic ID 9kJAA8KZRoWZtgk1eJTuEA and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:08:11,525] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-39 with topic ID 9kJAA8KZRoWZtgk1eJTuEA and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:08:11,525] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-40 with topic ID 9kJAA8KZRoWZtgk1eJTuEA and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:08:11,525] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-41 with topic ID 9kJAA8KZRoWZtgk1eJTuEA and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:08:11,525] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-42 with topic ID 9kJAA8KZRoWZtgk1eJTuEA and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:08:11,525] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-43 with topic ID 9kJAA8KZRoWZtgk1eJTuEA and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:08:11,525] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-44 with topic ID 9kJAA8KZRoWZtgk1eJTuEA and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:08:11,525] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-45 with topic ID 9kJAA8KZRoWZtgk1eJTuEA and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:08:11,525] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-46 with topic ID 9kJAA8KZRoWZtgk1eJTuEA and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:08:11,525] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-47 with topic ID 9kJAA8KZRoWZtgk1eJTuEA and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:08:11,525] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-48 with topic ID 9kJAA8KZRoWZtgk1eJTuEA and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:08:11,525] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-link-metadata-49 with topic ID 9kJAA8KZRoWZtgk1eJTuEA and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:08:11,530] INFO Loaded KafkaHttpServer implementation class io.confluent.http.server.KafkaHttpServerImpl (io.confluent.kafka.http.server.KafkaHttpServerLoader)
benchi-kafka     | [2025-03-18 16:08:11,530] INFO KafkaHttpServer transitioned from NEW to STARTING.. (io.confluent.http.server.KafkaHttpServerImpl)
benchi-kafka     | [2025-03-18 16:08:11,532] INFO Registered MetricsListener to connector of listener: null (io.confluent.rest.ApplicationServer)
benchi-kafka     | [2025-03-18 16:08:11,555] INFO SBC Event SbcMetadataUpdateEvent-11 generated 1 more events to enqueue in the following order - [SbcConfigUpdateEvent-12]. Enqueuing... (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:08:11,555] INFO Handling event SbcKraftStartupEvent-5 (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:08:11,555] INFO Balancer Status state for brokers [1] transitioned from BALANCER_EVENT_RECEIVED to STARTING due to event INITIALIZING_CRUISE_CONTROL. (io.confluent.databalancer.operation.StateMachine)
benchi-kafka     | [2025-03-18 16:08:11,555] INFO DataBalancer: Activating SBC with io.confluent.databalancer.BrokersMetadataSnapshot@92dde37 (io.confluent.databalancer.KafkaDataBalanceManager)
benchi-kafka     | [2025-03-18 16:08:11,555] INFO [Broker id=1] Transitioning 50 partition(s) to local leaders. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,556] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-link-metadata-43, _confluent-link-metadata-10, _confluent-link-metadata-39, _confluent-link-metadata-6, _confluent-link-metadata-18, _confluent-link-metadata-47, _confluent-link-metadata-14, _confluent-link-metadata-27, _confluent-link-metadata-23, _confluent-link-metadata-35, _confluent-link-metadata-2, _confluent-link-metadata-31, _confluent-link-metadata-42, _confluent-link-metadata-13, _confluent-link-metadata-38, _confluent-link-metadata-9, _confluent-link-metadata-21, _confluent-link-metadata-46, _confluent-link-metadata-17, _confluent-link-metadata-26, _confluent-link-metadata-22, _confluent-link-metadata-34, _confluent-link-metadata-5, _confluent-link-metadata-30, _confluent-link-metadata-1, _confluent-link-metadata-45, _confluent-link-metadata-12, _confluent-link-metadata-41, _confluent-link-metadata-8, _confluent-link-metadata-20, _confluent-link-metadata-49, _confluent-link-metadata-16, _confluent-link-metadata-29, _confluent-link-metadata-25, _confluent-link-metadata-37, _confluent-link-metadata-4, _confluent-link-metadata-33, _confluent-link-metadata-0, _confluent-link-metadata-11, _confluent-link-metadata-44, _confluent-link-metadata-7, _confluent-link-metadata-40, _confluent-link-metadata-19, _confluent-link-metadata-15, _confluent-link-metadata-48, _confluent-link-metadata-28, _confluent-link-metadata-24, _confluent-link-metadata-3, _confluent-link-metadata-36, _confluent-link-metadata-32) (kafka.server.ReplicaFetcherManager)
benchi-kafka     | [2025-03-18 16:08:11,556] INFO DataBalancer: Scheduling DataBalanceEngine Startup (io.confluent.databalancer.ConfluentDataBalanceEngine)
benchi-kafka     | [2025-03-18 16:08:11,557] INFO [Broker id=1] Creating new partition _confluent-link-metadata-43 with topic id 9kJAA8KZRoWZtgk1eJTuEA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,559] INFO Handling event SbcKraftBrokerAdditionEvent-10 (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:08:11,559] INFO Processing SbcKraftBrokerAdditionEvent-10 event with data: empty_brokers: [], new_brokers: [1] (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:08:11,559] WARN Notified of broker additions (empty broker ids [], new brokers [1]) but DataBalancer is disabled -- ignoring for now (io.confluent.databalancer.KafkaDataBalanceManager)
benchi-kafka     | [2025-03-18 16:08:11,559] INFO Handling event SbcConfigUpdateEvent-12 (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:08:11,559] INFO Balancer notified of a config change: ConfigurationsDelta(changes={ConfigResource(type=TOPIC, name='_confluent-link-metadata')=ConfigurationDelta(changedKeys=[cleanup.policy, min.insync.replicas])}) (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:08:11,559] INFO There were 0 change(s) and 0 deletion(s) to balancer configs. Changed Configs: {}, Deleted Configs: [] (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:08:11,560] INFO DataBalancer: Bootstrap server endpoint is Endpoint(listenerName='PLAINTEXT', securityProtocol=PLAINTEXT, host='broker', port=29092) (io.confluent.databalancer.startup.CruiseControlStartable)
benchi-kafka     | [2025-03-18 16:08:11,560] INFO DataBalancer: BOOTSTRAP_SERVERS determined to be broker:29092 (io.confluent.databalancer.startup.CruiseControlStartable)
benchi-kafka     | [2025-03-18 16:08:11,560] INFO KafkaCruiseControlConfig values: 
benchi-kafka     | 	alter.configs.response.timeout.ms = 30000
benchi-kafka     | 	anomaly.detection.allow.capacity.estimation = true
benchi-kafka     | 	anomaly.detection.goals = [io.confluent.cruisecontrol.analyzer.goals.ReplicaPlacementGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.RackAwareGoal, io.confluent.cruisecontrol.analyzer.goals.CellAwareGoal, io.confluent.cruisecontrol.analyzer.goals.TenantAwareGoal, io.confluent.cruisecontrol.analyzer.goals.MaxReplicaMovementParallelismGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.ReplicaCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.DiskCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.NetworkInboundCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.NetworkOutboundCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.ReplicationInboundCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.ProducerInboundCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.ReplicaDistributionGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.DiskUsageDistributionGoal]
benchi-kafka     | 	anomaly.detection.interval.ms = 60000
benchi-kafka     | 	bootstrap.servers = [broker:29092]
benchi-kafka     | 	broker.addition.elapsed.time.ms.completion.threshold = 57600000
benchi-kafka     | 	broker.addition.mean.cpu.percent.completion.threshold = 0.5
benchi-kafka     | 	broker.capacity.config.resolver.class = class com.linkedin.kafka.cruisecontrol.config.BrokerCapacityResolver
benchi-kafka     | 	broker.failure.alert.threshold.ms = 0
benchi-kafka     | 	broker.failure.exclude.recently.removed.brokers = true
benchi-kafka     | 	broker.failure.self.healing.threshold.ms = 3600000
benchi-kafka     | 	broker.metric.sample.aggregator.completeness.cache.size = 5
benchi-kafka     | 	broker.metric.sample.store.topic = _confluent_balancer_broker_samples
benchi-kafka     | 	broker.removal.shutdown.timeout.ms = 600000
benchi-kafka     | 	broker.replica.exclusion.timeout.ms = 120000
benchi-kafka     | 	bytes.cpu.contribution.weight = 0.2
benchi-kafka     | 	calculated.throttle.ratio = 0.8
benchi-kafka     | 	capacity.threshold.upper.limit = 0.95
benchi-kafka     | 	cdbe.shutdown.wait.ms = 15000
benchi-kafka     | 	cell.load.upper.bound = 0.7
benchi-kafka     | 	cell.overload.detection.interval.ms = 3600000
benchi-kafka     | 	cell.overload.duration.ms = 86400000
benchi-kafka     | 	client.id = kafka-cruise-control
benchi-kafka     | 	confluent.balancer.additional.invalidation.duration.ms = 60000
benchi-kafka     | 	confluent.balancer.plan.computation.retry.timeout.ms = 3600000
benchi-kafka     | 	confluent.cells.enable = false
benchi-kafka     | 	connections.max.idle.ms = 540000
benchi-kafka     | 	consume.out.bound.should.balance.FFF.traffic = false
benchi-kafka     | 	consumer.out.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	consumer.outbound.capacity.threshold = 0.9
benchi-kafka     | 	cpu.balance.threshold = 1.1
benchi-kafka     | 	cpu.capacity.threshold = 1.0
benchi-kafka     | 	cpu.low.utilization.threshold = 0.2
benchi-kafka     | 	cpu.low.utilization.threshold.for.broker.addition = 0.2
benchi-kafka     | 	cpu.utilization.detector.duration.ms = 600000
benchi-kafka     | 	cpu.utilization.detector.enabled = false
benchi-kafka     | 	cpu.utilization.detector.overutilization.threshold = 80.0
benchi-kafka     | 	cpu.utilization.detector.underutilization.threshold = 50.0
benchi-kafka     | 	default.api.timeout.ms = 60000
benchi-kafka     | 	default.replica.movement.strategies = [com.linkedin.kafka.cruisecontrol.executor.strategy.BaseReplicaMovementStrategy]
benchi-kafka     | 	describe.broker.exclusion.timeout.ms = 60000
benchi-kafka     | 	describe.cluster.response.timeout.ms = 30000
benchi-kafka     | 	describe.configs.batch.size = 1000
benchi-kafka     | 	describe.configs.response.timeout.ms = 30000
benchi-kafka     | 	describe.topics.response.timeout.ms = 30000
benchi-kafka     | 	disk.balance.threshold = 1.1
benchi-kafka     | 	disk.low.utilization.threshold = 0.2
benchi-kafka     | 	disk.max.load = 0.85
benchi-kafka     | 	disk.min.free.space.gb = 0
benchi-kafka     | 	disk.min.free.space.lower.limit.gb = 0
benchi-kafka     | 	disk.read.ratio = 0.2
benchi-kafka     | 	disk.utilization.detector.duration.ms = 600000
benchi-kafka     | 	disk.utilization.detector.enabled = false
benchi-kafka     | 	disk.utilization.detector.overutilization.threshold = 80.0
benchi-kafka     | 	disk.utilization.detector.reserved.capacity = 150000.0
benchi-kafka     | 	disk.utilization.detector.underutilization.threshold = 35.0
benchi-kafka     | 	dynamic.throttling.enabled = true
benchi-kafka     | 	execution.progress.check.interval.ms = 7000
benchi-kafka     | 	executor.leader.action.timeout.ms = 180000
benchi-kafka     | 	executor.notifier.class = class com.linkedin.kafka.cruisecontrol.executor.ExecutorNoopNotifier
benchi-kafka     | 	executor.reservation.refresh.time.ms = 60000
benchi-kafka     | 	follower.network.inbound.weight.for.cpu.util = 0.15
benchi-kafka     | 	goal.balancedness.priority.weight = 1.1
benchi-kafka     | 	goal.balancedness.strictness.weight = 1.5
benchi-kafka     | 	goal.violation.delay.on.new.brokers.ms = 1800000
benchi-kafka     | 	goal.violation.distribution.threshold.multiplier = 1.1
benchi-kafka     | 	goal.violation.exclude.recently.removed.brokers = true
benchi-kafka     | 	goals = [com.linkedin.kafka.cruisecontrol.analyzer.goals.MovementExclusionGoal, io.confluent.cruisecontrol.analyzer.goals.ReplicaPlacementGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.RackAwareGoal, io.confluent.cruisecontrol.analyzer.goals.CellAwareGoal, io.confluent.cruisecontrol.analyzer.goals.TenantAwareGoal, io.confluent.cruisecontrol.analyzer.goals.MaxReplicaMovementParallelismGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.ReplicaCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.DiskCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.NetworkInboundCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.NetworkOutboundCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.ReplicationInboundCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.ProducerInboundCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.MirrorInboundCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.ConsumerOutboundCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.SystemTopicEvenDistributionGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.ReplicaDistributionGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.DiskUsageDistributionGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.LeaderReplicaDistributionGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.NetworkInboundUsageDistributionGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.NetworkOutboundUsageDistributionGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.TopicReplicaDistributionGoal]
benchi-kafka     | 	hot.partition.capacity.utilization.threshold = 0.2
benchi-kafka     | 	incremental.balancing.cpu.top.proposal.tracking.enabled = true
benchi-kafka     | 	incremental.balancing.cpu.top.proposal.tracking.num.proposals = 15
benchi-kafka     | 	incremental.balancing.enabled = false
benchi-kafka     | 	incremental.balancing.goals = [com.linkedin.kafka.cruisecontrol.analyzer.goals.MovementExclusionGoal, io.confluent.cruisecontrol.analyzer.goals.ReplicaPlacementGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.RackAwareGoal, io.confluent.cruisecontrol.analyzer.goals.CellAwareGoal, io.confluent.cruisecontrol.analyzer.goals.TenantAwareGoal, io.confluent.cruisecontrol.analyzer.goals.MaxReplicaMovementParallelismGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.ReplicaCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.DiskCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.NetworkInboundCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.NetworkOutboundCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.ReplicationInboundCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.ProducerInboundCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.MirrorInboundCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.ConsumerOutboundCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.IncrementalCPUResourceDistributionGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.IncrementalTopicReplicaDistributionGoal]
benchi-kafka     | 	incremental.balancing.lower.bound = 0.02
benchi-kafka     | 	incremental.balancing.min.valid.windows = 5
benchi-kafka     | 	incremental.balancing.step.ratio = 0.2
benchi-kafka     | 	inter.cell.balancing.enabled = false
benchi-kafka     | 	invalid.replica.assignment.retry.timeout.ms = 300000
benchi-kafka     | 	leader.network.inbound.weight.for.cpu.util = 0.7
benchi-kafka     | 	leader.network.outbound.weight.for.cpu.util = 0.15
benchi-kafka     | 	leader.replica.count.balance.threshold = 1.1
benchi-kafka     | 	logdir.response.timeout.ms = 30000
benchi-kafka     | 	max.allowed.extrapolations.per.broker = 5
benchi-kafka     | 	max.allowed.extrapolations.per.partition = 5
benchi-kafka     | 	max.capacity.balancing.delta.percentage = 0.0
benchi-kafka     | 	max.replicas = 2147483647
benchi-kafka     | 	max.volume.throughput.mb = 0
benchi-kafka     | 	metadata.client.timeout.ms = 180000
benchi-kafka     | 	metadata.ttl = 10000
benchi-kafka     | 	metric.sampler.class = class io.confluent.cruisecontrol.metricsreporter.ConfluentTelemetryReporterSampler
benchi-kafka     | 	min.samples.per.partition.metrics.window = 1
benchi-kafka     | 	min.valid.partition.ratio = 0.95
benchi-kafka     | 	network.in.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	network.inbound.balance.threshold = 1.1
benchi-kafka     | 	network.inbound.capacity.threshold = 0.8
benchi-kafka     | 	network.inbound.low.utilization.threshold = 0.2
benchi-kafka     | 	network.out.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	network.outbound.balance.threshold = 1.1
benchi-kafka     | 	network.outbound.capacity.threshold = 0.8
benchi-kafka     | 	network.outbound.low.utilization.threshold = 0.2
benchi-kafka     | 	num.cached.recent.anomaly.states = 10
benchi-kafka     | 	num.concurrent.leader.movements = 1000
benchi-kafka     | 	num.concurrent.partition.movements.per.broker = 5
benchi-kafka     | 	num.metric.fetchers = 1
benchi-kafka     | 	num.partition.metrics.windows = 12
benchi-kafka     | 	partition.metric.sample.aggregator.completeness.cache.size = 5
benchi-kafka     | 	partition.metric.sample.store.topic = _confluent_balancer_partition_samples
benchi-kafka     | 	partition.metrics.window.ms = 180000
benchi-kafka     | 	populate.default.disk.capacity.from.local = true
benchi-kafka     | 	producer.in.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	producer.inbound.capacity.threshold = 0.9
benchi-kafka     | 	read.throughput.multiplier = 1.0
benchi-kafka     | 	receive.buffer.bytes = 32768
benchi-kafka     | 	reconnect.backoff.ms = 50
benchi-kafka     | 	removal.history.retention.time.ms = 86400000
benchi-kafka     | 	replica.count.balance.threshold = 1.1
benchi-kafka     | 	replica.movement.strategies = [com.linkedin.kafka.cruisecontrol.executor.strategy.PostponeUrpReplicaMovementStrategy, com.linkedin.kafka.cruisecontrol.executor.strategy.PrioritizeLargeReplicaMovementStrategy, com.linkedin.kafka.cruisecontrol.executor.strategy.PrioritizeSmallReplicaMovementStrategy, com.linkedin.kafka.cruisecontrol.executor.strategy.BaseReplicaMovementStrategy]
benchi-kafka     | 	replication.in.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	replication.inbound.capacity.threshold = 0.9
benchi-kafka     | 	request.cpu.contribution.weight = 0.8
benchi-kafka     | 	request.timeout.ms = 30000
benchi-kafka     | 	resource.utilization.detector.interval.ms = 60000
benchi-kafka     | 	sampling.allow.cpu.capacity.estimation = true
benchi-kafka     | 	sasl.client.callback.handler.class = null
benchi-kafka     | 	sasl.jaas.config = null
benchi-kafka     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
benchi-kafka     | 	sasl.kerberos.min.time.before.relogin = 60000
benchi-kafka     | 	sasl.kerberos.service.name = null
benchi-kafka     | 	sasl.kerberos.ticket.renew.jitter = 0.05
benchi-kafka     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
benchi-kafka     | 	sasl.login.callback.handler.class = null
benchi-kafka     | 	sasl.login.class = null
benchi-kafka     | 	sasl.login.connect.timeout.ms = null
benchi-kafka     | 	sasl.login.read.timeout.ms = null
benchi-kafka     | 	sasl.login.refresh.buffer.seconds = 300
benchi-kafka     | 	sasl.login.refresh.min.period.seconds = 60
benchi-kafka     | 	sasl.login.refresh.window.factor = 0.8
benchi-kafka     | 	sasl.login.refresh.window.jitter = 0.05
benchi-kafka     | 	sasl.login.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.login.retry.backoff.ms = 100
benchi-kafka     | 	sasl.mechanism = GSSAPI
benchi-kafka     | 	sasl.oauthbearer.clientassertion.audience = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.expiration = 5
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.jti.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.nbf.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.issuer = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.location = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key.passphrase = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.subject = null
benchi-kafka     | 	sasl.oauthbearer.clock.skew.seconds = 30
benchi-kafka     | 	sasl.oauthbearer.expected.audience = null
benchi-kafka     | 	sasl.oauthbearer.expected.issuer = null
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.url = null
benchi-kafka     | 	sasl.oauthbearer.scope.claim.name = scope
benchi-kafka     | 	sasl.oauthbearer.sub.claim.name = sub
benchi-kafka     | 	sasl.oauthbearer.token.endpoint.url = null
benchi-kafka     | 	sbc.metrics.parser.enabled = false
benchi-kafka     | 	security.protocol = PLAINTEXT
benchi-kafka     | 	self.healing.broker.failure.enabled = true
benchi-kafka     | 	self.healing.goal.violation.enabled = false
benchi-kafka     | 	self.healing.maximum.rounds = 1
benchi-kafka     | 	send.buffer.bytes = 131072
benchi-kafka     | 	ssl.cipher.suites = null
benchi-kafka     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	ssl.engine.factory.class = null
benchi-kafka     | 	ssl.key.password = null
benchi-kafka     | 	ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	ssl.keystore.certificate.chain = null
benchi-kafka     | 	ssl.keystore.key = null
benchi-kafka     | 	ssl.keystore.location = null
benchi-kafka     | 	ssl.keystore.password = null
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.protocol = TLSv1.3
benchi-kafka     | 	ssl.provider = null
benchi-kafka     | 	ssl.secure.random.implementation = null
benchi-kafka     | 	ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	ssl.truststore.certificates = null
benchi-kafka     | 	ssl.truststore.location = null
benchi-kafka     | 	ssl.truststore.password = null
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     | 	startup.retry.delay.minutes = 5
benchi-kafka     | 	startup.retry.max.hours = 2
benchi-kafka     | 	static.throttle.rate.override.enabled = false
benchi-kafka     | 	tenant.maximum.movements = 0
benchi-kafka     | 	tenant.suspension.ms = 86400000
benchi-kafka     | 	throttle.bytes.per.second = 10485760
benchi-kafka     | 	topic.balancing.badly.imbalanced.topic.imbalance.score.threshold = 0.3
benchi-kafka     | 	topic.balancing.balance.threshold.multiplier = 1.0
benchi-kafka     | 	topic.balancing.broker.addition.completion.percentage = 0.8
benchi-kafka     | 	topic.balancing.broker.addition.detector.with.trdg.enabled = false
benchi-kafka     | 	topic.balancing.imbalanced.score.threshold = 0.07
benchi-kafka     | 	topic.balancing.max.reassignments.per.iteration = -1
benchi-kafka     | 	topic.balancing.slightly.imbalanced.topic.imbalance.score.threshold = 0.05
benchi-kafka     | 	topic.balancing.slightly.imbalanced.topics.percentage.trigger = 0.2
benchi-kafka     | 	topic.balancing.trigger.threshold.multiplier = 3.0
benchi-kafka     | 	topic.partition.maximum.movements = 5
benchi-kafka     | 	topic.partition.movement.expiration.ms = 10800000
benchi-kafka     | 	topic.partition.suspension.ms = 18000000
benchi-kafka     | 	topics.excluded.from.partition.movement = 
benchi-kafka     | 	v2.addition.enabled = false
benchi-kafka     | 	write.throughput.multiplier = 1.0
benchi-kafka     | 	zookeeper.connect = 
benchi-kafka     | 	zookeeper.security.enabled = false
benchi-kafka     |  (com.linkedin.kafka.cruisecontrol.config.KafkaCruiseControlConfig)
benchi-kafka     | [2025-03-18 16:08:11,561] INFO DataBalancer: Instantiating DataBalanceEngine (io.confluent.databalancer.ConfluentDataBalanceEngine)
benchi-kafka     | [2025-03-18 16:08:11,563] INFO DataBalancer: Checking startup components (io.confluent.databalancer.startup.CruiseControlStartable)
benchi-kafka     | [2025-03-18 16:08:11,563] INFO DataBalancer: Checking startup component StartupComponent ConfluentTelemetryReporterSampler (io.confluent.databalancer.startup.StartupComponents)
benchi-kafka     | [2025-03-18 16:08:11,563] INFO [Broker id=1] Creating new partition _confluent-link-metadata-10 with topic id 9kJAA8KZRoWZtgk1eJTuEA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,563] INFO [Broker id=1] Creating new partition _confluent-link-metadata-39 with topic id 9kJAA8KZRoWZtgk1eJTuEA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,564] INFO [Broker id=1] Creating new partition _confluent-link-metadata-6 with topic id 9kJAA8KZRoWZtgk1eJTuEA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,564] INFO [Broker id=1] Creating new partition _confluent-link-metadata-18 with topic id 9kJAA8KZRoWZtgk1eJTuEA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,565] INFO [Broker id=1] Creating new partition _confluent-link-metadata-47 with topic id 9kJAA8KZRoWZtgk1eJTuEA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,565] INFO [Broker id=1] Creating new partition _confluent-link-metadata-14 with topic id 9kJAA8KZRoWZtgk1eJTuEA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,566] INFO [Broker id=1] Creating new partition _confluent-link-metadata-27 with topic id 9kJAA8KZRoWZtgk1eJTuEA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,566] INFO [Broker id=1] Creating new partition _confluent-link-metadata-23 with topic id 9kJAA8KZRoWZtgk1eJTuEA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,566] INFO [Broker id=1] Creating new partition _confluent-link-metadata-35 with topic id 9kJAA8KZRoWZtgk1eJTuEA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,567] INFO [Broker id=1] Creating new partition _confluent-link-metadata-2 with topic id 9kJAA8KZRoWZtgk1eJTuEA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,567] WARN Disabling exponential reconnect backoff because reconnect.backoff.ms is set, but reconnect.backoff.max.ms is not. (org.apache.kafka.clients.CommonClientConfigs)
benchi-kafka     | [2025-03-18 16:08:11,567] INFO [Broker id=1] Creating new partition _confluent-link-metadata-31 with topic id 9kJAA8KZRoWZtgk1eJTuEA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,567] INFO ConsumerConfig values: 
benchi-kafka     | 	allow.auto.create.topics = true
benchi-kafka     | 	auto.commit.interval.ms = 5000
benchi-kafka     | 	auto.include.jmx.reporter = true
benchi-kafka     | 	auto.offset.reset = latest
benchi-kafka     | 	bootstrap.servers = [broker:29092]
benchi-kafka     | 	check.crcs = true
benchi-kafka     | 	client.dns.lookup = use_all_dns_ips
benchi-kafka     | 	client.id = kafka-cruise-control
benchi-kafka     | 	client.rack = 
benchi-kafka     | 	confluent.lkc.id = null
benchi-kafka     | 	confluent.proxy.protocol.client.address = null
benchi-kafka     | 	confluent.proxy.protocol.client.mode = PROXY
benchi-kafka     | 	confluent.proxy.protocol.client.port = null
benchi-kafka     | 	confluent.proxy.protocol.client.version = NONE
benchi-kafka     | 	connections.max.idle.ms = 540000
benchi-kafka     | 	default.api.timeout.ms = 60000
benchi-kafka     | 	enable.auto.commit = false
benchi-kafka     | 	enable.metrics.push = true
benchi-kafka     | 	exclude.internal.topics = true
benchi-kafka     | 	fetch.max.bytes = 52428800
benchi-kafka     | 	fetch.max.wait.ms = 500
benchi-kafka     | 	fetch.min.bytes = 1
benchi-kafka     | 	group.id = ConfluentTelemetryReporterSampler-6820492430475519429
benchi-kafka     | 	group.instance.id = null
benchi-kafka     | 	group.protocol = classic
benchi-kafka     | 	group.remote.assignor = null
benchi-kafka     | 	heartbeat.interval.ms = 3000
benchi-kafka     | 	interceptor.classes = []
benchi-kafka     | 	internal.leave.group.on.close = true
benchi-kafka     | 	internal.throw.on.fetch.stable.offset.unsupported = false
benchi-kafka     | 	isolation.level = read_uncommitted
benchi-kafka     | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
benchi-kafka     | 	max.partition.fetch.bytes = 1048576
benchi-kafka     | 	max.poll.interval.ms = 2147483647
benchi-kafka     | 	max.poll.records = 2147483647
benchi-kafka     | 	metadata.max.age.ms = 300000
benchi-kafka     | 	metadata.recovery.strategy = none
benchi-kafka     | 	metric.reporters = []
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.recording.level = INFO
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
benchi-kafka     | 	receive.buffer.bytes = 32768
benchi-kafka     | 	reconnect.backoff.max.ms = 50
benchi-kafka     | 	reconnect.backoff.ms = 50
benchi-kafka     | 	request.timeout.ms = 30000
benchi-kafka     | 	retry.backoff.max.ms = 1000
benchi-kafka     | 	retry.backoff.ms = 100
benchi-kafka     | 	sasl.client.callback.handler.class = null
benchi-kafka     | 	sasl.jaas.config = null
benchi-kafka     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
benchi-kafka     | 	sasl.kerberos.min.time.before.relogin = 60000
benchi-kafka     | 	sasl.kerberos.service.name = null
benchi-kafka     | 	sasl.kerberos.ticket.renew.jitter = 0.05
benchi-kafka     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
benchi-kafka     | 	sasl.login.callback.handler.class = null
benchi-kafka     | 	sasl.login.class = null
benchi-kafka     | 	sasl.login.connect.timeout.ms = null
benchi-kafka     | 	sasl.login.read.timeout.ms = null
benchi-kafka     | 	sasl.login.refresh.buffer.seconds = 300
benchi-kafka     | 	sasl.login.refresh.min.period.seconds = 60
benchi-kafka     | 	sasl.login.refresh.window.factor = 0.8
benchi-kafka     | 	sasl.login.refresh.window.jitter = 0.05
benchi-kafka     | 	sasl.login.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.login.retry.backoff.ms = 100
benchi-kafka     | 	sasl.mechanism = GSSAPI
benchi-kafka     | 	sasl.oauthbearer.clientassertion.audience = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.expiration = 5
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.jti.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.nbf.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.issuer = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.location = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key.passphrase = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.subject = null
benchi-kafka     | 	sasl.oauthbearer.clock.skew.seconds = 30
benchi-kafka     | 	sasl.oauthbearer.expected.audience = null
benchi-kafka     | 	sasl.oauthbearer.expected.issuer = null
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.url = null
benchi-kafka     | 	sasl.oauthbearer.scope.claim.name = scope
benchi-kafka     | 	sasl.oauthbearer.sub.claim.name = sub
benchi-kafka     | 	sasl.oauthbearer.token.endpoint.url = null
benchi-kafka     | 	security.protocol = PLAINTEXT
benchi-kafka     | 	security.providers = null
benchi-kafka     | 	send.buffer.bytes = 131072
benchi-kafka     | 	session.timeout.ms = 45000
benchi-kafka     | 	socket.connection.setup.timeout.max.ms = 30000
benchi-kafka     | 	socket.connection.setup.timeout.ms = 10000
benchi-kafka     | 	ssl.cipher.suites = null
benchi-kafka     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	ssl.engine.factory.class = null
benchi-kafka     | 	ssl.key.password = null
benchi-kafka     | 	ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	ssl.keystore.certificate.chain = null
benchi-kafka     | 	ssl.keystore.key = null
benchi-kafka     | 	ssl.keystore.location = null
benchi-kafka     | 	ssl.keystore.password = null
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.protocol = TLSv1.3
benchi-kafka     | 	ssl.provider = null
benchi-kafka     | 	ssl.secure.random.implementation = null
benchi-kafka     | 	ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	ssl.truststore.certificates = null
benchi-kafka     | 	ssl.truststore.location = null
benchi-kafka     | 	ssl.truststore.password = null
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
benchi-kafka     |  (org.apache.kafka.clients.consumer.ConsumerConfig)
benchi-kafka     | [2025-03-18 16:08:11,567] INFO [Broker id=1] Creating new partition _confluent-link-metadata-42 with topic id 9kJAA8KZRoWZtgk1eJTuEA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,567] INFO Binding MetadataApiApplication to all listeners. (io.confluent.rest.Application)
benchi-kafka     | [2025-03-18 16:08:11,568] INFO [Broker id=1] Creating new partition _confluent-link-metadata-13 with topic id 9kJAA8KZRoWZtgk1eJTuEA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,568] INFO [Broker id=1] Creating new partition _confluent-link-metadata-38 with topic id 9kJAA8KZRoWZtgk1eJTuEA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,568] INFO [Broker id=1] Creating new partition _confluent-link-metadata-9 with topic id 9kJAA8KZRoWZtgk1eJTuEA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,569] INFO [Broker id=1] Creating new partition _confluent-link-metadata-21 with topic id 9kJAA8KZRoWZtgk1eJTuEA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,569] INFO [Broker id=1] Creating new partition _confluent-link-metadata-46 with topic id 9kJAA8KZRoWZtgk1eJTuEA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,569] INFO [Broker id=1] Creating new partition _confluent-link-metadata-17 with topic id 9kJAA8KZRoWZtgk1eJTuEA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,570] INFO [Broker id=1] Creating new partition _confluent-link-metadata-26 with topic id 9kJAA8KZRoWZtgk1eJTuEA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,570] INFO [Broker id=1] Creating new partition _confluent-link-metadata-22 with topic id 9kJAA8KZRoWZtgk1eJTuEA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,571] INFO [Broker id=1] Creating new partition _confluent-link-metadata-34 with topic id 9kJAA8KZRoWZtgk1eJTuEA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,571] INFO [Broker id=1] Creating new partition _confluent-link-metadata-5 with topic id 9kJAA8KZRoWZtgk1eJTuEA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,571] INFO [Broker id=1] Creating new partition _confluent-link-metadata-30 with topic id 9kJAA8KZRoWZtgk1eJTuEA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,571] INFO [Broker id=1] Creating new partition _confluent-link-metadata-1 with topic id 9kJAA8KZRoWZtgk1eJTuEA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,572] INFO [Broker id=1] Creating new partition _confluent-link-metadata-45 with topic id 9kJAA8KZRoWZtgk1eJTuEA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,572] INFO initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector)
benchi-kafka     | [2025-03-18 16:08:11,572] INFO [Broker id=1] Creating new partition _confluent-link-metadata-12 with topic id 9kJAA8KZRoWZtgk1eJTuEA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,573] INFO [Broker id=1] Creating new partition _confluent-link-metadata-41 with topic id 9kJAA8KZRoWZtgk1eJTuEA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,573] INFO [Broker id=1] Creating new partition _confluent-link-metadata-8 with topic id 9kJAA8KZRoWZtgk1eJTuEA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,573] INFO [Broker id=1] Creating new partition _confluent-link-metadata-20 with topic id 9kJAA8KZRoWZtgk1eJTuEA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,574] INFO [Broker id=1] Creating new partition _confluent-link-metadata-49 with topic id 9kJAA8KZRoWZtgk1eJTuEA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,574] INFO [Broker id=1] Creating new partition _confluent-link-metadata-16 with topic id 9kJAA8KZRoWZtgk1eJTuEA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,574] INFO [Broker id=1] Creating new partition _confluent-link-metadata-29 with topic id 9kJAA8KZRoWZtgk1eJTuEA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,575] INFO [Broker id=1] Creating new partition _confluent-link-metadata-25 with topic id 9kJAA8KZRoWZtgk1eJTuEA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,575] INFO [Broker id=1] Creating new partition _confluent-link-metadata-37 with topic id 9kJAA8KZRoWZtgk1eJTuEA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,576] INFO [Broker id=1] Creating new partition _confluent-link-metadata-4 with topic id 9kJAA8KZRoWZtgk1eJTuEA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,576] INFO [Broker id=1] Creating new partition _confluent-link-metadata-33 with topic id 9kJAA8KZRoWZtgk1eJTuEA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,577] INFO [Broker id=1] Creating new partition _confluent-link-metadata-0 with topic id 9kJAA8KZRoWZtgk1eJTuEA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,577] INFO [Broker id=1] Creating new partition _confluent-link-metadata-11 with topic id 9kJAA8KZRoWZtgk1eJTuEA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,577] INFO [Broker id=1] Creating new partition _confluent-link-metadata-44 with topic id 9kJAA8KZRoWZtgk1eJTuEA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,578] INFO [Broker id=1] Creating new partition _confluent-link-metadata-7 with topic id 9kJAA8KZRoWZtgk1eJTuEA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,578] INFO [Broker id=1] Creating new partition _confluent-link-metadata-40 with topic id 9kJAA8KZRoWZtgk1eJTuEA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,578] INFO [Broker id=1] Creating new partition _confluent-link-metadata-19 with topic id 9kJAA8KZRoWZtgk1eJTuEA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,579] INFO [Broker id=1] Creating new partition _confluent-link-metadata-15 with topic id 9kJAA8KZRoWZtgk1eJTuEA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,579] INFO [Broker id=1] Creating new partition _confluent-link-metadata-48 with topic id 9kJAA8KZRoWZtgk1eJTuEA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,580] INFO [Broker id=1] Creating new partition _confluent-link-metadata-28 with topic id 9kJAA8KZRoWZtgk1eJTuEA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,580] INFO [Broker id=1] Creating new partition _confluent-link-metadata-24 with topic id 9kJAA8KZRoWZtgk1eJTuEA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,581] INFO [Broker id=1] Creating new partition _confluent-link-metadata-3 with topic id 9kJAA8KZRoWZtgk1eJTuEA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,581] INFO [Broker id=1] Creating new partition _confluent-link-metadata-36 with topic id 9kJAA8KZRoWZtgk1eJTuEA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,582] INFO [Broker id=1] Creating new partition _confluent-link-metadata-32 with topic id 9kJAA8KZRoWZtgk1eJTuEA. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,584] INFO [Broker id=1] Stopped fetchers as part of become-leader transition for 50 partitions (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,593] INFO These configurations '[sasl.oauthbearer.jwks.endpoint.refresh.ms, sasl.oauthbearer.clientassertion.include.jti.claim, sasl.login.refresh.min.period.seconds, sasl.oauthbearer.scope.claim.name, sasl.login.refresh.window.factor, sasl.kerberos.ticket.renew.window.factor, sasl.login.retry.backoff.ms, sasl.oauthbearer.clientassertion.expiration, sasl.kerberos.kinit.cmd, sasl.kerberos.ticket.renew.jitter, ssl.keystore.type, ssl.trustmanager.algorithm, sasl.kerberos.min.time.before.relogin, ssl.endpoint.identification.algorithm, ssl.protocol, sasl.login.refresh.buffer.seconds, sasl.login.retry.backoff.max.ms, ssl.enabled.protocols, sasl.oauthbearer.sub.claim.name, ssl.truststore.type, sasl.oauthbearer.jwks.endpoint.retry.backoff.ms, sasl.oauthbearer.clock.skew.seconds, ssl.keymanager.algorithm, sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms, sasl.oauthbearer.clientassertion.include.nbf.claim, sasl.login.refresh.window.jitter]' were supplied but are not used yet. (org.apache.kafka.clients.consumer.ConsumerConfig)
benchi-kafka     | [2025-03-18 16:08:11,593] INFO Kafka version: 7.8.1-ce (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:08:11,593] INFO Kafka commitId: eefb51c6f2f1b888b8b03296353338afbac8e213 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:08:11,593] INFO Kafka startTimeMs: 1742314091593 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:08:11,593] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-6820492430475519429] Subscribed to pattern: '_confluent-telemetry-metrics' (org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer)
benchi-kafka     | [2025-03-18 16:08:11,596] INFO [MergedLog partition=_confluent-link-metadata-21, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:08:11,597] INFO Created log for partition _confluent-link-metadata-21 in /tmp/kraft-combined-logs/_confluent-link-metadata-21 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:08:11,598] INFO [Partition _confluent-link-metadata-21 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-21 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,598] INFO [Partition _confluent-link-metadata-21 broker=1] Log loaded for partition _confluent-link-metadata-21 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,599] INFO [MergedLog partition=_confluent-link-metadata-18, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:08:11,599] INFO Created log for partition _confluent-link-metadata-18 in /tmp/kraft-combined-logs/_confluent-link-metadata-18 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:08:11,599] INFO [Partition _confluent-link-metadata-18 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-18 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,600] INFO Setting topicIdPartition 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-21 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:08:11,600] INFO [Partition _confluent-link-metadata-18 broker=1] Log loaded for partition _confluent-link-metadata-18 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,600] INFO [MergedLog partition=_confluent-link-metadata-21, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-21 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:08:11,600] INFO Setting topicIdPartition 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-18 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:08:11,600] INFO [MergedLog partition=_confluent-link-metadata-18, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-18 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:08:11,601] INFO [MergedLog partition=_confluent-link-metadata-20, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:08:11,602] INFO Created log for partition _confluent-link-metadata-20 in /tmp/kraft-combined-logs/_confluent-link-metadata-20 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:08:11,602] INFO [Partition _confluent-link-metadata-20 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-20 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,602] INFO [Partition _confluent-link-metadata-20 broker=1] Log loaded for partition _confluent-link-metadata-20 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,602] INFO Setting topicIdPartition 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-20 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:08:11,602] INFO [MergedLog partition=_confluent-link-metadata-20, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-20 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:08:11,603] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-6820492430475519429] Cluster ID: MkU3OEVBNTcwNTJENDM2Qk (org.apache.kafka.clients.Metadata)
benchi-kafka     | [2025-03-18 16:08:11,603] INFO [MergedLog partition=_confluent-link-metadata-39, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:08:11,603] INFO Created log for partition _confluent-link-metadata-39 in /tmp/kraft-combined-logs/_confluent-link-metadata-39 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:08:11,603] INFO [Partition _confluent-link-metadata-39 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-39 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,603] INFO [Partition _confluent-link-metadata-39 broker=1] Log loaded for partition _confluent-link-metadata-39 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,603] INFO Setting topicIdPartition 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-39 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:08:11,603] INFO [MergedLog partition=_confluent-link-metadata-39, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-39 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:08:11,603] INFO [Broker id=1] Leader _confluent-link-metadata-39 with topic id Some(9kJAA8KZRoWZtgk1eJTuEA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,603] INFO [Broker id=1] Leader _confluent-link-metadata-21 with topic id Some(9kJAA8KZRoWZtgk1eJTuEA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,603] INFO [Broker id=1] Leader _confluent-link-metadata-20 with topic id Some(9kJAA8KZRoWZtgk1eJTuEA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,604] INFO [Broker id=1] Leader _confluent-link-metadata-18 with topic id Some(9kJAA8KZRoWZtgk1eJTuEA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,604] INFO Waiting for 1 seconds for metric reporter topic _confluent-telemetry-metrics to become available. (io.confluent.cruisecontrol.metricsreporter.ConfluentMetricsSamplerBase)
benchi-kafka     | [2025-03-18 16:08:11,605] INFO [MergedLog partition=_confluent-link-metadata-24, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:08:11,605] INFO Created log for partition _confluent-link-metadata-24 in /tmp/kraft-combined-logs/_confluent-link-metadata-24 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:08:11,605] INFO [Partition _confluent-link-metadata-24 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-24 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,605] INFO [Partition _confluent-link-metadata-24 broker=1] Log loaded for partition _confluent-link-metadata-24 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,606] INFO Setting topicIdPartition 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-24 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:08:11,606] INFO [MergedLog partition=_confluent-link-metadata-24, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-24 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:08:11,606] INFO [Broker id=1] Leader _confluent-link-metadata-24 with topic id Some(9kJAA8KZRoWZtgk1eJTuEA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,606] INFO Registered MetricsListener to connector of listener: null (io.confluent.rest.ApplicationServer)
benchi-kafka     | [2025-03-18 16:08:11,608] INFO [MergedLog partition=_confluent-link-metadata-23, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:08:11,608] INFO Created log for partition _confluent-link-metadata-23 in /tmp/kraft-combined-logs/_confluent-link-metadata-23 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:08:11,608] INFO [Partition _confluent-link-metadata-23 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-23 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,608] INFO [Partition _confluent-link-metadata-23 broker=1] Log loaded for partition _confluent-link-metadata-23 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,608] INFO Setting topicIdPartition 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-23 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:08:11,608] INFO [MergedLog partition=_confluent-link-metadata-23, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-23 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:08:11,608] INFO [Broker id=1] Leader _confluent-link-metadata-23 with topic id Some(9kJAA8KZRoWZtgk1eJTuEA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,610] INFO [MergedLog partition=_confluent-link-metadata-43, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:08:11,610] INFO Created log for partition _confluent-link-metadata-43 in /tmp/kraft-combined-logs/_confluent-link-metadata-43 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:08:11,610] INFO [Partition _confluent-link-metadata-43 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-43 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,610] INFO [Partition _confluent-link-metadata-43 broker=1] Log loaded for partition _confluent-link-metadata-43 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,610] INFO Setting topicIdPartition 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-43 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:08:11,610] INFO [MergedLog partition=_confluent-link-metadata-43, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-43 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:08:11,611] INFO [Broker id=1] Leader _confluent-link-metadata-43 with topic id Some(9kJAA8KZRoWZtgk1eJTuEA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,612] INFO [MergedLog partition=_confluent-link-metadata-34, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:08:11,612] INFO Created log for partition _confluent-link-metadata-34 in /tmp/kraft-combined-logs/_confluent-link-metadata-34 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:08:11,612] INFO [Partition _confluent-link-metadata-34 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-34 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,612] INFO [Partition _confluent-link-metadata-34 broker=1] Log loaded for partition _confluent-link-metadata-34 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,613] INFO Setting topicIdPartition 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-34 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:08:11,613] INFO [MergedLog partition=_confluent-link-metadata-34, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-34 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:08:11,613] INFO [Broker id=1] Leader _confluent-link-metadata-34 with topic id Some(9kJAA8KZRoWZtgk1eJTuEA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,614] INFO [MergedLog partition=_confluent-link-metadata-25, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:08:11,614] INFO Created log for partition _confluent-link-metadata-25 in /tmp/kraft-combined-logs/_confluent-link-metadata-25 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:08:11,614] INFO [Partition _confluent-link-metadata-25 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-25 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,614] INFO [Partition _confluent-link-metadata-25 broker=1] Log loaded for partition _confluent-link-metadata-25 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,614] INFO Setting topicIdPartition 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-25 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:08:11,615] INFO [MergedLog partition=_confluent-link-metadata-25, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-25 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:08:11,615] INFO [Broker id=1] Leader _confluent-link-metadata-25 with topic id Some(9kJAA8KZRoWZtgk1eJTuEA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,616] INFO [MergedLog partition=_confluent-link-metadata-22, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:08:11,616] INFO Created log for partition _confluent-link-metadata-22 in /tmp/kraft-combined-logs/_confluent-link-metadata-22 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:08:11,616] INFO [Partition _confluent-link-metadata-22 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-22 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,616] INFO [Partition _confluent-link-metadata-22 broker=1] Log loaded for partition _confluent-link-metadata-22 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,616] INFO Setting topicIdPartition 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-22 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:08:11,617] INFO [MergedLog partition=_confluent-link-metadata-22, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-22 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:08:11,617] INFO [Broker id=1] Leader _confluent-link-metadata-22 with topic id Some(9kJAA8KZRoWZtgk1eJTuEA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,618] INFO [MergedLog partition=_confluent-link-metadata-29, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:08:11,618] INFO Created log for partition _confluent-link-metadata-29 in /tmp/kraft-combined-logs/_confluent-link-metadata-29 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:08:11,618] INFO [Partition _confluent-link-metadata-29 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-29 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,618] INFO [Partition _confluent-link-metadata-29 broker=1] Log loaded for partition _confluent-link-metadata-29 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,618] INFO Setting topicIdPartition 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-29 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:08:11,618] INFO [MergedLog partition=_confluent-link-metadata-29, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-29 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:08:11,618] INFO [Broker id=1] Leader _confluent-link-metadata-29 with topic id Some(9kJAA8KZRoWZtgk1eJTuEA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,620] INFO [MergedLog partition=_confluent-link-metadata-5, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:08:11,620] INFO Created log for partition _confluent-link-metadata-5 in /tmp/kraft-combined-logs/_confluent-link-metadata-5 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:08:11,620] INFO [Partition _confluent-link-metadata-5 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-5 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,620] INFO [Partition _confluent-link-metadata-5 broker=1] Log loaded for partition _confluent-link-metadata-5 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,620] INFO Setting topicIdPartition 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-5 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:08:11,620] INFO [MergedLog partition=_confluent-link-metadata-5, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-5 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:08:11,621] INFO [Broker id=1] Leader _confluent-link-metadata-5 with topic id Some(9kJAA8KZRoWZtgk1eJTuEA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,621] INFO [MergedLog partition=_confluent-link-metadata-10, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:08:11,622] INFO Created log for partition _confluent-link-metadata-10 in /tmp/kraft-combined-logs/_confluent-link-metadata-10 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:08:11,622] INFO [Partition _confluent-link-metadata-10 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-10 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,622] INFO [Partition _confluent-link-metadata-10 broker=1] Log loaded for partition _confluent-link-metadata-10 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,622] INFO Setting topicIdPartition 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-10 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:08:11,622] INFO [MergedLog partition=_confluent-link-metadata-10, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-10 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:08:11,622] INFO [Broker id=1] Leader _confluent-link-metadata-10 with topic id Some(9kJAA8KZRoWZtgk1eJTuEA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,623] INFO [MergedLog partition=_confluent-link-metadata-47, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:08:11,623] INFO Created log for partition _confluent-link-metadata-47 in /tmp/kraft-combined-logs/_confluent-link-metadata-47 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:08:11,623] INFO [Partition _confluent-link-metadata-47 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-47 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,623] INFO [Partition _confluent-link-metadata-47 broker=1] Log loaded for partition _confluent-link-metadata-47 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,623] INFO Setting topicIdPartition 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-47 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:08:11,623] INFO [MergedLog partition=_confluent-link-metadata-47, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-47 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:08:11,624] INFO [Broker id=1] Leader _confluent-link-metadata-47 with topic id Some(9kJAA8KZRoWZtgk1eJTuEA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,625] INFO [MergedLog partition=_confluent-link-metadata-28, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:08:11,625] INFO Created log for partition _confluent-link-metadata-28 in /tmp/kraft-combined-logs/_confluent-link-metadata-28 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:08:11,625] INFO [Partition _confluent-link-metadata-28 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-28 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,625] INFO [Partition _confluent-link-metadata-28 broker=1] Log loaded for partition _confluent-link-metadata-28 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,625] INFO Setting topicIdPartition 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-28 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:08:11,625] INFO [MergedLog partition=_confluent-link-metadata-28, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-28 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:08:11,625] INFO [Broker id=1] Leader _confluent-link-metadata-28 with topic id Some(9kJAA8KZRoWZtgk1eJTuEA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,627] INFO [MergedLog partition=_confluent-link-metadata-49, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:08:11,627] INFO Created log for partition _confluent-link-metadata-49 in /tmp/kraft-combined-logs/_confluent-link-metadata-49 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:08:11,627] INFO [Partition _confluent-link-metadata-49 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-49 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,627] INFO [Partition _confluent-link-metadata-49 broker=1] Log loaded for partition _confluent-link-metadata-49 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,627] INFO Setting topicIdPartition 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-49 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:08:11,627] INFO [MergedLog partition=_confluent-link-metadata-49, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-49 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:08:11,628] INFO [Broker id=1] Leader _confluent-link-metadata-49 with topic id Some(9kJAA8KZRoWZtgk1eJTuEA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,629] INFO [MergedLog partition=_confluent-link-metadata-27, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:08:11,629] INFO Created log for partition _confluent-link-metadata-27 in /tmp/kraft-combined-logs/_confluent-link-metadata-27 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:08:11,629] INFO [Partition _confluent-link-metadata-27 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-27 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,629] INFO [Partition _confluent-link-metadata-27 broker=1] Log loaded for partition _confluent-link-metadata-27 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,629] INFO Setting topicIdPartition 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-27 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:08:11,629] INFO [MergedLog partition=_confluent-link-metadata-27, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-27 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:08:11,629] INFO [Broker id=1] Leader _confluent-link-metadata-27 with topic id Some(9kJAA8KZRoWZtgk1eJTuEA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,630] INFO [MergedLog partition=_confluent-link-metadata-6, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:08:11,631] INFO Created log for partition _confluent-link-metadata-6 in /tmp/kraft-combined-logs/_confluent-link-metadata-6 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:08:11,631] INFO [Partition _confluent-link-metadata-6 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-6 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,631] INFO [Partition _confluent-link-metadata-6 broker=1] Log loaded for partition _confluent-link-metadata-6 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,631] INFO Setting topicIdPartition 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-6 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:08:11,631] INFO [MergedLog partition=_confluent-link-metadata-6, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-6 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:08:11,631] INFO [Broker id=1] Leader _confluent-link-metadata-6 with topic id Some(9kJAA8KZRoWZtgk1eJTuEA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,632] INFO [MergedLog partition=_confluent-link-metadata-46, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:08:11,633] INFO Created log for partition _confluent-link-metadata-46 in /tmp/kraft-combined-logs/_confluent-link-metadata-46 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:08:11,633] INFO [Partition _confluent-link-metadata-46 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-46 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,633] INFO [Partition _confluent-link-metadata-46 broker=1] Log loaded for partition _confluent-link-metadata-46 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,633] INFO Setting topicIdPartition 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-46 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:08:11,633] INFO [MergedLog partition=_confluent-link-metadata-46, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-46 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:08:11,633] INFO [Broker id=1] Leader _confluent-link-metadata-46 with topic id Some(9kJAA8KZRoWZtgk1eJTuEA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,634] INFO [MergedLog partition=_confluent-link-metadata-35, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:08:11,634] INFO Created log for partition _confluent-link-metadata-35 in /tmp/kraft-combined-logs/_confluent-link-metadata-35 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:08:11,634] INFO [Partition _confluent-link-metadata-35 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-35 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,634] INFO [Partition _confluent-link-metadata-35 broker=1] Log loaded for partition _confluent-link-metadata-35 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,634] INFO Setting topicIdPartition 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-35 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:08:11,634] INFO [MergedLog partition=_confluent-link-metadata-35, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-35 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:08:11,635] INFO [Broker id=1] Leader _confluent-link-metadata-35 with topic id Some(9kJAA8KZRoWZtgk1eJTuEA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,636] INFO [MergedLog partition=_confluent-link-metadata-31, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:08:11,636] INFO Created log for partition _confluent-link-metadata-31 in /tmp/kraft-combined-logs/_confluent-link-metadata-31 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:08:11,636] INFO [Partition _confluent-link-metadata-31 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-31 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,636] INFO [Partition _confluent-link-metadata-31 broker=1] Log loaded for partition _confluent-link-metadata-31 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,636] INFO Setting topicIdPartition 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-31 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:08:11,636] INFO [MergedLog partition=_confluent-link-metadata-31, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-31 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:08:11,636] INFO [Broker id=1] Leader _confluent-link-metadata-31 with topic id Some(9kJAA8KZRoWZtgk1eJTuEA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,637] INFO [MergedLog partition=_confluent-link-metadata-16, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:08:11,637] INFO Created log for partition _confluent-link-metadata-16 in /tmp/kraft-combined-logs/_confluent-link-metadata-16 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:08:11,637] INFO [Partition _confluent-link-metadata-16 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-16 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,637] INFO [Partition _confluent-link-metadata-16 broker=1] Log loaded for partition _confluent-link-metadata-16 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,637] INFO Setting topicIdPartition 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-16 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:08:11,637] INFO [MergedLog partition=_confluent-link-metadata-16, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-16 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:08:11,637] INFO [Broker id=1] Leader _confluent-link-metadata-16 with topic id Some(9kJAA8KZRoWZtgk1eJTuEA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,639] INFO [MergedLog partition=_confluent-link-metadata-32, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:08:11,639] INFO Created log for partition _confluent-link-metadata-32 in /tmp/kraft-combined-logs/_confluent-link-metadata-32 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:08:11,639] INFO [Partition _confluent-link-metadata-32 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-32 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,639] INFO [Partition _confluent-link-metadata-32 broker=1] Log loaded for partition _confluent-link-metadata-32 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,639] INFO Setting topicIdPartition 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-32 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:08:11,639] INFO [MergedLog partition=_confluent-link-metadata-32, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-32 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:08:11,639] INFO [Broker id=1] Leader _confluent-link-metadata-32 with topic id Some(9kJAA8KZRoWZtgk1eJTuEA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,640] WARN Using default value http://localhost:8081 for config schema.registry.url. In a future release this config won't have a default value anymore. If you are using Schema Registry, please, specify schema.registry.url explicitly. Requests will fail in a future release if you try to use Schema Registry but have not specified a value for schema.registry.url. An empty value for this property means that the Schema Registry is disabled. (io.confluent.kafkarest.KafkaRestConfig)
benchi-kafka     | [2025-03-18 16:08:11,641] INFO SchemaRegistryConfig values: 
benchi-kafka     | 	auto.register.schemas = false
benchi-kafka     | 	basic.auth.credentials.source = URL
benchi-kafka     | 	basic.auth.user.info = [hidden]
benchi-kafka     | 	bearer.auth.cache.expiry.buffer.seconds = 300
benchi-kafka     | 	bearer.auth.client.id = null
benchi-kafka     | 	bearer.auth.client.secret = null
benchi-kafka     | 	bearer.auth.credentials.source = STATIC_TOKEN
benchi-kafka     | 	bearer.auth.custom.provider.class = null
benchi-kafka     | 	bearer.auth.identity.pool.id = null
benchi-kafka     | 	bearer.auth.issuer.endpoint.url = null
benchi-kafka     | 	bearer.auth.logical.cluster = null
benchi-kafka     | 	bearer.auth.scope = null
benchi-kafka     | 	bearer.auth.scope.claim.name = scope
benchi-kafka     | 	bearer.auth.sub.claim.name = sub
benchi-kafka     | 	bearer.auth.token = [hidden]
benchi-kafka     | 	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
benchi-kafka     | 	http.connect.timeout.ms = 60000
benchi-kafka     | 	http.read.timeout.ms = 60000
benchi-kafka     | 	id.compatibility.strict = true
benchi-kafka     | 	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
benchi-kafka     | 	latest.cache.size = 1000
benchi-kafka     | 	latest.cache.ttl.sec = -1
benchi-kafka     | 	latest.compatibility.strict = true
benchi-kafka     | 	max.schemas.per.subject = 1000
benchi-kafka     | 	normalize.schemas = false
benchi-kafka     | 	propagate.schema.tags = false
benchi-kafka     | 	proxy.host = 
benchi-kafka     | 	proxy.port = -1
benchi-kafka     | 	rule.actions = []
benchi-kafka     | 	rule.executors = []
benchi-kafka     | 	rule.service.loader.enable = true
benchi-kafka     | 	schema.format = null
benchi-kafka     | 	schema.reflection = false
benchi-kafka     | 	schema.registry.basic.auth.user.info = [hidden]
benchi-kafka     | 	schema.registry.ssl.cipher.suites = null
benchi-kafka     | 	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	schema.registry.ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	schema.registry.ssl.engine.factory.class = null
benchi-kafka     | 	schema.registry.ssl.key.password = null
benchi-kafka     | 	schema.registry.ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	schema.registry.ssl.keystore.certificate.chain = null
benchi-kafka     | 	schema.registry.ssl.keystore.key = null
benchi-kafka     | 	schema.registry.ssl.keystore.location = null
benchi-kafka     | 	schema.registry.ssl.keystore.password = null
benchi-kafka     | 	schema.registry.ssl.keystore.type = JKS
benchi-kafka     | 	schema.registry.ssl.protocol = TLSv1.3
benchi-kafka     | 	schema.registry.ssl.provider = null
benchi-kafka     | 	schema.registry.ssl.secure.random.implementation = null
benchi-kafka     | 	schema.registry.ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	schema.registry.ssl.truststore.certificates = null
benchi-kafka     | 	schema.registry.ssl.truststore.location = null
benchi-kafka     | 	schema.registry.ssl.truststore.password = null
benchi-kafka     | 	schema.registry.ssl.truststore.type = JKS
benchi-kafka     | 	schema.registry.url = [http://localhost:8081]
benchi-kafka     | 	use.latest.version = false
benchi-kafka     | 	use.latest.with.metadata = null
benchi-kafka     | 	use.schema.id = -1
benchi-kafka     | 	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
benchi-kafka     |  (io.confluent.kafkarest.config.SchemaRegistryConfig)
benchi-kafka     | [2025-03-18 16:08:11,641] INFO [MergedLog partition=_confluent-link-metadata-14, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:08:11,641] INFO Created log for partition _confluent-link-metadata-14 in /tmp/kraft-combined-logs/_confluent-link-metadata-14 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:08:11,641] INFO [Partition _confluent-link-metadata-14 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-14 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,641] INFO [Partition _confluent-link-metadata-14 broker=1] Log loaded for partition _confluent-link-metadata-14 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,642] INFO Setting topicIdPartition 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-14 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:08:11,642] INFO [MergedLog partition=_confluent-link-metadata-14, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-14 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:08:11,642] INFO [Broker id=1] Leader _confluent-link-metadata-14 with topic id Some(9kJAA8KZRoWZtgk1eJTuEA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,643] INFO [MergedLog partition=_confluent-link-metadata-37, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:08:11,643] INFO Created log for partition _confluent-link-metadata-37 in /tmp/kraft-combined-logs/_confluent-link-metadata-37 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:08:11,643] INFO [Partition _confluent-link-metadata-37 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-37 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,643] INFO [Partition _confluent-link-metadata-37 broker=1] Log loaded for partition _confluent-link-metadata-37 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,643] INFO Setting topicIdPartition 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-37 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:08:11,643] INFO [MergedLog partition=_confluent-link-metadata-37, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-37 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:08:11,643] INFO [Broker id=1] Leader _confluent-link-metadata-37 with topic id Some(9kJAA8KZRoWZtgk1eJTuEA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,644] INFO Binding EmbeddedKafkaRestApplication to all listeners. (io.confluent.rest.Application)
benchi-kafka     | [2025-03-18 16:08:11,645] INFO [MergedLog partition=_confluent-link-metadata-30, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:08:11,645] INFO Created log for partition _confluent-link-metadata-30 in /tmp/kraft-combined-logs/_confluent-link-metadata-30 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:08:11,645] INFO [Partition _confluent-link-metadata-30 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-30 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,645] INFO [Partition _confluent-link-metadata-30 broker=1] Log loaded for partition _confluent-link-metadata-30 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,645] INFO Setting topicIdPartition 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-30 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:08:11,645] INFO [MergedLog partition=_confluent-link-metadata-30, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-30 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:08:11,645] INFO [Broker id=1] Leader _confluent-link-metadata-30 with topic id Some(9kJAA8KZRoWZtgk1eJTuEA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,646] INFO [MergedLog partition=_confluent-link-metadata-33, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:08:11,646] INFO Created log for partition _confluent-link-metadata-33 in /tmp/kraft-combined-logs/_confluent-link-metadata-33 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:08:11,647] INFO [Partition _confluent-link-metadata-33 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-33 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,647] INFO [Partition _confluent-link-metadata-33 broker=1] Log loaded for partition _confluent-link-metadata-33 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,647] INFO Setting topicIdPartition 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-33 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:08:11,647] INFO [MergedLog partition=_confluent-link-metadata-33, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-33 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:08:11,647] INFO [Broker id=1] Leader _confluent-link-metadata-33 with topic id Some(9kJAA8KZRoWZtgk1eJTuEA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,649] INFO [MergedLog partition=_confluent-link-metadata-26, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:08:11,649] INFO Created log for partition _confluent-link-metadata-26 in /tmp/kraft-combined-logs/_confluent-link-metadata-26 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:08:11,649] INFO [Partition _confluent-link-metadata-26 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-26 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,649] INFO [Partition _confluent-link-metadata-26 broker=1] Log loaded for partition _confluent-link-metadata-26 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,649] INFO Setting topicIdPartition 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-26 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:08:11,649] INFO [MergedLog partition=_confluent-link-metadata-26, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-26 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:08:11,649] INFO [Broker id=1] Leader _confluent-link-metadata-26 with topic id Some(9kJAA8KZRoWZtgk1eJTuEA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,650] INFO [MergedLog partition=_confluent-link-metadata-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:08:11,650] INFO Created log for partition _confluent-link-metadata-0 in /tmp/kraft-combined-logs/_confluent-link-metadata-0 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:08:11,650] INFO [Partition _confluent-link-metadata-0 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,650] INFO [Partition _confluent-link-metadata-0 broker=1] Log loaded for partition _confluent-link-metadata-0 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,651] INFO Setting topicIdPartition 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-0 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:08:11,651] INFO [MergedLog partition=_confluent-link-metadata-0, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:08:11,651] INFO [Broker id=1] Leader _confluent-link-metadata-0 with topic id Some(9kJAA8KZRoWZtgk1eJTuEA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,655] INFO [MergedLog partition=_confluent-link-metadata-1, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:08:11,655] INFO Created log for partition _confluent-link-metadata-1 in /tmp/kraft-combined-logs/_confluent-link-metadata-1 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:08:11,655] INFO [Partition _confluent-link-metadata-1 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-1 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,656] INFO [Partition _confluent-link-metadata-1 broker=1] Log loaded for partition _confluent-link-metadata-1 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,656] INFO Setting topicIdPartition 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-1 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:08:11,656] INFO [MergedLog partition=_confluent-link-metadata-1, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-1 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:08:11,656] INFO [Broker id=1] Leader _confluent-link-metadata-1 with topic id Some(9kJAA8KZRoWZtgk1eJTuEA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,657] INFO [MergedLog partition=_confluent-link-metadata-4, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:08:11,657] INFO Created log for partition _confluent-link-metadata-4 in /tmp/kraft-combined-logs/_confluent-link-metadata-4 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:08:11,657] INFO [Partition _confluent-link-metadata-4 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-4 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,657] INFO [Partition _confluent-link-metadata-4 broker=1] Log loaded for partition _confluent-link-metadata-4 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,657] INFO Setting topicIdPartition 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-4 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:08:11,657] INFO [MergedLog partition=_confluent-link-metadata-4, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-4 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:08:11,657] INFO [Broker id=1] Leader _confluent-link-metadata-4 with topic id Some(9kJAA8KZRoWZtgk1eJTuEA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,658] INFO [Producer clientId=confluent-telemetry-reporter-local-producer] Cluster ID: MkU3OEVBNTcwNTJENDM2Qk (org.apache.kafka.clients.Metadata)
benchi-kafka     | [2025-03-18 16:08:11,658] INFO [MergedLog partition=_confluent-link-metadata-42, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:08:11,659] INFO Created log for partition _confluent-link-metadata-42 in /tmp/kraft-combined-logs/_confluent-link-metadata-42 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:08:11,659] INFO [Partition _confluent-link-metadata-42 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-42 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,659] INFO [Partition _confluent-link-metadata-42 broker=1] Log loaded for partition _confluent-link-metadata-42 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,659] INFO Setting topicIdPartition 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-42 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:08:11,659] INFO [MergedLog partition=_confluent-link-metadata-42, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-42 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:08:11,659] INFO [Broker id=1] Leader _confluent-link-metadata-42 with topic id Some(9kJAA8KZRoWZtgk1eJTuEA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,660] INFO [MergedLog partition=_confluent-link-metadata-3, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:08:11,660] INFO Created log for partition _confluent-link-metadata-3 in /tmp/kraft-combined-logs/_confluent-link-metadata-3 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:08:11,660] INFO [Partition _confluent-link-metadata-3 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-3 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,660] INFO [Partition _confluent-link-metadata-3 broker=1] Log loaded for partition _confluent-link-metadata-3 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,660] INFO Setting topicIdPartition 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-3 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:08:11,660] INFO [MergedLog partition=_confluent-link-metadata-3, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-3 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:08:11,661] INFO [Broker id=1] Leader _confluent-link-metadata-3 with topic id Some(9kJAA8KZRoWZtgk1eJTuEA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,661] INFO [MergedLog partition=_confluent-link-metadata-41, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:08:11,662] INFO Created log for partition _confluent-link-metadata-41 in /tmp/kraft-combined-logs/_confluent-link-metadata-41 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:08:11,662] INFO [Partition _confluent-link-metadata-41 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-41 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,662] INFO [Partition _confluent-link-metadata-41 broker=1] Log loaded for partition _confluent-link-metadata-41 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,662] INFO Setting topicIdPartition 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-41 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:08:11,662] INFO [MergedLog partition=_confluent-link-metadata-41, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-41 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:08:11,662] INFO [Broker id=1] Leader _confluent-link-metadata-41 with topic id Some(9kJAA8KZRoWZtgk1eJTuEA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,663] INFO [MergedLog partition=_confluent-link-metadata-38, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:08:11,663] INFO Created log for partition _confluent-link-metadata-38 in /tmp/kraft-combined-logs/_confluent-link-metadata-38 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:08:11,663] INFO [Partition _confluent-link-metadata-38 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-38 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,663] INFO [Partition _confluent-link-metadata-38 broker=1] Log loaded for partition _confluent-link-metadata-38 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,663] INFO Setting topicIdPartition 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-38 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:08:11,663] INFO [MergedLog partition=_confluent-link-metadata-38, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-38 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:08:11,663] INFO [Broker id=1] Leader _confluent-link-metadata-38 with topic id Some(9kJAA8KZRoWZtgk1eJTuEA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,664] INFO [MergedLog partition=_confluent-link-metadata-2, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:08:11,665] INFO Created log for partition _confluent-link-metadata-2 in /tmp/kraft-combined-logs/_confluent-link-metadata-2 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:08:11,665] INFO [Partition _confluent-link-metadata-2 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-2 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,665] INFO [Partition _confluent-link-metadata-2 broker=1] Log loaded for partition _confluent-link-metadata-2 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,665] INFO Setting topicIdPartition 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-2 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:08:11,665] INFO [MergedLog partition=_confluent-link-metadata-2, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-2 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:08:11,665] INFO [Broker id=1] Leader _confluent-link-metadata-2 with topic id Some(9kJAA8KZRoWZtgk1eJTuEA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,666] INFO [MergedLog partition=_confluent-link-metadata-17, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:08:11,666] INFO Created log for partition _confluent-link-metadata-17 in /tmp/kraft-combined-logs/_confluent-link-metadata-17 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:08:11,666] INFO [Partition _confluent-link-metadata-17 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-17 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,666] INFO [Partition _confluent-link-metadata-17 broker=1] Log loaded for partition _confluent-link-metadata-17 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,666] INFO Setting topicIdPartition 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-17 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:08:11,666] INFO [MergedLog partition=_confluent-link-metadata-17, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-17 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:08:11,667] INFO [Broker id=1] Leader _confluent-link-metadata-17 with topic id Some(9kJAA8KZRoWZtgk1eJTuEA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,668] INFO [MergedLog partition=_confluent-link-metadata-11, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:08:11,668] INFO Created log for partition _confluent-link-metadata-11 in /tmp/kraft-combined-logs/_confluent-link-metadata-11 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:08:11,668] INFO [Partition _confluent-link-metadata-11 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-11 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,668] INFO [Partition _confluent-link-metadata-11 broker=1] Log loaded for partition _confluent-link-metadata-11 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,668] INFO Setting topicIdPartition 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-11 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:08:11,668] INFO [MergedLog partition=_confluent-link-metadata-11, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-11 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:08:11,668] INFO [Broker id=1] Leader _confluent-link-metadata-11 with topic id Some(9kJAA8KZRoWZtgk1eJTuEA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,669] INFO [MergedLog partition=_confluent-link-metadata-9, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:08:11,669] INFO Created log for partition _confluent-link-metadata-9 in /tmp/kraft-combined-logs/_confluent-link-metadata-9 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:08:11,669] INFO [Partition _confluent-link-metadata-9 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-9 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,669] INFO [Partition _confluent-link-metadata-9 broker=1] Log loaded for partition _confluent-link-metadata-9 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,669] INFO Setting topicIdPartition 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-9 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:08:11,669] INFO [MergedLog partition=_confluent-link-metadata-9, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-9 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:08:11,669] INFO [Broker id=1] Leader _confluent-link-metadata-9 with topic id Some(9kJAA8KZRoWZtgk1eJTuEA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,671] INFO [MergedLog partition=_confluent-link-metadata-8, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:08:11,671] INFO Created log for partition _confluent-link-metadata-8 in /tmp/kraft-combined-logs/_confluent-link-metadata-8 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:08:11,671] INFO [Partition _confluent-link-metadata-8 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-8 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,671] INFO [Partition _confluent-link-metadata-8 broker=1] Log loaded for partition _confluent-link-metadata-8 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,671] INFO Setting topicIdPartition 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-8 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:08:11,672] INFO [MergedLog partition=_confluent-link-metadata-8, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-8 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:08:11,672] INFO [Broker id=1] Leader _confluent-link-metadata-8 with topic id Some(9kJAA8KZRoWZtgk1eJTuEA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,672] INFO jetty-9.4.54.v20240208; built: 2024-02-08T19:42:39.027Z; git: cef3fbd6d736a21e7d541a5db490381d95a2047d; jvm 17.0.13+11 (org.eclipse.jetty.server.Server)
benchi-kafka     | [2025-03-18 16:08:11,673] INFO [MergedLog partition=_confluent-link-metadata-36, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:08:11,673] INFO Created log for partition _confluent-link-metadata-36 in /tmp/kraft-combined-logs/_confluent-link-metadata-36 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:08:11,673] INFO [Partition _confluent-link-metadata-36 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-36 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,673] INFO [Partition _confluent-link-metadata-36 broker=1] Log loaded for partition _confluent-link-metadata-36 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,673] INFO Setting topicIdPartition 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-36 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:08:11,673] INFO [MergedLog partition=_confluent-link-metadata-36, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-36 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:08:11,673] INFO [Broker id=1] Leader _confluent-link-metadata-36 with topic id Some(9kJAA8KZRoWZtgk1eJTuEA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,675] INFO [MergedLog partition=_confluent-link-metadata-13, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:08:11,675] INFO Created log for partition _confluent-link-metadata-13 in /tmp/kraft-combined-logs/_confluent-link-metadata-13 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:08:11,675] INFO [Partition _confluent-link-metadata-13 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-13 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,675] INFO [Partition _confluent-link-metadata-13 broker=1] Log loaded for partition _confluent-link-metadata-13 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,675] INFO Setting topicIdPartition 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-13 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:08:11,676] INFO [MergedLog partition=_confluent-link-metadata-13, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-13 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:08:11,676] INFO [Broker id=1] Leader _confluent-link-metadata-13 with topic id Some(9kJAA8KZRoWZtgk1eJTuEA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,677] INFO [MergedLog partition=_confluent-link-metadata-15, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:08:11,677] INFO Created log for partition _confluent-link-metadata-15 in /tmp/kraft-combined-logs/_confluent-link-metadata-15 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:08:11,677] INFO [Partition _confluent-link-metadata-15 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-15 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,677] INFO [Partition _confluent-link-metadata-15 broker=1] Log loaded for partition _confluent-link-metadata-15 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,677] INFO Setting topicIdPartition 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-15 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:08:11,677] INFO [MergedLog partition=_confluent-link-metadata-15, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-15 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:08:11,677] INFO [Broker id=1] Leader _confluent-link-metadata-15 with topic id Some(9kJAA8KZRoWZtgk1eJTuEA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,678] INFO [MergedLog partition=_confluent-link-metadata-7, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:08:11,679] INFO Created log for partition _confluent-link-metadata-7 in /tmp/kraft-combined-logs/_confluent-link-metadata-7 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:08:11,679] INFO [Partition _confluent-link-metadata-7 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-7 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,679] INFO [Partition _confluent-link-metadata-7 broker=1] Log loaded for partition _confluent-link-metadata-7 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,679] INFO Setting topicIdPartition 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-7 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:08:11,679] INFO [MergedLog partition=_confluent-link-metadata-7, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-7 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:08:11,679] INFO [Broker id=1] Leader _confluent-link-metadata-7 with topic id Some(9kJAA8KZRoWZtgk1eJTuEA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,680] INFO [MergedLog partition=_confluent-link-metadata-19, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:08:11,680] INFO Created log for partition _confluent-link-metadata-19 in /tmp/kraft-combined-logs/_confluent-link-metadata-19 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:08:11,680] INFO [Partition _confluent-link-metadata-19 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-19 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,680] INFO [Partition _confluent-link-metadata-19 broker=1] Log loaded for partition _confluent-link-metadata-19 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,680] INFO Setting topicIdPartition 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-19 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:08:11,680] INFO [MergedLog partition=_confluent-link-metadata-19, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-19 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:08:11,680] INFO [Broker id=1] Leader _confluent-link-metadata-19 with topic id Some(9kJAA8KZRoWZtgk1eJTuEA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,682] INFO [MergedLog partition=_confluent-link-metadata-45, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:08:11,682] INFO Created log for partition _confluent-link-metadata-45 in /tmp/kraft-combined-logs/_confluent-link-metadata-45 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:08:11,682] INFO [Partition _confluent-link-metadata-45 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-45 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,682] INFO [Partition _confluent-link-metadata-45 broker=1] Log loaded for partition _confluent-link-metadata-45 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,682] INFO Setting topicIdPartition 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-45 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:08:11,682] INFO [MergedLog partition=_confluent-link-metadata-45, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-45 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:08:11,682] INFO [Broker id=1] Leader _confluent-link-metadata-45 with topic id Some(9kJAA8KZRoWZtgk1eJTuEA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,684] INFO [MergedLog partition=_confluent-link-metadata-40, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:08:11,684] INFO Created log for partition _confluent-link-metadata-40 in /tmp/kraft-combined-logs/_confluent-link-metadata-40 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:08:11,684] INFO [Partition _confluent-link-metadata-40 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-40 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,684] INFO [Partition _confluent-link-metadata-40 broker=1] Log loaded for partition _confluent-link-metadata-40 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,684] INFO Setting topicIdPartition 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-40 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:08:11,684] INFO [MergedLog partition=_confluent-link-metadata-40, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-40 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:08:11,684] INFO [Broker id=1] Leader _confluent-link-metadata-40 with topic id Some(9kJAA8KZRoWZtgk1eJTuEA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,685] INFO [MergedLog partition=_confluent-link-metadata-48, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:08:11,686] INFO Created log for partition _confluent-link-metadata-48 in /tmp/kraft-combined-logs/_confluent-link-metadata-48 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:08:11,686] INFO [Partition _confluent-link-metadata-48 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-48 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,686] INFO [Partition _confluent-link-metadata-48 broker=1] Log loaded for partition _confluent-link-metadata-48 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,686] INFO Setting topicIdPartition 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-48 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:08:11,686] INFO [MergedLog partition=_confluent-link-metadata-48, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-48 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:08:11,686] INFO [Broker id=1] Leader _confluent-link-metadata-48 with topic id Some(9kJAA8KZRoWZtgk1eJTuEA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,687] INFO [MergedLog partition=_confluent-link-metadata-44, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:08:11,687] INFO Created log for partition _confluent-link-metadata-44 in /tmp/kraft-combined-logs/_confluent-link-metadata-44 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:08:11,687] INFO [Partition _confluent-link-metadata-44 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-44 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,687] INFO [Partition _confluent-link-metadata-44 broker=1] Log loaded for partition _confluent-link-metadata-44 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,687] INFO Setting topicIdPartition 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-44 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:08:11,687] INFO [MergedLog partition=_confluent-link-metadata-44, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-44 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:08:11,687] INFO [Broker id=1] Leader _confluent-link-metadata-44 with topic id Some(9kJAA8KZRoWZtgk1eJTuEA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,688] INFO [MergedLog partition=_confluent-link-metadata-12, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:08:11,688] INFO Created log for partition _confluent-link-metadata-12 in /tmp/kraft-combined-logs/_confluent-link-metadata-12 with properties {cleanup.policy=compact, min.insync.replicas=2} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:08:11,688] INFO [Partition _confluent-link-metadata-12 broker=1] No checkpointed highwatermark is found for partition _confluent-link-metadata-12 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,689] INFO [Partition _confluent-link-metadata-12 broker=1] Log loaded for partition _confluent-link-metadata-12 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:11,689] INFO Setting topicIdPartition 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-12 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:08:11,689] INFO [MergedLog partition=_confluent-link-metadata-12, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-link-metadata-12 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:08:11,689] INFO [Broker id=1] Leader _confluent-link-metadata-12 with topic id Some(9kJAA8KZRoWZtgk1eJTuEA) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:11,694] INFO [DynamicConfigPublisher broker id=1] Updating topic _confluent-link-metadata with new configuration : cleanup.policy -> compact,min.insync.replicas -> 2 (kafka.server.metadata.DynamicConfigPublisher)
benchi-kafka     | [2025-03-18 16:08:11,698] INFO DefaultSessionIdManager workerName=node0 (org.eclipse.jetty.server.session)
benchi-kafka     | [2025-03-18 16:08:11,698] INFO No SessionScavenger set, using defaults (org.eclipse.jetty.server.session)
benchi-kafka     | [2025-03-18 16:08:11,699] INFO node0 Scavenging every 660000ms (org.eclipse.jetty.server.session)
benchi-kafka     | Mar 18, 2025 4:08:11 PM org.glassfish.jersey.internal.inject.Providers checkProviderRuntime
benchi-kafka     | WARNING: A provider io.confluent.metadataapi.resources.MetadataResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.metadataapi.resources.MetadataResource will be ignored. 
benchi-kafka     | [2025-03-18 16:08:11,850] INFO HV000001: Hibernate Validator 6.1.7.Final (org.hibernate.validator.internal.util.Version)
benchi-kafka     | [2025-03-18 16:08:11,936] INFO Started o.e.j.s.ServletContextHandler@666c2f40{/v1/metadata,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler)
benchi-kafka     | [2025-03-18 16:08:11,949] WARN Using default value http://localhost:8081 for config schema.registry.url. In a future release this config won't have a default value anymore. If you are using Schema Registry, please, specify schema.registry.url explicitly. Requests will fail in a future release if you try to use Schema Registry but have not specified a value for schema.registry.url. An empty value for this property means that the Schema Registry is disabled. (io.confluent.kafkarest.KafkaRestConfig)
benchi-kafka     | [2025-03-18 16:08:11,950] WARN Using default value http://localhost:8081 for config schema.registry.url. In a future release this config won't have a default value anymore. If you are using Schema Registry, please, specify schema.registry.url explicitly. Requests will fail in a future release if you try to use Schema Registry but have not specified a value for schema.registry.url. An empty value for this property means that the Schema Registry is disabled. (io.confluent.kafkarest.KafkaRestConfig)
benchi-kafka     | [2025-03-18 16:08:11,955] WARN Using default value http://localhost:8081 for config schema.registry.url. In a future release this config won't have a default value anymore. If you are using Schema Registry, please, specify schema.registry.url explicitly. Requests will fail in a future release if you try to use Schema Registry but have not specified a value for schema.registry.url. An empty value for this property means that the Schema Registry is disabled. (io.confluent.kafkarest.KafkaRestConfig)
benchi-kafka     | [2025-03-18 16:08:11,958] WARN Using default value http://localhost:8081 for config schema.registry.url. In a future release this config won't have a default value anymore. If you are using Schema Registry, please, specify schema.registry.url explicitly. Requests will fail in a future release if you try to use Schema Registry but have not specified a value for schema.registry.url. An empty value for this property means that the Schema Registry is disabled. (io.confluent.kafkarest.KafkaRestConfig)
benchi-kafka     | [2025-03-18 16:08:12,150] INFO Started o.e.j.s.ServletContextHandler@3e73d551{/kafka,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler)
benchi-kafka     | [2025-03-18 16:08:12,158] INFO Started o.e.j.s.ServletContextHandler@10e9059e{/ws,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler)
benchi-kafka     | [2025-03-18 16:08:12,158] INFO Started o.e.j.s.ServletContextHandler@7feb5a3e{/ws,null,AVAILABLE} (org.eclipse.jetty.server.handler.ContextHandler)
benchi-kafka     | [2025-03-18 16:08:12,166] INFO Started NetworkTrafficServerConnector@77e1dacd{HTTP/1.1, (http/1.1, h2c)}{0.0.0.0:8090} (org.eclipse.jetty.server.AbstractConnector)
benchi-kafka     | [2025-03-18 16:08:12,166] INFO Started @2462ms (org.eclipse.jetty.server.Server)
benchi-kafka     | [2025-03-18 16:08:12,166] INFO KafkaHttpServer transitioned from STARTING to RUNNING.. (io.confluent.http.server.KafkaHttpServerImpl)
benchi-kafka     | [2025-03-18 16:08:12,171] INFO LicenseConfig values: 
benchi-kafka     | 	confluent.license = [hidden]
benchi-kafka     | 	confluent.license.retry.backoff.max.ms = 100000
benchi-kafka     | 	confluent.license.retry.backoff.min.ms = 1000
benchi-kafka     | 	confluent.license.topic = _confluent-license
benchi-kafka     | 	confluent.license.topic.create.timeout.ms = 600000
benchi-kafka     | 	confluent.license.topic.replication.factor = 1
benchi-kafka     |  (io.confluent.license.validator.LicenseConfig)
benchi-kafka     | [2025-03-18 16:08:12,171] INFO LicenseConfig values: 
benchi-kafka     | 	confluent.license = [hidden]
benchi-kafka     | 	confluent.license.retry.backoff.max.ms = 100000
benchi-kafka     | 	confluent.license.retry.backoff.min.ms = 1000
benchi-kafka     | 	confluent.license.topic = _confluent-license
benchi-kafka     | 	confluent.license.topic.create.timeout.ms = 600000
benchi-kafka     | 	confluent.license.topic.replication.factor = 1
benchi-kafka     |  (io.confluent.license.validator.LicenseConfig)
benchi-kafka     | [2025-03-18 16:08:12,179] INFO AdminClientConfig values: 
benchi-kafka     | 	auto.include.jmx.reporter = true
benchi-kafka     | 	bootstrap.controllers = []
benchi-kafka     | 	bootstrap.servers = [broker:29092]
benchi-kafka     | 	client.dns.lookup = use_all_dns_ips
benchi-kafka     | 	client.id = _confluent-license-admin-1
benchi-kafka     | 	confluent.lkc.id = null
benchi-kafka     | 	confluent.metrics.reporter.bootstrap.servers = localhost:9092
benchi-kafka     | 	confluent.proxy.protocol.client.address = null
benchi-kafka     | 	confluent.proxy.protocol.client.mode = PROXY
benchi-kafka     | 	confluent.proxy.protocol.client.port = null
benchi-kafka     | 	confluent.proxy.protocol.client.version = NONE
benchi-kafka     | 	connections.max.idle.ms = 300000
benchi-kafka     | 	default.api.timeout.ms = 60000
benchi-kafka     | 	enable.metrics.push = true
benchi-kafka     | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
benchi-kafka     | 	metadata.max.age.ms = 300000
benchi-kafka     | 	metadata.recovery.strategy = none
benchi-kafka     | 	metric.reporters = []
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.recording.level = INFO
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	receive.buffer.bytes = 65536
benchi-kafka     | 	reconnect.backoff.max.ms = 1000
benchi-kafka     | 	reconnect.backoff.ms = 50
benchi-kafka     | 	request.timeout.ms = 30000
benchi-kafka     | 	retries = 2147483647
benchi-kafka     | 	retry.backoff.max.ms = 1000
benchi-kafka     | 	retry.backoff.ms = 100
benchi-kafka     | 	sasl.client.callback.handler.class = null
benchi-kafka     | 	sasl.jaas.config = null
benchi-kafka     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
benchi-kafka     | 	sasl.kerberos.min.time.before.relogin = 60000
benchi-kafka     | 	sasl.kerberos.service.name = null
benchi-kafka     | 	sasl.kerberos.ticket.renew.jitter = 0.05
benchi-kafka     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
benchi-kafka     | 	sasl.login.callback.handler.class = null
benchi-kafka     | 	sasl.login.class = null
benchi-kafka     | 	sasl.login.connect.timeout.ms = null
benchi-kafka     | 	sasl.login.read.timeout.ms = null
benchi-kafka     | 	sasl.login.refresh.buffer.seconds = 300
benchi-kafka     | 	sasl.login.refresh.min.period.seconds = 60
benchi-kafka     | 	sasl.login.refresh.window.factor = 0.8
benchi-kafka     | 	sasl.login.refresh.window.jitter = 0.05
benchi-kafka     | 	sasl.login.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.login.retry.backoff.ms = 100
benchi-kafka     | 	sasl.mechanism = GSSAPI
benchi-kafka     | 	sasl.oauthbearer.clientassertion.audience = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.expiration = 5
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.jti.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.nbf.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.issuer = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.location = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key.passphrase = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.subject = null
benchi-kafka     | 	sasl.oauthbearer.clock.skew.seconds = 30
benchi-kafka     | 	sasl.oauthbearer.expected.audience = null
benchi-kafka     | 	sasl.oauthbearer.expected.issuer = null
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.url = null
benchi-kafka     | 	sasl.oauthbearer.scope.claim.name = scope
benchi-kafka     | 	sasl.oauthbearer.sub.claim.name = sub
benchi-kafka     | 	sasl.oauthbearer.token.endpoint.url = null
benchi-kafka     | 	security.protocol = PLAINTEXT
benchi-kafka     | 	security.providers = null
benchi-kafka     | 	send.buffer.bytes = 131072
benchi-kafka     | 	socket.connection.setup.timeout.max.ms = 30000
benchi-kafka     | 	socket.connection.setup.timeout.ms = 10000
benchi-kafka     | 	ssl.cipher.suites = null
benchi-kafka     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	ssl.engine.factory.class = null
benchi-kafka     | 	ssl.key.password = null
benchi-kafka     | 	ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	ssl.keystore.certificate.chain = null
benchi-kafka     | 	ssl.keystore.key = null
benchi-kafka     | 	ssl.keystore.location = null
benchi-kafka     | 	ssl.keystore.password = null
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.protocol = TLSv1.3
benchi-kafka     | 	ssl.provider = null
benchi-kafka     | 	ssl.secure.random.implementation = null
benchi-kafka     | 	ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	ssl.truststore.certificates = null
benchi-kafka     | 	ssl.truststore.location = null
benchi-kafka     | 	ssl.truststore.password = null
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     |  (org.apache.kafka.clients.admin.AdminClientConfig)
benchi-kafka     | [2025-03-18 16:08:12,180] INFO These configurations '[confluent.metrics.topic.replicas, replication.factor, confluent.metrics.reporter.bootstrap.servers, jmx.hostname, min.insync.replicas, cluster.link.metadata.topic.replication.factor, jmx.port]' were supplied but are not used yet. (org.apache.kafka.clients.admin.AdminClientConfig)
benchi-kafka     | [2025-03-18 16:08:12,180] INFO Kafka version: 7.8.1-ce (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:08:12,180] INFO Kafka commitId: eefb51c6f2f1b888b8b03296353338afbac8e213 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:08:12,180] INFO Kafka startTimeMs: 1742314092180 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:08:12,188] INFO [AdminClient clientId=_confluent-license-admin-1] The DescribeTopicPartitions API is not supported, using Metadata API to describe topics. (org.apache.kafka.clients.admin.KafkaAdminClient)
benchi-kafka     | [2025-03-18 16:08:12,191] INFO App info kafka.admin.client for _confluent-license-admin-1 unregistered (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:08:12,195] INFO Starting License Store (io.confluent.license.LicenseStore)
benchi-kafka     | [2025-03-18 16:08:12,195] INFO Starting KafkaBasedLog with topic _confluent-command reportErrorsToCallback=false (org.apache.kafka.connect.util.KafkaBasedLog)
benchi-kafka     | [2025-03-18 16:08:12,195] INFO AdminClientConfig values: 
benchi-kafka     | 	auto.include.jmx.reporter = true
benchi-kafka     | 	bootstrap.controllers = []
benchi-kafka     | 	bootstrap.servers = [broker:29092]
benchi-kafka     | 	client.dns.lookup = use_all_dns_ips
benchi-kafka     | 	client.id = _confluent-license-admin-1
benchi-kafka     | 	confluent.lkc.id = null
benchi-kafka     | 	confluent.metrics.reporter.bootstrap.servers = localhost:9092
benchi-kafka     | 	confluent.proxy.protocol.client.address = null
benchi-kafka     | 	confluent.proxy.protocol.client.mode = PROXY
benchi-kafka     | 	confluent.proxy.protocol.client.port = null
benchi-kafka     | 	confluent.proxy.protocol.client.version = NONE
benchi-kafka     | 	connections.max.idle.ms = 300000
benchi-kafka     | 	default.api.timeout.ms = 60000
benchi-kafka     | 	enable.metrics.push = true
benchi-kafka     | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
benchi-kafka     | 	metadata.max.age.ms = 300000
benchi-kafka     | 	metadata.recovery.strategy = none
benchi-kafka     | 	metric.reporters = []
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.recording.level = INFO
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	receive.buffer.bytes = 65536
benchi-kafka     | 	reconnect.backoff.max.ms = 1000
benchi-kafka     | 	reconnect.backoff.ms = 50
benchi-kafka     | 	request.timeout.ms = 30000
benchi-kafka     | 	retries = 2147483647
benchi-kafka     | 	retry.backoff.max.ms = 1000
benchi-kafka     | 	retry.backoff.ms = 100
benchi-kafka     | 	sasl.client.callback.handler.class = null
benchi-kafka     | 	sasl.jaas.config = null
benchi-kafka     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
benchi-kafka     | 	sasl.kerberos.min.time.before.relogin = 60000
benchi-kafka     | 	sasl.kerberos.service.name = null
benchi-kafka     | 	sasl.kerberos.ticket.renew.jitter = 0.05
benchi-kafka     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
benchi-kafka     | 	sasl.login.callback.handler.class = null
benchi-kafka     | 	sasl.login.class = null
benchi-kafka     | 	sasl.login.connect.timeout.ms = null
benchi-kafka     | 	sasl.login.read.timeout.ms = null
benchi-kafka     | 	sasl.login.refresh.buffer.seconds = 300
benchi-kafka     | 	sasl.login.refresh.min.period.seconds = 60
benchi-kafka     | 	sasl.login.refresh.window.factor = 0.8
benchi-kafka     | 	sasl.login.refresh.window.jitter = 0.05
benchi-kafka     | 	sasl.login.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.login.retry.backoff.ms = 100
benchi-kafka     | 	sasl.mechanism = GSSAPI
benchi-kafka     | 	sasl.oauthbearer.clientassertion.audience = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.expiration = 5
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.jti.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.nbf.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.issuer = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.location = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key.passphrase = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.subject = null
benchi-kafka     | 	sasl.oauthbearer.clock.skew.seconds = 30
benchi-kafka     | 	sasl.oauthbearer.expected.audience = null
benchi-kafka     | 	sasl.oauthbearer.expected.issuer = null
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.url = null
benchi-kafka     | 	sasl.oauthbearer.scope.claim.name = scope
benchi-kafka     | 	sasl.oauthbearer.sub.claim.name = sub
benchi-kafka     | 	sasl.oauthbearer.token.endpoint.url = null
benchi-kafka     | 	security.protocol = PLAINTEXT
benchi-kafka     | 	security.providers = null
benchi-kafka     | 	send.buffer.bytes = 131072
benchi-kafka     | 	socket.connection.setup.timeout.max.ms = 30000
benchi-kafka     | 	socket.connection.setup.timeout.ms = 10000
benchi-kafka     | 	ssl.cipher.suites = null
benchi-kafka     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	ssl.engine.factory.class = null
benchi-kafka     | 	ssl.key.password = null
benchi-kafka     | 	ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	ssl.keystore.certificate.chain = null
benchi-kafka     | 	ssl.keystore.key = null
benchi-kafka     | 	ssl.keystore.location = null
benchi-kafka     | 	ssl.keystore.password = null
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.protocol = TLSv1.3
benchi-kafka     | 	ssl.provider = null
benchi-kafka     | 	ssl.secure.random.implementation = null
benchi-kafka     | 	ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	ssl.truststore.certificates = null
benchi-kafka     | 	ssl.truststore.location = null
benchi-kafka     | 	ssl.truststore.password = null
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     |  (org.apache.kafka.clients.admin.AdminClientConfig)
benchi-kafka     | [2025-03-18 16:08:12,196] INFO These configurations '[confluent.metrics.topic.replicas, replication.factor, confluent.metrics.reporter.bootstrap.servers, jmx.hostname, min.insync.replicas, cluster.link.metadata.topic.replication.factor, jmx.port]' were supplied but are not used yet. (org.apache.kafka.clients.admin.AdminClientConfig)
benchi-kafka     | [2025-03-18 16:08:12,196] INFO Kafka version: 7.8.1-ce (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:08:12,196] INFO Kafka commitId: eefb51c6f2f1b888b8b03296353338afbac8e213 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:08:12,196] INFO Kafka startTimeMs: 1742314092196 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:08:12,200] INFO [AdminClient clientId=_confluent-license-admin-1] The DescribeTopicPartitions API is not supported, using Metadata API to describe topics. (org.apache.kafka.clients.admin.KafkaAdminClient)
benchi-kafka     | [2025-03-18 16:08:12,204] INFO [ControllerServer id=1] CreateTopics result(s): CreatableTopic(name='_confluent-command', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='compact'), CreateableTopicConfig(name='min.insync.replicas', value='1')], linkName=null, mirrorTopic=null, sourceTopicId=AAAAAAAAAAAAAAAAAAAAAA, mirrorStartOffsetSpec=-9223372036854775808, mirrorStartOffsets=[], stoppedSequenceNumber=0): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:08:12,204] INFO [ControllerServer id=1] Replayed TopicRecord for topic _confluent-command with topic ID Tl_wzRtXT7-I7ZNugmF7hg. (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:08:12,204] INFO [ControllerServer id=1] Replayed ConfigRecord for ConfigResource(type=TOPIC, name='_confluent-command') which set configuration cleanup.policy to compact (org.apache.kafka.controller.ConfigurationControlManager)
benchi-kafka     | [2025-03-18 16:08:12,204] INFO [ControllerServer id=1] Replayed ConfigRecord for ConfigResource(type=TOPIC, name='_confluent-command') which set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
benchi-kafka     | [2025-03-18 16:08:12,204] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-command-0 with topic ID Tl_wzRtXT7-I7ZNugmF7hg and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:08:12,233] INFO SBC Event SbcMetadataUpdateEvent-15 generated 1 more events to enqueue in the following order - [SbcConfigUpdateEvent-16]. Enqueuing... (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:08:12,233] INFO Handling event SbcConfigUpdateEvent-16 (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:08:12,233] INFO Balancer notified of a config change: ConfigurationsDelta(changes={ConfigResource(type=TOPIC, name='_confluent-command')=ConfigurationDelta(changedKeys=[cleanup.policy, min.insync.replicas])}) (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:08:12,233] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:12,233] INFO There were 0 change(s) and 0 deletion(s) to balancer configs. Changed Configs: {}, Deleted Configs: [] (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:08:12,233] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-command-0) (kafka.server.ReplicaFetcherManager)
benchi-kafka     | [2025-03-18 16:08:12,233] INFO [Broker id=1] Creating new partition _confluent-command-0 with topic id Tl_wzRtXT7-I7ZNugmF7hg. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:12,233] INFO [Broker id=1] Stopped fetchers as part of become-leader transition for 1 partitions (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:12,235] INFO App info kafka.admin.client for _confluent-license-admin-1 unregistered (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:08:12,235] INFO [MergedLog partition=_confluent-command-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:08:12,235] INFO Created log for partition _confluent-command-0 in /tmp/kraft-combined-logs/_confluent-command-0 with properties {cleanup.policy=compact, min.insync.replicas=1} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:08:12,235] INFO [Partition _confluent-command-0 broker=1] No checkpointed highwatermark is found for partition _confluent-command-0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:12,235] INFO [Partition _confluent-command-0 broker=1] Log loaded for partition _confluent-command-0 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:08:12,235] INFO Setting topicIdPartition Tl_wzRtXT7-I7ZNugmF7hg:_confluent-command-0 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:08:12,235] INFO [MergedLog partition=_confluent-command-0, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-command-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:08:12,235] INFO [Broker id=1] Leader _confluent-command-0 with topic id Some(Tl_wzRtXT7-I7ZNugmF7hg) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:08:12,236] INFO ConsumerConfig values: 
benchi-kafka     | 	allow.auto.create.topics = true
benchi-kafka     | 	auto.commit.interval.ms = 5000
benchi-kafka     | 	auto.include.jmx.reporter = true
benchi-kafka     | 	auto.offset.reset = latest
benchi-kafka     | 	bootstrap.servers = [broker:29092]
benchi-kafka     | 	check.crcs = true
benchi-kafka     | 	client.dns.lookup = use_all_dns_ips
benchi-kafka     | 	client.id = _confluent-license-consumer-1
benchi-kafka     | 	client.rack = 
benchi-kafka     | 	confluent.lkc.id = null
benchi-kafka     | 	confluent.proxy.protocol.client.address = null
benchi-kafka     | 	confluent.proxy.protocol.client.mode = PROXY
benchi-kafka     | 	confluent.proxy.protocol.client.port = null
benchi-kafka     | 	confluent.proxy.protocol.client.version = NONE
benchi-kafka     | 	connections.max.idle.ms = 540000
benchi-kafka     | 	default.api.timeout.ms = 60000
benchi-kafka     | 	enable.auto.commit = false
benchi-kafka     | 	enable.metrics.push = true
benchi-kafka     | 	exclude.internal.topics = true
benchi-kafka     | 	fetch.max.bytes = 52428800
benchi-kafka     | 	fetch.max.wait.ms = 500
benchi-kafka     | 	fetch.min.bytes = 1
benchi-kafka     | 	group.id = null
benchi-kafka     | 	group.instance.id = null
benchi-kafka     | 	group.protocol = classic
benchi-kafka     | 	group.remote.assignor = null
benchi-kafka     | 	heartbeat.interval.ms = 3000
benchi-kafka     | 	interceptor.classes = []
benchi-kafka     | 	internal.leave.group.on.close = true
benchi-kafka     | 	internal.throw.on.fetch.stable.offset.unsupported = false
benchi-kafka     | 	isolation.level = read_uncommitted
benchi-kafka     | 	key.deserializer = class io.confluent.license.LicenseStore$LicenseKeySerde
benchi-kafka     | 	max.partition.fetch.bytes = 1048576
benchi-kafka     | 	max.poll.interval.ms = 300000
benchi-kafka     | 	max.poll.records = 500
benchi-kafka     | 	metadata.max.age.ms = 300000
benchi-kafka     | 	metadata.recovery.strategy = none
benchi-kafka     | 	metric.reporters = []
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.recording.level = INFO
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
benchi-kafka     | 	receive.buffer.bytes = 65536
benchi-kafka     | 	reconnect.backoff.max.ms = 1000
benchi-kafka     | 	reconnect.backoff.ms = 50
benchi-kafka     | 	request.timeout.ms = 120000
benchi-kafka     | 	retry.backoff.max.ms = 1000
benchi-kafka     | 	retry.backoff.ms = 100
benchi-kafka     | 	sasl.client.callback.handler.class = null
benchi-kafka     | 	sasl.jaas.config = null
benchi-kafka     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
benchi-kafka     | 	sasl.kerberos.min.time.before.relogin = 60000
benchi-kafka     | 	sasl.kerberos.service.name = null
benchi-kafka     | 	sasl.kerberos.ticket.renew.jitter = 0.05
benchi-kafka     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
benchi-kafka     | 	sasl.login.callback.handler.class = null
benchi-kafka     | 	sasl.login.class = null
benchi-kafka     | 	sasl.login.connect.timeout.ms = null
benchi-kafka     | 	sasl.login.read.timeout.ms = null
benchi-kafka     | 	sasl.login.refresh.buffer.seconds = 300
benchi-kafka     | 	sasl.login.refresh.min.period.seconds = 60
benchi-kafka     | 	sasl.login.refresh.window.factor = 0.8
benchi-kafka     | 	sasl.login.refresh.window.jitter = 0.05
benchi-kafka     | 	sasl.login.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.login.retry.backoff.ms = 100
benchi-kafka     | 	sasl.mechanism = GSSAPI
benchi-kafka     | 	sasl.oauthbearer.clientassertion.audience = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.expiration = 5
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.jti.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.nbf.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.issuer = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.location = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key.passphrase = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.subject = null
benchi-kafka     | 	sasl.oauthbearer.clock.skew.seconds = 30
benchi-kafka     | 	sasl.oauthbearer.expected.audience = null
benchi-kafka     | 	sasl.oauthbearer.expected.issuer = null
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.url = null
benchi-kafka     | 	sasl.oauthbearer.scope.claim.name = scope
benchi-kafka     | 	sasl.oauthbearer.sub.claim.name = sub
benchi-kafka     | 	sasl.oauthbearer.token.endpoint.url = null
benchi-kafka     | 	security.protocol = PLAINTEXT
benchi-kafka     | 	security.providers = null
benchi-kafka     | 	send.buffer.bytes = 131072
benchi-kafka     | 	session.timeout.ms = 45000
benchi-kafka     | 	socket.connection.setup.timeout.max.ms = 30000
benchi-kafka     | 	socket.connection.setup.timeout.ms = 10000
benchi-kafka     | 	ssl.cipher.suites = null
benchi-kafka     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	ssl.engine.factory.class = null
benchi-kafka     | 	ssl.key.password = null
benchi-kafka     | 	ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	ssl.keystore.certificate.chain = null
benchi-kafka     | 	ssl.keystore.key = null
benchi-kafka     | 	ssl.keystore.location = null
benchi-kafka     | 	ssl.keystore.password = null
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.protocol = TLSv1.3
benchi-kafka     | 	ssl.provider = null
benchi-kafka     | 	ssl.secure.random.implementation = null
benchi-kafka     | 	ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	ssl.truststore.certificates = null
benchi-kafka     | 	ssl.truststore.location = null
benchi-kafka     | 	ssl.truststore.password = null
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     | 	value.deserializer = class io.confluent.license.LicenseStore$LicenseMessageSerde
benchi-kafka     |  (org.apache.kafka.clients.consumer.ConsumerConfig)
benchi-kafka     | [2025-03-18 16:08:12,236] INFO [DynamicConfigPublisher broker id=1] Updating topic _confluent-command with new configuration : cleanup.policy -> compact,min.insync.replicas -> 1 (kafka.server.metadata.DynamicConfigPublisher)
benchi-kafka     | [2025-03-18 16:08:12,236] INFO initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector)
benchi-kafka     | [2025-03-18 16:08:12,239] INFO These configurations '[confluent.metrics.topic.replicas, confluent.metrics.reporter.bootstrap.servers, jmx.hostname, cluster.link.metadata.topic.replication.factor, jmx.port]' were supplied but are not used yet. (org.apache.kafka.clients.consumer.ConsumerConfig)
benchi-kafka     | [2025-03-18 16:08:12,239] INFO Kafka version: 7.8.1-ce (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:08:12,239] INFO Kafka commitId: eefb51c6f2f1b888b8b03296353338afbac8e213 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:08:12,239] INFO Kafka startTimeMs: 1742314092239 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:08:12,241] INFO [Consumer clientId=_confluent-license-consumer-1, groupId=null] Cluster ID: MkU3OEVBNTcwNTJENDM2Qk (org.apache.kafka.clients.Metadata)
benchi-kafka     | [2025-03-18 16:08:12,245] INFO App info kafka.consumer for _confluent-license-consumer-1 unregistered (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:08:12,246] INFO ProducerConfig values: 
benchi-kafka     | 	acks = -1
benchi-kafka     | 	auto.include.jmx.reporter = true
benchi-kafka     | 	batch.size = 16384
benchi-kafka     | 	bootstrap.servers = [broker:29092]
benchi-kafka     | 	buffer.memory = 33554432
benchi-kafka     | 	client.dns.lookup = use_all_dns_ips
benchi-kafka     | 	client.id = _confluent-license-producer-1
benchi-kafka     | 	compression.gzip.level = -1
benchi-kafka     | 	compression.lz4.level = 9
benchi-kafka     | 	compression.type = none
benchi-kafka     | 	compression.zstd.level = 3
benchi-kafka     | 	confluent.lkc.id = null
benchi-kafka     | 	confluent.proxy.protocol.client.address = null
benchi-kafka     | 	confluent.proxy.protocol.client.mode = PROXY
benchi-kafka     | 	confluent.proxy.protocol.client.port = null
benchi-kafka     | 	confluent.proxy.protocol.client.version = NONE
benchi-kafka     | 	connections.max.idle.ms = 540000
benchi-kafka     | 	delivery.timeout.ms = 120000
benchi-kafka     | 	enable.idempotence = false
benchi-kafka     | 	enable.metrics.push = true
benchi-kafka     | 	interceptor.classes = []
benchi-kafka     | 	key.serializer = class io.confluent.license.LicenseStore$LicenseKeySerde
benchi-kafka     | 	linger.ms = 0
benchi-kafka     | 	max.block.ms = 60000
benchi-kafka     | 	max.in.flight.requests.per.connection = 1
benchi-kafka     | 	max.request.size = 1048576
benchi-kafka     | 	metadata.max.age.ms = 300000
benchi-kafka     | 	metadata.max.idle.ms = 300000
benchi-kafka     | 	metadata.recovery.strategy = none
benchi-kafka     | 	metric.reporters = []
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.recording.level = INFO
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	partitioner.adaptive.partitioning.enable = true
benchi-kafka     | 	partitioner.availability.timeout.ms = 0
benchi-kafka     | 	partitioner.class = null
benchi-kafka     | 	partitioner.ignore.keys = false
benchi-kafka     | 	receive.buffer.bytes = 32768
benchi-kafka     | 	reconnect.backoff.max.ms = 1000
benchi-kafka     | 	reconnect.backoff.ms = 50
benchi-kafka     | 	request.timeout.ms = 30000
benchi-kafka     | 	retries = 2147483647
benchi-kafka     | 	retry.backoff.max.ms = 1000
benchi-kafka     | 	retry.backoff.ms = 100
benchi-kafka     | 	sasl.client.callback.handler.class = null
benchi-kafka     | 	sasl.jaas.config = null
benchi-kafka     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
benchi-kafka     | 	sasl.kerberos.min.time.before.relogin = 60000
benchi-kafka     | 	sasl.kerberos.service.name = null
benchi-kafka     | 	sasl.kerberos.ticket.renew.jitter = 0.05
benchi-kafka     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
benchi-kafka     | 	sasl.login.callback.handler.class = null
benchi-kafka     | 	sasl.login.class = null
benchi-kafka     | 	sasl.login.connect.timeout.ms = null
benchi-kafka     | 	sasl.login.read.timeout.ms = null
benchi-kafka     | 	sasl.login.refresh.buffer.seconds = 300
benchi-kafka     | 	sasl.login.refresh.min.period.seconds = 60
benchi-kafka     | 	sasl.login.refresh.window.factor = 0.8
benchi-kafka     | 	sasl.login.refresh.window.jitter = 0.05
benchi-kafka     | 	sasl.login.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.login.retry.backoff.ms = 100
benchi-kafka     | 	sasl.mechanism = GSSAPI
benchi-kafka     | 	sasl.oauthbearer.clientassertion.audience = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.expiration = 5
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.jti.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.nbf.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.issuer = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.location = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key.passphrase = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.subject = null
benchi-kafka     | 	sasl.oauthbearer.clock.skew.seconds = 30
benchi-kafka     | 	sasl.oauthbearer.expected.audience = null
benchi-kafka     | 	sasl.oauthbearer.expected.issuer = null
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.url = null
benchi-kafka     | 	sasl.oauthbearer.scope.claim.name = scope
benchi-kafka     | 	sasl.oauthbearer.sub.claim.name = sub
benchi-kafka     | 	sasl.oauthbearer.token.endpoint.url = null
benchi-kafka     | 	security.protocol = PLAINTEXT
benchi-kafka     | 	security.providers = null
benchi-kafka     | 	send.buffer.bytes = 131072
benchi-kafka     | 	socket.connection.setup.timeout.max.ms = 30000
benchi-kafka     | 	socket.connection.setup.timeout.ms = 10000
benchi-kafka     | 	ssl.cipher.suites = null
benchi-kafka     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	ssl.engine.factory.class = null
benchi-kafka     | 	ssl.key.password = null
benchi-kafka     | 	ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	ssl.keystore.certificate.chain = null
benchi-kafka     | 	ssl.keystore.key = null
benchi-kafka     | 	ssl.keystore.location = null
benchi-kafka     | 	ssl.keystore.password = null
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.protocol = TLSv1.3
benchi-kafka     | 	ssl.provider = null
benchi-kafka     | 	ssl.secure.random.implementation = null
benchi-kafka     | 	ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	ssl.truststore.certificates = null
benchi-kafka     | 	ssl.truststore.location = null
benchi-kafka     | 	ssl.truststore.password = null
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     | 	transaction.timeout.ms = 60000
benchi-kafka     | 	transactional.id = null
benchi-kafka     | 	value.serializer = class io.confluent.license.LicenseStore$LicenseMessageSerde
benchi-kafka     |  (org.apache.kafka.clients.producer.ProducerConfig)
benchi-kafka     | [2025-03-18 16:08:12,246] INFO initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector)
benchi-kafka     | [2025-03-18 16:08:12,247] INFO These configurations '[confluent.metrics.topic.replicas, confluent.metrics.reporter.bootstrap.servers, jmx.hostname, cluster.link.metadata.topic.replication.factor, jmx.port]' were supplied but are not used yet. (org.apache.kafka.clients.producer.ProducerConfig)
benchi-kafka     | [2025-03-18 16:08:12,247] INFO Kafka version: 7.8.1-ce (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:08:12,247] INFO Kafka commitId: eefb51c6f2f1b888b8b03296353338afbac8e213 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:08:12,247] INFO Kafka startTimeMs: 1742314092247 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:08:12,247] INFO ConsumerConfig values: 
benchi-kafka     | 	allow.auto.create.topics = true
benchi-kafka     | 	auto.commit.interval.ms = 5000
benchi-kafka     | 	auto.include.jmx.reporter = true
benchi-kafka     | 	auto.offset.reset = earliest
benchi-kafka     | 	bootstrap.servers = [broker:29092]
benchi-kafka     | 	check.crcs = true
benchi-kafka     | 	client.dns.lookup = use_all_dns_ips
benchi-kafka     | 	client.id = _confluent-license-consumer-1
benchi-kafka     | 	client.rack = 
benchi-kafka     | 	confluent.lkc.id = null
benchi-kafka     | 	confluent.proxy.protocol.client.address = null
benchi-kafka     | 	confluent.proxy.protocol.client.mode = PROXY
benchi-kafka     | 	confluent.proxy.protocol.client.port = null
benchi-kafka     | 	confluent.proxy.protocol.client.version = NONE
benchi-kafka     | 	connections.max.idle.ms = 540000
benchi-kafka     | 	default.api.timeout.ms = 60000
benchi-kafka     | 	enable.auto.commit = false
benchi-kafka     | 	enable.metrics.push = true
benchi-kafka     | 	exclude.internal.topics = true
benchi-kafka     | 	fetch.max.bytes = 52428800
benchi-kafka     | 	fetch.max.wait.ms = 500
benchi-kafka     | 	fetch.min.bytes = 1
benchi-kafka     | 	group.id = null
benchi-kafka     | 	group.instance.id = null
benchi-kafka     | 	group.protocol = classic
benchi-kafka     | 	group.remote.assignor = null
benchi-kafka     | 	heartbeat.interval.ms = 3000
benchi-kafka     | 	interceptor.classes = []
benchi-kafka     | 	internal.leave.group.on.close = true
benchi-kafka     | 	internal.throw.on.fetch.stable.offset.unsupported = false
benchi-kafka     | 	isolation.level = read_uncommitted
benchi-kafka     | 	key.deserializer = class io.confluent.license.LicenseStore$LicenseKeySerde
benchi-kafka     | 	max.partition.fetch.bytes = 1048576
benchi-kafka     | 	max.poll.interval.ms = 300000
benchi-kafka     | 	max.poll.records = 500
benchi-kafka     | 	metadata.max.age.ms = 300000
benchi-kafka     | 	metadata.recovery.strategy = none
benchi-kafka     | 	metric.reporters = []
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.recording.level = INFO
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
benchi-kafka     | 	receive.buffer.bytes = 65536
benchi-kafka     | 	reconnect.backoff.max.ms = 1000
benchi-kafka     | 	reconnect.backoff.ms = 50
benchi-kafka     | 	request.timeout.ms = 120000
benchi-kafka     | 	retry.backoff.max.ms = 1000
benchi-kafka     | 	retry.backoff.ms = 100
benchi-kafka     | 	sasl.client.callback.handler.class = null
benchi-kafka     | 	sasl.jaas.config = null
benchi-kafka     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
benchi-kafka     | 	sasl.kerberos.min.time.before.relogin = 60000
benchi-kafka     | 	sasl.kerberos.service.name = null
benchi-kafka     | 	sasl.kerberos.ticket.renew.jitter = 0.05
benchi-kafka     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
benchi-kafka     | 	sasl.login.callback.handler.class = null
benchi-kafka     | 	sasl.login.class = null
benchi-kafka     | 	sasl.login.connect.timeout.ms = null
benchi-kafka     | 	sasl.login.read.timeout.ms = null
benchi-kafka     | 	sasl.login.refresh.buffer.seconds = 300
benchi-kafka     | 	sasl.login.refresh.min.period.seconds = 60
benchi-kafka     | 	sasl.login.refresh.window.factor = 0.8
benchi-kafka     | 	sasl.login.refresh.window.jitter = 0.05
benchi-kafka     | 	sasl.login.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.login.retry.backoff.ms = 100
benchi-kafka     | 	sasl.mechanism = GSSAPI
benchi-kafka     | 	sasl.oauthbearer.clientassertion.audience = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.expiration = 5
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.jti.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.nbf.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.issuer = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.location = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key.passphrase = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.subject = null
benchi-kafka     | 	sasl.oauthbearer.clock.skew.seconds = 30
benchi-kafka     | 	sasl.oauthbearer.expected.audience = null
benchi-kafka     | 	sasl.oauthbearer.expected.issuer = null
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.url = null
benchi-kafka     | 	sasl.oauthbearer.scope.claim.name = scope
benchi-kafka     | 	sasl.oauthbearer.sub.claim.name = sub
benchi-kafka     | 	sasl.oauthbearer.token.endpoint.url = null
benchi-kafka     | 	security.protocol = PLAINTEXT
benchi-kafka     | 	security.providers = null
benchi-kafka     | 	send.buffer.bytes = 131072
benchi-kafka     | 	session.timeout.ms = 45000
benchi-kafka     | 	socket.connection.setup.timeout.max.ms = 30000
benchi-kafka     | 	socket.connection.setup.timeout.ms = 10000
benchi-kafka     | 	ssl.cipher.suites = null
benchi-kafka     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	ssl.engine.factory.class = null
benchi-kafka     | 	ssl.key.password = null
benchi-kafka     | 	ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	ssl.keystore.certificate.chain = null
benchi-kafka     | 	ssl.keystore.key = null
benchi-kafka     | 	ssl.keystore.location = null
benchi-kafka     | 	ssl.keystore.password = null
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.protocol = TLSv1.3
benchi-kafka     | 	ssl.provider = null
benchi-kafka     | 	ssl.secure.random.implementation = null
benchi-kafka     | 	ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	ssl.truststore.certificates = null
benchi-kafka     | 	ssl.truststore.location = null
benchi-kafka     | 	ssl.truststore.password = null
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     | 	value.deserializer = class io.confluent.license.LicenseStore$LicenseMessageSerde
benchi-kafka     |  (org.apache.kafka.clients.consumer.ConsumerConfig)
benchi-kafka     | [2025-03-18 16:08:12,247] INFO initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector)
benchi-kafka     | [2025-03-18 16:08:12,248] INFO These configurations '[confluent.metrics.topic.replicas, confluent.metrics.reporter.bootstrap.servers, jmx.hostname, cluster.link.metadata.topic.replication.factor, jmx.port]' were supplied but are not used yet. (org.apache.kafka.clients.consumer.ConsumerConfig)
benchi-kafka     | [2025-03-18 16:08:12,248] INFO Kafka version: 7.8.1-ce (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:08:12,248] INFO Kafka commitId: eefb51c6f2f1b888b8b03296353338afbac8e213 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:08:12,248] INFO Kafka startTimeMs: 1742314092248 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:08:12,249] INFO [Producer clientId=_confluent-license-producer-1] Cluster ID: MkU3OEVBNTcwNTJENDM2Qk (org.apache.kafka.clients.Metadata)
benchi-kafka     | [2025-03-18 16:08:12,250] INFO [Consumer clientId=_confluent-license-consumer-1, groupId=null] Cluster ID: MkU3OEVBNTcwNTJENDM2Qk (org.apache.kafka.clients.Metadata)
benchi-kafka     | [2025-03-18 16:08:12,251] INFO [Consumer clientId=_confluent-license-consumer-1, groupId=null] Assigned to partition(s): _confluent-command-0 (org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer)
benchi-kafka     | [2025-03-18 16:08:12,252] INFO [Consumer clientId=_confluent-license-consumer-1, groupId=null] Seeking to earliest offset of partition _confluent-command-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
benchi-kafka     | [2025-03-18 16:08:12,266] INFO Finished reading KafkaBasedLog for topic _confluent-command (org.apache.kafka.connect.util.KafkaBasedLog)
benchi-kafka     | [2025-03-18 16:08:12,266] INFO Started KafkaBasedLog for topic _confluent-command (org.apache.kafka.connect.util.KafkaBasedLog)
benchi-kafka     | [2025-03-18 16:08:12,266] INFO Started License Store (io.confluent.license.LicenseStore)
benchi-kafka     | [2025-03-18 16:08:12,334] INFO [Producer clientId=confluent-metrics-reporter] Cluster ID: MkU3OEVBNTcwNTJENDM2Qk (org.apache.kafka.clients.Metadata)
benchi-kafka     | [2025-03-18 16:08:12,618] INFO Waiting for 2 seconds for metric reporter topic _confluent-telemetry-metrics to become available. (io.confluent.cruisecontrol.metricsreporter.ConfluentMetricsSamplerBase)
benchi-kafka     | [2025-03-18 16:08:12,803] INFO AdminClientConfig values: 
benchi-kafka     | 	auto.include.jmx.reporter = true
benchi-kafka     | 	bootstrap.controllers = []
benchi-kafka     | 	bootstrap.servers = [broker:29092]
benchi-kafka     | 	client.dns.lookup = use_all_dns_ips
benchi-kafka     | 	client.id = _confluent-license-admin-1
benchi-kafka     | 	confluent.lkc.id = null
benchi-kafka     | 	confluent.metrics.reporter.bootstrap.servers = localhost:9092
benchi-kafka     | 	confluent.proxy.protocol.client.address = null
benchi-kafka     | 	confluent.proxy.protocol.client.mode = PROXY
benchi-kafka     | 	confluent.proxy.protocol.client.port = null
benchi-kafka     | 	confluent.proxy.protocol.client.version = NONE
benchi-kafka     | 	connections.max.idle.ms = 300000
benchi-kafka     | 	default.api.timeout.ms = 60000
benchi-kafka     | 	enable.metrics.push = true
benchi-kafka     | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
benchi-kafka     | 	metadata.max.age.ms = 300000
benchi-kafka     | 	metadata.recovery.strategy = none
benchi-kafka     | 	metric.reporters = []
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.recording.level = INFO
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	receive.buffer.bytes = 65536
benchi-kafka     | 	reconnect.backoff.max.ms = 1000
benchi-kafka     | 	reconnect.backoff.ms = 50
benchi-kafka     | 	request.timeout.ms = 30000
benchi-kafka     | 	retries = 2147483647
benchi-kafka     | 	retry.backoff.max.ms = 1000
benchi-kafka     | 	retry.backoff.ms = 100
benchi-kafka     | 	sasl.client.callback.handler.class = null
benchi-kafka     | 	sasl.jaas.config = null
benchi-kafka     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
benchi-kafka     | 	sasl.kerberos.min.time.before.relogin = 60000
benchi-kafka     | 	sasl.kerberos.service.name = null
benchi-kafka     | 	sasl.kerberos.ticket.renew.jitter = 0.05
benchi-kafka     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
benchi-kafka     | 	sasl.login.callback.handler.class = null
benchi-kafka     | 	sasl.login.class = null
benchi-kafka     | 	sasl.login.connect.timeout.ms = null
benchi-kafka     | 	sasl.login.read.timeout.ms = null
benchi-kafka     | 	sasl.login.refresh.buffer.seconds = 300
benchi-kafka     | 	sasl.login.refresh.min.period.seconds = 60
benchi-kafka     | 	sasl.login.refresh.window.factor = 0.8
benchi-kafka     | 	sasl.login.refresh.window.jitter = 0.05
benchi-kafka     | 	sasl.login.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.login.retry.backoff.ms = 100
benchi-kafka     | 	sasl.mechanism = GSSAPI
benchi-kafka     | 	sasl.oauthbearer.clientassertion.audience = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.expiration = 5
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.jti.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.nbf.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.issuer = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.location = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key.passphrase = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.subject = null
benchi-kafka     | 	sasl.oauthbearer.clock.skew.seconds = 30
benchi-kafka     | 	sasl.oauthbearer.expected.audience = null
benchi-kafka     | 	sasl.oauthbearer.expected.issuer = null
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.url = null
benchi-kafka     | 	sasl.oauthbearer.scope.claim.name = scope
benchi-kafka     | 	sasl.oauthbearer.sub.claim.name = sub
benchi-kafka     | 	sasl.oauthbearer.token.endpoint.url = null
benchi-kafka     | 	security.protocol = PLAINTEXT
benchi-kafka     | 	security.providers = null
benchi-kafka     | 	send.buffer.bytes = 131072
benchi-kafka     | 	socket.connection.setup.timeout.max.ms = 30000
benchi-kafka     | 	socket.connection.setup.timeout.ms = 10000
benchi-kafka     | 	ssl.cipher.suites = null
benchi-kafka     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	ssl.engine.factory.class = null
benchi-kafka     | 	ssl.key.password = null
benchi-kafka     | 	ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	ssl.keystore.certificate.chain = null
benchi-kafka     | 	ssl.keystore.key = null
benchi-kafka     | 	ssl.keystore.location = null
benchi-kafka     | 	ssl.keystore.password = null
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.protocol = TLSv1.3
benchi-kafka     | 	ssl.provider = null
benchi-kafka     | 	ssl.secure.random.implementation = null
benchi-kafka     | 	ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	ssl.truststore.certificates = null
benchi-kafka     | 	ssl.truststore.location = null
benchi-kafka     | 	ssl.truststore.password = null
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     |  (org.apache.kafka.clients.admin.AdminClientConfig)
benchi-kafka     | [2025-03-18 16:08:12,804] INFO These configurations '[confluent.metrics.topic.replicas, replication.factor, confluent.metrics.reporter.bootstrap.servers, jmx.hostname, min.insync.replicas, cluster.link.metadata.topic.replication.factor, jmx.port]' were supplied but are not used yet. (org.apache.kafka.clients.admin.AdminClientConfig)
benchi-kafka     | [2025-03-18 16:08:12,804] INFO Kafka version: 7.8.1-ce (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:08:12,804] INFO Kafka commitId: eefb51c6f2f1b888b8b03296353338afbac8e213 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:08:12,804] INFO Kafka startTimeMs: 1742314092804 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:08:12,809] INFO App info kafka.admin.client for _confluent-license-admin-1 unregistered (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:08:12,844] INFO AdminClientConfig values: 
benchi-kafka     | 	auto.include.jmx.reporter = true
benchi-kafka     | 	bootstrap.controllers = []
benchi-kafka     | 	bootstrap.servers = [broker:29092]
benchi-kafka     | 	client.dns.lookup = use_all_dns_ips
benchi-kafka     | 	client.id = _confluent-license-admin-1
benchi-kafka     | 	confluent.lkc.id = null
benchi-kafka     | 	confluent.metrics.reporter.bootstrap.servers = localhost:9092
benchi-kafka     | 	confluent.proxy.protocol.client.address = null
benchi-kafka     | 	confluent.proxy.protocol.client.mode = PROXY
benchi-kafka     | 	confluent.proxy.protocol.client.port = null
benchi-kafka     | 	confluent.proxy.protocol.client.version = NONE
benchi-kafka     | 	connections.max.idle.ms = 300000
benchi-kafka     | 	default.api.timeout.ms = 60000
benchi-kafka     | 	enable.metrics.push = true
benchi-kafka     | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
benchi-kafka     | 	metadata.max.age.ms = 300000
benchi-kafka     | 	metadata.recovery.strategy = none
benchi-kafka     | 	metric.reporters = []
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.recording.level = INFO
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	receive.buffer.bytes = 65536
benchi-kafka     | 	reconnect.backoff.max.ms = 1000
benchi-kafka     | 	reconnect.backoff.ms = 50
benchi-kafka     | 	request.timeout.ms = 30000
benchi-kafka     | 	retries = 2147483647
benchi-kafka     | 	retry.backoff.max.ms = 1000
benchi-kafka     | 	retry.backoff.ms = 100
benchi-kafka     | 	sasl.client.callback.handler.class = null
benchi-kafka     | 	sasl.jaas.config = null
benchi-kafka     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
benchi-kafka     | 	sasl.kerberos.min.time.before.relogin = 60000
benchi-kafka     | 	sasl.kerberos.service.name = null
benchi-kafka     | 	sasl.kerberos.ticket.renew.jitter = 0.05
benchi-kafka     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
benchi-kafka     | 	sasl.login.callback.handler.class = null
benchi-kafka     | 	sasl.login.class = null
benchi-kafka     | 	sasl.login.connect.timeout.ms = null
benchi-kafka     | 	sasl.login.read.timeout.ms = null
benchi-kafka     | 	sasl.login.refresh.buffer.seconds = 300
benchi-kafka     | 	sasl.login.refresh.min.period.seconds = 60
benchi-kafka     | 	sasl.login.refresh.window.factor = 0.8
benchi-kafka     | 	sasl.login.refresh.window.jitter = 0.05
benchi-kafka     | 	sasl.login.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.login.retry.backoff.ms = 100
benchi-kafka     | 	sasl.mechanism = GSSAPI
benchi-kafka     | 	sasl.oauthbearer.clientassertion.audience = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.expiration = 5
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.jti.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.nbf.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.issuer = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.location = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key.passphrase = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.subject = null
benchi-kafka     | 	sasl.oauthbearer.clock.skew.seconds = 30
benchi-kafka     | 	sasl.oauthbearer.expected.audience = null
benchi-kafka     | 	sasl.oauthbearer.expected.issuer = null
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.url = null
benchi-kafka     | 	sasl.oauthbearer.scope.claim.name = scope
benchi-kafka     | 	sasl.oauthbearer.sub.claim.name = sub
benchi-kafka     | 	sasl.oauthbearer.token.endpoint.url = null
benchi-kafka     | 	security.protocol = PLAINTEXT
benchi-kafka     | 	security.providers = null
benchi-kafka     | 	send.buffer.bytes = 131072
benchi-kafka     | 	socket.connection.setup.timeout.max.ms = 30000
benchi-kafka     | 	socket.connection.setup.timeout.ms = 10000
benchi-kafka     | 	ssl.cipher.suites = null
benchi-kafka     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	ssl.engine.factory.class = null
benchi-kafka     | 	ssl.key.password = null
benchi-kafka     | 	ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	ssl.keystore.certificate.chain = null
benchi-kafka     | 	ssl.keystore.key = null
benchi-kafka     | 	ssl.keystore.location = null
benchi-kafka     | 	ssl.keystore.password = null
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.protocol = TLSv1.3
benchi-kafka     | 	ssl.provider = null
benchi-kafka     | 	ssl.secure.random.implementation = null
benchi-kafka     | 	ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	ssl.truststore.certificates = null
benchi-kafka     | 	ssl.truststore.location = null
benchi-kafka     | 	ssl.truststore.password = null
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     |  (org.apache.kafka.clients.admin.AdminClientConfig)
benchi-kafka     | [2025-03-18 16:08:12,844] INFO These configurations '[confluent.metrics.topic.replicas, replication.factor, confluent.metrics.reporter.bootstrap.servers, jmx.hostname, min.insync.replicas, cluster.link.metadata.topic.replication.factor, jmx.port]' were supplied but are not used yet. (org.apache.kafka.clients.admin.AdminClientConfig)
benchi-kafka     | [2025-03-18 16:08:12,844] INFO Kafka version: 7.8.1-ce (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:08:12,844] INFO Kafka commitId: eefb51c6f2f1b888b8b03296353338afbac8e213 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:08:12,844] INFO Kafka startTimeMs: 1742314092844 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:08:12,849] INFO App info kafka.admin.client for _confluent-license-admin-1 unregistered (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:08:12,849] INFO License for single cluster, single node (io.confluent.license.LicenseManager)
benchi-kafka     | [2025-03-18 16:08:12,852] INFO [BrokerServer id=1] Transition from STARTING to STARTED (kafka.server.BrokerServer)
benchi-kafka     | [2025-03-18 16:08:12,852] INFO Kafka version: 7.8.1-ce (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:08:12,852] INFO Kafka commitId: eefb51c6f2f1b888b8b03296353338afbac8e213 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:08:12,852] INFO Kafka startTimeMs: 1742314092852 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:08:12,852] INFO [KafkaRaftServer nodeId=1] Kafka Server started (kafka.server.KafkaRaftServer)
benchi-kafka     | [2025-03-18 16:08:14,626] INFO Waiting for 4 seconds for metric reporter topic _confluent-telemetry-metrics to become available. (io.confluent.cruisecontrol.metricsreporter.ConfluentMetricsSamplerBase)
benchi-kafka     | [2025-03-18 16:08:18,631] INFO Waiting for 8 seconds for metric reporter topic _confluent-telemetry-metrics to become available. (io.confluent.cruisecontrol.metricsreporter.ConfluentMetricsSamplerBase)
benchi-kafka     | [2025-03-18 16:08:26,344] INFO AdminClientConfig values: 
benchi-kafka     | 	auto.include.jmx.reporter = true
benchi-kafka     | 	bootstrap.controllers = []
benchi-kafka     | 	bootstrap.servers = [localhost:9092]
benchi-kafka     | 	client.dns.lookup = use_all_dns_ips
benchi-kafka     | 	client.id = 
benchi-kafka     | 	confluent.lkc.id = null
benchi-kafka     | 	confluent.metrics.reporter.bootstrap.servers = kafka-0:9071
benchi-kafka     | 	confluent.proxy.protocol.client.address = null
benchi-kafka     | 	confluent.proxy.protocol.client.mode = PROXY
benchi-kafka     | 	confluent.proxy.protocol.client.port = null
benchi-kafka     | 	confluent.proxy.protocol.client.version = NONE
benchi-kafka     | 	connections.max.idle.ms = 300000
benchi-kafka     | 	default.api.timeout.ms = 60000
benchi-kafka     | 	enable.metrics.push = true
benchi-kafka     | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
benchi-kafka     | 	metadata.max.age.ms = 300000
benchi-kafka     | 	metadata.recovery.strategy = none
benchi-kafka     | 	metric.reporters = []
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.recording.level = INFO
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	receive.buffer.bytes = 65536
benchi-kafka     | 	reconnect.backoff.max.ms = 1000
benchi-kafka     | 	reconnect.backoff.ms = 50
benchi-kafka     | 	request.timeout.ms = 30000
benchi-kafka     | 	retries = 2147483647
benchi-kafka     | 	retry.backoff.max.ms = 1000
benchi-kafka     | 	retry.backoff.ms = 100
benchi-kafka     | 	sasl.client.callback.handler.class = null
benchi-kafka     | 	sasl.jaas.config = null
benchi-kafka     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
benchi-kafka     | 	sasl.kerberos.min.time.before.relogin = 60000
benchi-kafka     | 	sasl.kerberos.service.name = null
benchi-kafka     | 	sasl.kerberos.ticket.renew.jitter = 0.05
benchi-kafka     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
benchi-kafka     | 	sasl.login.callback.handler.class = null
benchi-kafka     | 	sasl.login.class = null
benchi-kafka     | 	sasl.login.connect.timeout.ms = null
benchi-kafka     | 	sasl.login.read.timeout.ms = null
benchi-kafka     | 	sasl.login.refresh.buffer.seconds = 300
benchi-kafka     | 	sasl.login.refresh.min.period.seconds = 60
benchi-kafka     | 	sasl.login.refresh.window.factor = 0.8
benchi-kafka     | 	sasl.login.refresh.window.jitter = 0.05
benchi-kafka     | 	sasl.login.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.login.retry.backoff.ms = 100
benchi-kafka     | 	sasl.mechanism = GSSAPI
benchi-kafka     | 	sasl.oauthbearer.clientassertion.audience = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.expiration = 5
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.jti.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.nbf.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.issuer = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.location = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key.passphrase = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.subject = null
benchi-kafka     | 	sasl.oauthbearer.clock.skew.seconds = 30
benchi-kafka     | 	sasl.oauthbearer.expected.audience = null
benchi-kafka     | 	sasl.oauthbearer.expected.issuer = null
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.url = null
benchi-kafka     | 	sasl.oauthbearer.scope.claim.name = scope
benchi-kafka     | 	sasl.oauthbearer.sub.claim.name = sub
benchi-kafka     | 	sasl.oauthbearer.token.endpoint.url = null
benchi-kafka     | 	security.protocol = PLAINTEXT
benchi-kafka     | 	security.providers = null
benchi-kafka     | 	send.buffer.bytes = 131072
benchi-kafka     | 	socket.connection.setup.timeout.max.ms = 30000
benchi-kafka     | 	socket.connection.setup.timeout.ms = 10000
benchi-kafka     | 	ssl.cipher.suites = null
benchi-kafka     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	ssl.engine.factory.class = null
benchi-kafka     | 	ssl.key.password = null
benchi-kafka     | 	ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	ssl.keystore.certificate.chain = null
benchi-kafka     | 	ssl.keystore.key = null
benchi-kafka     | 	ssl.keystore.location = null
benchi-kafka     | 	ssl.keystore.password = null
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.protocol = TLSv1.3
benchi-kafka     | 	ssl.provider = null
benchi-kafka     | 	ssl.secure.random.implementation = null
benchi-kafka     | 	ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	ssl.truststore.certificates = null
benchi-kafka     | 	ssl.truststore.location = null
benchi-kafka     | 	ssl.truststore.password = null
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     |  (org.apache.kafka.clients.admin.AdminClientConfig)
benchi-kafka     | [2025-03-18 16:08:26,347] INFO Kafka version: 7.8.1-ce (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:08:26,347] INFO Kafka commitId: eefb51c6f2f1b888b8b03296353338afbac8e213 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:08:26,347] INFO Kafka startTimeMs: 1742314106347 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:08:26,356] INFO [AdminClient clientId=adminclient-1] The DescribeTopicPartitions API is not supported, using Metadata API to describe topics. (org.apache.kafka.clients.admin.KafkaAdminClient)
benchi-kafka     | [2025-03-18 16:08:26,359] INFO [ControllerServer id=1] CreateTopics result(s): CreatableTopic(name='_confluent-metrics', numPartitions=12, replicationFactor=3, assignments=[], configs=[CreateableTopicConfig(name='max.message.bytes', value='10485760'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='259200000'), CreateableTopicConfig(name='segment.ms', value='14400000'), CreateableTopicConfig(name='retention.bytes', value='-1')], linkName=null, mirrorTopic=null, sourceTopicId=AAAAAAAAAAAAAAAAAAAAAA, mirrorStartOffsetSpec=-9223372036854775808, mirrorStartOffsets=[], stoppedSequenceNumber=0): INVALID_REPLICATION_FACTOR (Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.) (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:08:26,361] INFO App info kafka.admin.client for adminclient-1 unregistered (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:08:26,362] ERROR Error checking or creating metrics topic (io.confluent.metrics.reporter.ConfluentMetricsReporter)
benchi-kafka     | org.apache.kafka.common.errors.InvalidReplicationFactorException: Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.
benchi-kafka     | [2025-03-18 16:08:26,636] INFO Waiting for 16 seconds for metric reporter topic _confluent-telemetry-metrics to become available. (io.confluent.cruisecontrol.metricsreporter.ConfluentMetricsSamplerBase)
benchi-kafka     | [2025-03-18 16:08:41,344] INFO AdminClientConfig values: 
benchi-kafka     | 	auto.include.jmx.reporter = true
benchi-kafka     | 	bootstrap.controllers = []
benchi-kafka     | 	bootstrap.servers = [localhost:9092]
benchi-kafka     | 	client.dns.lookup = use_all_dns_ips
benchi-kafka     | 	client.id = 
benchi-kafka     | 	confluent.lkc.id = null
benchi-kafka     | 	confluent.metrics.reporter.bootstrap.servers = kafka-0:9071
benchi-kafka     | 	confluent.proxy.protocol.client.address = null
benchi-kafka     | 	confluent.proxy.protocol.client.mode = PROXY
benchi-kafka     | 	confluent.proxy.protocol.client.port = null
benchi-kafka     | 	confluent.proxy.protocol.client.version = NONE
benchi-kafka     | 	connections.max.idle.ms = 300000
benchi-kafka     | 	default.api.timeout.ms = 60000
benchi-kafka     | 	enable.metrics.push = true
benchi-kafka     | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
benchi-kafka     | 	metadata.max.age.ms = 300000
benchi-kafka     | 	metadata.recovery.strategy = none
benchi-kafka     | 	metric.reporters = []
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.recording.level = INFO
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	receive.buffer.bytes = 65536
benchi-kafka     | 	reconnect.backoff.max.ms = 1000
benchi-kafka     | 	reconnect.backoff.ms = 50
benchi-kafka     | 	request.timeout.ms = 30000
benchi-kafka     | 	retries = 2147483647
benchi-kafka     | 	retry.backoff.max.ms = 1000
benchi-kafka     | 	retry.backoff.ms = 100
benchi-kafka     | 	sasl.client.callback.handler.class = null
benchi-kafka     | 	sasl.jaas.config = null
benchi-kafka     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
benchi-kafka     | 	sasl.kerberos.min.time.before.relogin = 60000
benchi-kafka     | 	sasl.kerberos.service.name = null
benchi-kafka     | 	sasl.kerberos.ticket.renew.jitter = 0.05
benchi-kafka     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
benchi-kafka     | 	sasl.login.callback.handler.class = null
benchi-kafka     | 	sasl.login.class = null
benchi-kafka     | 	sasl.login.connect.timeout.ms = null
benchi-kafka     | 	sasl.login.read.timeout.ms = null
benchi-kafka     | 	sasl.login.refresh.buffer.seconds = 300
benchi-kafka     | 	sasl.login.refresh.min.period.seconds = 60
benchi-kafka     | 	sasl.login.refresh.window.factor = 0.8
benchi-kafka     | 	sasl.login.refresh.window.jitter = 0.05
benchi-kafka     | 	sasl.login.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.login.retry.backoff.ms = 100
benchi-kafka     | 	sasl.mechanism = GSSAPI
benchi-kafka     | 	sasl.oauthbearer.clientassertion.audience = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.expiration = 5
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.jti.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.nbf.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.issuer = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.location = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key.passphrase = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.subject = null
benchi-kafka     | 	sasl.oauthbearer.clock.skew.seconds = 30
benchi-kafka     | 	sasl.oauthbearer.expected.audience = null
benchi-kafka     | 	sasl.oauthbearer.expected.issuer = null
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.url = null
benchi-kafka     | 	sasl.oauthbearer.scope.claim.name = scope
benchi-kafka     | 	sasl.oauthbearer.sub.claim.name = sub
benchi-kafka     | 	sasl.oauthbearer.token.endpoint.url = null
benchi-kafka     | 	security.protocol = PLAINTEXT
benchi-kafka     | 	security.providers = null
benchi-kafka     | 	send.buffer.bytes = 131072
benchi-kafka     | 	socket.connection.setup.timeout.max.ms = 30000
benchi-kafka     | 	socket.connection.setup.timeout.ms = 10000
benchi-kafka     | 	ssl.cipher.suites = null
benchi-kafka     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	ssl.engine.factory.class = null
benchi-kafka     | 	ssl.key.password = null
benchi-kafka     | 	ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	ssl.keystore.certificate.chain = null
benchi-kafka     | 	ssl.keystore.key = null
benchi-kafka     | 	ssl.keystore.location = null
benchi-kafka     | 	ssl.keystore.password = null
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.protocol = TLSv1.3
benchi-kafka     | 	ssl.provider = null
benchi-kafka     | 	ssl.secure.random.implementation = null
benchi-kafka     | 	ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	ssl.truststore.certificates = null
benchi-kafka     | 	ssl.truststore.location = null
benchi-kafka     | 	ssl.truststore.password = null
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     |  (org.apache.kafka.clients.admin.AdminClientConfig)
benchi-kafka     | [2025-03-18 16:08:41,348] INFO Kafka version: 7.8.1-ce (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:08:41,348] INFO Kafka commitId: eefb51c6f2f1b888b8b03296353338afbac8e213 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:08:41,348] INFO Kafka startTimeMs: 1742314121348 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:08:41,353] INFO Beginning log roller... (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:08:41,354] INFO Log roller completed in 0 seconds (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:08:41,360] INFO [AdminClient clientId=adminclient-2] The DescribeTopicPartitions API is not supported, using Metadata API to describe topics. (org.apache.kafka.clients.admin.KafkaAdminClient)
benchi-kafka     | [2025-03-18 16:08:41,362] INFO [ControllerServer id=1] CreateTopics result(s): CreatableTopic(name='_confluent-metrics', numPartitions=12, replicationFactor=3, assignments=[], configs=[CreateableTopicConfig(name='max.message.bytes', value='10485760'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='259200000'), CreateableTopicConfig(name='segment.ms', value='14400000'), CreateableTopicConfig(name='retention.bytes', value='-1')], linkName=null, mirrorTopic=null, sourceTopicId=AAAAAAAAAAAAAAAAAAAAAA, mirrorStartOffsetSpec=-9223372036854775808, mirrorStartOffsets=[], stoppedSequenceNumber=0): INVALID_REPLICATION_FACTOR (Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.) (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:08:41,364] INFO App info kafka.admin.client for adminclient-2 unregistered (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:08:41,365] ERROR Error checking or creating metrics topic (io.confluent.metrics.reporter.ConfluentMetricsReporter)
benchi-kafka     | org.apache.kafka.common.errors.InvalidReplicationFactorException: Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.
benchi-kafka     | [2025-03-18 16:08:42,642] INFO Waiting for 32 seconds for metric reporter topic _confluent-telemetry-metrics to become available. (io.confluent.cruisecontrol.metricsreporter.ConfluentMetricsSamplerBase)
benchi-kafka     | [2025-03-18 16:08:56,344] INFO AdminClientConfig values: 
benchi-kafka     | 	auto.include.jmx.reporter = true
benchi-kafka     | 	bootstrap.controllers = []
benchi-kafka     | 	bootstrap.servers = [localhost:9092]
benchi-kafka     | 	client.dns.lookup = use_all_dns_ips
benchi-kafka     | 	client.id = 
benchi-kafka     | 	confluent.lkc.id = null
benchi-kafka     | 	confluent.metrics.reporter.bootstrap.servers = kafka-0:9071
benchi-kafka     | 	confluent.proxy.protocol.client.address = null
benchi-kafka     | 	confluent.proxy.protocol.client.mode = PROXY
benchi-kafka     | 	confluent.proxy.protocol.client.port = null
benchi-kafka     | 	confluent.proxy.protocol.client.version = NONE
benchi-kafka     | 	connections.max.idle.ms = 300000
benchi-kafka     | 	default.api.timeout.ms = 60000
benchi-kafka     | 	enable.metrics.push = true
benchi-kafka     | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
benchi-kafka     | 	metadata.max.age.ms = 300000
benchi-kafka     | 	metadata.recovery.strategy = none
benchi-kafka     | 	metric.reporters = []
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.recording.level = INFO
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	receive.buffer.bytes = 65536
benchi-kafka     | 	reconnect.backoff.max.ms = 1000
benchi-kafka     | 	reconnect.backoff.ms = 50
benchi-kafka     | 	request.timeout.ms = 30000
benchi-kafka     | 	retries = 2147483647
benchi-kafka     | 	retry.backoff.max.ms = 1000
benchi-kafka     | 	retry.backoff.ms = 100
benchi-kafka     | 	sasl.client.callback.handler.class = null
benchi-kafka     | 	sasl.jaas.config = null
benchi-kafka     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
benchi-kafka     | 	sasl.kerberos.min.time.before.relogin = 60000
benchi-kafka     | 	sasl.kerberos.service.name = null
benchi-kafka     | 	sasl.kerberos.ticket.renew.jitter = 0.05
benchi-kafka     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
benchi-kafka     | 	sasl.login.callback.handler.class = null
benchi-kafka     | 	sasl.login.class = null
benchi-kafka     | 	sasl.login.connect.timeout.ms = null
benchi-kafka     | 	sasl.login.read.timeout.ms = null
benchi-kafka     | 	sasl.login.refresh.buffer.seconds = 300
benchi-kafka     | 	sasl.login.refresh.min.period.seconds = 60
benchi-kafka     | 	sasl.login.refresh.window.factor = 0.8
benchi-kafka     | 	sasl.login.refresh.window.jitter = 0.05
benchi-kafka     | 	sasl.login.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.login.retry.backoff.ms = 100
benchi-kafka     | 	sasl.mechanism = GSSAPI
benchi-kafka     | 	sasl.oauthbearer.clientassertion.audience = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.expiration = 5
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.jti.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.nbf.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.issuer = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.location = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key.passphrase = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.subject = null
benchi-kafka     | 	sasl.oauthbearer.clock.skew.seconds = 30
benchi-kafka     | 	sasl.oauthbearer.expected.audience = null
benchi-kafka     | 	sasl.oauthbearer.expected.issuer = null
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.url = null
benchi-kafka     | 	sasl.oauthbearer.scope.claim.name = scope
benchi-kafka     | 	sasl.oauthbearer.sub.claim.name = sub
benchi-kafka     | 	sasl.oauthbearer.token.endpoint.url = null
benchi-kafka     | 	security.protocol = PLAINTEXT
benchi-kafka     | 	security.providers = null
benchi-kafka     | 	send.buffer.bytes = 131072
benchi-kafka     | 	socket.connection.setup.timeout.max.ms = 30000
benchi-kafka     | 	socket.connection.setup.timeout.ms = 10000
benchi-kafka     | 	ssl.cipher.suites = null
benchi-kafka     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	ssl.engine.factory.class = null
benchi-kafka     | 	ssl.key.password = null
benchi-kafka     | 	ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	ssl.keystore.certificate.chain = null
benchi-kafka     | 	ssl.keystore.key = null
benchi-kafka     | 	ssl.keystore.location = null
benchi-kafka     | 	ssl.keystore.password = null
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.protocol = TLSv1.3
benchi-kafka     | 	ssl.provider = null
benchi-kafka     | 	ssl.secure.random.implementation = null
benchi-kafka     | 	ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	ssl.truststore.certificates = null
benchi-kafka     | 	ssl.truststore.location = null
benchi-kafka     | 	ssl.truststore.password = null
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     |  (org.apache.kafka.clients.admin.AdminClientConfig)
benchi-kafka     | [2025-03-18 16:08:56,347] INFO Kafka version: 7.8.1-ce (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:08:56,347] INFO Kafka commitId: eefb51c6f2f1b888b8b03296353338afbac8e213 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:08:56,347] INFO Kafka startTimeMs: 1742314136347 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:08:56,354] INFO [AdminClient clientId=adminclient-3] The DescribeTopicPartitions API is not supported, using Metadata API to describe topics. (org.apache.kafka.clients.admin.KafkaAdminClient)
benchi-kafka     | [2025-03-18 16:08:56,358] INFO [ControllerServer id=1] CreateTopics result(s): CreatableTopic(name='_confluent-metrics', numPartitions=12, replicationFactor=3, assignments=[], configs=[CreateableTopicConfig(name='max.message.bytes', value='10485760'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='259200000'), CreateableTopicConfig(name='segment.ms', value='14400000'), CreateableTopicConfig(name='retention.bytes', value='-1')], linkName=null, mirrorTopic=null, sourceTopicId=AAAAAAAAAAAAAAAAAAAAAA, mirrorStartOffsetSpec=-9223372036854775808, mirrorStartOffsets=[], stoppedSequenceNumber=0): INVALID_REPLICATION_FACTOR (Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.) (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:08:56,359] INFO App info kafka.admin.client for adminclient-3 unregistered (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:08:56,360] ERROR Error checking or creating metrics topic (io.confluent.metrics.reporter.ConfluentMetricsReporter)
benchi-kafka     | org.apache.kafka.common.errors.InvalidReplicationFactorException: Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.
benchi-kafka     | [2025-03-18 16:09:10,705] INFO [CelltControllerMetricsPublisher id=1] No cells found. Skipping cell metrics refresh. (org.apache.kafka.controller.metrics.CellControllerMetricsPublisher)
benchi-kafka     | [2025-03-18 16:09:11,338] INFO AdminClientConfig values: 
benchi-kafka     | 	auto.include.jmx.reporter = true
benchi-kafka     | 	bootstrap.controllers = []
benchi-kafka     | 	bootstrap.servers = [localhost:9092]
benchi-kafka     | 	client.dns.lookup = use_all_dns_ips
benchi-kafka     | 	client.id = 
benchi-kafka     | 	confluent.lkc.id = null
benchi-kafka     | 	confluent.metrics.reporter.bootstrap.servers = kafka-0:9071
benchi-kafka     | 	confluent.proxy.protocol.client.address = null
benchi-kafka     | 	confluent.proxy.protocol.client.mode = PROXY
benchi-kafka     | 	confluent.proxy.protocol.client.port = null
benchi-kafka     | 	confluent.proxy.protocol.client.version = NONE
benchi-kafka     | 	connections.max.idle.ms = 300000
benchi-kafka     | 	default.api.timeout.ms = 60000
benchi-kafka     | 	enable.metrics.push = true
benchi-kafka     | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
benchi-kafka     | 	metadata.max.age.ms = 300000
benchi-kafka     | 	metadata.recovery.strategy = none
benchi-kafka     | 	metric.reporters = []
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.recording.level = INFO
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	receive.buffer.bytes = 65536
benchi-kafka     | 	reconnect.backoff.max.ms = 1000
benchi-kafka     | 	reconnect.backoff.ms = 50
benchi-kafka     | 	request.timeout.ms = 30000
benchi-kafka     | 	retries = 2147483647
benchi-kafka     | 	retry.backoff.max.ms = 1000
benchi-kafka     | 	retry.backoff.ms = 100
benchi-kafka     | 	sasl.client.callback.handler.class = null
benchi-kafka     | 	sasl.jaas.config = null
benchi-kafka     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
benchi-kafka     | 	sasl.kerberos.min.time.before.relogin = 60000
benchi-kafka     | 	sasl.kerberos.service.name = null
benchi-kafka     | 	sasl.kerberos.ticket.renew.jitter = 0.05
benchi-kafka     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
benchi-kafka     | 	sasl.login.callback.handler.class = null
benchi-kafka     | 	sasl.login.class = null
benchi-kafka     | 	sasl.login.connect.timeout.ms = null
benchi-kafka     | 	sasl.login.read.timeout.ms = null
benchi-kafka     | 	sasl.login.refresh.buffer.seconds = 300
benchi-kafka     | 	sasl.login.refresh.min.period.seconds = 60
benchi-kafka     | 	sasl.login.refresh.window.factor = 0.8
benchi-kafka     | 	sasl.login.refresh.window.jitter = 0.05
benchi-kafka     | 	sasl.login.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.login.retry.backoff.ms = 100
benchi-kafka     | 	sasl.mechanism = GSSAPI
benchi-kafka     | 	sasl.oauthbearer.clientassertion.audience = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.expiration = 5
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.jti.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.nbf.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.issuer = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.location = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key.passphrase = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.subject = null
benchi-kafka     | 	sasl.oauthbearer.clock.skew.seconds = 30
benchi-kafka     | 	sasl.oauthbearer.expected.audience = null
benchi-kafka     | 	sasl.oauthbearer.expected.issuer = null
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.url = null
benchi-kafka     | 	sasl.oauthbearer.scope.claim.name = scope
benchi-kafka     | 	sasl.oauthbearer.sub.claim.name = sub
benchi-kafka     | 	sasl.oauthbearer.token.endpoint.url = null
benchi-kafka     | 	security.protocol = PLAINTEXT
benchi-kafka     | 	security.providers = null
benchi-kafka     | 	send.buffer.bytes = 131072
benchi-kafka     | 	socket.connection.setup.timeout.max.ms = 30000
benchi-kafka     | 	socket.connection.setup.timeout.ms = 10000
benchi-kafka     | 	ssl.cipher.suites = null
benchi-kafka     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	ssl.engine.factory.class = null
benchi-kafka     | 	ssl.key.password = null
benchi-kafka     | 	ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	ssl.keystore.certificate.chain = null
benchi-kafka     | 	ssl.keystore.key = null
benchi-kafka     | 	ssl.keystore.location = null
benchi-kafka     | 	ssl.keystore.password = null
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.protocol = TLSv1.3
benchi-kafka     | 	ssl.provider = null
benchi-kafka     | 	ssl.secure.random.implementation = null
benchi-kafka     | 	ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	ssl.truststore.certificates = null
benchi-kafka     | 	ssl.truststore.location = null
benchi-kafka     | 	ssl.truststore.password = null
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     |  (org.apache.kafka.clients.admin.AdminClientConfig)
benchi-kafka     | [2025-03-18 16:09:11,339] INFO Kafka version: 7.8.1-ce (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:09:11,339] INFO Kafka commitId: eefb51c6f2f1b888b8b03296353338afbac8e213 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:09:11,339] INFO Kafka startTimeMs: 1742314151339 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:09:11,343] INFO [AdminClient clientId=adminclient-4] The DescribeTopicPartitions API is not supported, using Metadata API to describe topics. (org.apache.kafka.clients.admin.KafkaAdminClient)
benchi-kafka     | [2025-03-18 16:09:11,347] INFO [ControllerServer id=1] CreateTopics result(s): CreatableTopic(name='_confluent-metrics', numPartitions=12, replicationFactor=3, assignments=[], configs=[CreateableTopicConfig(name='max.message.bytes', value='10485760'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='259200000'), CreateableTopicConfig(name='segment.ms', value='14400000'), CreateableTopicConfig(name='retention.bytes', value='-1')], linkName=null, mirrorTopic=null, sourceTopicId=AAAAAAAAAAAAAAAAAAAAAA, mirrorStartOffsetSpec=-9223372036854775808, mirrorStartOffsets=[], stoppedSequenceNumber=0): INVALID_REPLICATION_FACTOR (Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.) (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:09:11,348] INFO App info kafka.admin.client for adminclient-4 unregistered (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:09:11,349] ERROR Error checking or creating metrics topic (io.confluent.metrics.reporter.ConfluentMetricsReporter)
benchi-kafka     | org.apache.kafka.common.errors.InvalidReplicationFactorException: Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.
benchi-kafka     | [2025-03-18 16:09:11,370] INFO AdminClientConfig values: 
benchi-kafka     | 	auto.include.jmx.reporter = true
benchi-kafka     | 	bootstrap.controllers = []
benchi-kafka     | 	bootstrap.servers = [broker:29092]
benchi-kafka     | 	client.dns.lookup = use_all_dns_ips
benchi-kafka     | 	client.id = confluent-telemetry-reporter-local-producer
benchi-kafka     | 	confluent.lkc.id = null
benchi-kafka     | 	confluent.metrics.reporter.bootstrap.servers = localhost:9092
benchi-kafka     | 	confluent.proxy.protocol.client.address = null
benchi-kafka     | 	confluent.proxy.protocol.client.mode = PROXY
benchi-kafka     | 	confluent.proxy.protocol.client.port = null
benchi-kafka     | 	confluent.proxy.protocol.client.version = NONE
benchi-kafka     | 	connections.max.idle.ms = 300000
benchi-kafka     | 	default.api.timeout.ms = 60000
benchi-kafka     | 	enable.metrics.push = true
benchi-kafka     | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
benchi-kafka     | 	metadata.max.age.ms = 300000
benchi-kafka     | 	metadata.recovery.strategy = none
benchi-kafka     | 	metric.reporters = []
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.recording.level = INFO
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	receive.buffer.bytes = 65536
benchi-kafka     | 	reconnect.backoff.max.ms = 1000
benchi-kafka     | 	reconnect.backoff.ms = 50
benchi-kafka     | 	request.timeout.ms = 30000
benchi-kafka     | 	retries = 2147483647
benchi-kafka     | 	retry.backoff.max.ms = 1000
benchi-kafka     | 	retry.backoff.ms = 500
benchi-kafka     | 	sasl.client.callback.handler.class = null
benchi-kafka     | 	sasl.jaas.config = null
benchi-kafka     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
benchi-kafka     | 	sasl.kerberos.min.time.before.relogin = 60000
benchi-kafka     | 	sasl.kerberos.service.name = null
benchi-kafka     | 	sasl.kerberos.ticket.renew.jitter = 0.05
benchi-kafka     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
benchi-kafka     | 	sasl.login.callback.handler.class = null
benchi-kafka     | 	sasl.login.class = null
benchi-kafka     | 	sasl.login.connect.timeout.ms = null
benchi-kafka     | 	sasl.login.read.timeout.ms = null
benchi-kafka     | 	sasl.login.refresh.buffer.seconds = 300
benchi-kafka     | 	sasl.login.refresh.min.period.seconds = 60
benchi-kafka     | 	sasl.login.refresh.window.factor = 0.8
benchi-kafka     | 	sasl.login.refresh.window.jitter = 0.05
benchi-kafka     | 	sasl.login.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.login.retry.backoff.ms = 100
benchi-kafka     | 	sasl.mechanism = GSSAPI
benchi-kafka     | 	sasl.oauthbearer.clientassertion.audience = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.expiration = 5
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.jti.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.nbf.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.issuer = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.location = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key.passphrase = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.subject = null
benchi-kafka     | 	sasl.oauthbearer.clock.skew.seconds = 30
benchi-kafka     | 	sasl.oauthbearer.expected.audience = null
benchi-kafka     | 	sasl.oauthbearer.expected.issuer = null
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.url = null
benchi-kafka     | 	sasl.oauthbearer.scope.claim.name = scope
benchi-kafka     | 	sasl.oauthbearer.sub.claim.name = sub
benchi-kafka     | 	sasl.oauthbearer.token.endpoint.url = null
benchi-kafka     | 	security.protocol = PLAINTEXT
benchi-kafka     | 	security.providers = null
benchi-kafka     | 	send.buffer.bytes = 131072
benchi-kafka     | 	socket.connection.setup.timeout.max.ms = 30000
benchi-kafka     | 	socket.connection.setup.timeout.ms = 10000
benchi-kafka     | 	ssl.cipher.suites = null
benchi-kafka     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	ssl.engine.factory.class = null
benchi-kafka     | 	ssl.key.password = null
benchi-kafka     | 	ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	ssl.keystore.certificate.chain = null
benchi-kafka     | 	ssl.keystore.key = null
benchi-kafka     | 	ssl.keystore.location = null
benchi-kafka     | 	ssl.keystore.password = null
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.protocol = TLSv1.3
benchi-kafka     | 	ssl.provider = null
benchi-kafka     | 	ssl.secure.random.implementation = null
benchi-kafka     | 	ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	ssl.truststore.certificates = null
benchi-kafka     | 	ssl.truststore.location = null
benchi-kafka     | 	ssl.truststore.password = null
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     |  (org.apache.kafka.clients.admin.AdminClientConfig)
benchi-kafka     | [2025-03-18 16:09:11,370] INFO These configurations '[compression.type, confluent.metrics.reporter.bootstrap.servers, enable.idempotence, acks, key.serializer, max.request.size, value.serializer, partitioner.class, interceptor.classes, max.in.flight.requests.per.connection, linger.ms]' were supplied but are not used yet. (org.apache.kafka.clients.admin.AdminClientConfig)
benchi-kafka     | [2025-03-18 16:09:11,371] INFO Kafka version: 7.8.1-ce (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:09:11,371] INFO Kafka commitId: eefb51c6f2f1b888b8b03296353338afbac8e213 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:09:11,371] INFO Kafka startTimeMs: 1742314151370 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:09:11,374] INFO [AdminClient clientId=confluent-telemetry-reporter-local-producer] The DescribeTopicPartitions API is not supported, using Metadata API to describe topics. (org.apache.kafka.clients.admin.KafkaAdminClient)
benchi-kafka     | [2025-03-18 16:09:11,377] INFO [ControllerServer id=1] CreateTopics result(s): CreatableTopic(name='_confluent-telemetry-metrics', numPartitions=12, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='max.message.bytes', value='10485760'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='259200000'), CreateableTopicConfig(name='segment.ms', value='14400000'), CreateableTopicConfig(name='retention.bytes', value='-1')], linkName=null, mirrorTopic=null, sourceTopicId=AAAAAAAAAAAAAAAAAAAAAA, mirrorStartOffsetSpec=-9223372036854775808, mirrorStartOffsets=[], stoppedSequenceNumber=0): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:09:11,377] INFO [ControllerServer id=1] Replayed TopicRecord for topic _confluent-telemetry-metrics with topic ID rACWIAgeSLaNjkitMhNiUQ. (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:09:11,377] INFO [ControllerServer id=1] Replayed ConfigRecord for ConfigResource(type=TOPIC, name='_confluent-telemetry-metrics') which set configuration max.message.bytes to 10485760 (org.apache.kafka.controller.ConfigurationControlManager)
benchi-kafka     | [2025-03-18 16:09:11,377] INFO [ControllerServer id=1] Replayed ConfigRecord for ConfigResource(type=TOPIC, name='_confluent-telemetry-metrics') which set configuration message.timestamp.type to CreateTime (org.apache.kafka.controller.ConfigurationControlManager)
benchi-kafka     | [2025-03-18 16:09:11,377] INFO [ControllerServer id=1] Replayed ConfigRecord for ConfigResource(type=TOPIC, name='_confluent-telemetry-metrics') which set configuration min.insync.replicas to 1 (org.apache.kafka.controller.ConfigurationControlManager)
benchi-kafka     | [2025-03-18 16:09:11,377] INFO [ControllerServer id=1] Replayed ConfigRecord for ConfigResource(type=TOPIC, name='_confluent-telemetry-metrics') which set configuration retention.ms to 259200000 (org.apache.kafka.controller.ConfigurationControlManager)
benchi-kafka     | [2025-03-18 16:09:11,377] INFO [ControllerServer id=1] Replayed ConfigRecord for ConfigResource(type=TOPIC, name='_confluent-telemetry-metrics') which set configuration segment.ms to 14400000 (org.apache.kafka.controller.ConfigurationControlManager)
benchi-kafka     | [2025-03-18 16:09:11,377] INFO [ControllerServer id=1] Replayed ConfigRecord for ConfigResource(type=TOPIC, name='_confluent-telemetry-metrics') which set configuration retention.bytes to -1 (org.apache.kafka.controller.ConfigurationControlManager)
benchi-kafka     | [2025-03-18 16:09:11,377] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-telemetry-metrics-0 with topic ID rACWIAgeSLaNjkitMhNiUQ and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:09:11,377] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-telemetry-metrics-1 with topic ID rACWIAgeSLaNjkitMhNiUQ and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:09:11,377] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-telemetry-metrics-2 with topic ID rACWIAgeSLaNjkitMhNiUQ and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:09:11,377] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-telemetry-metrics-3 with topic ID rACWIAgeSLaNjkitMhNiUQ and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:09:11,377] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-telemetry-metrics-4 with topic ID rACWIAgeSLaNjkitMhNiUQ and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:09:11,377] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-telemetry-metrics-5 with topic ID rACWIAgeSLaNjkitMhNiUQ and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:09:11,377] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-telemetry-metrics-6 with topic ID rACWIAgeSLaNjkitMhNiUQ and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:09:11,378] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-telemetry-metrics-7 with topic ID rACWIAgeSLaNjkitMhNiUQ and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:09:11,378] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-telemetry-metrics-8 with topic ID rACWIAgeSLaNjkitMhNiUQ and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:09:11,378] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-telemetry-metrics-9 with topic ID rACWIAgeSLaNjkitMhNiUQ and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:09:11,378] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-telemetry-metrics-10 with topic ID rACWIAgeSLaNjkitMhNiUQ and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:09:11,378] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent-telemetry-metrics-11 with topic ID rACWIAgeSLaNjkitMhNiUQ and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:09:11,405] INFO SBC Event SbcMetadataUpdateEvent-134 generated 1 more events to enqueue in the following order - [SbcConfigUpdateEvent-135]. Enqueuing... (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:09:11,405] INFO Handling event SbcConfigUpdateEvent-135 (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:09:11,405] INFO [Broker id=1] Transitioning 12 partition(s) to local leaders. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:11,405] INFO Balancer notified of a config change: ConfigurationsDelta(changes={ConfigResource(type=TOPIC, name='_confluent-telemetry-metrics')=ConfigurationDelta(changedKeys=[max.message.bytes, message.timestamp.type, min.insync.replicas, retention.ms, segment.ms, retention.bytes])}) (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:09:11,405] INFO There were 0 change(s) and 0 deletion(s) to balancer configs. Changed Configs: {}, Deleted Configs: [] (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:09:11,405] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent-telemetry-metrics-3, _confluent-telemetry-metrics-4, _confluent-telemetry-metrics-5, _confluent-telemetry-metrics-6, _confluent-telemetry-metrics-7, _confluent-telemetry-metrics-8, _confluent-telemetry-metrics-9, _confluent-telemetry-metrics-10, _confluent-telemetry-metrics-11, _confluent-telemetry-metrics-0, _confluent-telemetry-metrics-1, _confluent-telemetry-metrics-2) (kafka.server.ReplicaFetcherManager)
benchi-kafka     | [2025-03-18 16:09:11,405] INFO [Broker id=1] Creating new partition _confluent-telemetry-metrics-3 with topic id rACWIAgeSLaNjkitMhNiUQ. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:11,406] INFO [Broker id=1] Creating new partition _confluent-telemetry-metrics-4 with topic id rACWIAgeSLaNjkitMhNiUQ. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:11,406] INFO [Broker id=1] Creating new partition _confluent-telemetry-metrics-5 with topic id rACWIAgeSLaNjkitMhNiUQ. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:11,407] INFO Created telemetry topic _confluent-telemetry-metrics (io.confluent.telemetry.exporter.kafka.KafkaExporter)
benchi-kafka     | [2025-03-18 16:09:11,407] INFO [Broker id=1] Creating new partition _confluent-telemetry-metrics-6 with topic id rACWIAgeSLaNjkitMhNiUQ. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:11,407] INFO App info kafka.admin.client for confluent-telemetry-reporter-local-producer unregistered (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:09:11,407] INFO [Broker id=1] Creating new partition _confluent-telemetry-metrics-7 with topic id rACWIAgeSLaNjkitMhNiUQ. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:11,408] INFO [Broker id=1] Creating new partition _confluent-telemetry-metrics-8 with topic id rACWIAgeSLaNjkitMhNiUQ. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:11,408] INFO [Broker id=1] Creating new partition _confluent-telemetry-metrics-9 with topic id rACWIAgeSLaNjkitMhNiUQ. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:11,409] INFO [Broker id=1] Creating new partition _confluent-telemetry-metrics-10 with topic id rACWIAgeSLaNjkitMhNiUQ. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:11,409] INFO [Broker id=1] Creating new partition _confluent-telemetry-metrics-11 with topic id rACWIAgeSLaNjkitMhNiUQ. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:11,409] INFO [Broker id=1] Creating new partition _confluent-telemetry-metrics-0 with topic id rACWIAgeSLaNjkitMhNiUQ. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:11,409] INFO [Broker id=1] Creating new partition _confluent-telemetry-metrics-1 with topic id rACWIAgeSLaNjkitMhNiUQ. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:11,409] INFO [Broker id=1] Creating new partition _confluent-telemetry-metrics-2 with topic id rACWIAgeSLaNjkitMhNiUQ. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:11,410] INFO [Broker id=1] Stopped fetchers as part of become-leader transition for 12 partitions (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:11,412] INFO [MergedLog partition=_confluent-telemetry-metrics-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:09:11,412] INFO Created log for partition _confluent-telemetry-metrics-0 in /tmp/kraft-combined-logs/_confluent-telemetry-metrics-0 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:11,413] INFO [Partition _confluent-telemetry-metrics-0 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:11,413] INFO [Partition _confluent-telemetry-metrics-0 broker=1] Log loaded for partition _confluent-telemetry-metrics-0 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:11,413] INFO Setting topicIdPartition rACWIAgeSLaNjkitMhNiUQ:_confluent-telemetry-metrics-0 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:09:11,413] INFO [MergedLog partition=_confluent-telemetry-metrics-0, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-telemetry-metrics-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:09:11,413] INFO [Broker id=1] Leader _confluent-telemetry-metrics-0 with topic id Some(rACWIAgeSLaNjkitMhNiUQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:11,413] INFO [MergedLog partition=_confluent-telemetry-metrics-11, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:09:11,414] INFO Created log for partition _confluent-telemetry-metrics-11 in /tmp/kraft-combined-logs/_confluent-telemetry-metrics-11 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:11,414] INFO [Partition _confluent-telemetry-metrics-11 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-11 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:11,414] INFO [Partition _confluent-telemetry-metrics-11 broker=1] Log loaded for partition _confluent-telemetry-metrics-11 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:11,414] INFO Setting topicIdPartition rACWIAgeSLaNjkitMhNiUQ:_confluent-telemetry-metrics-11 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:09:11,414] INFO [MergedLog partition=_confluent-telemetry-metrics-11, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-telemetry-metrics-11 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:09:11,414] INFO [Broker id=1] Leader _confluent-telemetry-metrics-11 with topic id Some(rACWIAgeSLaNjkitMhNiUQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:11,415] INFO [MergedLog partition=_confluent-telemetry-metrics-3, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:09:11,415] INFO Created log for partition _confluent-telemetry-metrics-3 in /tmp/kraft-combined-logs/_confluent-telemetry-metrics-3 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:11,416] INFO [Partition _confluent-telemetry-metrics-3 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-3 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:11,416] INFO [Partition _confluent-telemetry-metrics-3 broker=1] Log loaded for partition _confluent-telemetry-metrics-3 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:11,416] INFO Setting topicIdPartition rACWIAgeSLaNjkitMhNiUQ:_confluent-telemetry-metrics-3 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:09:11,416] INFO [MergedLog partition=_confluent-telemetry-metrics-3, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-telemetry-metrics-3 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:09:11,416] INFO [Broker id=1] Leader _confluent-telemetry-metrics-3 with topic id Some(rACWIAgeSLaNjkitMhNiUQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:11,417] INFO [MergedLog partition=_confluent-telemetry-metrics-7, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:09:11,417] INFO Created log for partition _confluent-telemetry-metrics-7 in /tmp/kraft-combined-logs/_confluent-telemetry-metrics-7 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:11,417] INFO [Partition _confluent-telemetry-metrics-7 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-7 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:11,417] INFO [Partition _confluent-telemetry-metrics-7 broker=1] Log loaded for partition _confluent-telemetry-metrics-7 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:11,417] INFO Setting topicIdPartition rACWIAgeSLaNjkitMhNiUQ:_confluent-telemetry-metrics-7 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:09:11,417] INFO [MergedLog partition=_confluent-telemetry-metrics-7, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-telemetry-metrics-7 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:09:11,417] INFO [Broker id=1] Leader _confluent-telemetry-metrics-7 with topic id Some(rACWIAgeSLaNjkitMhNiUQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:11,418] INFO [MergedLog partition=_confluent-telemetry-metrics-4, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:09:11,418] INFO Created log for partition _confluent-telemetry-metrics-4 in /tmp/kraft-combined-logs/_confluent-telemetry-metrics-4 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:11,418] INFO [Partition _confluent-telemetry-metrics-4 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-4 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:11,418] INFO [Partition _confluent-telemetry-metrics-4 broker=1] Log loaded for partition _confluent-telemetry-metrics-4 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:11,419] INFO Setting topicIdPartition rACWIAgeSLaNjkitMhNiUQ:_confluent-telemetry-metrics-4 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:09:11,419] INFO [MergedLog partition=_confluent-telemetry-metrics-4, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-telemetry-metrics-4 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:09:11,419] INFO [Broker id=1] Leader _confluent-telemetry-metrics-4 with topic id Some(rACWIAgeSLaNjkitMhNiUQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:11,419] INFO [MergedLog partition=_confluent-telemetry-metrics-1, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:09:11,420] INFO Created log for partition _confluent-telemetry-metrics-1 in /tmp/kraft-combined-logs/_confluent-telemetry-metrics-1 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:11,420] INFO [Partition _confluent-telemetry-metrics-1 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-1 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:11,420] INFO [Partition _confluent-telemetry-metrics-1 broker=1] Log loaded for partition _confluent-telemetry-metrics-1 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:11,420] INFO Setting topicIdPartition rACWIAgeSLaNjkitMhNiUQ:_confluent-telemetry-metrics-1 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:09:11,420] INFO [MergedLog partition=_confluent-telemetry-metrics-1, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-telemetry-metrics-1 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:09:11,420] INFO [Broker id=1] Leader _confluent-telemetry-metrics-1 with topic id Some(rACWIAgeSLaNjkitMhNiUQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:11,421] INFO Partitioner has null list of partitions to produce to. Calculating partitions to produce to (io.confluent.shaded.io.confluent.telemetry.events.exporter.kafka.RandomBrokerPartitionSubsetPartitioner)
benchi-kafka     | [2025-03-18 16:09:11,421] INFO Kafka Producer producing to the following subset partitions: {_confluent-telemetry-metrics=[8, 11]} (io.confluent.shaded.io.confluent.telemetry.events.exporter.kafka.RandomBrokerPartitionSubsetPartitioner)
benchi-kafka     | [2025-03-18 16:09:11,422] INFO [MergedLog partition=_confluent-telemetry-metrics-5, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:09:11,422] INFO Created log for partition _confluent-telemetry-metrics-5 in /tmp/kraft-combined-logs/_confluent-telemetry-metrics-5 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:11,422] INFO [Partition _confluent-telemetry-metrics-5 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-5 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:11,422] INFO [Partition _confluent-telemetry-metrics-5 broker=1] Log loaded for partition _confluent-telemetry-metrics-5 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:11,422] INFO Setting topicIdPartition rACWIAgeSLaNjkitMhNiUQ:_confluent-telemetry-metrics-5 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:09:11,423] INFO [MergedLog partition=_confluent-telemetry-metrics-5, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-telemetry-metrics-5 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:09:11,423] INFO [Broker id=1] Leader _confluent-telemetry-metrics-5 with topic id Some(rACWIAgeSLaNjkitMhNiUQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:11,423] INFO [MergedLog partition=_confluent-telemetry-metrics-8, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:09:11,423] INFO Created log for partition _confluent-telemetry-metrics-8 in /tmp/kraft-combined-logs/_confluent-telemetry-metrics-8 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:11,424] INFO [Partition _confluent-telemetry-metrics-8 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-8 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:11,424] INFO [Partition _confluent-telemetry-metrics-8 broker=1] Log loaded for partition _confluent-telemetry-metrics-8 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:11,424] INFO Setting topicIdPartition rACWIAgeSLaNjkitMhNiUQ:_confluent-telemetry-metrics-8 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:09:11,424] INFO [MergedLog partition=_confluent-telemetry-metrics-8, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-telemetry-metrics-8 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:09:11,424] INFO [Broker id=1] Leader _confluent-telemetry-metrics-8 with topic id Some(rACWIAgeSLaNjkitMhNiUQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:11,425] INFO [MergedLog partition=_confluent-telemetry-metrics-6, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:09:11,425] INFO Created log for partition _confluent-telemetry-metrics-6 in /tmp/kraft-combined-logs/_confluent-telemetry-metrics-6 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:11,425] INFO [Partition _confluent-telemetry-metrics-6 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-6 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:11,425] INFO [Partition _confluent-telemetry-metrics-6 broker=1] Log loaded for partition _confluent-telemetry-metrics-6 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:11,425] INFO Setting topicIdPartition rACWIAgeSLaNjkitMhNiUQ:_confluent-telemetry-metrics-6 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:09:11,425] INFO [MergedLog partition=_confluent-telemetry-metrics-6, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-telemetry-metrics-6 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:09:11,425] INFO [Broker id=1] Leader _confluent-telemetry-metrics-6 with topic id Some(rACWIAgeSLaNjkitMhNiUQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:11,426] INFO [MergedLog partition=_confluent-telemetry-metrics-2, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:09:11,426] INFO Created log for partition _confluent-telemetry-metrics-2 in /tmp/kraft-combined-logs/_confluent-telemetry-metrics-2 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:11,426] INFO [Partition _confluent-telemetry-metrics-2 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-2 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:11,426] INFO [Partition _confluent-telemetry-metrics-2 broker=1] Log loaded for partition _confluent-telemetry-metrics-2 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:11,426] INFO Setting topicIdPartition rACWIAgeSLaNjkitMhNiUQ:_confluent-telemetry-metrics-2 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:09:11,426] INFO [MergedLog partition=_confluent-telemetry-metrics-2, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-telemetry-metrics-2 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:09:11,427] INFO [Broker id=1] Leader _confluent-telemetry-metrics-2 with topic id Some(rACWIAgeSLaNjkitMhNiUQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:11,428] INFO [MergedLog partition=_confluent-telemetry-metrics-9, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:09:11,428] INFO Created log for partition _confluent-telemetry-metrics-9 in /tmp/kraft-combined-logs/_confluent-telemetry-metrics-9 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:11,428] INFO [Partition _confluent-telemetry-metrics-9 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-9 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:11,428] INFO [Partition _confluent-telemetry-metrics-9 broker=1] Log loaded for partition _confluent-telemetry-metrics-9 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:11,428] INFO Setting topicIdPartition rACWIAgeSLaNjkitMhNiUQ:_confluent-telemetry-metrics-9 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:09:11,428] INFO [MergedLog partition=_confluent-telemetry-metrics-9, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-telemetry-metrics-9 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:09:11,428] INFO [Broker id=1] Leader _confluent-telemetry-metrics-9 with topic id Some(rACWIAgeSLaNjkitMhNiUQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:11,429] INFO [MergedLog partition=_confluent-telemetry-metrics-10, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:09:11,429] INFO Created log for partition _confluent-telemetry-metrics-10 in /tmp/kraft-combined-logs/_confluent-telemetry-metrics-10 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:11,429] INFO [Partition _confluent-telemetry-metrics-10 broker=1] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-10 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:11,429] INFO [Partition _confluent-telemetry-metrics-10 broker=1] Log loaded for partition _confluent-telemetry-metrics-10 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:11,430] INFO Setting topicIdPartition rACWIAgeSLaNjkitMhNiUQ:_confluent-telemetry-metrics-10 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:09:11,430] INFO [MergedLog partition=_confluent-telemetry-metrics-10, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent-telemetry-metrics-10 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:09:11,430] INFO [Broker id=1] Leader _confluent-telemetry-metrics-10 with topic id Some(rACWIAgeSLaNjkitMhNiUQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:11,430] INFO [DynamicConfigPublisher broker id=1] Updating topic _confluent-telemetry-metrics with new configuration : max.message.bytes -> 10485760,message.timestamp.type -> CreateTime,min.insync.replicas -> 1,retention.ms -> 259200000,segment.ms -> 14400000,retention.bytes -> -1 (kafka.server.metadata.DynamicConfigPublisher)
benchi-kafka     | [2025-03-18 16:09:11,447] INFO Broker Addition context is yet to be initialized, hence BrokerAddCount metrics will be reported as 0. (io.confluent.databalancer.ConfluentDataBalanceEngine)
benchi-kafka     | [2025-03-18 16:09:11,476] INFO [Partition _confluent-telemetry-metrics-8 broker=1] roll: _confluent-telemetry-metrics-8: first produce received, lastOffset: 22, leaderEpoch: 0, numMessages:23, time diff: 59 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:11,477] INFO [Partition _confluent-telemetry-metrics-8 broker=1] roll: _confluent-telemetry-metrics-8: first HWM advanced, leaderEpoch: 0, old HW: 0, new HW: 23, become leader time: 1742314151417 ms, time diff: 60 ms (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:11,478] INFO [Partition _confluent-telemetry-metrics-11 broker=1] roll: _confluent-telemetry-metrics-11: first produce received, lastOffset: 22, leaderEpoch: 0, numMessages:23, time diff: 68 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:11,478] INFO [Partition _confluent-telemetry-metrics-11 broker=1] roll: _confluent-telemetry-metrics-11: first HWM advanced, leaderEpoch: 0, old HW: 0, new HW: 23, become leader time: 1742314151410 ms, time diff: 68 ms (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:14,655] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-6820492430475519429] Resetting generation and member id due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
benchi-kafka     | [2025-03-18 16:09:14,655] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler-6820492430475519429] Request joining group due to: consumer pro-actively leaving the group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
benchi-kafka     | [2025-03-18 16:09:14,658] INFO App info kafka.consumer for kafka-cruise-control unregistered (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:09:14,658] INFO Metric Reporter Sampler ready to start. (io.confluent.cruisecontrol.metricsreporter.ConfluentMetricsSamplerBase)
benchi-kafka     | [2025-03-18 16:09:14,658] INFO DataBalancer: Startup component StartupComponent ConfluentTelemetryReporterSampler ready to proceed (io.confluent.databalancer.startup.StartupComponents)
benchi-kafka     | [2025-03-18 16:09:14,658] INFO DataBalancer: Checking startup component StartupComponent SampleStoreTopicCleanUp (io.confluent.databalancer.startup.StartupComponents)
benchi-kafka     | [2025-03-18 16:09:14,659] INFO AdminClientConfig values: 
benchi-kafka     | 	auto.include.jmx.reporter = true
benchi-kafka     | 	bootstrap.controllers = []
benchi-kafka     | 	bootstrap.servers = [broker:29092]
benchi-kafka     | 	client.dns.lookup = use_all_dns_ips
benchi-kafka     | 	client.id = kafka-cruise-control
benchi-kafka     | 	confluent.lkc.id = null
benchi-kafka     | 	confluent.metrics.reporter.bootstrap.servers = localhost:9092
benchi-kafka     | 	confluent.proxy.protocol.client.address = null
benchi-kafka     | 	confluent.proxy.protocol.client.mode = PROXY
benchi-kafka     | 	confluent.proxy.protocol.client.port = null
benchi-kafka     | 	confluent.proxy.protocol.client.version = NONE
benchi-kafka     | 	connections.max.idle.ms = 540000
benchi-kafka     | 	default.api.timeout.ms = 60000
benchi-kafka     | 	enable.metrics.push = true
benchi-kafka     | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
benchi-kafka     | 	metadata.max.age.ms = 300000
benchi-kafka     | 	metadata.recovery.strategy = none
benchi-kafka     | 	metric.reporters = []
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.recording.level = INFO
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	receive.buffer.bytes = 32768
benchi-kafka     | 	reconnect.backoff.max.ms = 5000
benchi-kafka     | 	reconnect.backoff.ms = 500
benchi-kafka     | 	request.timeout.ms = 30000
benchi-kafka     | 	retries = 2147483647
benchi-kafka     | 	retry.backoff.max.ms = 1000
benchi-kafka     | 	retry.backoff.ms = 500
benchi-kafka     | 	sasl.client.callback.handler.class = null
benchi-kafka     | 	sasl.jaas.config = null
benchi-kafka     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
benchi-kafka     | 	sasl.kerberos.min.time.before.relogin = 60000
benchi-kafka     | 	sasl.kerberos.service.name = null
benchi-kafka     | 	sasl.kerberos.ticket.renew.jitter = 0.05
benchi-kafka     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
benchi-kafka     | 	sasl.login.callback.handler.class = null
benchi-kafka     | 	sasl.login.class = null
benchi-kafka     | 	sasl.login.connect.timeout.ms = null
benchi-kafka     | 	sasl.login.read.timeout.ms = null
benchi-kafka     | 	sasl.login.refresh.buffer.seconds = 300
benchi-kafka     | 	sasl.login.refresh.min.period.seconds = 60
benchi-kafka     | 	sasl.login.refresh.window.factor = 0.8
benchi-kafka     | 	sasl.login.refresh.window.jitter = 0.05
benchi-kafka     | 	sasl.login.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.login.retry.backoff.ms = 100
benchi-kafka     | 	sasl.mechanism = GSSAPI
benchi-kafka     | 	sasl.oauthbearer.clientassertion.audience = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.expiration = 5
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.jti.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.nbf.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.issuer = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.location = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key.passphrase = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.subject = null
benchi-kafka     | 	sasl.oauthbearer.clock.skew.seconds = 30
benchi-kafka     | 	sasl.oauthbearer.expected.audience = null
benchi-kafka     | 	sasl.oauthbearer.expected.issuer = null
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.url = null
benchi-kafka     | 	sasl.oauthbearer.scope.claim.name = scope
benchi-kafka     | 	sasl.oauthbearer.sub.claim.name = sub
benchi-kafka     | 	sasl.oauthbearer.token.endpoint.url = null
benchi-kafka     | 	security.protocol = PLAINTEXT
benchi-kafka     | 	security.providers = null
benchi-kafka     | 	send.buffer.bytes = 131072
benchi-kafka     | 	socket.connection.setup.timeout.max.ms = 30000
benchi-kafka     | 	socket.connection.setup.timeout.ms = 10000
benchi-kafka     | 	ssl.cipher.suites = null
benchi-kafka     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	ssl.engine.factory.class = null
benchi-kafka     | 	ssl.key.password = null
benchi-kafka     | 	ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	ssl.keystore.certificate.chain = null
benchi-kafka     | 	ssl.keystore.key = null
benchi-kafka     | 	ssl.keystore.location = null
benchi-kafka     | 	ssl.keystore.password = null
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.protocol = TLSv1.3
benchi-kafka     | 	ssl.provider = null
benchi-kafka     | 	ssl.secure.random.implementation = null
benchi-kafka     | 	ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	ssl.truststore.certificates = null
benchi-kafka     | 	ssl.truststore.location = null
benchi-kafka     | 	ssl.truststore.password = null
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     |  (org.apache.kafka.clients.admin.AdminClientConfig)
benchi-kafka     | [2025-03-18 16:09:14,660] INFO These configurations '[sasl.oauthbearer.jwks.endpoint.refresh.ms, sasl.oauthbearer.clientassertion.include.jti.claim, sasl.login.refresh.min.period.seconds, sasl.oauthbearer.scope.claim.name, sasl.login.refresh.window.factor, sasl.kerberos.ticket.renew.window.factor, sasl.login.retry.backoff.ms, sasl.oauthbearer.clientassertion.expiration, sasl.kerberos.kinit.cmd, sasl.kerberos.ticket.renew.jitter, ssl.keystore.type, ssl.trustmanager.algorithm, sasl.kerberos.min.time.before.relogin, ssl.endpoint.identification.algorithm, ssl.protocol, confluent.metrics.reporter.bootstrap.servers, sasl.login.refresh.buffer.seconds, sasl.login.retry.backoff.max.ms, ssl.enabled.protocols, sasl.oauthbearer.sub.claim.name, ssl.truststore.type, sasl.oauthbearer.jwks.endpoint.retry.backoff.ms, sasl.oauthbearer.clock.skew.seconds, ssl.keymanager.algorithm, sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms, sasl.oauthbearer.clientassertion.include.nbf.claim, sasl.login.refresh.window.jitter]' were supplied but are not used yet. (org.apache.kafka.clients.admin.AdminClientConfig)
benchi-kafka     | [2025-03-18 16:09:14,660] INFO Kafka version: 7.8.1-ce (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:09:14,660] INFO Kafka commitId: eefb51c6f2f1b888b8b03296353338afbac8e213 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:09:14,660] INFO Kafka startTimeMs: 1742314154660 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:09:14,669] INFO DataBalancer: No topics to be deleted. (com.linkedin.kafka.cruisecontrol.SbkTopicUtils)
benchi-kafka     | [2025-03-18 16:09:14,669] INFO App info kafka.admin.client for kafka-cruise-control unregistered (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:09:14,670] INFO DataBalancer: Startup component StartupComponent SampleStoreTopicCleanUp ready to proceed (io.confluent.databalancer.startup.StartupComponents)
benchi-kafka     | [2025-03-18 16:09:14,670] INFO DataBalancer: Checking startup component StartupComponent ApiStatePersistenceStore (io.confluent.databalancer.startup.StartupComponents)
benchi-kafka     | [2025-03-18 16:09:14,671] INFO AdminClientConfig values: 
benchi-kafka     | 	auto.include.jmx.reporter = true
benchi-kafka     | 	bootstrap.controllers = []
benchi-kafka     | 	bootstrap.servers = [broker:29092]
benchi-kafka     | 	client.dns.lookup = use_all_dns_ips
benchi-kafka     | 	client.id = kafka-cruise-control
benchi-kafka     | 	confluent.lkc.id = null
benchi-kafka     | 	confluent.metrics.reporter.bootstrap.servers = localhost:9092
benchi-kafka     | 	confluent.proxy.protocol.client.address = null
benchi-kafka     | 	confluent.proxy.protocol.client.mode = PROXY
benchi-kafka     | 	confluent.proxy.protocol.client.port = null
benchi-kafka     | 	confluent.proxy.protocol.client.version = NONE
benchi-kafka     | 	connections.max.idle.ms = 540000
benchi-kafka     | 	default.api.timeout.ms = 60000
benchi-kafka     | 	enable.metrics.push = true
benchi-kafka     | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
benchi-kafka     | 	metadata.max.age.ms = 300000
benchi-kafka     | 	metadata.recovery.strategy = none
benchi-kafka     | 	metric.reporters = []
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.recording.level = INFO
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	receive.buffer.bytes = 32768
benchi-kafka     | 	reconnect.backoff.max.ms = 5000
benchi-kafka     | 	reconnect.backoff.ms = 500
benchi-kafka     | 	request.timeout.ms = 30000
benchi-kafka     | 	retries = 2147483647
benchi-kafka     | 	retry.backoff.max.ms = 1000
benchi-kafka     | 	retry.backoff.ms = 500
benchi-kafka     | 	sasl.client.callback.handler.class = null
benchi-kafka     | 	sasl.jaas.config = null
benchi-kafka     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
benchi-kafka     | 	sasl.kerberos.min.time.before.relogin = 60000
benchi-kafka     | 	sasl.kerberos.service.name = null
benchi-kafka     | 	sasl.kerberos.ticket.renew.jitter = 0.05
benchi-kafka     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
benchi-kafka     | 	sasl.login.callback.handler.class = null
benchi-kafka     | 	sasl.login.class = null
benchi-kafka     | 	sasl.login.connect.timeout.ms = null
benchi-kafka     | 	sasl.login.read.timeout.ms = null
benchi-kafka     | 	sasl.login.refresh.buffer.seconds = 300
benchi-kafka     | 	sasl.login.refresh.min.period.seconds = 60
benchi-kafka     | 	sasl.login.refresh.window.factor = 0.8
benchi-kafka     | 	sasl.login.refresh.window.jitter = 0.05
benchi-kafka     | 	sasl.login.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.login.retry.backoff.ms = 100
benchi-kafka     | 	sasl.mechanism = GSSAPI
benchi-kafka     | 	sasl.oauthbearer.clientassertion.audience = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.expiration = 5
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.jti.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.nbf.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.issuer = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.location = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key.passphrase = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.subject = null
benchi-kafka     | 	sasl.oauthbearer.clock.skew.seconds = 30
benchi-kafka     | 	sasl.oauthbearer.expected.audience = null
benchi-kafka     | 	sasl.oauthbearer.expected.issuer = null
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.url = null
benchi-kafka     | 	sasl.oauthbearer.scope.claim.name = scope
benchi-kafka     | 	sasl.oauthbearer.sub.claim.name = sub
benchi-kafka     | 	sasl.oauthbearer.token.endpoint.url = null
benchi-kafka     | 	security.protocol = PLAINTEXT
benchi-kafka     | 	security.providers = null
benchi-kafka     | 	send.buffer.bytes = 131072
benchi-kafka     | 	socket.connection.setup.timeout.max.ms = 30000
benchi-kafka     | 	socket.connection.setup.timeout.ms = 10000
benchi-kafka     | 	ssl.cipher.suites = null
benchi-kafka     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	ssl.engine.factory.class = null
benchi-kafka     | 	ssl.key.password = null
benchi-kafka     | 	ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	ssl.keystore.certificate.chain = null
benchi-kafka     | 	ssl.keystore.key = null
benchi-kafka     | 	ssl.keystore.location = null
benchi-kafka     | 	ssl.keystore.password = null
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.protocol = TLSv1.3
benchi-kafka     | 	ssl.provider = null
benchi-kafka     | 	ssl.secure.random.implementation = null
benchi-kafka     | 	ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	ssl.truststore.certificates = null
benchi-kafka     | 	ssl.truststore.location = null
benchi-kafka     | 	ssl.truststore.password = null
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     |  (org.apache.kafka.clients.admin.AdminClientConfig)
benchi-kafka     | [2025-03-18 16:09:14,671] INFO These configurations '[sasl.oauthbearer.jwks.endpoint.refresh.ms, sasl.oauthbearer.clientassertion.include.jti.claim, sasl.login.refresh.min.period.seconds, sasl.oauthbearer.scope.claim.name, sasl.login.refresh.window.factor, sasl.kerberos.ticket.renew.window.factor, sasl.login.retry.backoff.ms, sasl.oauthbearer.clientassertion.expiration, sasl.kerberos.kinit.cmd, sasl.kerberos.ticket.renew.jitter, ssl.keystore.type, ssl.trustmanager.algorithm, sasl.kerberos.min.time.before.relogin, ssl.endpoint.identification.algorithm, ssl.protocol, confluent.metrics.reporter.bootstrap.servers, sasl.login.refresh.buffer.seconds, sasl.login.retry.backoff.max.ms, ssl.enabled.protocols, sasl.oauthbearer.sub.claim.name, ssl.truststore.type, sasl.oauthbearer.jwks.endpoint.retry.backoff.ms, sasl.oauthbearer.clock.skew.seconds, ssl.keymanager.algorithm, sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms, sasl.oauthbearer.clientassertion.include.nbf.claim, sasl.login.refresh.window.jitter]' were supplied but are not used yet. (org.apache.kafka.clients.admin.AdminClientConfig)
benchi-kafka     | [2025-03-18 16:09:14,671] INFO Kafka version: 7.8.1-ce (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:09:14,671] INFO Kafka commitId: eefb51c6f2f1b888b8b03296353338afbac8e213 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:09:14,671] INFO Kafka startTimeMs: 1742314154671 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:09:14,677] INFO DataBalancer: Creating topic _confluent_balancer_api_state  (com.linkedin.kafka.cruisecontrol.SbkTopicUtils)
benchi-kafka     | [2025-03-18 16:09:14,679] INFO [ControllerServer id=1] CreateTopics result(s): CreatableTopic(name='_confluent_balancer_api_state', numPartitions=1, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='cleanup.policy', value='delete,compact'), CreateableTopicConfig(name='retention.ms', value='2592000000')], linkName=null, mirrorTopic=null, sourceTopicId=AAAAAAAAAAAAAAAAAAAAAA, mirrorStartOffsetSpec=-9223372036854775808, mirrorStartOffsets=[], stoppedSequenceNumber=0): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:09:14,679] INFO [ControllerServer id=1] Replayed TopicRecord for topic _confluent_balancer_api_state with topic ID pCMvnNWjTh6QVp7erZq34g. (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:09:14,679] INFO [ControllerServer id=1] Replayed ConfigRecord for ConfigResource(type=TOPIC, name='_confluent_balancer_api_state') which set configuration cleanup.policy to delete,compact (org.apache.kafka.controller.ConfigurationControlManager)
benchi-kafka     | [2025-03-18 16:09:14,679] INFO [ControllerServer id=1] Replayed ConfigRecord for ConfigResource(type=TOPIC, name='_confluent_balancer_api_state') which set configuration retention.ms to 2592000000 (org.apache.kafka.controller.ConfigurationControlManager)
benchi-kafka     | [2025-03-18 16:09:14,679] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition _confluent_balancer_api_state-0 with topic ID pCMvnNWjTh6QVp7erZq34g and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:09:14,707] INFO SBC Event SbcMetadataUpdateEvent-143 generated 1 more events to enqueue in the following order - [SbcConfigUpdateEvent-144]. Enqueuing... (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:09:14,707] INFO Handling event SbcConfigUpdateEvent-144 (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:09:14,707] INFO [Broker id=1] Transitioning 1 partition(s) to local leaders. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:14,707] INFO Balancer notified of a config change: ConfigurationsDelta(changes={ConfigResource(type=TOPIC, name='_confluent_balancer_api_state')=ConfigurationDelta(changedKeys=[cleanup.policy, retention.ms])}) (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:09:14,707] INFO There were 0 change(s) and 0 deletion(s) to balancer configs. Changed Configs: {}, Deleted Configs: [] (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:09:14,707] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(_confluent_balancer_api_state-0) (kafka.server.ReplicaFetcherManager)
benchi-kafka     | [2025-03-18 16:09:14,707] INFO [Broker id=1] Creating new partition _confluent_balancer_api_state-0 with topic id pCMvnNWjTh6QVp7erZq34g. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:14,708] INFO [Broker id=1] Stopped fetchers as part of become-leader transition for 1 partitions (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:14,708] INFO App info kafka.admin.client for kafka-cruise-control unregistered (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:09:14,709] INFO Waiting for 1 seconds to ensure that api persistent store topic is created/exists. (io.confluent.databalancer.persistence.ApiStatePersistenceStore)
benchi-kafka     | [2025-03-18 16:09:14,709] INFO [MergedLog partition=_confluent_balancer_api_state-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:09:14,710] INFO Created log for partition _confluent_balancer_api_state-0 in /tmp/kraft-combined-logs/_confluent_balancer_api_state-0 with properties {cleanup.policy=delete,compact, retention.ms=2592000000} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:14,710] INFO [Partition _confluent_balancer_api_state-0 broker=1] No checkpointed highwatermark is found for partition _confluent_balancer_api_state-0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:14,710] INFO [Partition _confluent_balancer_api_state-0 broker=1] Log loaded for partition _confluent_balancer_api_state-0 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:14,710] INFO Setting topicIdPartition pCMvnNWjTh6QVp7erZq34g:_confluent_balancer_api_state-0 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:09:14,710] INFO [MergedLog partition=_confluent_balancer_api_state-0, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for _confluent_balancer_api_state-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:09:14,710] INFO [Broker id=1] Leader _confluent_balancer_api_state-0 with topic id Some(pCMvnNWjTh6QVp7erZq34g) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:14,711] INFO [DynamicConfigPublisher broker id=1] Updating topic _confluent_balancer_api_state with new configuration : cleanup.policy -> delete,compact,retention.ms -> 2592000000 (kafka.server.metadata.DynamicConfigPublisher)
benchi-kafka     | [2025-03-18 16:09:15,712] INFO AdminClientConfig values: 
benchi-kafka     | 	auto.include.jmx.reporter = true
benchi-kafka     | 	bootstrap.controllers = []
benchi-kafka     | 	bootstrap.servers = [broker:29092]
benchi-kafka     | 	client.dns.lookup = use_all_dns_ips
benchi-kafka     | 	client.id = kafka-cruise-control
benchi-kafka     | 	confluent.lkc.id = null
benchi-kafka     | 	confluent.metrics.reporter.bootstrap.servers = localhost:9092
benchi-kafka     | 	confluent.proxy.protocol.client.address = null
benchi-kafka     | 	confluent.proxy.protocol.client.mode = PROXY
benchi-kafka     | 	confluent.proxy.protocol.client.port = null
benchi-kafka     | 	confluent.proxy.protocol.client.version = NONE
benchi-kafka     | 	connections.max.idle.ms = 540000
benchi-kafka     | 	default.api.timeout.ms = 60000
benchi-kafka     | 	enable.metrics.push = true
benchi-kafka     | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
benchi-kafka     | 	metadata.max.age.ms = 300000
benchi-kafka     | 	metadata.recovery.strategy = none
benchi-kafka     | 	metric.reporters = []
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.recording.level = INFO
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	receive.buffer.bytes = 32768
benchi-kafka     | 	reconnect.backoff.max.ms = 5000
benchi-kafka     | 	reconnect.backoff.ms = 500
benchi-kafka     | 	request.timeout.ms = 30000
benchi-kafka     | 	retries = 2147483647
benchi-kafka     | 	retry.backoff.max.ms = 1000
benchi-kafka     | 	retry.backoff.ms = 500
benchi-kafka     | 	sasl.client.callback.handler.class = null
benchi-kafka     | 	sasl.jaas.config = null
benchi-kafka     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
benchi-kafka     | 	sasl.kerberos.min.time.before.relogin = 60000
benchi-kafka     | 	sasl.kerberos.service.name = null
benchi-kafka     | 	sasl.kerberos.ticket.renew.jitter = 0.05
benchi-kafka     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
benchi-kafka     | 	sasl.login.callback.handler.class = null
benchi-kafka     | 	sasl.login.class = null
benchi-kafka     | 	sasl.login.connect.timeout.ms = null
benchi-kafka     | 	sasl.login.read.timeout.ms = null
benchi-kafka     | 	sasl.login.refresh.buffer.seconds = 300
benchi-kafka     | 	sasl.login.refresh.min.period.seconds = 60
benchi-kafka     | 	sasl.login.refresh.window.factor = 0.8
benchi-kafka     | 	sasl.login.refresh.window.jitter = 0.05
benchi-kafka     | 	sasl.login.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.login.retry.backoff.ms = 100
benchi-kafka     | 	sasl.mechanism = GSSAPI
benchi-kafka     | 	sasl.oauthbearer.clientassertion.audience = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.expiration = 5
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.jti.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.nbf.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.issuer = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.location = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key.passphrase = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.subject = null
benchi-kafka     | 	sasl.oauthbearer.clock.skew.seconds = 30
benchi-kafka     | 	sasl.oauthbearer.expected.audience = null
benchi-kafka     | 	sasl.oauthbearer.expected.issuer = null
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.url = null
benchi-kafka     | 	sasl.oauthbearer.scope.claim.name = scope
benchi-kafka     | 	sasl.oauthbearer.sub.claim.name = sub
benchi-kafka     | 	sasl.oauthbearer.token.endpoint.url = null
benchi-kafka     | 	security.protocol = PLAINTEXT
benchi-kafka     | 	security.providers = null
benchi-kafka     | 	send.buffer.bytes = 131072
benchi-kafka     | 	socket.connection.setup.timeout.max.ms = 30000
benchi-kafka     | 	socket.connection.setup.timeout.ms = 10000
benchi-kafka     | 	ssl.cipher.suites = null
benchi-kafka     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	ssl.engine.factory.class = null
benchi-kafka     | 	ssl.key.password = null
benchi-kafka     | 	ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	ssl.keystore.certificate.chain = null
benchi-kafka     | 	ssl.keystore.key = null
benchi-kafka     | 	ssl.keystore.location = null
benchi-kafka     | 	ssl.keystore.password = null
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.protocol = TLSv1.3
benchi-kafka     | 	ssl.provider = null
benchi-kafka     | 	ssl.secure.random.implementation = null
benchi-kafka     | 	ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	ssl.truststore.certificates = null
benchi-kafka     | 	ssl.truststore.location = null
benchi-kafka     | 	ssl.truststore.password = null
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     |  (org.apache.kafka.clients.admin.AdminClientConfig)
benchi-kafka     | [2025-03-18 16:09:15,715] INFO These configurations '[sasl.oauthbearer.jwks.endpoint.refresh.ms, sasl.oauthbearer.clientassertion.include.jti.claim, sasl.login.refresh.min.period.seconds, sasl.oauthbearer.scope.claim.name, sasl.login.refresh.window.factor, sasl.kerberos.ticket.renew.window.factor, sasl.login.retry.backoff.ms, sasl.oauthbearer.clientassertion.expiration, sasl.kerberos.kinit.cmd, sasl.kerberos.ticket.renew.jitter, ssl.keystore.type, ssl.trustmanager.algorithm, sasl.kerberos.min.time.before.relogin, ssl.endpoint.identification.algorithm, ssl.protocol, confluent.metrics.reporter.bootstrap.servers, sasl.login.refresh.buffer.seconds, sasl.login.retry.backoff.max.ms, ssl.enabled.protocols, sasl.oauthbearer.sub.claim.name, ssl.truststore.type, sasl.oauthbearer.jwks.endpoint.retry.backoff.ms, sasl.oauthbearer.clock.skew.seconds, ssl.keymanager.algorithm, sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms, sasl.oauthbearer.clientassertion.include.nbf.claim, sasl.login.refresh.window.jitter]' were supplied but are not used yet. (org.apache.kafka.clients.admin.AdminClientConfig)
benchi-kafka     | [2025-03-18 16:09:15,716] INFO Kafka version: 7.8.1-ce (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:09:15,716] INFO Kafka commitId: eefb51c6f2f1b888b8b03296353338afbac8e213 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:09:15,716] INFO Kafka startTimeMs: 1742314155716 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:09:15,725] INFO DataBalancer: Adjusting topic _confluent_balancer_api_state configuration (com.linkedin.kafka.cruisecontrol.SbkTopicUtils)
benchi-kafka     | [2025-03-18 16:09:15,746] INFO [AdminClient clientId=kafka-cruise-control] The DescribeTopicPartitions API is not supported, using Metadata API to describe topics. (org.apache.kafka.clients.admin.KafkaAdminClient)
benchi-kafka     | [2025-03-18 16:09:15,748] INFO App info kafka.admin.client for kafka-cruise-control unregistered (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:09:15,749] INFO Confirmed that topic _confluent_balancer_api_state exists. (io.confluent.databalancer.persistence.ApiStatePersistenceStore)
benchi-kafka     | [2025-03-18 16:09:15,749] INFO DataBalancer: Startup component StartupComponent ApiStatePersistenceStore ready to proceed (io.confluent.databalancer.startup.StartupComponents)
benchi-kafka     | [2025-03-18 16:09:15,749] INFO DataBalancer: Startup checking succeeded, proceeding to full validation. (io.confluent.databalancer.startup.StartupComponents)
benchi-kafka     | [2025-03-18 16:09:15,749] INFO DataBalancer: Creating CruiseControl (io.confluent.databalancer.startup.CruiseControlStartable)
benchi-kafka     | [2025-03-18 16:09:15,758] INFO DataBalancer: Bootstrap server endpoint is Endpoint(listenerName='PLAINTEXT', securityProtocol=PLAINTEXT, host='broker', port=29092) (io.confluent.databalancer.startup.CruiseControlStartable)
benchi-kafka     | [2025-03-18 16:09:15,759] INFO AdminClientConfig values: 
benchi-kafka     | 	auto.include.jmx.reporter = true
benchi-kafka     | 	bootstrap.controllers = []
benchi-kafka     | 	bootstrap.servers = [broker:29092]
benchi-kafka     | 	client.dns.lookup = use_all_dns_ips
benchi-kafka     | 	client.id = null-admin-1
benchi-kafka     | 	confluent.lkc.id = null
benchi-kafka     | 	confluent.metrics.reporter.bootstrap.servers = localhost:9092
benchi-kafka     | 	confluent.proxy.protocol.client.address = null
benchi-kafka     | 	confluent.proxy.protocol.client.mode = PROXY
benchi-kafka     | 	confluent.proxy.protocol.client.port = null
benchi-kafka     | 	confluent.proxy.protocol.client.version = NONE
benchi-kafka     | 	connections.max.idle.ms = 300000
benchi-kafka     | 	default.api.timeout.ms = 60000
benchi-kafka     | 	enable.metrics.push = true
benchi-kafka     | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
benchi-kafka     | 	metadata.max.age.ms = 300000
benchi-kafka     | 	metadata.recovery.strategy = none
benchi-kafka     | 	metric.reporters = []
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.recording.level = INFO
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	receive.buffer.bytes = 65536
benchi-kafka     | 	reconnect.backoff.max.ms = 5000
benchi-kafka     | 	reconnect.backoff.ms = 500
benchi-kafka     | 	request.timeout.ms = 30000
benchi-kafka     | 	retries = 2147483647
benchi-kafka     | 	retry.backoff.max.ms = 1000
benchi-kafka     | 	retry.backoff.ms = 500
benchi-kafka     | 	sasl.client.callback.handler.class = null
benchi-kafka     | 	sasl.jaas.config = null
benchi-kafka     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
benchi-kafka     | 	sasl.kerberos.min.time.before.relogin = 60000
benchi-kafka     | 	sasl.kerberos.service.name = null
benchi-kafka     | 	sasl.kerberos.ticket.renew.jitter = 0.05
benchi-kafka     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
benchi-kafka     | 	sasl.login.callback.handler.class = null
benchi-kafka     | 	sasl.login.class = null
benchi-kafka     | 	sasl.login.connect.timeout.ms = null
benchi-kafka     | 	sasl.login.read.timeout.ms = null
benchi-kafka     | 	sasl.login.refresh.buffer.seconds = 300
benchi-kafka     | 	sasl.login.refresh.min.period.seconds = 60
benchi-kafka     | 	sasl.login.refresh.window.factor = 0.8
benchi-kafka     | 	sasl.login.refresh.window.jitter = 0.05
benchi-kafka     | 	sasl.login.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.login.retry.backoff.ms = 100
benchi-kafka     | 	sasl.mechanism = GSSAPI
benchi-kafka     | 	sasl.oauthbearer.clientassertion.audience = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.expiration = 5
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.jti.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.nbf.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.issuer = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.location = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key.passphrase = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.subject = null
benchi-kafka     | 	sasl.oauthbearer.clock.skew.seconds = 30
benchi-kafka     | 	sasl.oauthbearer.expected.audience = null
benchi-kafka     | 	sasl.oauthbearer.expected.issuer = null
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.url = null
benchi-kafka     | 	sasl.oauthbearer.scope.claim.name = scope
benchi-kafka     | 	sasl.oauthbearer.sub.claim.name = sub
benchi-kafka     | 	sasl.oauthbearer.token.endpoint.url = null
benchi-kafka     | 	security.protocol = PLAINTEXT
benchi-kafka     | 	security.providers = null
benchi-kafka     | 	send.buffer.bytes = 131072
benchi-kafka     | 	socket.connection.setup.timeout.max.ms = 30000
benchi-kafka     | 	socket.connection.setup.timeout.ms = 10000
benchi-kafka     | 	ssl.cipher.suites = null
benchi-kafka     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	ssl.engine.factory.class = null
benchi-kafka     | 	ssl.key.password = null
benchi-kafka     | 	ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	ssl.keystore.certificate.chain = null
benchi-kafka     | 	ssl.keystore.key = null
benchi-kafka     | 	ssl.keystore.location = null
benchi-kafka     | 	ssl.keystore.password = null
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.protocol = TLSv1.3
benchi-kafka     | 	ssl.provider = null
benchi-kafka     | 	ssl.secure.random.implementation = null
benchi-kafka     | 	ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	ssl.truststore.certificates = null
benchi-kafka     | 	ssl.truststore.location = null
benchi-kafka     | 	ssl.truststore.password = null
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     |  (org.apache.kafka.clients.admin.AdminClientConfig)
benchi-kafka     | [2025-03-18 16:09:15,760] INFO These configurations '[confluent.metrics.reporter.bootstrap.servers]' were supplied but are not used yet. (org.apache.kafka.clients.admin.AdminClientConfig)
benchi-kafka     | [2025-03-18 16:09:15,760] INFO Kafka version: 7.8.1-ce (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:09:15,760] INFO Kafka commitId: eefb51c6f2f1b888b8b03296353338afbac8e213 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:09:15,760] INFO Kafka startTimeMs: 1742314155760 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:09:15,762] INFO Starting KafkaBasedLog with topic _confluent_balancer_api_state reportErrorsToCallback=false (org.apache.kafka.connect.util.KafkaBasedLog)
benchi-kafka     | [2025-03-18 16:09:15,762] INFO ProducerConfig values: 
benchi-kafka     | 	acks = -1
benchi-kafka     | 	auto.include.jmx.reporter = true
benchi-kafka     | 	batch.size = 16384
benchi-kafka     | 	bootstrap.servers = [broker:29092]
benchi-kafka     | 	buffer.memory = 33554432
benchi-kafka     | 	client.dns.lookup = use_all_dns_ips
benchi-kafka     | 	client.id = _confluent_balancer_api_state-producer-1
benchi-kafka     | 	compression.gzip.level = -1
benchi-kafka     | 	compression.lz4.level = 9
benchi-kafka     | 	compression.type = none
benchi-kafka     | 	compression.zstd.level = 3
benchi-kafka     | 	confluent.lkc.id = null
benchi-kafka     | 	confluent.proxy.protocol.client.address = null
benchi-kafka     | 	confluent.proxy.protocol.client.mode = PROXY
benchi-kafka     | 	confluent.proxy.protocol.client.port = null
benchi-kafka     | 	confluent.proxy.protocol.client.version = NONE
benchi-kafka     | 	connections.max.idle.ms = 540000
benchi-kafka     | 	delivery.timeout.ms = 120000
benchi-kafka     | 	enable.idempotence = false
benchi-kafka     | 	enable.metrics.push = true
benchi-kafka     | 	interceptor.classes = []
benchi-kafka     | 	key.serializer = class io.confluent.databalancer.persistence.ApiStatePersistenceStore$SbkApiStatusKeySerde
benchi-kafka     | 	linger.ms = 0
benchi-kafka     | 	max.block.ms = 60000
benchi-kafka     | 	max.in.flight.requests.per.connection = 1
benchi-kafka     | 	max.request.size = 1048576
benchi-kafka     | 	metadata.max.age.ms = 300000
benchi-kafka     | 	metadata.max.idle.ms = 300000
benchi-kafka     | 	metadata.recovery.strategy = none
benchi-kafka     | 	metric.reporters = []
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.recording.level = INFO
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	partitioner.adaptive.partitioning.enable = true
benchi-kafka     | 	partitioner.availability.timeout.ms = 0
benchi-kafka     | 	partitioner.class = null
benchi-kafka     | 	partitioner.ignore.keys = false
benchi-kafka     | 	receive.buffer.bytes = 32768
benchi-kafka     | 	reconnect.backoff.max.ms = 1000
benchi-kafka     | 	reconnect.backoff.ms = 50
benchi-kafka     | 	request.timeout.ms = 30000
benchi-kafka     | 	retries = 2147483647
benchi-kafka     | 	retry.backoff.max.ms = 1000
benchi-kafka     | 	retry.backoff.ms = 100
benchi-kafka     | 	sasl.client.callback.handler.class = null
benchi-kafka     | 	sasl.jaas.config = null
benchi-kafka     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
benchi-kafka     | 	sasl.kerberos.min.time.before.relogin = 60000
benchi-kafka     | 	sasl.kerberos.service.name = null
benchi-kafka     | 	sasl.kerberos.ticket.renew.jitter = 0.05
benchi-kafka     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
benchi-kafka     | 	sasl.login.callback.handler.class = null
benchi-kafka     | 	sasl.login.class = null
benchi-kafka     | 	sasl.login.connect.timeout.ms = null
benchi-kafka     | 	sasl.login.read.timeout.ms = null
benchi-kafka     | 	sasl.login.refresh.buffer.seconds = 300
benchi-kafka     | 	sasl.login.refresh.min.period.seconds = 60
benchi-kafka     | 	sasl.login.refresh.window.factor = 0.8
benchi-kafka     | 	sasl.login.refresh.window.jitter = 0.05
benchi-kafka     | 	sasl.login.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.login.retry.backoff.ms = 100
benchi-kafka     | 	sasl.mechanism = GSSAPI
benchi-kafka     | 	sasl.oauthbearer.clientassertion.audience = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.expiration = 5
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.jti.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.nbf.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.issuer = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.location = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key.passphrase = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.subject = null
benchi-kafka     | 	sasl.oauthbearer.clock.skew.seconds = 30
benchi-kafka     | 	sasl.oauthbearer.expected.audience = null
benchi-kafka     | 	sasl.oauthbearer.expected.issuer = null
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.url = null
benchi-kafka     | 	sasl.oauthbearer.scope.claim.name = scope
benchi-kafka     | 	sasl.oauthbearer.sub.claim.name = sub
benchi-kafka     | 	sasl.oauthbearer.token.endpoint.url = null
benchi-kafka     | 	security.protocol = PLAINTEXT
benchi-kafka     | 	security.providers = null
benchi-kafka     | 	send.buffer.bytes = 131072
benchi-kafka     | 	socket.connection.setup.timeout.max.ms = 30000
benchi-kafka     | 	socket.connection.setup.timeout.ms = 10000
benchi-kafka     | 	ssl.cipher.suites = null
benchi-kafka     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	ssl.engine.factory.class = null
benchi-kafka     | 	ssl.key.password = null
benchi-kafka     | 	ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	ssl.keystore.certificate.chain = null
benchi-kafka     | 	ssl.keystore.key = null
benchi-kafka     | 	ssl.keystore.location = null
benchi-kafka     | 	ssl.keystore.password = null
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.protocol = TLSv1.3
benchi-kafka     | 	ssl.provider = null
benchi-kafka     | 	ssl.secure.random.implementation = null
benchi-kafka     | 	ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	ssl.truststore.certificates = null
benchi-kafka     | 	ssl.truststore.location = null
benchi-kafka     | 	ssl.truststore.password = null
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     | 	transaction.timeout.ms = 60000
benchi-kafka     | 	transactional.id = null
benchi-kafka     | 	value.serializer = class io.confluent.databalancer.persistence.ApiStatePersistenceStore$SbkApiStatusMessageSerde
benchi-kafka     |  (org.apache.kafka.clients.producer.ProducerConfig)
benchi-kafka     | [2025-03-18 16:09:15,762] INFO initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector)
benchi-kafka     | [2025-03-18 16:09:15,766] INFO Kafka version: 7.8.1-ce (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:09:15,766] INFO Kafka commitId: eefb51c6f2f1b888b8b03296353338afbac8e213 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:09:15,766] INFO Kafka startTimeMs: 1742314155766 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:09:15,766] INFO ConsumerConfig values: 
benchi-kafka     | 	allow.auto.create.topics = true
benchi-kafka     | 	auto.commit.interval.ms = 5000
benchi-kafka     | 	auto.include.jmx.reporter = true
benchi-kafka     | 	auto.offset.reset = earliest
benchi-kafka     | 	bootstrap.servers = [broker:29092]
benchi-kafka     | 	check.crcs = true
benchi-kafka     | 	client.dns.lookup = use_all_dns_ips
benchi-kafka     | 	client.id = _confluent_balancer_api_state-consumer-1
benchi-kafka     | 	client.rack = 
benchi-kafka     | 	confluent.lkc.id = null
benchi-kafka     | 	confluent.proxy.protocol.client.address = null
benchi-kafka     | 	confluent.proxy.protocol.client.mode = PROXY
benchi-kafka     | 	confluent.proxy.protocol.client.port = null
benchi-kafka     | 	confluent.proxy.protocol.client.version = NONE
benchi-kafka     | 	connections.max.idle.ms = 540000
benchi-kafka     | 	default.api.timeout.ms = 60000
benchi-kafka     | 	enable.auto.commit = false
benchi-kafka     | 	enable.metrics.push = true
benchi-kafka     | 	exclude.internal.topics = true
benchi-kafka     | 	fetch.max.bytes = 52428800
benchi-kafka     | 	fetch.max.wait.ms = 500
benchi-kafka     | 	fetch.min.bytes = 1
benchi-kafka     | 	group.id = null
benchi-kafka     | 	group.instance.id = null
benchi-kafka     | 	group.protocol = classic
benchi-kafka     | 	group.remote.assignor = null
benchi-kafka     | 	heartbeat.interval.ms = 3000
benchi-kafka     | 	interceptor.classes = []
benchi-kafka     | 	internal.leave.group.on.close = true
benchi-kafka     | 	internal.throw.on.fetch.stable.offset.unsupported = false
benchi-kafka     | 	isolation.level = read_uncommitted
benchi-kafka     | 	key.deserializer = class io.confluent.databalancer.persistence.ApiStatePersistenceStore$SbkApiStatusKeySerde
benchi-kafka     | 	max.partition.fetch.bytes = 1048576
benchi-kafka     | 	max.poll.interval.ms = 300000
benchi-kafka     | 	max.poll.records = 500
benchi-kafka     | 	metadata.max.age.ms = 300000
benchi-kafka     | 	metadata.recovery.strategy = none
benchi-kafka     | 	metric.reporters = []
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.recording.level = INFO
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
benchi-kafka     | 	receive.buffer.bytes = 65536
benchi-kafka     | 	reconnect.backoff.max.ms = 1000
benchi-kafka     | 	reconnect.backoff.ms = 50
benchi-kafka     | 	request.timeout.ms = 30000
benchi-kafka     | 	retry.backoff.max.ms = 1000
benchi-kafka     | 	retry.backoff.ms = 100
benchi-kafka     | 	sasl.client.callback.handler.class = null
benchi-kafka     | 	sasl.jaas.config = null
benchi-kafka     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
benchi-kafka     | 	sasl.kerberos.min.time.before.relogin = 60000
benchi-kafka     | 	sasl.kerberos.service.name = null
benchi-kafka     | 	sasl.kerberos.ticket.renew.jitter = 0.05
benchi-kafka     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
benchi-kafka     | 	sasl.login.callback.handler.class = null
benchi-kafka     | 	sasl.login.class = null
benchi-kafka     | 	sasl.login.connect.timeout.ms = null
benchi-kafka     | 	sasl.login.read.timeout.ms = null
benchi-kafka     | 	sasl.login.refresh.buffer.seconds = 300
benchi-kafka     | 	sasl.login.refresh.min.period.seconds = 60
benchi-kafka     | 	sasl.login.refresh.window.factor = 0.8
benchi-kafka     | 	sasl.login.refresh.window.jitter = 0.05
benchi-kafka     | 	sasl.login.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.login.retry.backoff.ms = 100
benchi-kafka     | 	sasl.mechanism = GSSAPI
benchi-kafka     | 	sasl.oauthbearer.clientassertion.audience = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.expiration = 5
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.jti.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.nbf.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.issuer = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.location = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key.passphrase = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.subject = null
benchi-kafka     | 	sasl.oauthbearer.clock.skew.seconds = 30
benchi-kafka     | 	sasl.oauthbearer.expected.audience = null
benchi-kafka     | 	sasl.oauthbearer.expected.issuer = null
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.url = null
benchi-kafka     | 	sasl.oauthbearer.scope.claim.name = scope
benchi-kafka     | 	sasl.oauthbearer.sub.claim.name = sub
benchi-kafka     | 	sasl.oauthbearer.token.endpoint.url = null
benchi-kafka     | 	security.protocol = PLAINTEXT
benchi-kafka     | 	security.providers = null
benchi-kafka     | 	send.buffer.bytes = 131072
benchi-kafka     | 	session.timeout.ms = 45000
benchi-kafka     | 	socket.connection.setup.timeout.max.ms = 30000
benchi-kafka     | 	socket.connection.setup.timeout.ms = 10000
benchi-kafka     | 	ssl.cipher.suites = null
benchi-kafka     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	ssl.engine.factory.class = null
benchi-kafka     | 	ssl.key.password = null
benchi-kafka     | 	ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	ssl.keystore.certificate.chain = null
benchi-kafka     | 	ssl.keystore.key = null
benchi-kafka     | 	ssl.keystore.location = null
benchi-kafka     | 	ssl.keystore.password = null
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.protocol = TLSv1.3
benchi-kafka     | 	ssl.provider = null
benchi-kafka     | 	ssl.secure.random.implementation = null
benchi-kafka     | 	ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	ssl.truststore.certificates = null
benchi-kafka     | 	ssl.truststore.location = null
benchi-kafka     | 	ssl.truststore.password = null
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     | 	value.deserializer = class io.confluent.databalancer.persistence.ApiStatePersistenceStore$SbkApiStatusMessageSerde
benchi-kafka     |  (org.apache.kafka.clients.consumer.ConsumerConfig)
benchi-kafka     | [2025-03-18 16:09:15,766] INFO initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector)
benchi-kafka     | [2025-03-18 16:09:15,767] INFO Kafka version: 7.8.1-ce (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:09:15,767] INFO Kafka commitId: eefb51c6f2f1b888b8b03296353338afbac8e213 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:09:15,767] INFO Kafka startTimeMs: 1742314155767 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:09:15,768] INFO [Producer clientId=_confluent_balancer_api_state-producer-1] Cluster ID: MkU3OEVBNTcwNTJENDM2Qk (org.apache.kafka.clients.Metadata)
benchi-kafka     | [2025-03-18 16:09:15,768] INFO [Consumer clientId=_confluent_balancer_api_state-consumer-1, groupId=null] Cluster ID: MkU3OEVBNTcwNTJENDM2Qk (org.apache.kafka.clients.Metadata)
benchi-kafka     | [2025-03-18 16:09:15,769] INFO [Consumer clientId=_confluent_balancer_api_state-consumer-1, groupId=null] Assigned to partition(s): _confluent_balancer_api_state-0 (org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer)
benchi-kafka     | [2025-03-18 16:09:15,769] INFO [Consumer clientId=_confluent_balancer_api_state-consumer-1, groupId=null] Seeking to earliest offset of partition _confluent_balancer_api_state-0 (org.apache.kafka.clients.consumer.internals.SubscriptionState)
benchi-kafka     | [2025-03-18 16:09:15,778] INFO Finished reading KafkaBasedLog for topic _confluent_balancer_api_state (org.apache.kafka.connect.util.KafkaBasedLog)
benchi-kafka     | [2025-03-18 16:09:15,778] INFO Started KafkaBasedLog for topic _confluent_balancer_api_state (org.apache.kafka.connect.util.KafkaBasedLog)
benchi-kafka     | [2025-03-18 16:09:15,778] INFO Started DataBalancer Api State Persistence Store (io.confluent.databalancer.persistence.ApiStatePersistenceStore)
benchi-kafka     | [2025-03-18 16:09:15,780] INFO Starting Kafka Cruise Control... (com.linkedin.kafka.cruisecontrol.KafkaCruiseControl)
benchi-kafka     | [2025-03-18 16:09:15,781] INFO Initializing DataBalancer with goals UpdatableSbcGoalsConfig{rebalancingGoals=GoalsConfig{requirements=(requiredNumWindows=6, minMonitoredPartitionPercentage=0.950, includedAllTopics=true, fetchTopicPlacements=true), goals=[MovementExclusionGoal, ReplicaPlacementGoal, RackAwareGoal, CellAwareGoal, TenantAwareGoal, MaxReplicaMovementParallelismGoal, ReplicaCapacityGoal, DiskCapacityGoal, NetworkInboundCapacityGoal, NetworkOutboundCapacityGoal, ReplicationInboundCapacityGoal, ProducerInboundCapacityGoal, MirrorInboundCapacityGoal, ConsumerOutboundCapacityGoal, SystemTopicEvenDistributionGoal, ReplicaDistributionGoal, DiskUsageDistributionGoal, LeaderReplicaDistributionGoal, NetworkInboundUsageDistributionGoal, NetworkOutboundUsageDistributionGoal, TopicReplicaDistributionGoal]}, triggeringGoals=GoalsConfig{requirements=(requiredNumWindows=1, minMonitoredPartitionPercentage=0.950, includedAllTopics=true, fetchTopicPlacements=true), goals=[ReplicaPlacementGoal, RackAwareGoal, CellAwareGoal, TenantAwareGoal, MaxReplicaMovementParallelismGoal, ReplicaCapacityGoal, DiskCapacityGoal, NetworkInboundCapacityGoal, NetworkOutboundCapacityGoal, ReplicationInboundCapacityGoal, ProducerInboundCapacityGoal, ReplicaDistributionGoal, DiskUsageDistributionGoal]}, incrementalBalancingEnabled=false, incrementalBalancingGoals=GoalsConfig{requirements=(requiredNumWindows=5, minMonitoredPartitionPercentage=0.950, includedAllTopics=true, fetchTopicPlacements=true), goals=[MovementExclusionGoal, ReplicaPlacementGoal, RackAwareGoal, CellAwareGoal, TenantAwareGoal, MaxReplicaMovementParallelismGoal, ReplicaCapacityGoal, DiskCapacityGoal, NetworkInboundCapacityGoal, NetworkOutboundCapacityGoal, ReplicationInboundCapacityGoal, ProducerInboundCapacityGoal, MirrorInboundCapacityGoal, ConsumerOutboundCapacityGoal, IncrementalCPUResourceDistributionGoal, IncrementalTopicReplicaDistributionGoal]}} (com.linkedin.kafka.cruisecontrol.KafkaCruiseControl)
benchi-kafka     | [2025-03-18 16:09:15,781] INFO AdminClientConfig values: 
benchi-kafka     | 	auto.include.jmx.reporter = true
benchi-kafka     | 	bootstrap.controllers = []
benchi-kafka     | 	bootstrap.servers = [broker:29092]
benchi-kafka     | 	client.dns.lookup = use_all_dns_ips
benchi-kafka     | 	client.id = 
benchi-kafka     | 	confluent.lkc.id = null
benchi-kafka     | 	confluent.metrics.reporter.bootstrap.servers = localhost:9092
benchi-kafka     | 	confluent.proxy.protocol.client.address = null
benchi-kafka     | 	confluent.proxy.protocol.client.mode = PROXY
benchi-kafka     | 	confluent.proxy.protocol.client.port = null
benchi-kafka     | 	confluent.proxy.protocol.client.version = NONE
benchi-kafka     | 	connections.max.idle.ms = 300000
benchi-kafka     | 	default.api.timeout.ms = 60000
benchi-kafka     | 	enable.metrics.push = true
benchi-kafka     | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
benchi-kafka     | 	metadata.max.age.ms = 300000
benchi-kafka     | 	metadata.recovery.strategy = none
benchi-kafka     | 	metric.reporters = []
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.recording.level = INFO
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	receive.buffer.bytes = 65536
benchi-kafka     | 	reconnect.backoff.max.ms = 5000
benchi-kafka     | 	reconnect.backoff.ms = 500
benchi-kafka     | 	request.timeout.ms = 30000
benchi-kafka     | 	retries = 2147483647
benchi-kafka     | 	retry.backoff.max.ms = 1000
benchi-kafka     | 	retry.backoff.ms = 500
benchi-kafka     | 	sasl.client.callback.handler.class = null
benchi-kafka     | 	sasl.jaas.config = null
benchi-kafka     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
benchi-kafka     | 	sasl.kerberos.min.time.before.relogin = 60000
benchi-kafka     | 	sasl.kerberos.service.name = null
benchi-kafka     | 	sasl.kerberos.ticket.renew.jitter = 0.05
benchi-kafka     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
benchi-kafka     | 	sasl.login.callback.handler.class = null
benchi-kafka     | 	sasl.login.class = null
benchi-kafka     | 	sasl.login.connect.timeout.ms = null
benchi-kafka     | 	sasl.login.read.timeout.ms = null
benchi-kafka     | 	sasl.login.refresh.buffer.seconds = 300
benchi-kafka     | 	sasl.login.refresh.min.period.seconds = 60
benchi-kafka     | 	sasl.login.refresh.window.factor = 0.8
benchi-kafka     | 	sasl.login.refresh.window.jitter = 0.05
benchi-kafka     | 	sasl.login.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.login.retry.backoff.ms = 100
benchi-kafka     | 	sasl.mechanism = GSSAPI
benchi-kafka     | 	sasl.oauthbearer.clientassertion.audience = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.expiration = 5
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.jti.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.nbf.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.issuer = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.location = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key.passphrase = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.subject = null
benchi-kafka     | 	sasl.oauthbearer.clock.skew.seconds = 30
benchi-kafka     | 	sasl.oauthbearer.expected.audience = null
benchi-kafka     | 	sasl.oauthbearer.expected.issuer = null
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.url = null
benchi-kafka     | 	sasl.oauthbearer.scope.claim.name = scope
benchi-kafka     | 	sasl.oauthbearer.sub.claim.name = sub
benchi-kafka     | 	sasl.oauthbearer.token.endpoint.url = null
benchi-kafka     | 	security.protocol = PLAINTEXT
benchi-kafka     | 	security.providers = null
benchi-kafka     | 	send.buffer.bytes = 131072
benchi-kafka     | 	socket.connection.setup.timeout.max.ms = 30000
benchi-kafka     | 	socket.connection.setup.timeout.ms = 10000
benchi-kafka     | 	ssl.cipher.suites = null
benchi-kafka     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	ssl.engine.factory.class = null
benchi-kafka     | 	ssl.key.password = null
benchi-kafka     | 	ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	ssl.keystore.certificate.chain = null
benchi-kafka     | 	ssl.keystore.key = null
benchi-kafka     | 	ssl.keystore.location = null
benchi-kafka     | 	ssl.keystore.password = null
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.protocol = TLSv1.3
benchi-kafka     | 	ssl.provider = null
benchi-kafka     | 	ssl.secure.random.implementation = null
benchi-kafka     | 	ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	ssl.truststore.certificates = null
benchi-kafka     | 	ssl.truststore.location = null
benchi-kafka     | 	ssl.truststore.password = null
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     |  (org.apache.kafka.clients.admin.AdminClientConfig)
benchi-kafka     | [2025-03-18 16:09:15,781] INFO These configurations '[confluent.metrics.reporter.bootstrap.servers]' were supplied but are not used yet. (org.apache.kafka.clients.admin.AdminClientConfig)
benchi-kafka     | [2025-03-18 16:09:15,781] INFO Kafka version: 7.8.1-ce (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:09:15,781] INFO Kafka commitId: eefb51c6f2f1b888b8b03296353338afbac8e213 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:09:15,781] INFO Kafka startTimeMs: 1742314155781 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:09:15,785] INFO CruiseControl: Attempting to configure Broker Capacity from config properties (com.linkedin.kafka.cruisecontrol.config.BrokerCapacityResolver)
benchi-kafka     | [2025-03-18 16:09:15,802] INFO [AdminClient clientId=adminclient-5] The DescribeTopicPartitions API is not supported, using Metadata API to describe topics. (org.apache.kafka.clients.admin.KafkaAdminClient)
benchi-kafka     | [2025-03-18 16:09:15,815] INFO Notifying listeners about metadata change. (com.linkedin.kafka.cruisecontrol.common.MetadataClient)
benchi-kafka     | [2025-03-18 16:09:15,833] INFO [Partition _confluent_balancer_api_state-0 broker=1] roll: _confluent_balancer_api_state-0: first produce received, lastOffset: 0, leaderEpoch: 0, numMessages:1, time diff: 1125 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:15,833] INFO [Partition _confluent_balancer_api_state-0 broker=1] roll: _confluent_balancer_api_state-0: first HWM advanced, leaderEpoch: 0, old HW: 0, new HW: 1, become leader time: 1742314154708 ms, time diff: 1125 ms (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:15,873] INFO Loaded an even cluster load state record EvenClusterLoadStateRecord{ currentState=null, currentStateCreatedAt=0, currentStateLastUpdatedAt=0, currentStateException=null, previousState=null, previousStateCreatedAt=0, previousStateLastUpdatedAt=0, previousStateException=null} (io.confluent.databalancer.persistence.ApiStatePersistenceStore)
benchi-kafka     | [2025-03-18 16:09:15,878] INFO Set throttle rate 10485760. Will not override static throttles when setting the rate. (com.linkedin.kafka.cruisecontrol.executor.ReplicationThrottleHelper)
benchi-kafka     | [2025-03-18 16:09:15,894] INFO Removed throttled replicas config for topics: [_confluent_balancer_api_state, _confluent-command, _confluent-telemetry-metrics, _confluent-link-metadata] (com.linkedin.kafka.cruisecontrol.executor.ReplicationThrottleHelper)
benchi-kafka     | [2025-03-18 16:09:15,902] INFO Removed throttle rate config from 0 brokers (com.linkedin.kafka.cruisecontrol.executor.ReplicationThrottleHelper)
benchi-kafka     | [2025-03-18 16:09:15,902] INFO Starting anomaly detector. (com.linkedin.kafka.cruisecontrol.detector.AnomalyDetector)
benchi-kafka     | [2025-03-18 16:09:15,902] INFO Starting metric sampling task. (com.linkedin.kafka.cruisecontrol.monitor.task.MetricSamplingTask)
benchi-kafka     | [2025-03-18 16:09:15,902] INFO [SBK_BrokerFailureDetector]: Starting (com.linkedin.kafka.cruisecontrol.detector.BrokerFailureDetector)
benchi-kafka     | [2025-03-18 16:09:15,903] INFO Starting anomaly handler (com.linkedin.kafka.cruisecontrol.detector.AnomalyDetector)
benchi-kafka     | [2025-03-18 16:09:15,903] INFO KafkaCruiseControlConfig values: 
benchi-kafka     | 	alter.configs.response.timeout.ms = 30000
benchi-kafka     | 	anomaly.detection.allow.capacity.estimation = true
benchi-kafka     | 	anomaly.detection.goals = [io.confluent.cruisecontrol.analyzer.goals.ReplicaPlacementGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.RackAwareGoal, io.confluent.cruisecontrol.analyzer.goals.CellAwareGoal, io.confluent.cruisecontrol.analyzer.goals.TenantAwareGoal, io.confluent.cruisecontrol.analyzer.goals.MaxReplicaMovementParallelismGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.ReplicaCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.DiskCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.NetworkInboundCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.NetworkOutboundCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.ReplicationInboundCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.ProducerInboundCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.ReplicaDistributionGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.DiskUsageDistributionGoal]
benchi-kafka     | 	anomaly.detection.interval.ms = 60000
benchi-kafka     | 	bootstrap.servers = [broker:29092]
benchi-kafka     | 	broker.addition.elapsed.time.ms.completion.threshold = 57600000
benchi-kafka     | 	broker.addition.mean.cpu.percent.completion.threshold = 0.5
benchi-kafka     | 	broker.capacity.config.resolver.class = class com.linkedin.kafka.cruisecontrol.config.BrokerCapacityResolver
benchi-kafka     | 	broker.failure.alert.threshold.ms = 0
benchi-kafka     | 	broker.failure.exclude.recently.removed.brokers = true
benchi-kafka     | 	broker.failure.self.healing.threshold.ms = 3600000
benchi-kafka     | 	broker.metric.sample.aggregator.completeness.cache.size = 5
benchi-kafka     | 	broker.metric.sample.store.topic = _confluent_balancer_broker_samples
benchi-kafka     | 	broker.removal.shutdown.timeout.ms = 600000
benchi-kafka     | 	broker.replica.exclusion.timeout.ms = 120000
benchi-kafka     | 	bytes.cpu.contribution.weight = 0.2
benchi-kafka     | 	calculated.throttle.ratio = 0.8
benchi-kafka     | 	capacity.threshold.upper.limit = 0.95
benchi-kafka     | 	cdbe.shutdown.wait.ms = 15000
benchi-kafka     | 	cell.load.upper.bound = 0.7
benchi-kafka     | 	cell.overload.detection.interval.ms = 3600000
benchi-kafka     | 	cell.overload.duration.ms = 86400000
benchi-kafka     | 	client.id = kafka-cruise-control
benchi-kafka     | 	confluent.balancer.additional.invalidation.duration.ms = 60000
benchi-kafka     | 	confluent.balancer.plan.computation.retry.timeout.ms = 3600000
benchi-kafka     | 	confluent.cells.enable = false
benchi-kafka     | 	connections.max.idle.ms = 540000
benchi-kafka     | 	consume.out.bound.should.balance.FFF.traffic = false
benchi-kafka     | 	consumer.out.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	consumer.outbound.capacity.threshold = 0.9
benchi-kafka     | 	cpu.balance.threshold = 1.1
benchi-kafka     | 	cpu.capacity.threshold = 1.0
benchi-kafka     | 	cpu.low.utilization.threshold = 0.2
benchi-kafka     | 	cpu.low.utilization.threshold.for.broker.addition = 0.2
benchi-kafka     | 	cpu.utilization.detector.duration.ms = 600000
benchi-kafka     | 	cpu.utilization.detector.enabled = false
benchi-kafka     | 	cpu.utilization.detector.overutilization.threshold = 80.0
benchi-kafka     | 	cpu.utilization.detector.underutilization.threshold = 50.0
benchi-kafka     | 	default.api.timeout.ms = 60000
benchi-kafka     | 	default.replica.movement.strategies = [com.linkedin.kafka.cruisecontrol.executor.strategy.BaseReplicaMovementStrategy]
benchi-kafka     | 	describe.broker.exclusion.timeout.ms = 60000
benchi-kafka     | 	describe.cluster.response.timeout.ms = 30000
benchi-kafka     | 	describe.configs.batch.size = 1000
benchi-kafka     | 	describe.configs.response.timeout.ms = 30000
benchi-kafka     | 	describe.topics.response.timeout.ms = 30000
benchi-kafka     | 	disk.balance.threshold = 1.1
benchi-kafka     | 	disk.low.utilization.threshold = 0.2
benchi-kafka     | 	disk.max.load = 0.85
benchi-kafka     | 	disk.min.free.space.gb = 0
benchi-kafka     | 	disk.min.free.space.lower.limit.gb = 0
benchi-kafka     | 	disk.read.ratio = 0.2
benchi-kafka     | 	disk.utilization.detector.duration.ms = 600000
benchi-kafka     | 	disk.utilization.detector.enabled = false
benchi-kafka     | 	disk.utilization.detector.overutilization.threshold = 80.0
benchi-kafka     | 	disk.utilization.detector.reserved.capacity = 150000.0
benchi-kafka     | 	disk.utilization.detector.underutilization.threshold = 35.0
benchi-kafka     | 	dynamic.throttling.enabled = true
benchi-kafka     | 	execution.progress.check.interval.ms = 7000
benchi-kafka     | 	executor.leader.action.timeout.ms = 180000
benchi-kafka     | 	executor.notifier.class = class com.linkedin.kafka.cruisecontrol.executor.ExecutorNoopNotifier
benchi-kafka     | 	executor.reservation.refresh.time.ms = 60000
benchi-kafka     | 	follower.network.inbound.weight.for.cpu.util = 0.15
benchi-kafka     | 	goal.balancedness.priority.weight = 1.1
benchi-kafka     | 	goal.balancedness.strictness.weight = 1.5
benchi-kafka     | 	goal.violation.delay.on.new.brokers.ms = 1800000
benchi-kafka     | 	goal.violation.distribution.threshold.multiplier = 1.1
benchi-kafka     | 	goal.violation.exclude.recently.removed.brokers = true
benchi-kafka     | 	goals = [com.linkedin.kafka.cruisecontrol.analyzer.goals.MovementExclusionGoal, io.confluent.cruisecontrol.analyzer.goals.ReplicaPlacementGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.RackAwareGoal, io.confluent.cruisecontrol.analyzer.goals.CellAwareGoal, io.confluent.cruisecontrol.analyzer.goals.TenantAwareGoal, io.confluent.cruisecontrol.analyzer.goals.MaxReplicaMovementParallelismGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.ReplicaCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.DiskCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.NetworkInboundCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.NetworkOutboundCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.ReplicationInboundCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.ProducerInboundCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.MirrorInboundCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.ConsumerOutboundCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.SystemTopicEvenDistributionGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.ReplicaDistributionGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.DiskUsageDistributionGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.LeaderReplicaDistributionGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.NetworkInboundUsageDistributionGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.NetworkOutboundUsageDistributionGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.TopicReplicaDistributionGoal]
benchi-kafka     | 	hot.partition.capacity.utilization.threshold = 0.2
benchi-kafka     | 	incremental.balancing.cpu.top.proposal.tracking.enabled = true
benchi-kafka     | 	incremental.balancing.cpu.top.proposal.tracking.num.proposals = 15
benchi-kafka     | 	incremental.balancing.enabled = false
benchi-kafka     | 	incremental.balancing.goals = [com.linkedin.kafka.cruisecontrol.analyzer.goals.MovementExclusionGoal, io.confluent.cruisecontrol.analyzer.goals.ReplicaPlacementGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.RackAwareGoal, io.confluent.cruisecontrol.analyzer.goals.CellAwareGoal, io.confluent.cruisecontrol.analyzer.goals.TenantAwareGoal, io.confluent.cruisecontrol.analyzer.goals.MaxReplicaMovementParallelismGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.ReplicaCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.DiskCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.NetworkInboundCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.NetworkOutboundCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.ReplicationInboundCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.ProducerInboundCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.MirrorInboundCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.ConsumerOutboundCapacityGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.IncrementalCPUResourceDistributionGoal, com.linkedin.kafka.cruisecontrol.analyzer.goals.IncrementalTopicReplicaDistributionGoal]
benchi-kafka     | 	incremental.balancing.lower.bound = 0.02
benchi-kafka     | 	incremental.balancing.min.valid.windows = 5
benchi-kafka     | 	incremental.balancing.step.ratio = 0.2
benchi-kafka     | 	inter.cell.balancing.enabled = false
benchi-kafka     | 	invalid.replica.assignment.retry.timeout.ms = 300000
benchi-kafka     | 	leader.network.inbound.weight.for.cpu.util = 0.7
benchi-kafka     | 	leader.network.outbound.weight.for.cpu.util = 0.15
benchi-kafka     | 	leader.replica.count.balance.threshold = 1.1
benchi-kafka     | 	logdir.response.timeout.ms = 30000
benchi-kafka     | 	max.allowed.extrapolations.per.broker = 5
benchi-kafka     | 	max.allowed.extrapolations.per.partition = 5
benchi-kafka     | 	max.capacity.balancing.delta.percentage = 0.0
benchi-kafka     | 	max.replicas = 2147483647
benchi-kafka     | 	max.volume.throughput.mb = 0
benchi-kafka     | 	metadata.client.timeout.ms = 180000
benchi-kafka     | 	metadata.ttl = 10000
benchi-kafka     | 	metric.sampler.class = class io.confluent.cruisecontrol.metricsreporter.ConfluentTelemetryReporterSampler
benchi-kafka     | 	min.samples.per.partition.metrics.window = 1
benchi-kafka     | 	min.valid.partition.ratio = 0.95
benchi-kafka     | 	network.in.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	network.inbound.balance.threshold = 1.1
benchi-kafka     | 	network.inbound.capacity.threshold = 0.8
benchi-kafka     | 	network.inbound.low.utilization.threshold = 0.2
benchi-kafka     | 	network.out.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	network.outbound.balance.threshold = 1.1
benchi-kafka     | 	network.outbound.capacity.threshold = 0.8
benchi-kafka     | 	network.outbound.low.utilization.threshold = 0.2
benchi-kafka     | 	num.cached.recent.anomaly.states = 10
benchi-kafka     | 	num.concurrent.leader.movements = 1000
benchi-kafka     | 	num.concurrent.partition.movements.per.broker = 5
benchi-kafka     | 	num.metric.fetchers = 1
benchi-kafka     | 	num.partition.metrics.windows = 12
benchi-kafka     | 	partition.metric.sample.aggregator.completeness.cache.size = 5
benchi-kafka     | 	partition.metric.sample.store.topic = _confluent_balancer_partition_samples
benchi-kafka     | 	partition.metrics.window.ms = 180000
benchi-kafka     | 	populate.default.disk.capacity.from.local = true
benchi-kafka     | 	producer.in.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	producer.inbound.capacity.threshold = 0.9
benchi-kafka     | 	read.throughput.multiplier = 1.0
benchi-kafka     | 	receive.buffer.bytes = 32768
benchi-kafka     | 	reconnect.backoff.ms = 50
benchi-kafka     | 	removal.history.retention.time.ms = 86400000
benchi-kafka     | 	replica.count.balance.threshold = 1.1
benchi-kafka     | 	replica.movement.strategies = [com.linkedin.kafka.cruisecontrol.executor.strategy.PostponeUrpReplicaMovementStrategy, com.linkedin.kafka.cruisecontrol.executor.strategy.PrioritizeLargeReplicaMovementStrategy, com.linkedin.kafka.cruisecontrol.executor.strategy.PrioritizeSmallReplicaMovementStrategy, com.linkedin.kafka.cruisecontrol.executor.strategy.BaseReplicaMovementStrategy]
benchi-kafka     | 	replication.in.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	replication.inbound.capacity.threshold = 0.9
benchi-kafka     | 	request.cpu.contribution.weight = 0.8
benchi-kafka     | 	request.timeout.ms = 30000
benchi-kafka     | 	resource.utilization.detector.interval.ms = 60000
benchi-kafka     | 	sampling.allow.cpu.capacity.estimation = true
benchi-kafka     | 	sasl.client.callback.handler.class = null
benchi-kafka     | 	sasl.jaas.config = null
benchi-kafka     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
benchi-kafka     | 	sasl.kerberos.min.time.before.relogin = 60000
benchi-kafka     | 	sasl.kerberos.service.name = null
benchi-kafka     | 	sasl.kerberos.ticket.renew.jitter = 0.05
benchi-kafka     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
benchi-kafka     | 	sasl.login.callback.handler.class = null
benchi-kafka     | 	sasl.login.class = null
benchi-kafka     | 	sasl.login.connect.timeout.ms = null
benchi-kafka     | 	sasl.login.read.timeout.ms = null
benchi-kafka     | 	sasl.login.refresh.buffer.seconds = 300
benchi-kafka     | 	sasl.login.refresh.min.period.seconds = 60
benchi-kafka     | 	sasl.login.refresh.window.factor = 0.8
benchi-kafka     | 	sasl.login.refresh.window.jitter = 0.05
benchi-kafka     | 	sasl.login.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.login.retry.backoff.ms = 100
benchi-kafka     | 	sasl.mechanism = GSSAPI
benchi-kafka     | 	sasl.oauthbearer.clientassertion.audience = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.expiration = 5
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.jti.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.nbf.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.issuer = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.location = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key.passphrase = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.subject = null
benchi-kafka     | 	sasl.oauthbearer.clock.skew.seconds = 30
benchi-kafka     | 	sasl.oauthbearer.expected.audience = null
benchi-kafka     | 	sasl.oauthbearer.expected.issuer = null
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.url = null
benchi-kafka     | 	sasl.oauthbearer.scope.claim.name = scope
benchi-kafka     | 	sasl.oauthbearer.sub.claim.name = sub
benchi-kafka     | 	sasl.oauthbearer.token.endpoint.url = null
benchi-kafka     | 	sbc.metrics.parser.enabled = false
benchi-kafka     | 	security.protocol = PLAINTEXT
benchi-kafka     | 	self.healing.broker.failure.enabled = true
benchi-kafka     | 	self.healing.goal.violation.enabled = false
benchi-kafka     | 	self.healing.maximum.rounds = 1
benchi-kafka     | 	send.buffer.bytes = 131072
benchi-kafka     | 	ssl.cipher.suites = null
benchi-kafka     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	ssl.engine.factory.class = null
benchi-kafka     | 	ssl.key.password = null
benchi-kafka     | 	ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	ssl.keystore.certificate.chain = null
benchi-kafka     | 	ssl.keystore.key = null
benchi-kafka     | 	ssl.keystore.location = null
benchi-kafka     | 	ssl.keystore.password = null
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.protocol = TLSv1.3
benchi-kafka     | 	ssl.provider = null
benchi-kafka     | 	ssl.secure.random.implementation = null
benchi-kafka     | 	ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	ssl.truststore.certificates = null
benchi-kafka     | 	ssl.truststore.location = null
benchi-kafka     | 	ssl.truststore.password = null
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     | 	startup.retry.delay.minutes = 5
benchi-kafka     | 	startup.retry.max.hours = 2
benchi-kafka     | 	static.throttle.rate.override.enabled = false
benchi-kafka     | 	tenant.maximum.movements = 0
benchi-kafka     | 	tenant.suspension.ms = 86400000
benchi-kafka     | 	throttle.bytes.per.second = 10485760
benchi-kafka     | 	topic.balancing.badly.imbalanced.topic.imbalance.score.threshold = 0.3
benchi-kafka     | 	topic.balancing.balance.threshold.multiplier = 1.0
benchi-kafka     | 	topic.balancing.broker.addition.completion.percentage = 0.8
benchi-kafka     | 	topic.balancing.broker.addition.detector.with.trdg.enabled = false
benchi-kafka     | 	topic.balancing.imbalanced.score.threshold = 0.07
benchi-kafka     | 	topic.balancing.max.reassignments.per.iteration = -1
benchi-kafka     | 	topic.balancing.slightly.imbalanced.topic.imbalance.score.threshold = 0.05
benchi-kafka     | 	topic.balancing.slightly.imbalanced.topics.percentage.trigger = 0.2
benchi-kafka     | 	topic.balancing.trigger.threshold.multiplier = 3.0
benchi-kafka     | 	topic.partition.maximum.movements = 5
benchi-kafka     | 	topic.partition.movement.expiration.ms = 10800000
benchi-kafka     | 	topic.partition.suspension.ms = 18000000
benchi-kafka     | 	topics.excluded.from.partition.movement = 
benchi-kafka     | 	v2.addition.enabled = false
benchi-kafka     | 	write.throughput.multiplier = 1.0
benchi-kafka     | 	zookeeper.connect = 
benchi-kafka     | 	zookeeper.security.enabled = false
benchi-kafka     |  (com.linkedin.kafka.cruisecontrol.config.KafkaCruiseControlConfig)
benchi-kafka     | [2025-03-18 16:09:15,904] INFO Kafka Cruise Control started. (com.linkedin.kafka.cruisecontrol.KafkaCruiseControl)
benchi-kafka     | [2025-03-18 16:09:15,904] WARN Disabling exponential reconnect backoff because reconnect.backoff.ms is set, but reconnect.backoff.max.ms is not. (org.apache.kafka.clients.CommonClientConfigs)
benchi-kafka     | [2025-03-18 16:09:15,904] INFO Alive brokers: [1], failed brokers: [] (com.linkedin.kafka.cruisecontrol.detector.BrokerFailureDetector)
benchi-kafka     | [2025-03-18 16:09:15,904] INFO ConsumerConfig values: 
benchi-kafka     | 	allow.auto.create.topics = true
benchi-kafka     | 	auto.commit.interval.ms = 5000
benchi-kafka     | 	auto.include.jmx.reporter = true
benchi-kafka     | 	auto.offset.reset = latest
benchi-kafka     | 	bootstrap.servers = [broker:29092]
benchi-kafka     | 	check.crcs = true
benchi-kafka     | 	client.dns.lookup = use_all_dns_ips
benchi-kafka     | 	client.id = kafka-cruise-control
benchi-kafka     | 	client.rack = 
benchi-kafka     | 	confluent.lkc.id = null
benchi-kafka     | 	confluent.proxy.protocol.client.address = null
benchi-kafka     | 	confluent.proxy.protocol.client.mode = PROXY
benchi-kafka     | 	confluent.proxy.protocol.client.port = null
benchi-kafka     | 	confluent.proxy.protocol.client.version = NONE
benchi-kafka     | 	connections.max.idle.ms = 540000
benchi-kafka     | 	default.api.timeout.ms = 60000
benchi-kafka     | 	enable.auto.commit = false
benchi-kafka     | 	enable.metrics.push = true
benchi-kafka     | 	exclude.internal.topics = true
benchi-kafka     | 	fetch.max.bytes = 52428800
benchi-kafka     | 	fetch.max.wait.ms = 500
benchi-kafka     | 	fetch.min.bytes = 1
benchi-kafka     | 	group.id = ConfluentTelemetryReporterSampler--2946986101702943459
benchi-kafka     | 	group.instance.id = null
benchi-kafka     | 	group.protocol = classic
benchi-kafka     | 	group.remote.assignor = null
benchi-kafka     | 	heartbeat.interval.ms = 3000
benchi-kafka     | 	interceptor.classes = []
benchi-kafka     | 	internal.leave.group.on.close = true
benchi-kafka     | 	internal.throw.on.fetch.stable.offset.unsupported = false
benchi-kafka     | 	isolation.level = read_uncommitted
benchi-kafka     | 	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
benchi-kafka     | 	max.partition.fetch.bytes = 1048576
benchi-kafka     | 	max.poll.interval.ms = 2147483647
benchi-kafka     | 	max.poll.records = 2147483647
benchi-kafka     | 	metadata.max.age.ms = 300000
benchi-kafka     | 	metadata.recovery.strategy = none
benchi-kafka     | 	metric.reporters = []
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.recording.level = INFO
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
benchi-kafka     | 	receive.buffer.bytes = 32768
benchi-kafka     | 	reconnect.backoff.max.ms = 50
benchi-kafka     | 	reconnect.backoff.ms = 50
benchi-kafka     | 	request.timeout.ms = 30000
benchi-kafka     | 	retry.backoff.max.ms = 1000
benchi-kafka     | 	retry.backoff.ms = 100
benchi-kafka     | 	sasl.client.callback.handler.class = null
benchi-kafka     | 	sasl.jaas.config = null
benchi-kafka     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
benchi-kafka     | 	sasl.kerberos.min.time.before.relogin = 60000
benchi-kafka     | 	sasl.kerberos.service.name = null
benchi-kafka     | 	sasl.kerberos.ticket.renew.jitter = 0.05
benchi-kafka     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
benchi-kafka     | 	sasl.login.callback.handler.class = null
benchi-kafka     | 	sasl.login.class = null
benchi-kafka     | 	sasl.login.connect.timeout.ms = null
benchi-kafka     | 	sasl.login.read.timeout.ms = null
benchi-kafka     | 	sasl.login.refresh.buffer.seconds = 300
benchi-kafka     | 	sasl.login.refresh.min.period.seconds = 60
benchi-kafka     | 	sasl.login.refresh.window.factor = 0.8
benchi-kafka     | 	sasl.login.refresh.window.jitter = 0.05
benchi-kafka     | 	sasl.login.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.login.retry.backoff.ms = 100
benchi-kafka     | 	sasl.mechanism = GSSAPI
benchi-kafka     | 	sasl.oauthbearer.clientassertion.audience = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.expiration = 5
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.jti.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.nbf.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.issuer = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.location = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key.passphrase = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.subject = null
benchi-kafka     | 	sasl.oauthbearer.clock.skew.seconds = 30
benchi-kafka     | 	sasl.oauthbearer.expected.audience = null
benchi-kafka     | 	sasl.oauthbearer.expected.issuer = null
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.url = null
benchi-kafka     | 	sasl.oauthbearer.scope.claim.name = scope
benchi-kafka     | 	sasl.oauthbearer.sub.claim.name = sub
benchi-kafka     | 	sasl.oauthbearer.token.endpoint.url = null
benchi-kafka     | 	security.protocol = PLAINTEXT
benchi-kafka     | 	security.providers = null
benchi-kafka     | 	send.buffer.bytes = 131072
benchi-kafka     | 	session.timeout.ms = 45000
benchi-kafka     | 	socket.connection.setup.timeout.max.ms = 30000
benchi-kafka     | 	socket.connection.setup.timeout.ms = 10000
benchi-kafka     | 	ssl.cipher.suites = null
benchi-kafka     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	ssl.engine.factory.class = null
benchi-kafka     | 	ssl.key.password = null
benchi-kafka     | 	ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	ssl.keystore.certificate.chain = null
benchi-kafka     | 	ssl.keystore.key = null
benchi-kafka     | 	ssl.keystore.location = null
benchi-kafka     | 	ssl.keystore.password = null
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.protocol = TLSv1.3
benchi-kafka     | 	ssl.provider = null
benchi-kafka     | 	ssl.secure.random.implementation = null
benchi-kafka     | 	ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	ssl.truststore.certificates = null
benchi-kafka     | 	ssl.truststore.location = null
benchi-kafka     | 	ssl.truststore.password = null
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     | 	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
benchi-kafka     |  (org.apache.kafka.clients.consumer.ConsumerConfig)
benchi-kafka     | [2025-03-18 16:09:15,904] INFO initializing Kafka metrics collector (org.apache.kafka.common.telemetry.internals.KafkaMetricsCollector)
benchi-kafka     | [2025-03-18 16:09:15,905] INFO No pending DataBalancer operations found at startup. (io.confluent.databalancer.ConfluentDataBalanceEngine)
benchi-kafka     | [2025-03-18 16:09:15,905] INFO Balancer Status state for brokers [1] transitioned from STARTING to RUNNING due to event CRUISE_CONTROL_INITIALIZATION_COMPLETED. (io.confluent.databalancer.operation.StateMachine)
benchi-kafka     | [2025-03-18 16:09:15,905] INFO DataBalancer: Scheduling DataBalanceEngine auto-heal update (setting to false) (io.confluent.databalancer.ConfluentDataBalanceEngine)
benchi-kafka     | [2025-03-18 16:09:15,905] INFO Updated list of failed broker: {} (com.linkedin.kafka.cruisecontrol.detector.BrokerFailureDetector)
benchi-kafka     | [2025-03-18 16:09:15,905] INFO DataBalancer: DataBalanceEngine started (io.confluent.databalancer.ConfluentDataBalanceEngine)
benchi-kafka     | [2025-03-18 16:09:15,905] INFO Databalancer: Updating auto-heal mode to (false) (io.confluent.databalancer.ConfluentDataBalanceEngine)
benchi-kafka     | [2025-03-18 16:09:15,905] INFO Changing GOAL_VIOLATION anomaly self-healing actions to false (io.confluent.databalancer.ConfluentDataBalanceEngine)
benchi-kafka     | [2025-03-18 16:09:15,905] INFO Goal violation self-healing left disabled (no change) (com.linkedin.kafka.cruisecontrol.KafkaCruiseControl)
benchi-kafka     | [2025-03-18 16:09:15,905] INFO These configurations '[sasl.oauthbearer.jwks.endpoint.refresh.ms, sasl.oauthbearer.clientassertion.include.jti.claim, sasl.login.refresh.min.period.seconds, sasl.oauthbearer.scope.claim.name, sasl.login.refresh.window.factor, sasl.kerberos.ticket.renew.window.factor, sasl.login.retry.backoff.ms, sasl.oauthbearer.clientassertion.expiration, sasl.kerberos.kinit.cmd, sasl.kerberos.ticket.renew.jitter, ssl.keystore.type, ssl.trustmanager.algorithm, sasl.kerberos.min.time.before.relogin, ssl.endpoint.identification.algorithm, ssl.protocol, sasl.login.refresh.buffer.seconds, sasl.login.retry.backoff.max.ms, ssl.enabled.protocols, sasl.oauthbearer.sub.claim.name, ssl.truststore.type, sasl.oauthbearer.jwks.endpoint.retry.backoff.ms, sasl.oauthbearer.clock.skew.seconds, ssl.keymanager.algorithm, sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms, sasl.oauthbearer.clientassertion.include.nbf.claim, sasl.login.refresh.window.jitter]' were supplied but are not used yet. (org.apache.kafka.clients.consumer.ConsumerConfig)
benchi-kafka     | [2025-03-18 16:09:15,905] INFO Kafka version: 7.8.1-ce (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:09:15,905] INFO Kafka commitId: eefb51c6f2f1b888b8b03296353338afbac8e213 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:09:15,906] INFO Kafka startTimeMs: 1742314155905 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:09:15,906] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Subscribed to pattern: '_confluent-telemetry-metrics' (org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer)
benchi-kafka     | [2025-03-18 16:09:15,908] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Cluster ID: MkU3OEVBNTcwNTJENDM2Qk (org.apache.kafka.clients.Metadata)
benchi-kafka     | [2025-03-18 16:09:15,910] INFO Beginning to sample MetricsWindow{sizeMs=180000, startMs=1742313960000, endMs=1742314140000, endMsInclusive=1742314139999, index=9679523, baseTimestamp=0}(16:06:00 - 16:08:59.999) (com.linkedin.kafka.cruisecontrol.monitor.task.MetricSamplingTask)
benchi-kafka     | [2025-03-18 16:09:15,914] INFO Sent auto-creation request for Set(__consumer_offsets) to the active controller. (kafka.server.DefaultAutoTopicCreationManager)
benchi-kafka     | [2025-03-18 16:09:15,915] INFO [ControllerServer id=1] CreateTopics result(s): CreatableTopic(name='__consumer_offsets', numPartitions=50, replicationFactor=1, assignments=[], configs=[CreateableTopicConfig(name='compression.type', value='producer'), CreateableTopicConfig(name='cleanup.policy', value='compact'), CreateableTopicConfig(name='max.compaction.lag.ms', value='9223372036854775807'), CreateableTopicConfig(name='segment.bytes', value='104857600'), CreateableTopicConfig(name='confluent.placement.constraints', value=''), CreateableTopicConfig(name='min.cleanable.dirty.ratio', value='0.5'), CreateableTopicConfig(name='delete.retention.ms', value='86400000')], linkName=null, mirrorTopic=null, sourceTopicId=AAAAAAAAAAAAAAAAAAAAAA, mirrorStartOffsetSpec=-9223372036854775808, mirrorStartOffsets=[], stoppedSequenceNumber=0): SUCCESS (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:09:15,915] INFO [ControllerServer id=1] Replayed TopicRecord for topic __consumer_offsets with topic ID 3VYZksSVS72hxNY0NkQPGQ. (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:09:15,915] INFO [ControllerServer id=1] Replayed ConfigRecord for ConfigResource(type=TOPIC, name='__consumer_offsets') which set configuration compression.type to producer (org.apache.kafka.controller.ConfigurationControlManager)
benchi-kafka     | [2025-03-18 16:09:15,915] INFO [ControllerServer id=1] Replayed ConfigRecord for ConfigResource(type=TOPIC, name='__consumer_offsets') which set configuration cleanup.policy to compact (org.apache.kafka.controller.ConfigurationControlManager)
benchi-kafka     | [2025-03-18 16:09:15,916] INFO [ControllerServer id=1] Replayed ConfigRecord for ConfigResource(type=TOPIC, name='__consumer_offsets') which set configuration max.compaction.lag.ms to 9223372036854775807 (org.apache.kafka.controller.ConfigurationControlManager)
benchi-kafka     | [2025-03-18 16:09:15,916] INFO [ControllerServer id=1] Replayed ConfigRecord for ConfigResource(type=TOPIC, name='__consumer_offsets') which set configuration segment.bytes to 104857600 (org.apache.kafka.controller.ConfigurationControlManager)
benchi-kafka     | [2025-03-18 16:09:15,916] INFO [ControllerServer id=1] Replayed ConfigRecord for ConfigResource(type=TOPIC, name='__consumer_offsets') which set configuration confluent.placement.constraints to  (org.apache.kafka.controller.ConfigurationControlManager)
benchi-kafka     | [2025-03-18 16:09:15,916] INFO [ControllerServer id=1] Replayed ConfigRecord for ConfigResource(type=TOPIC, name='__consumer_offsets') which set configuration min.cleanable.dirty.ratio to 0.5 (org.apache.kafka.controller.ConfigurationControlManager)
benchi-kafka     | [2025-03-18 16:09:15,916] INFO [ControllerServer id=1] Replayed ConfigRecord for ConfigResource(type=TOPIC, name='__consumer_offsets') which set configuration delete.retention.ms to 86400000 (org.apache.kafka.controller.ConfigurationControlManager)
benchi-kafka     | [2025-03-18 16:09:15,916] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-0 with topic ID 3VYZksSVS72hxNY0NkQPGQ and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:09:15,916] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-1 with topic ID 3VYZksSVS72hxNY0NkQPGQ and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:09:15,916] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-2 with topic ID 3VYZksSVS72hxNY0NkQPGQ and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:09:15,916] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-3 with topic ID 3VYZksSVS72hxNY0NkQPGQ and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:09:15,916] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-4 with topic ID 3VYZksSVS72hxNY0NkQPGQ and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:09:15,916] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-5 with topic ID 3VYZksSVS72hxNY0NkQPGQ and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:09:15,916] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-6 with topic ID 3VYZksSVS72hxNY0NkQPGQ and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:09:15,916] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-7 with topic ID 3VYZksSVS72hxNY0NkQPGQ and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:09:15,916] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-8 with topic ID 3VYZksSVS72hxNY0NkQPGQ and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:09:15,916] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-9 with topic ID 3VYZksSVS72hxNY0NkQPGQ and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:09:15,916] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-10 with topic ID 3VYZksSVS72hxNY0NkQPGQ and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:09:15,916] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-11 with topic ID 3VYZksSVS72hxNY0NkQPGQ and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:09:15,916] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-12 with topic ID 3VYZksSVS72hxNY0NkQPGQ and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:09:15,916] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-13 with topic ID 3VYZksSVS72hxNY0NkQPGQ and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:09:15,916] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-14 with topic ID 3VYZksSVS72hxNY0NkQPGQ and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:09:15,916] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-15 with topic ID 3VYZksSVS72hxNY0NkQPGQ and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:09:15,916] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-16 with topic ID 3VYZksSVS72hxNY0NkQPGQ and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:09:15,916] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-17 with topic ID 3VYZksSVS72hxNY0NkQPGQ and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:09:15,916] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-18 with topic ID 3VYZksSVS72hxNY0NkQPGQ and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:09:15,916] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-19 with topic ID 3VYZksSVS72hxNY0NkQPGQ and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:09:15,916] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-20 with topic ID 3VYZksSVS72hxNY0NkQPGQ and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:09:15,916] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-21 with topic ID 3VYZksSVS72hxNY0NkQPGQ and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:09:15,916] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-22 with topic ID 3VYZksSVS72hxNY0NkQPGQ and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:09:15,916] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-23 with topic ID 3VYZksSVS72hxNY0NkQPGQ and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:09:15,916] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-24 with topic ID 3VYZksSVS72hxNY0NkQPGQ and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:09:15,916] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-25 with topic ID 3VYZksSVS72hxNY0NkQPGQ and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:09:15,916] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-26 with topic ID 3VYZksSVS72hxNY0NkQPGQ and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:09:15,916] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-27 with topic ID 3VYZksSVS72hxNY0NkQPGQ and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:09:15,916] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-28 with topic ID 3VYZksSVS72hxNY0NkQPGQ and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:09:15,917] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-29 with topic ID 3VYZksSVS72hxNY0NkQPGQ and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:09:15,917] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-30 with topic ID 3VYZksSVS72hxNY0NkQPGQ and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:09:15,917] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-31 with topic ID 3VYZksSVS72hxNY0NkQPGQ and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:09:15,917] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-32 with topic ID 3VYZksSVS72hxNY0NkQPGQ and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:09:15,917] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-33 with topic ID 3VYZksSVS72hxNY0NkQPGQ and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:09:15,917] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-34 with topic ID 3VYZksSVS72hxNY0NkQPGQ and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:09:15,917] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-35 with topic ID 3VYZksSVS72hxNY0NkQPGQ and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:09:15,917] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-36 with topic ID 3VYZksSVS72hxNY0NkQPGQ and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:09:15,917] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-37 with topic ID 3VYZksSVS72hxNY0NkQPGQ and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:09:15,917] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-38 with topic ID 3VYZksSVS72hxNY0NkQPGQ and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:09:15,917] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-39 with topic ID 3VYZksSVS72hxNY0NkQPGQ and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:09:15,917] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-40 with topic ID 3VYZksSVS72hxNY0NkQPGQ and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:09:15,917] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-41 with topic ID 3VYZksSVS72hxNY0NkQPGQ and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:09:15,917] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-42 with topic ID 3VYZksSVS72hxNY0NkQPGQ and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:09:15,917] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-43 with topic ID 3VYZksSVS72hxNY0NkQPGQ and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:09:15,917] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-44 with topic ID 3VYZksSVS72hxNY0NkQPGQ and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:09:15,917] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-45 with topic ID 3VYZksSVS72hxNY0NkQPGQ and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:09:15,917] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-46 with topic ID 3VYZksSVS72hxNY0NkQPGQ and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:09:15,917] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-47 with topic ID 3VYZksSVS72hxNY0NkQPGQ and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:09:15,917] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-48 with topic ID 3VYZksSVS72hxNY0NkQPGQ and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:09:15,917] INFO [ControllerServer id=1] Replayed PartitionRecord for new partition __consumer_offsets-49 with topic ID 3VYZksSVS72hxNY0NkQPGQ and PartitionRegistration(replicas=[1], observers=[], directories=[Y-adVyLUb4IllEQbE2t_Kg], isr=[1], removingReplicas=[], addingReplicas=[], removingObservers=[], addingObservers=[], elr=[], lastKnownElr=[], leader=1, leaderRecoveryState=RECOVERED, leaderEpoch=0, partitionEpoch=0, linkedLeaderEpoch=-1, linkState=NOT_MIRROR). (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:09:15,947] INFO SBC Event SbcMetadataUpdateEvent-147 generated 1 more events to enqueue in the following order - [SbcConfigUpdateEvent-148]. Enqueuing... (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:09:15,947] INFO Handling event SbcConfigUpdateEvent-148 (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:09:15,947] INFO [Broker id=1] Transitioning 50 partition(s) to local leaders. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:15,947] INFO Balancer notified of a config change: ConfigurationsDelta(changes={ConfigResource(type=TOPIC, name='__consumer_offsets')=ConfigurationDelta(changedKeys=[compression.type, cleanup.policy, max.compaction.lag.ms, segment.bytes, confluent.placement.constraints, min.cleanable.dirty.ratio, delete.retention.ms])}) (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:09:15,947] INFO There were 0 change(s) and 0 deletion(s) to balancer configs. Changed Configs: {}, Deleted Configs: [] (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:09:15,947] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(__consumer_offsets-13, __consumer_offsets-46, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-21, __consumer_offsets-17, __consumer_offsets-30, __consumer_offsets-26, __consumer_offsets-5, __consumer_offsets-38, __consumer_offsets-1, __consumer_offsets-34, __consumer_offsets-16, __consumer_offsets-45, __consumer_offsets-12, __consumer_offsets-41, __consumer_offsets-24, __consumer_offsets-20, __consumer_offsets-49, __consumer_offsets-0, __consumer_offsets-29, __consumer_offsets-25, __consumer_offsets-8, __consumer_offsets-37, __consumer_offsets-4, __consumer_offsets-33, __consumer_offsets-15, __consumer_offsets-48, __consumer_offsets-11, __consumer_offsets-44, __consumer_offsets-23, __consumer_offsets-19, __consumer_offsets-32, __consumer_offsets-28, __consumer_offsets-7, __consumer_offsets-40, __consumer_offsets-3, __consumer_offsets-36, __consumer_offsets-47, __consumer_offsets-14, __consumer_offsets-43, __consumer_offsets-10, __consumer_offsets-22, __consumer_offsets-18, __consumer_offsets-31, __consumer_offsets-27, __consumer_offsets-39, __consumer_offsets-6, __consumer_offsets-35, __consumer_offsets-2) (kafka.server.ReplicaFetcherManager)
benchi-kafka     | [2025-03-18 16:09:15,947] INFO [Broker id=1] Creating new partition __consumer_offsets-13 with topic id 3VYZksSVS72hxNY0NkQPGQ. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:15,948] INFO [Broker id=1] Creating new partition __consumer_offsets-46 with topic id 3VYZksSVS72hxNY0NkQPGQ. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:15,948] INFO [Broker id=1] Creating new partition __consumer_offsets-9 with topic id 3VYZksSVS72hxNY0NkQPGQ. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:15,948] INFO [Broker id=1] Creating new partition __consumer_offsets-42 with topic id 3VYZksSVS72hxNY0NkQPGQ. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:15,948] INFO [Broker id=1] Creating new partition __consumer_offsets-21 with topic id 3VYZksSVS72hxNY0NkQPGQ. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:15,948] INFO [Broker id=1] Creating new partition __consumer_offsets-17 with topic id 3VYZksSVS72hxNY0NkQPGQ. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:15,949] INFO [Broker id=1] Creating new partition __consumer_offsets-30 with topic id 3VYZksSVS72hxNY0NkQPGQ. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:15,949] INFO [Broker id=1] Creating new partition __consumer_offsets-26 with topic id 3VYZksSVS72hxNY0NkQPGQ. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:15,949] INFO [Broker id=1] Creating new partition __consumer_offsets-5 with topic id 3VYZksSVS72hxNY0NkQPGQ. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:15,949] INFO [Broker id=1] Creating new partition __consumer_offsets-38 with topic id 3VYZksSVS72hxNY0NkQPGQ. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:15,949] INFO [Broker id=1] Creating new partition __consumer_offsets-1 with topic id 3VYZksSVS72hxNY0NkQPGQ. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:15,950] INFO [Broker id=1] Creating new partition __consumer_offsets-34 with topic id 3VYZksSVS72hxNY0NkQPGQ. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:15,950] INFO [Broker id=1] Creating new partition __consumer_offsets-16 with topic id 3VYZksSVS72hxNY0NkQPGQ. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:15,951] INFO [Broker id=1] Creating new partition __consumer_offsets-45 with topic id 3VYZksSVS72hxNY0NkQPGQ. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:15,951] INFO [Broker id=1] Creating new partition __consumer_offsets-12 with topic id 3VYZksSVS72hxNY0NkQPGQ. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:15,951] INFO [Broker id=1] Creating new partition __consumer_offsets-41 with topic id 3VYZksSVS72hxNY0NkQPGQ. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:15,951] INFO [Broker id=1] Creating new partition __consumer_offsets-24 with topic id 3VYZksSVS72hxNY0NkQPGQ. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:15,951] INFO [Broker id=1] Creating new partition __consumer_offsets-20 with topic id 3VYZksSVS72hxNY0NkQPGQ. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:15,951] INFO [Broker id=1] Creating new partition __consumer_offsets-49 with topic id 3VYZksSVS72hxNY0NkQPGQ. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:15,952] INFO [Broker id=1] Creating new partition __consumer_offsets-0 with topic id 3VYZksSVS72hxNY0NkQPGQ. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:15,952] INFO [Broker id=1] Creating new partition __consumer_offsets-29 with topic id 3VYZksSVS72hxNY0NkQPGQ. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:15,952] INFO [Broker id=1] Creating new partition __consumer_offsets-25 with topic id 3VYZksSVS72hxNY0NkQPGQ. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:15,952] INFO [Broker id=1] Creating new partition __consumer_offsets-8 with topic id 3VYZksSVS72hxNY0NkQPGQ. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:15,952] INFO [Broker id=1] Creating new partition __consumer_offsets-37 with topic id 3VYZksSVS72hxNY0NkQPGQ. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:15,953] INFO [Broker id=1] Creating new partition __consumer_offsets-4 with topic id 3VYZksSVS72hxNY0NkQPGQ. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:15,953] INFO [Broker id=1] Creating new partition __consumer_offsets-33 with topic id 3VYZksSVS72hxNY0NkQPGQ. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:15,953] INFO [Broker id=1] Creating new partition __consumer_offsets-15 with topic id 3VYZksSVS72hxNY0NkQPGQ. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:15,953] INFO [Broker id=1] Creating new partition __consumer_offsets-48 with topic id 3VYZksSVS72hxNY0NkQPGQ. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:15,953] INFO [Broker id=1] Creating new partition __consumer_offsets-11 with topic id 3VYZksSVS72hxNY0NkQPGQ. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:15,953] INFO [Broker id=1] Creating new partition __consumer_offsets-44 with topic id 3VYZksSVS72hxNY0NkQPGQ. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:15,953] INFO [Broker id=1] Creating new partition __consumer_offsets-23 with topic id 3VYZksSVS72hxNY0NkQPGQ. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:15,954] INFO [Broker id=1] Creating new partition __consumer_offsets-19 with topic id 3VYZksSVS72hxNY0NkQPGQ. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:15,954] INFO [Broker id=1] Creating new partition __consumer_offsets-32 with topic id 3VYZksSVS72hxNY0NkQPGQ. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:15,954] INFO [Broker id=1] Creating new partition __consumer_offsets-28 with topic id 3VYZksSVS72hxNY0NkQPGQ. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:15,954] INFO [Broker id=1] Creating new partition __consumer_offsets-7 with topic id 3VYZksSVS72hxNY0NkQPGQ. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:15,954] INFO [Broker id=1] Creating new partition __consumer_offsets-40 with topic id 3VYZksSVS72hxNY0NkQPGQ. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:15,954] INFO [Broker id=1] Creating new partition __consumer_offsets-3 with topic id 3VYZksSVS72hxNY0NkQPGQ. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:15,954] INFO [Broker id=1] Creating new partition __consumer_offsets-36 with topic id 3VYZksSVS72hxNY0NkQPGQ. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:15,954] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
benchi-kafka     | [2025-03-18 16:09:15,955] INFO [Broker id=1] Creating new partition __consumer_offsets-47 with topic id 3VYZksSVS72hxNY0NkQPGQ. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:15,955] INFO [Broker id=1] Creating new partition __consumer_offsets-14 with topic id 3VYZksSVS72hxNY0NkQPGQ. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:15,955] INFO [Broker id=1] Creating new partition __consumer_offsets-43 with topic id 3VYZksSVS72hxNY0NkQPGQ. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:15,956] INFO [Broker id=1] Creating new partition __consumer_offsets-10 with topic id 3VYZksSVS72hxNY0NkQPGQ. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:15,956] INFO [Broker id=1] Creating new partition __consumer_offsets-22 with topic id 3VYZksSVS72hxNY0NkQPGQ. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:15,956] INFO [Broker id=1] Creating new partition __consumer_offsets-18 with topic id 3VYZksSVS72hxNY0NkQPGQ. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:15,956] INFO [Broker id=1] Creating new partition __consumer_offsets-31 with topic id 3VYZksSVS72hxNY0NkQPGQ. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:15,956] INFO [Broker id=1] Creating new partition __consumer_offsets-27 with topic id 3VYZksSVS72hxNY0NkQPGQ. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:15,956] INFO [Broker id=1] Creating new partition __consumer_offsets-39 with topic id 3VYZksSVS72hxNY0NkQPGQ. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:15,957] INFO [Broker id=1] Creating new partition __consumer_offsets-6 with topic id 3VYZksSVS72hxNY0NkQPGQ. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:15,957] INFO [Broker id=1] Creating new partition __consumer_offsets-35 with topic id 3VYZksSVS72hxNY0NkQPGQ. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:15,957] INFO [Broker id=1] Creating new partition __consumer_offsets-2 with topic id 3VYZksSVS72hxNY0NkQPGQ. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:15,957] INFO [Broker id=1] Stopped fetchers as part of become-leader transition for 50 partitions (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:15,959] INFO [MergedLog partition=__consumer_offsets-28, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:09:15,959] INFO Created log for partition __consumer_offsets-28 in /tmp/kraft-combined-logs/__consumer_offsets-28 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:15,959] INFO [Partition __consumer_offsets-28 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-28 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:15,959] INFO [Partition __consumer_offsets-28 broker=1] Log loaded for partition __consumer_offsets-28 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:15,960] INFO Setting topicIdPartition 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-28 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:09:15,960] INFO [MergedLog partition=__consumer_offsets-28, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-28 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:09:15,960] INFO [Broker id=1] Leader __consumer_offsets-28 with topic id Some(3VYZksSVS72hxNY0NkQPGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:15,960] INFO [MergedLog partition=__consumer_offsets-19, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:09:15,960] INFO Created log for partition __consumer_offsets-19 in /tmp/kraft-combined-logs/__consumer_offsets-19 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:15,961] INFO [Partition __consumer_offsets-19 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-19 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:15,961] INFO [Partition __consumer_offsets-19 broker=1] Log loaded for partition __consumer_offsets-19 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:15,961] INFO Setting topicIdPartition 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-19 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:09:15,961] INFO [MergedLog partition=__consumer_offsets-19, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-19 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:09:15,961] INFO [Broker id=1] Leader __consumer_offsets-19 with topic id Some(3VYZksSVS72hxNY0NkQPGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:15,962] INFO [MergedLog partition=__consumer_offsets-35, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:09:15,962] INFO Created log for partition __consumer_offsets-35 in /tmp/kraft-combined-logs/__consumer_offsets-35 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:15,962] INFO [Partition __consumer_offsets-35 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-35 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:15,962] INFO [Partition __consumer_offsets-35 broker=1] Log loaded for partition __consumer_offsets-35 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:15,962] INFO Setting topicIdPartition 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-35 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:09:15,962] INFO [MergedLog partition=__consumer_offsets-35, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-35 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:09:15,962] INFO [Broker id=1] Leader __consumer_offsets-35 with topic id Some(3VYZksSVS72hxNY0NkQPGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:15,963] INFO [MergedLog partition=__consumer_offsets-3, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:09:15,963] INFO Created log for partition __consumer_offsets-3 in /tmp/kraft-combined-logs/__consumer_offsets-3 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:15,963] INFO [Partition __consumer_offsets-3 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-3 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:15,963] INFO [Partition __consumer_offsets-3 broker=1] Log loaded for partition __consumer_offsets-3 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:15,963] INFO Setting topicIdPartition 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-3 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:09:15,963] INFO [MergedLog partition=__consumer_offsets-3, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-3 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:09:15,963] INFO [Broker id=1] Leader __consumer_offsets-3 with topic id Some(3VYZksSVS72hxNY0NkQPGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:15,964] INFO [MergedLog partition=__consumer_offsets-22, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:09:15,964] INFO Created log for partition __consumer_offsets-22 in /tmp/kraft-combined-logs/__consumer_offsets-22 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:15,964] INFO [Partition __consumer_offsets-22 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-22 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:15,964] INFO [Partition __consumer_offsets-22 broker=1] Log loaded for partition __consumer_offsets-22 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:15,964] INFO Setting topicIdPartition 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-22 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:09:15,964] INFO [MergedLog partition=__consumer_offsets-22, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-22 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:09:15,964] INFO [Broker id=1] Leader __consumer_offsets-22 with topic id Some(3VYZksSVS72hxNY0NkQPGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:15,966] INFO [MergedLog partition=__consumer_offsets-27, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:09:15,966] INFO Created log for partition __consumer_offsets-27 in /tmp/kraft-combined-logs/__consumer_offsets-27 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:15,966] INFO [Partition __consumer_offsets-27 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-27 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:15,966] INFO [Partition __consumer_offsets-27 broker=1] Log loaded for partition __consumer_offsets-27 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:15,966] INFO Setting topicIdPartition 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-27 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:09:15,966] INFO [MergedLog partition=__consumer_offsets-27, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-27 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:09:15,966] INFO [Broker id=1] Leader __consumer_offsets-27 with topic id Some(3VYZksSVS72hxNY0NkQPGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:15,967] INFO [MergedLog partition=__consumer_offsets-11, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:09:15,967] INFO Created log for partition __consumer_offsets-11 in /tmp/kraft-combined-logs/__consumer_offsets-11 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:15,967] INFO [Partition __consumer_offsets-11 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-11 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:15,967] INFO [Partition __consumer_offsets-11 broker=1] Log loaded for partition __consumer_offsets-11 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:15,967] INFO Setting topicIdPartition 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-11 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:09:15,967] INFO [MergedLog partition=__consumer_offsets-11, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-11 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:09:15,967] INFO [Broker id=1] Leader __consumer_offsets-11 with topic id Some(3VYZksSVS72hxNY0NkQPGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:15,968] INFO [MergedLog partition=__consumer_offsets-26, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:09:15,969] INFO Created log for partition __consumer_offsets-26 in /tmp/kraft-combined-logs/__consumer_offsets-26 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:15,969] INFO [Partition __consumer_offsets-26 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-26 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:15,969] INFO [Partition __consumer_offsets-26 broker=1] Log loaded for partition __consumer_offsets-26 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:15,969] INFO Setting topicIdPartition 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-26 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:09:15,969] INFO [MergedLog partition=__consumer_offsets-26, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-26 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:09:15,969] INFO [Broker id=1] Leader __consumer_offsets-26 with topic id Some(3VYZksSVS72hxNY0NkQPGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:15,970] INFO [MergedLog partition=__consumer_offsets-24, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:09:15,970] INFO Created log for partition __consumer_offsets-24 in /tmp/kraft-combined-logs/__consumer_offsets-24 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:15,970] INFO [Partition __consumer_offsets-24 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-24 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:15,970] INFO [Partition __consumer_offsets-24 broker=1] Log loaded for partition __consumer_offsets-24 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:15,970] INFO Setting topicIdPartition 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-24 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:09:15,970] INFO [MergedLog partition=__consumer_offsets-24, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-24 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:09:15,970] INFO [Broker id=1] Leader __consumer_offsets-24 with topic id Some(3VYZksSVS72hxNY0NkQPGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:15,971] INFO [MergedLog partition=__consumer_offsets-23, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:09:15,972] INFO Created log for partition __consumer_offsets-23 in /tmp/kraft-combined-logs/__consumer_offsets-23 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:15,972] INFO [Partition __consumer_offsets-23 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-23 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:15,972] INFO [Partition __consumer_offsets-23 broker=1] Log loaded for partition __consumer_offsets-23 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:15,972] INFO Setting topicIdPartition 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-23 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:09:15,972] INFO [MergedLog partition=__consumer_offsets-23, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-23 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:09:15,972] INFO [Broker id=1] Leader __consumer_offsets-23 with topic id Some(3VYZksSVS72hxNY0NkQPGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:15,973] INFO [MergedLog partition=__consumer_offsets-20, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:09:15,973] INFO Created log for partition __consumer_offsets-20 in /tmp/kraft-combined-logs/__consumer_offsets-20 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:15,973] INFO [Partition __consumer_offsets-20 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-20 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:15,973] INFO [Partition __consumer_offsets-20 broker=1] Log loaded for partition __consumer_offsets-20 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:15,973] INFO Setting topicIdPartition 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-20 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:09:15,973] INFO [MergedLog partition=__consumer_offsets-20, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-20 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:09:15,973] INFO [Broker id=1] Leader __consumer_offsets-20 with topic id Some(3VYZksSVS72hxNY0NkQPGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:15,974] INFO [MergedLog partition=__consumer_offsets-30, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:09:15,974] INFO Created log for partition __consumer_offsets-30 in /tmp/kraft-combined-logs/__consumer_offsets-30 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:15,975] INFO [Partition __consumer_offsets-30 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-30 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:15,975] INFO [Partition __consumer_offsets-30 broker=1] Log loaded for partition __consumer_offsets-30 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:15,975] INFO Setting topicIdPartition 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-30 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:09:15,975] INFO [MergedLog partition=__consumer_offsets-30, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-30 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:09:15,975] INFO [Broker id=1] Leader __consumer_offsets-30 with topic id Some(3VYZksSVS72hxNY0NkQPGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:15,976] INFO [MergedLog partition=__consumer_offsets-44, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:09:15,976] INFO Created log for partition __consumer_offsets-44 in /tmp/kraft-combined-logs/__consumer_offsets-44 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:15,976] INFO [Partition __consumer_offsets-44 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-44 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:15,976] INFO [Partition __consumer_offsets-44 broker=1] Log loaded for partition __consumer_offsets-44 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:15,976] INFO Setting topicIdPartition 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-44 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:09:15,976] INFO [MergedLog partition=__consumer_offsets-44, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-44 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:09:15,976] INFO [Broker id=1] Leader __consumer_offsets-44 with topic id Some(3VYZksSVS72hxNY0NkQPGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:15,977] INFO [MergedLog partition=__consumer_offsets-31, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:09:15,977] INFO Created log for partition __consumer_offsets-31 in /tmp/kraft-combined-logs/__consumer_offsets-31 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:15,977] INFO [Partition __consumer_offsets-31 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-31 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:15,977] INFO [Partition __consumer_offsets-31 broker=1] Log loaded for partition __consumer_offsets-31 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:15,977] INFO Setting topicIdPartition 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-31 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:09:15,977] INFO [MergedLog partition=__consumer_offsets-31, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-31 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:09:15,977] INFO [Broker id=1] Leader __consumer_offsets-31 with topic id Some(3VYZksSVS72hxNY0NkQPGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:15,978] INFO [MergedLog partition=__consumer_offsets-18, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:09:15,978] INFO Created log for partition __consumer_offsets-18 in /tmp/kraft-combined-logs/__consumer_offsets-18 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:15,978] INFO [Partition __consumer_offsets-18 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-18 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:15,978] INFO [Partition __consumer_offsets-18 broker=1] Log loaded for partition __consumer_offsets-18 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:15,978] INFO Setting topicIdPartition 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-18 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:09:15,978] INFO [MergedLog partition=__consumer_offsets-18, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-18 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:09:15,978] INFO [Broker id=1] Leader __consumer_offsets-18 with topic id Some(3VYZksSVS72hxNY0NkQPGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:15,980] INFO [MergedLog partition=__consumer_offsets-36, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:09:15,980] INFO Created log for partition __consumer_offsets-36 in /tmp/kraft-combined-logs/__consumer_offsets-36 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:15,980] INFO [Partition __consumer_offsets-36 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-36 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:15,980] INFO [Partition __consumer_offsets-36 broker=1] Log loaded for partition __consumer_offsets-36 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:15,980] INFO Setting topicIdPartition 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-36 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:09:15,980] INFO [MergedLog partition=__consumer_offsets-36, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-36 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:09:15,980] INFO [Broker id=1] Leader __consumer_offsets-36 with topic id Some(3VYZksSVS72hxNY0NkQPGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:15,981] INFO [MergedLog partition=__consumer_offsets-2, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:09:15,981] INFO Created log for partition __consumer_offsets-2 in /tmp/kraft-combined-logs/__consumer_offsets-2 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:15,981] INFO [Partition __consumer_offsets-2 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-2 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:15,981] INFO [Partition __consumer_offsets-2 broker=1] Log loaded for partition __consumer_offsets-2 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:15,981] INFO Setting topicIdPartition 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-2 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:09:15,981] INFO [MergedLog partition=__consumer_offsets-2, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-2 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:09:15,981] INFO [Broker id=1] Leader __consumer_offsets-2 with topic id Some(3VYZksSVS72hxNY0NkQPGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:15,982] INFO [MergedLog partition=__consumer_offsets-39, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:09:15,982] INFO Created log for partition __consumer_offsets-39 in /tmp/kraft-combined-logs/__consumer_offsets-39 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:15,983] INFO [Partition __consumer_offsets-39 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-39 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:15,983] INFO [Partition __consumer_offsets-39 broker=1] Log loaded for partition __consumer_offsets-39 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:15,983] INFO Setting topicIdPartition 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-39 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:09:15,983] INFO [MergedLog partition=__consumer_offsets-39, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-39 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:09:15,983] INFO [Broker id=1] Leader __consumer_offsets-39 with topic id Some(3VYZksSVS72hxNY0NkQPGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:15,983] INFO [MergedLog partition=__consumer_offsets-32, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:09:15,984] INFO Created log for partition __consumer_offsets-32 in /tmp/kraft-combined-logs/__consumer_offsets-32 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:15,984] INFO [Partition __consumer_offsets-32 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-32 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:15,984] INFO [Partition __consumer_offsets-32 broker=1] Log loaded for partition __consumer_offsets-32 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:15,984] INFO Setting topicIdPartition 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-32 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:09:15,984] INFO [MergedLog partition=__consumer_offsets-32, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-32 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:09:15,984] INFO [Broker id=1] Leader __consumer_offsets-32 with topic id Some(3VYZksSVS72hxNY0NkQPGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:15,985] INFO [MergedLog partition=__consumer_offsets-6, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:09:15,985] INFO Created log for partition __consumer_offsets-6 in /tmp/kraft-combined-logs/__consumer_offsets-6 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:15,985] INFO [Partition __consumer_offsets-6 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-6 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:15,985] INFO [Partition __consumer_offsets-6 broker=1] Log loaded for partition __consumer_offsets-6 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:15,985] INFO Setting topicIdPartition 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-6 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:09:15,985] INFO [MergedLog partition=__consumer_offsets-6, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-6 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:09:15,985] INFO [Broker id=1] Leader __consumer_offsets-6 with topic id Some(3VYZksSVS72hxNY0NkQPGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:15,986] INFO [MergedLog partition=__consumer_offsets-8, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:09:15,986] INFO Created log for partition __consumer_offsets-8 in /tmp/kraft-combined-logs/__consumer_offsets-8 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:15,986] INFO [Partition __consumer_offsets-8 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-8 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:15,986] INFO [Partition __consumer_offsets-8 broker=1] Log loaded for partition __consumer_offsets-8 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:15,986] INFO Setting topicIdPartition 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-8 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:09:15,986] INFO [MergedLog partition=__consumer_offsets-8, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-8 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:09:15,986] INFO [Broker id=1] Leader __consumer_offsets-8 with topic id Some(3VYZksSVS72hxNY0NkQPGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:15,987] INFO [MergedLog partition=__consumer_offsets-7, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:09:15,987] INFO Created log for partition __consumer_offsets-7 in /tmp/kraft-combined-logs/__consumer_offsets-7 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:15,987] INFO [Partition __consumer_offsets-7 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-7 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:15,987] INFO [Partition __consumer_offsets-7 broker=1] Log loaded for partition __consumer_offsets-7 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:15,988] INFO Setting topicIdPartition 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-7 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:09:15,988] INFO [MergedLog partition=__consumer_offsets-7, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-7 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:09:15,988] INFO [Broker id=1] Leader __consumer_offsets-7 with topic id Some(3VYZksSVS72hxNY0NkQPGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:15,989] INFO [MergedLog partition=__consumer_offsets-43, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:09:15,989] INFO Created log for partition __consumer_offsets-43 in /tmp/kraft-combined-logs/__consumer_offsets-43 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:15,989] INFO [Partition __consumer_offsets-43 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-43 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:15,989] INFO [Partition __consumer_offsets-43 broker=1] Log loaded for partition __consumer_offsets-43 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:15,989] INFO Setting topicIdPartition 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-43 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:09:15,989] INFO [MergedLog partition=__consumer_offsets-43, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-43 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:09:15,989] INFO [Broker id=1] Leader __consumer_offsets-43 with topic id Some(3VYZksSVS72hxNY0NkQPGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:15,990] INFO [MergedLog partition=__consumer_offsets-5, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:09:15,990] INFO Created log for partition __consumer_offsets-5 in /tmp/kraft-combined-logs/__consumer_offsets-5 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:15,990] INFO [Partition __consumer_offsets-5 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-5 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:15,990] INFO [Partition __consumer_offsets-5 broker=1] Log loaded for partition __consumer_offsets-5 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:15,990] INFO Setting topicIdPartition 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-5 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:09:15,990] INFO [MergedLog partition=__consumer_offsets-5, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-5 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:09:15,990] INFO [Broker id=1] Leader __consumer_offsets-5 with topic id Some(3VYZksSVS72hxNY0NkQPGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:15,991] INFO [MergedLog partition=__consumer_offsets-15, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:09:15,991] INFO Created log for partition __consumer_offsets-15 in /tmp/kraft-combined-logs/__consumer_offsets-15 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:15,991] INFO [Partition __consumer_offsets-15 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-15 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:15,991] INFO [Partition __consumer_offsets-15 broker=1] Log loaded for partition __consumer_offsets-15 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:15,991] INFO Setting topicIdPartition 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-15 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:09:15,991] INFO [MergedLog partition=__consumer_offsets-15, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-15 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:09:15,991] INFO [Broker id=1] Leader __consumer_offsets-15 with topic id Some(3VYZksSVS72hxNY0NkQPGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:15,992] INFO [MergedLog partition=__consumer_offsets-1, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:09:15,992] INFO Created log for partition __consumer_offsets-1 in /tmp/kraft-combined-logs/__consumer_offsets-1 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:15,992] INFO [Partition __consumer_offsets-1 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-1 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:15,992] INFO [Partition __consumer_offsets-1 broker=1] Log loaded for partition __consumer_offsets-1 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:15,992] INFO Setting topicIdPartition 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-1 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:09:15,992] INFO [MergedLog partition=__consumer_offsets-1, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-1 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:09:15,992] INFO [Broker id=1] Leader __consumer_offsets-1 with topic id Some(3VYZksSVS72hxNY0NkQPGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:15,994] INFO [MergedLog partition=__consumer_offsets-49, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:09:15,994] INFO Created log for partition __consumer_offsets-49 in /tmp/kraft-combined-logs/__consumer_offsets-49 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:15,994] INFO [Partition __consumer_offsets-49 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-49 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:15,994] INFO [Partition __consumer_offsets-49 broker=1] Log loaded for partition __consumer_offsets-49 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:15,995] INFO Setting topicIdPartition 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-49 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:09:15,995] INFO [MergedLog partition=__consumer_offsets-49, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-49 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:09:15,995] INFO [Broker id=1] Leader __consumer_offsets-49 with topic id Some(3VYZksSVS72hxNY0NkQPGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:15,995] INFO [MergedLog partition=__consumer_offsets-21, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:09:15,996] INFO Created log for partition __consumer_offsets-21 in /tmp/kraft-combined-logs/__consumer_offsets-21 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:15,996] INFO [Partition __consumer_offsets-21 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-21 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:15,996] INFO [Partition __consumer_offsets-21 broker=1] Log loaded for partition __consumer_offsets-21 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:15,996] INFO Setting topicIdPartition 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-21 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:09:15,996] INFO [MergedLog partition=__consumer_offsets-21, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-21 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:09:15,996] INFO [Broker id=1] Leader __consumer_offsets-21 with topic id Some(3VYZksSVS72hxNY0NkQPGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:15,996] INFO [MergedLog partition=__consumer_offsets-16, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:09:15,997] INFO Created log for partition __consumer_offsets-16 in /tmp/kraft-combined-logs/__consumer_offsets-16 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:15,997] INFO [Partition __consumer_offsets-16 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-16 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:15,997] INFO [Partition __consumer_offsets-16 broker=1] Log loaded for partition __consumer_offsets-16 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:15,997] INFO Setting topicIdPartition 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-16 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:09:15,997] INFO [MergedLog partition=__consumer_offsets-16, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-16 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:09:15,997] INFO [Broker id=1] Leader __consumer_offsets-16 with topic id Some(3VYZksSVS72hxNY0NkQPGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:15,998] INFO [MergedLog partition=__consumer_offsets-34, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:09:15,998] INFO Created log for partition __consumer_offsets-34 in /tmp/kraft-combined-logs/__consumer_offsets-34 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:15,998] INFO [Partition __consumer_offsets-34 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-34 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:15,998] INFO [Partition __consumer_offsets-34 broker=1] Log loaded for partition __consumer_offsets-34 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:15,998] INFO Setting topicIdPartition 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-34 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:09:15,998] INFO [MergedLog partition=__consumer_offsets-34, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-34 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:09:15,998] INFO [Broker id=1] Leader __consumer_offsets-34 with topic id Some(3VYZksSVS72hxNY0NkQPGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:15,999] INFO [MergedLog partition=__consumer_offsets-48, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:09:15,999] INFO Created log for partition __consumer_offsets-48 in /tmp/kraft-combined-logs/__consumer_offsets-48 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:15,999] INFO [Partition __consumer_offsets-48 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-48 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:15,999] INFO [Partition __consumer_offsets-48 broker=1] Log loaded for partition __consumer_offsets-48 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:15,999] INFO Setting topicIdPartition 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-48 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:09:15,999] INFO [MergedLog partition=__consumer_offsets-48, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-48 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:09:15,999] INFO [Broker id=1] Leader __consumer_offsets-48 with topic id Some(3VYZksSVS72hxNY0NkQPGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:16,000] INFO [MergedLog partition=__consumer_offsets-38, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:09:16,001] INFO Created log for partition __consumer_offsets-38 in /tmp/kraft-combined-logs/__consumer_offsets-38 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,001] INFO [Partition __consumer_offsets-38 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-38 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:16,001] INFO [Partition __consumer_offsets-38 broker=1] Log loaded for partition __consumer_offsets-38 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:16,001] INFO Setting topicIdPartition 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-38 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:09:16,001] INFO [MergedLog partition=__consumer_offsets-38, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-38 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:09:16,001] INFO [Broker id=1] Leader __consumer_offsets-38 with topic id Some(3VYZksSVS72hxNY0NkQPGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:16,002] INFO [MergedLog partition=__consumer_offsets-10, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:09:16,002] INFO Created log for partition __consumer_offsets-10 in /tmp/kraft-combined-logs/__consumer_offsets-10 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,002] INFO [Partition __consumer_offsets-10 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-10 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:16,002] INFO [Partition __consumer_offsets-10 broker=1] Log loaded for partition __consumer_offsets-10 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:16,003] INFO Setting topicIdPartition 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-10 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:09:16,003] INFO [MergedLog partition=__consumer_offsets-10, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-10 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:09:16,003] INFO [Broker id=1] Leader __consumer_offsets-10 with topic id Some(3VYZksSVS72hxNY0NkQPGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:16,004] INFO [MergedLog partition=__consumer_offsets-40, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:09:16,004] INFO Created log for partition __consumer_offsets-40 in /tmp/kraft-combined-logs/__consumer_offsets-40 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,004] INFO [Partition __consumer_offsets-40 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-40 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:16,004] INFO [Partition __consumer_offsets-40 broker=1] Log loaded for partition __consumer_offsets-40 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:16,004] INFO Setting topicIdPartition 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-40 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:09:16,004] INFO [MergedLog partition=__consumer_offsets-40, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-40 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:09:16,004] INFO [Broker id=1] Leader __consumer_offsets-40 with topic id Some(3VYZksSVS72hxNY0NkQPGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:16,005] INFO [MergedLog partition=__consumer_offsets-37, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:09:16,005] INFO Created log for partition __consumer_offsets-37 in /tmp/kraft-combined-logs/__consumer_offsets-37 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,005] INFO [Partition __consumer_offsets-37 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-37 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:16,005] INFO [Partition __consumer_offsets-37 broker=1] Log loaded for partition __consumer_offsets-37 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:16,005] INFO Setting topicIdPartition 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-37 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:09:16,005] INFO [MergedLog partition=__consumer_offsets-37, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-37 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:09:16,005] INFO [Broker id=1] Leader __consumer_offsets-37 with topic id Some(3VYZksSVS72hxNY0NkQPGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:16,006] INFO [MergedLog partition=__consumer_offsets-25, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:09:16,006] INFO Created log for partition __consumer_offsets-25 in /tmp/kraft-combined-logs/__consumer_offsets-25 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,006] INFO [Partition __consumer_offsets-25 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-25 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:16,006] INFO [Partition __consumer_offsets-25 broker=1] Log loaded for partition __consumer_offsets-25 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:16,006] INFO Setting topicIdPartition 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-25 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:09:16,007] INFO [MergedLog partition=__consumer_offsets-25, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-25 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:09:16,007] INFO [Broker id=1] Leader __consumer_offsets-25 with topic id Some(3VYZksSVS72hxNY0NkQPGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:16,007] INFO [MergedLog partition=__consumer_offsets-12, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:09:16,007] INFO Created log for partition __consumer_offsets-12 in /tmp/kraft-combined-logs/__consumer_offsets-12 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,007] INFO [Partition __consumer_offsets-12 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-12 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:16,007] INFO [Partition __consumer_offsets-12 broker=1] Log loaded for partition __consumer_offsets-12 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:16,008] INFO Setting topicIdPartition 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-12 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:09:16,008] INFO [MergedLog partition=__consumer_offsets-12, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-12 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:09:16,008] INFO [Broker id=1] Leader __consumer_offsets-12 with topic id Some(3VYZksSVS72hxNY0NkQPGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:16,008] INFO [MergedLog partition=__consumer_offsets-4, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:09:16,008] INFO Created log for partition __consumer_offsets-4 in /tmp/kraft-combined-logs/__consumer_offsets-4 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,009] INFO [Partition __consumer_offsets-4 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-4 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:16,009] INFO [Partition __consumer_offsets-4 broker=1] Log loaded for partition __consumer_offsets-4 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:16,009] INFO Setting topicIdPartition 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-4 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:09:16,009] INFO [MergedLog partition=__consumer_offsets-4, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-4 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:09:16,009] INFO [Broker id=1] Leader __consumer_offsets-4 with topic id Some(3VYZksSVS72hxNY0NkQPGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:16,009] INFO [MergedLog partition=__consumer_offsets-47, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:09:16,010] INFO Created log for partition __consumer_offsets-47 in /tmp/kraft-combined-logs/__consumer_offsets-47 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,010] INFO [Partition __consumer_offsets-47 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-47 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:16,010] INFO [Partition __consumer_offsets-47 broker=1] Log loaded for partition __consumer_offsets-47 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:16,010] INFO Setting topicIdPartition 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-47 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:09:16,010] INFO [MergedLog partition=__consumer_offsets-47, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-47 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:09:16,010] INFO [Broker id=1] Leader __consumer_offsets-47 with topic id Some(3VYZksSVS72hxNY0NkQPGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:16,011] INFO [MergedLog partition=__consumer_offsets-13, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:09:16,011] INFO Created log for partition __consumer_offsets-13 in /tmp/kraft-combined-logs/__consumer_offsets-13 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,011] INFO [Partition __consumer_offsets-13 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-13 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:16,011] INFO [Partition __consumer_offsets-13 broker=1] Log loaded for partition __consumer_offsets-13 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:16,011] INFO Setting topicIdPartition 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-13 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:09:16,011] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,011] INFO [MergedLog partition=__consumer_offsets-13, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-13 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:09:16,011] INFO [Broker id=1] Leader __consumer_offsets-13 with topic id Some(3VYZksSVS72hxNY0NkQPGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:16,012] INFO [MergedLog partition=__consumer_offsets-9, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:09:16,012] INFO Created log for partition __consumer_offsets-9 in /tmp/kraft-combined-logs/__consumer_offsets-9 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,012] INFO [Partition __consumer_offsets-9 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-9 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:16,012] INFO [Partition __consumer_offsets-9 broker=1] Log loaded for partition __consumer_offsets-9 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:16,012] INFO Setting topicIdPartition 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-9 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:09:16,013] INFO [MergedLog partition=__consumer_offsets-9, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-9 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:09:16,013] INFO [Broker id=1] Leader __consumer_offsets-9 with topic id Some(3VYZksSVS72hxNY0NkQPGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:16,014] INFO [MergedLog partition=__consumer_offsets-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:09:16,014] INFO Created log for partition __consumer_offsets-0 in /tmp/kraft-combined-logs/__consumer_offsets-0 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,014] INFO [Partition __consumer_offsets-0 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:16,014] INFO [Partition __consumer_offsets-0 broker=1] Log loaded for partition __consumer_offsets-0 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:16,014] INFO Setting topicIdPartition 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-0 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:09:16,014] INFO [MergedLog partition=__consumer_offsets-0, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-0 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:09:16,014] INFO [Broker id=1] Leader __consumer_offsets-0 with topic id Some(3VYZksSVS72hxNY0NkQPGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:16,016] INFO [MergedLog partition=__consumer_offsets-45, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:09:16,016] INFO Created log for partition __consumer_offsets-45 in /tmp/kraft-combined-logs/__consumer_offsets-45 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,016] INFO [Partition __consumer_offsets-45 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-45 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:16,016] INFO [Partition __consumer_offsets-45 broker=1] Log loaded for partition __consumer_offsets-45 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:16,016] INFO Setting topicIdPartition 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-45 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:09:16,016] INFO [MergedLog partition=__consumer_offsets-45, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-45 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:09:16,016] INFO [Broker id=1] Leader __consumer_offsets-45 with topic id Some(3VYZksSVS72hxNY0NkQPGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:16,016] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Group coordinator broker:29092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: error response NOT_COORDINATOR. isDisconnected: false. Rediscovery will be attempted. (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,016] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Requesting disconnect from last known coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,016] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] JoinGroup failed: This is not the correct coordinator. Marking coordinator unknown. Sent generation was Generation{generationId=-1, memberId='', protocol='null'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,016] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Client requested disconnect from node 2147483646 (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:09:16,017] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,017] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Group coordinator broker:29092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: false. Rediscovery will be attempted. (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,017] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Requesting disconnect from last known coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,017] INFO [MergedLog partition=__consumer_offsets-17, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:09:16,017] INFO Created log for partition __consumer_offsets-17 in /tmp/kraft-combined-logs/__consumer_offsets-17 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,017] INFO [Partition __consumer_offsets-17 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-17 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:16,018] INFO [Partition __consumer_offsets-17 broker=1] Log loaded for partition __consumer_offsets-17 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:16,018] INFO Setting topicIdPartition 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-17 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:09:16,018] INFO [MergedLog partition=__consumer_offsets-17, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-17 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:09:16,018] INFO [Broker id=1] Leader __consumer_offsets-17 with topic id Some(3VYZksSVS72hxNY0NkQPGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:16,019] INFO [MergedLog partition=__consumer_offsets-29, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:09:16,019] INFO Created log for partition __consumer_offsets-29 in /tmp/kraft-combined-logs/__consumer_offsets-29 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,019] INFO [Partition __consumer_offsets-29 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-29 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:16,019] INFO [Partition __consumer_offsets-29 broker=1] Log loaded for partition __consumer_offsets-29 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:16,019] INFO Setting topicIdPartition 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-29 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:09:16,019] INFO [MergedLog partition=__consumer_offsets-29, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-29 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:09:16,019] INFO [Broker id=1] Leader __consumer_offsets-29 with topic id Some(3VYZksSVS72hxNY0NkQPGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:16,020] INFO [MergedLog partition=__consumer_offsets-42, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:09:16,020] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,020] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Group coordinator broker:29092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: false. Rediscovery will be attempted. (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,020] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Requesting disconnect from last known coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,020] INFO Created log for partition __consumer_offsets-42 in /tmp/kraft-combined-logs/__consumer_offsets-42 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,020] INFO [Partition __consumer_offsets-42 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-42 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:16,020] INFO [Partition __consumer_offsets-42 broker=1] Log loaded for partition __consumer_offsets-42 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:16,021] INFO Setting topicIdPartition 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-42 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:09:16,021] INFO [MergedLog partition=__consumer_offsets-42, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-42 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:09:16,021] INFO [Broker id=1] Leader __consumer_offsets-42 with topic id Some(3VYZksSVS72hxNY0NkQPGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:16,022] INFO [MergedLog partition=__consumer_offsets-46, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:09:16,022] INFO Created log for partition __consumer_offsets-46 in /tmp/kraft-combined-logs/__consumer_offsets-46 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,022] INFO [Partition __consumer_offsets-46 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-46 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:16,022] INFO [Partition __consumer_offsets-46 broker=1] Log loaded for partition __consumer_offsets-46 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:16,022] INFO Setting topicIdPartition 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-46 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:09:16,022] INFO [MergedLog partition=__consumer_offsets-46, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-46 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:09:16,022] INFO [Broker id=1] Leader __consumer_offsets-46 with topic id Some(3VYZksSVS72hxNY0NkQPGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:16,023] INFO [MergedLog partition=__consumer_offsets-14, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:09:16,023] INFO Created log for partition __consumer_offsets-14 in /tmp/kraft-combined-logs/__consumer_offsets-14 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,023] INFO [Partition __consumer_offsets-14 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-14 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:16,023] INFO [Partition __consumer_offsets-14 broker=1] Log loaded for partition __consumer_offsets-14 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:16,023] INFO Setting topicIdPartition 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-14 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:09:16,023] INFO [MergedLog partition=__consumer_offsets-14, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-14 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:09:16,024] INFO [Broker id=1] Leader __consumer_offsets-14 with topic id Some(3VYZksSVS72hxNY0NkQPGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:16,025] INFO [MergedLog partition=__consumer_offsets-33, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:09:16,025] INFO Created log for partition __consumer_offsets-33 in /tmp/kraft-combined-logs/__consumer_offsets-33 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,025] INFO [Partition __consumer_offsets-33 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-33 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:16,025] INFO [Partition __consumer_offsets-33 broker=1] Log loaded for partition __consumer_offsets-33 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:16,025] INFO Setting topicIdPartition 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-33 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:09:16,025] INFO [MergedLog partition=__consumer_offsets-33, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-33 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:09:16,025] INFO [Broker id=1] Leader __consumer_offsets-33 with topic id Some(3VYZksSVS72hxNY0NkQPGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:16,026] INFO [MergedLog partition=__consumer_offsets-41, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.MergedLog$)
benchi-kafka     | [2025-03-18 16:09:16,026] INFO Created log for partition __consumer_offsets-41 in /tmp/kraft-combined-logs/__consumer_offsets-41 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", delete.retention.ms=86400000, max.compaction.lag.ms=9223372036854775807, min.cleanable.dirty.ratio=0.5, segment.bytes=104857600} (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,026] INFO [Partition __consumer_offsets-41 broker=1] No checkpointed highwatermark is found for partition __consumer_offsets-41 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:16,026] INFO [Partition __consumer_offsets-41 broker=1] Log loaded for partition __consumer_offsets-41 with initial high watermark 0 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:16,026] INFO Setting topicIdPartition 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-41 (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:09:16,026] INFO [MergedLog partition=__consumer_offsets-41, dir=/tmp/kraft-combined-logs] Initializing tier metadata without recovery for __consumer_offsets-41 because, either the recovery is active (false) or local log start offset 0 and check-pointed log start offset 0 do not indicate any missing tier metadata. (kafka.log.MergedLog)
benchi-kafka     | [2025-03-18 16:09:16,026] INFO [Broker id=1] Leader __consumer_offsets-41 with topic id Some(3VYZksSVS72hxNY0NkQPGQ) starts at leader epoch 0 from offset 0 with partition epoch 0, high watermark 0, ISR [1], adding replicas [], removing replicas [] , and recovery state RECOVERED. Previous leader epoch was -1. (state.change.logger)
benchi-kafka     | [2025-03-18 16:09:16,027] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 13 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,027] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-13 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,027] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 46 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,028] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-46 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,028] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 9 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,028] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-9 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,028] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 42 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,028] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-42 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,028] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 21 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,028] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-21 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,028] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 17 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,028] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-17 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,028] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 30 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,028] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-30 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,029] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 26 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,029] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-26 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,029] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 5 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,029] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-5 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,029] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 38 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,029] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-38 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,029] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 1 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,029] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-1 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,029] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 34 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,029] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-34 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,029] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 16 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,029] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-16 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,030] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 45 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,030] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-45 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,030] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 12 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,030] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-12 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,030] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 41 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,030] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-41 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,030] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 24 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,030] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-24 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,030] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 20 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,030] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-20 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,030] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 49 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,030] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-49 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,031] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 0 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,031] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-0 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,031] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 29 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,031] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-29 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,031] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 25 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,031] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-25 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,031] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 8 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,031] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-8 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,032] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 37 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,032] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-37 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,032] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 4 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,032] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-4 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,032] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,032] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Group coordinator broker:29092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: false. Rediscovery will be attempted. (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,032] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Requesting disconnect from last known coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,032] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 33 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,032] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-33 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,033] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 15 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,033] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-15 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,033] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 48 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,033] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-48 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,033] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 11 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,033] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-11 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,033] INFO The cleaning for partition __consumer_offsets-16 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,033] INFO The cleaning for partition __consumer_offsets-45 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,033] INFO The cleaning for partition __consumer_offsets-8 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,033] INFO The cleaning for partition __consumer_offsets-11 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,033] INFO The cleaning for partition __consumer_offsets-38 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,033] INFO The cleaning for partition __consumer_offsets-1 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,034] INFO The cleaning for partition __consumer_offsets-12 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,034] INFO The cleaning for partition __consumer_offsets-34 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,033] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 44 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,034] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-44 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,033] INFO The cleaning for partition __consumer_offsets-46 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,033] INFO The cleaning for partition __consumer_offsets-29 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,034] INFO The cleaning for partition __consumer_offsets-42 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,033] INFO The cleaning for partition __consumer_offsets-48 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,034] INFO The cleaning for partition __consumer_offsets-49 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,034] INFO The cleaning for partition __consumer_offsets-15 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,034] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 23 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,034] INFO The cleaning for partition __consumer_offsets-21 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,034] INFO The cleaning for partition __consumer_offsets-13 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,035] INFO The cleaning for partition __consumer_offsets-0 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,035] INFO The cleaning for partition __consumer_offsets-37 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,035] INFO The cleaning for partition __consumer_offsets-4 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,035] INFO The cleaning for partition __consumer_offsets-41 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,035] INFO The cleaning for partition __consumer_offsets-33 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,035] INFO The cleaning for partition __consumer_offsets-44 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,035] INFO The cleaning for partition __consumer_offsets-20 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,034] INFO The cleaning for partition __consumer_offsets-26 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,035] INFO The cleaning for partition __consumer_offsets-25 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,035] INFO The cleaning for partition __consumer_offsets-24 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,034] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-23 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,035] INFO The cleaning for partition __consumer_offsets-17 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,035] INFO The cleaning for partition __consumer_offsets-5 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,036] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 19 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,036] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-19 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,036] INFO The cleaning for partition __consumer_offsets-30 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,036] INFO The cleaning for partition __consumer_offsets-23 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,036] INFO The cleaning for partition __consumer_offsets-9 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,036] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 32 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,036] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-32 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,036] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 28 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,037] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-28 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,037] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 7 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,037] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-7 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,037] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 40 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,037] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-40 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,037] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 3 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,037] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-3 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,037] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 36 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,037] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-36 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,037] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 47 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,037] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-47 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,037] INFO The cleaning for partition __consumer_offsets-19 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,037] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 14 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,037] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-14 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,037] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 43 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,037] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-43 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,037] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 10 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,037] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-10 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,037] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 22 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,037] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-22 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,037] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 18 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,037] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-18 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,037] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 31 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,037] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-31 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,037] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 27 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,037] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-27 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,037] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 39 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,037] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-39 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,037] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 6 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,037] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-6 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,037] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 35 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,037] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-35 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,037] INFO [GroupCoordinator 1]: Elected as the group coordinator for partition 2 in epoch 0 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,037] INFO [GroupMetadataManager brokerId=1] Scheduling loading of offsets and group metadata from __consumer_offsets-2 for epoch 0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,037] INFO [DynamicConfigPublisher broker id=1] Updating topic __consumer_offsets with new configuration : compression.type -> producer,cleanup.policy -> compact,max.compaction.lag.ms -> 9223372036854775807,segment.bytes -> 104857600,confluent.placement.constraints -> ,min.cleanable.dirty.ratio -> 0.5,delete.retention.ms -> 86400000 (kafka.server.metadata.DynamicConfigPublisher)
benchi-kafka     | [2025-03-18 16:09:16,040] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-23 in 4 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,040] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-5 in 10 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,040] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-42 in 11 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,040] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-13 in 13 milliseconds for epoch 0, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,040] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-46 in 11 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,040] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-17 in 11 milliseconds for epoch 0, of which 7 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,040] INFO Cleaning for partition __consumer_offsets-23 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,040] INFO Cleaning for partition __consumer_offsets-5 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,040] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-29 in 9 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,040] INFO Cleaning for partition __consumer_offsets-29 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,040] INFO Cleaning for partition __consumer_offsets-46 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,040] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-12 in 10 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,040] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-37 in 8 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,041] INFO Cleaning for partition __consumer_offsets-37 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,041] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-49 in 11 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,041] INFO Cleaning for partition __consumer_offsets-49 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,041] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-8 in 10 milliseconds for epoch 0, of which 2 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,041] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-44 in 5 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,041] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-1 in 12 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,041] INFO Cleaning for partition __consumer_offsets-44 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,041] INFO Cleaning for partition __consumer_offsets-1 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,040] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-11 in 7 milliseconds for epoch 0, of which 0 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,041] INFO Cleaning for partition __consumer_offsets-11 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,040] INFO Cleaning for partition __consumer_offsets-17 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,041] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-33 in 9 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,041] INFO Cleaning for partition __consumer_offsets-33 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,041] INFO Cleaning for partition __consumer_offsets-42 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,041] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-26 in 12 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,041] INFO Cleaning for partition __consumer_offsets-26 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,041] INFO Cleaning for partition __consumer_offsets-8 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,040] INFO Cleaning for partition __consumer_offsets-12 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,041] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-34 in 10 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,041] INFO Cleaning for partition __consumer_offsets-34 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,041] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-4 in 9 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,041] INFO Cleaning for partition __consumer_offsets-4 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,041] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-25 in 10 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,040] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-41 in 9 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,041] INFO Cleaning for partition __consumer_offsets-41 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,040] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-0 in 9 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,041] INFO Cleaning for partition __consumer_offsets-0 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,040] INFO Cleaning for partition __consumer_offsets-13 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,040] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-45 in 10 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,042] INFO Cleaning for partition __consumer_offsets-45 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,042] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-20 in 9 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,042] INFO Cleaning for partition __consumer_offsets-20 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,042] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-24 in 9 milliseconds for epoch 0, of which 5 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,042] INFO Cleaning for partition __consumer_offsets-24 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,040] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-16 in 11 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,042] INFO Cleaning for partition __consumer_offsets-16 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,040] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-9 in 11 milliseconds for epoch 0, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,042] INFO Cleaning for partition __consumer_offsets-9 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,042] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-48 in 6 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,042] INFO Cleaning for partition __consumer_offsets-48 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,042] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-15 in 6 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,041] INFO Cleaning for partition __consumer_offsets-25 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,042] INFO Cleaning for partition __consumer_offsets-15 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,041] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-38 in 10 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,041] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-30 in 11 milliseconds for epoch 0, of which 8 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,043] INFO Cleaning for partition __consumer_offsets-30 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,041] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-19 in 5 milliseconds for epoch 0, of which 1 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,043] INFO Cleaning for partition __consumer_offsets-19 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,041] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-21 in 11 milliseconds for epoch 0, of which 6 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,043] INFO Cleaning for partition __consumer_offsets-38 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,043] INFO Cleaning for partition __consumer_offsets-21 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,043] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,043] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Group coordinator broker:29092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: false. Rediscovery will be attempted. (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,043] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Requesting disconnect from last known coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,046] INFO The cleaning for partition __consumer_offsets-32 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,046] INFO The cleaning for partition __consumer_offsets-28 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,046] INFO The cleaning for partition __consumer_offsets-47 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,046] INFO The cleaning for partition __consumer_offsets-36 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,046] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-47 in 9 milliseconds for epoch 0, of which 9 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,046] INFO Cleaning for partition __consumer_offsets-47 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,046] INFO The cleaning for partition __consumer_offsets-14 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,047] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-14 in 10 milliseconds for epoch 0, of which 9 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,047] INFO Cleaning for partition __consumer_offsets-14 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,046] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-36 in 9 milliseconds for epoch 0, of which 9 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,047] INFO Cleaning for partition __consumer_offsets-36 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,047] INFO The cleaning for partition __consumer_offsets-43 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,046] INFO The cleaning for partition __consumer_offsets-7 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,047] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-43 in 10 milliseconds for epoch 0, of which 10 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,047] INFO The cleaning for partition __consumer_offsets-27 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,047] INFO The cleaning for partition __consumer_offsets-6 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,047] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-27 in 10 milliseconds for epoch 0, of which 10 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,047] INFO The cleaning for partition __consumer_offsets-35 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,047] INFO Cleaning for partition __consumer_offsets-27 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,047] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-6 in 10 milliseconds for epoch 0, of which 10 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,047] INFO Cleaning for partition __consumer_offsets-6 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,047] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-35 in 10 milliseconds for epoch 0, of which 10 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,047] INFO The cleaning for partition __consumer_offsets-40 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,046] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-28 in 9 milliseconds for epoch 0, of which 9 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,047] INFO Cleaning for partition __consumer_offsets-28 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,046] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-32 in 10 milliseconds for epoch 0, of which 10 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,047] INFO Cleaning for partition __consumer_offsets-32 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,047] INFO Cleaning for partition __consumer_offsets-43 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,047] INFO The cleaning for partition __consumer_offsets-10 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,047] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-10 in 10 milliseconds for epoch 0, of which 10 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,047] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-40 in 10 milliseconds for epoch 0, of which 10 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,047] INFO Cleaning for partition __consumer_offsets-35 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,047] INFO The cleaning for partition __consumer_offsets-2 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,047] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-2 in 10 milliseconds for epoch 0, of which 10 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,047] INFO Cleaning for partition __consumer_offsets-2 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,047] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-7 in 10 milliseconds for epoch 0, of which 10 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,047] INFO The cleaning for partition __consumer_offsets-3 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,048] INFO Cleaning for partition __consumer_offsets-7 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,048] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-3 in 11 milliseconds for epoch 0, of which 11 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,048] INFO Cleaning for partition __consumer_offsets-3 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,047] INFO The cleaning for partition __consumer_offsets-39 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,047] INFO The cleaning for partition __consumer_offsets-31 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,048] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-39 in 11 milliseconds for epoch 0, of which 11 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,047] INFO The cleaning for partition __consumer_offsets-18 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,048] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-31 in 11 milliseconds for epoch 0, of which 11 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,048] INFO Cleaning for partition __consumer_offsets-31 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,047] INFO The cleaning for partition __consumer_offsets-22 is aborted and paused (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,048] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-18 in 11 milliseconds for epoch 0, of which 11 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,048] INFO Cleaning for partition __consumer_offsets-39 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,047] INFO Cleaning for partition __consumer_offsets-40 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,047] INFO Cleaning for partition __consumer_offsets-10 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,048] INFO [GroupMetadataManager brokerId=1] Finished loading offsets and group metadata from __consumer_offsets-22 in 11 milliseconds for epoch 0, of which 11 milliseconds was spent in the scheduler. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:09:16,048] INFO Cleaning for partition __consumer_offsets-22 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,048] INFO Cleaning for partition __consumer_offsets-18 is resumed (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:09:16,054] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,054] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Group coordinator broker:29092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: false. Rediscovery will be attempted. (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,054] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Requesting disconnect from last known coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,067] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Discovered group coordinator broker:29092 (id: 2147483646 rack: null) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,067] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Request joining group due to: rebalance failed due to 'This is not the correct coordinator.' (NotCoordinatorException) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,067] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,071] INFO [GroupCoordinator 1]: Dynamic member with unknown member id joins group ConfluentTelemetryReporterSampler--2946986101702943459 in Empty state. Created a new member id kafka-cruise-control-6e3c9224-dbc7-4d28-a92a-7bc65219219e and request the member to rejoin with this id. (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,072] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Request joining group due to: need to re-join with the given member-id: kafka-cruise-control-6e3c9224-dbc7-4d28-a92a-7bc65219219e (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,072] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] (Re-)joining group (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,075] INFO [GroupCoordinator 1]: Preparing to rebalance group ConfluentTelemetryReporterSampler--2946986101702943459 in state PreparingRebalance with old generation 0 (__consumer_offsets-23) (reason: Adding new member kafka-cruise-control-6e3c9224-dbc7-4d28-a92a-7bc65219219e with group instance id None; client reason: need to re-join with the given member-id: kafka-cruise-control-6e3c9224-dbc7-4d28-a92a-7bc65219219e) (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,077] INFO [GroupCoordinator 1]: Stabilized group ConfluentTelemetryReporterSampler--2946986101702943459 generation 1 (__consumer_offsets-23) with 1 members (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,077] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Successfully joined group with generation Generation{generationId=1, memberId='kafka-cruise-control-6e3c9224-dbc7-4d28-a92a-7bc65219219e', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,080] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Finished assignment for group at generation 1: {kafka-cruise-control-6e3c9224-dbc7-4d28-a92a-7bc65219219e=Assignment(partitions=[_confluent-telemetry-metrics-0, _confluent-telemetry-metrics-1, _confluent-telemetry-metrics-2, _confluent-telemetry-metrics-3, _confluent-telemetry-metrics-4, _confluent-telemetry-metrics-5, _confluent-telemetry-metrics-6, _confluent-telemetry-metrics-7, _confluent-telemetry-metrics-8, _confluent-telemetry-metrics-9, _confluent-telemetry-metrics-10, _confluent-telemetry-metrics-11])} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,083] INFO [GroupCoordinator 1]: Assignment received from leader kafka-cruise-control-6e3c9224-dbc7-4d28-a92a-7bc65219219e for group ConfluentTelemetryReporterSampler--2946986101702943459 for generation 1. The group has 1 members, 0 of which are static. (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,086] INFO [Partition __consumer_offsets-23 broker=1] roll: __consumer_offsets-23: first produce received, lastOffset: 0, leaderEpoch: 0, numMessages:1, time diff: 129 (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:16,086] INFO [Partition __consumer_offsets-23 broker=1] roll: __consumer_offsets-23: first HWM advanced, leaderEpoch: 0, old HW: 0, new HW: 1, become leader time: 1742314155957 ms, time diff: 129 ms (kafka.cluster.Partition)
benchi-kafka     | [2025-03-18 16:09:16,087] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Successfully synced group in generation Generation{generationId=1, memberId='kafka-cruise-control-6e3c9224-dbc7-4d28-a92a-7bc65219219e', protocol='range'} (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,087] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Notifying assignor about the new Assignment(partitions=[_confluent-telemetry-metrics-0, _confluent-telemetry-metrics-1, _confluent-telemetry-metrics-2, _confluent-telemetry-metrics-3, _confluent-telemetry-metrics-4, _confluent-telemetry-metrics-5, _confluent-telemetry-metrics-6, _confluent-telemetry-metrics-7, _confluent-telemetry-metrics-8, _confluent-telemetry-metrics-9, _confluent-telemetry-metrics-10, _confluent-telemetry-metrics-11]) (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,088] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Adding newly assigned partitions: _confluent-telemetry-metrics-0, _confluent-telemetry-metrics-1, _confluent-telemetry-metrics-2, _confluent-telemetry-metrics-3, _confluent-telemetry-metrics-4, _confluent-telemetry-metrics-5, _confluent-telemetry-metrics-6, _confluent-telemetry-metrics-7, _confluent-telemetry-metrics-8, _confluent-telemetry-metrics-9, _confluent-telemetry-metrics-10, _confluent-telemetry-metrics-11 (org.apache.kafka.clients.consumer.internals.ConsumerRebalanceListenerInvoker)
benchi-kafka     | [2025-03-18 16:09:16,091] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Found no committed offset for partition _confluent-telemetry-metrics-3 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,091] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Found no committed offset for partition _confluent-telemetry-metrics-4 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,091] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Found no committed offset for partition _confluent-telemetry-metrics-5 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,091] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Found no committed offset for partition _confluent-telemetry-metrics-6 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,091] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Found no committed offset for partition _confluent-telemetry-metrics-7 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,091] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Found no committed offset for partition _confluent-telemetry-metrics-8 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,091] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Found no committed offset for partition _confluent-telemetry-metrics-9 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,091] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Found no committed offset for partition _confluent-telemetry-metrics-10 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,091] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Found no committed offset for partition _confluent-telemetry-metrics-11 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,091] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Found no committed offset for partition _confluent-telemetry-metrics-0 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,091] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Found no committed offset for partition _confluent-telemetry-metrics-1 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,091] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Found no committed offset for partition _confluent-telemetry-metrics-2 (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
benchi-kafka     | [2025-03-18 16:09:16,606] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Seeking to offset 0 for partition _confluent-telemetry-metrics-3 (org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer)
benchi-kafka     | [2025-03-18 16:09:16,606] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Seeking to offset 0 for partition _confluent-telemetry-metrics-4 (org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer)
benchi-kafka     | [2025-03-18 16:09:16,606] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Seeking to offset 0 for partition _confluent-telemetry-metrics-5 (org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer)
benchi-kafka     | [2025-03-18 16:09:16,606] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Seeking to offset 0 for partition _confluent-telemetry-metrics-6 (org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer)
benchi-kafka     | [2025-03-18 16:09:16,606] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Seeking to offset 0 for partition _confluent-telemetry-metrics-7 (org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer)
benchi-kafka     | [2025-03-18 16:09:16,606] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Seeking to offset 0 for partition _confluent-telemetry-metrics-8 (org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer)
benchi-kafka     | [2025-03-18 16:09:16,606] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Seeking to offset 0 for partition _confluent-telemetry-metrics-9 (org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer)
benchi-kafka     | [2025-03-18 16:09:16,606] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Seeking to offset 0 for partition _confluent-telemetry-metrics-10 (org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer)
benchi-kafka     | [2025-03-18 16:09:16,606] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Seeking to offset 0 for partition _confluent-telemetry-metrics-11 (org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer)
benchi-kafka     | [2025-03-18 16:09:16,606] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Seeking to offset 0 for partition _confluent-telemetry-metrics-0 (org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer)
benchi-kafka     | [2025-03-18 16:09:16,606] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Seeking to offset 0 for partition _confluent-telemetry-metrics-1 (org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer)
benchi-kafka     | [2025-03-18 16:09:16,606] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Seeking to offset 0 for partition _confluent-telemetry-metrics-2 (org.apache.kafka.clients.consumer.internals.LegacyKafkaConsumer)
benchi-kafka     | [2025-03-18 16:09:16,638] INFO Finished sampling for 12 partitions - processed 79 metrics over 1 polls for the time range [16:06:00,16:08:59.999], with 0 of them added to the metrics processor, 79 of them being later than the desired end time and 0 being earlier than the desired start time. (io.confluent.cruisecontrol.metricsreporter.ConfluentMetricsSamplerBase)
benchi-kafka     | [2025-03-18 16:09:16,639] WARN Zero replica samples were added! Collected 0 (0 discarded, 0 added) replica metric samples for 0 replicas. Total partition assigned: 64. Total unrecognized replicas: 0 (com.linkedin.kafka.cruisecontrol.monitor.sampling.MetricFetcherManager)
benchi-kafka     | [2025-03-18 16:09:16,639] WARN Zero partition samples were added! Collected 0 (0 discarded, 0 added) partition metric samples for 0 partitions. Total partition assigned: 64. Total unrecognized partitions: 0 (com.linkedin.kafka.cruisecontrol.monitor.sampling.MetricFetcherManager)
benchi-kafka     | [2025-03-18 16:09:16,639] INFO Successfully finished metric sampling for time period 16:06:00 to 16:08:59.999 (1742313960000 to 1742314139999). (com.linkedin.kafka.cruisecontrol.monitor.task.MetricSamplingTask)
benchi-kafka     | [2025-03-18 16:09:16,639] INFO Sleeping the SamplingScheduler until 16:11:59.999 (for 163360ms) as instructed due to reason The last eligible window for sampling was already sampled. Sleeping until the end of the current window...  (lastSampledWindow: (index: 9679523, 16:06:00 - 16:08:59.999), currentWindow: (index: 9679524, 16:09:00 - 16:11:59.999)) (com.linkedin.kafka.cruisecontrol.monitor.task.MetricSamplingTask)
benchi-kafka     | [2025-03-18 16:09:26,344] INFO AdminClientConfig values: 
benchi-kafka     | 	auto.include.jmx.reporter = true
benchi-kafka     | 	bootstrap.controllers = []
benchi-kafka     | 	bootstrap.servers = [localhost:9092]
benchi-kafka     | 	client.dns.lookup = use_all_dns_ips
benchi-kafka     | 	client.id = 
benchi-kafka     | 	confluent.lkc.id = null
benchi-kafka     | 	confluent.metrics.reporter.bootstrap.servers = kafka-0:9071
benchi-kafka     | 	confluent.proxy.protocol.client.address = null
benchi-kafka     | 	confluent.proxy.protocol.client.mode = PROXY
benchi-kafka     | 	confluent.proxy.protocol.client.port = null
benchi-kafka     | 	confluent.proxy.protocol.client.version = NONE
benchi-kafka     | 	connections.max.idle.ms = 300000
benchi-kafka     | 	default.api.timeout.ms = 60000
benchi-kafka     | 	enable.metrics.push = true
benchi-kafka     | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
benchi-kafka     | 	metadata.max.age.ms = 300000
benchi-kafka     | 	metadata.recovery.strategy = none
benchi-kafka     | 	metric.reporters = []
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.recording.level = INFO
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	receive.buffer.bytes = 65536
benchi-kafka     | 	reconnect.backoff.max.ms = 1000
benchi-kafka     | 	reconnect.backoff.ms = 50
benchi-kafka     | 	request.timeout.ms = 30000
benchi-kafka     | 	retries = 2147483647
benchi-kafka     | 	retry.backoff.max.ms = 1000
benchi-kafka     | 	retry.backoff.ms = 100
benchi-kafka     | 	sasl.client.callback.handler.class = null
benchi-kafka     | 	sasl.jaas.config = null
benchi-kafka     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
benchi-kafka     | 	sasl.kerberos.min.time.before.relogin = 60000
benchi-kafka     | 	sasl.kerberos.service.name = null
benchi-kafka     | 	sasl.kerberos.ticket.renew.jitter = 0.05
benchi-kafka     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
benchi-kafka     | 	sasl.login.callback.handler.class = null
benchi-kafka     | 	sasl.login.class = null
benchi-kafka     | 	sasl.login.connect.timeout.ms = null
benchi-kafka     | 	sasl.login.read.timeout.ms = null
benchi-kafka     | 	sasl.login.refresh.buffer.seconds = 300
benchi-kafka     | 	sasl.login.refresh.min.period.seconds = 60
benchi-kafka     | 	sasl.login.refresh.window.factor = 0.8
benchi-kafka     | 	sasl.login.refresh.window.jitter = 0.05
benchi-kafka     | 	sasl.login.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.login.retry.backoff.ms = 100
benchi-kafka     | 	sasl.mechanism = GSSAPI
benchi-kafka     | 	sasl.oauthbearer.clientassertion.audience = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.expiration = 5
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.jti.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.nbf.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.issuer = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.location = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key.passphrase = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.subject = null
benchi-kafka     | 	sasl.oauthbearer.clock.skew.seconds = 30
benchi-kafka     | 	sasl.oauthbearer.expected.audience = null
benchi-kafka     | 	sasl.oauthbearer.expected.issuer = null
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.url = null
benchi-kafka     | 	sasl.oauthbearer.scope.claim.name = scope
benchi-kafka     | 	sasl.oauthbearer.sub.claim.name = sub
benchi-kafka     | 	sasl.oauthbearer.token.endpoint.url = null
benchi-kafka     | 	security.protocol = PLAINTEXT
benchi-kafka     | 	security.providers = null
benchi-kafka     | 	send.buffer.bytes = 131072
benchi-kafka     | 	socket.connection.setup.timeout.max.ms = 30000
benchi-kafka     | 	socket.connection.setup.timeout.ms = 10000
benchi-kafka     | 	ssl.cipher.suites = null
benchi-kafka     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	ssl.engine.factory.class = null
benchi-kafka     | 	ssl.key.password = null
benchi-kafka     | 	ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	ssl.keystore.certificate.chain = null
benchi-kafka     | 	ssl.keystore.key = null
benchi-kafka     | 	ssl.keystore.location = null
benchi-kafka     | 	ssl.keystore.password = null
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.protocol = TLSv1.3
benchi-kafka     | 	ssl.provider = null
benchi-kafka     | 	ssl.secure.random.implementation = null
benchi-kafka     | 	ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	ssl.truststore.certificates = null
benchi-kafka     | 	ssl.truststore.location = null
benchi-kafka     | 	ssl.truststore.password = null
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     |  (org.apache.kafka.clients.admin.AdminClientConfig)
benchi-kafka     | [2025-03-18 16:09:26,346] INFO Kafka version: 7.8.1-ce (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:09:26,346] INFO Kafka commitId: eefb51c6f2f1b888b8b03296353338afbac8e213 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:09:26,346] INFO Kafka startTimeMs: 1742314166346 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:09:26,356] INFO [AdminClient clientId=adminclient-6] The DescribeTopicPartitions API is not supported, using Metadata API to describe topics. (org.apache.kafka.clients.admin.KafkaAdminClient)
benchi-kafka     | [2025-03-18 16:09:26,358] INFO [ControllerServer id=1] CreateTopics result(s): CreatableTopic(name='_confluent-metrics', numPartitions=12, replicationFactor=3, assignments=[], configs=[CreateableTopicConfig(name='max.message.bytes', value='10485760'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='259200000'), CreateableTopicConfig(name='segment.ms', value='14400000'), CreateableTopicConfig(name='retention.bytes', value='-1')], linkName=null, mirrorTopic=null, sourceTopicId=AAAAAAAAAAAAAAAAAAAAAA, mirrorStartOffsetSpec=-9223372036854775808, mirrorStartOffsets=[], stoppedSequenceNumber=0): INVALID_REPLICATION_FACTOR (Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.) (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:09:26,358] INFO App info kafka.admin.client for adminclient-6 unregistered (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:09:26,359] ERROR Error checking or creating metrics topic (io.confluent.metrics.reporter.ConfluentMetricsReporter)
benchi-kafka     | org.apache.kafka.common.errors.InvalidReplicationFactorException: Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.
benchi-kafka     | [2025-03-18 16:09:41,342] INFO AdminClientConfig values: 
benchi-kafka     | 	auto.include.jmx.reporter = true
benchi-kafka     | 	bootstrap.controllers = []
benchi-kafka     | 	bootstrap.servers = [localhost:9092]
benchi-kafka     | 	client.dns.lookup = use_all_dns_ips
benchi-kafka     | 	client.id = 
benchi-kafka     | 	confluent.lkc.id = null
benchi-kafka     | 	confluent.metrics.reporter.bootstrap.servers = kafka-0:9071
benchi-kafka     | 	confluent.proxy.protocol.client.address = null
benchi-kafka     | 	confluent.proxy.protocol.client.mode = PROXY
benchi-kafka     | 	confluent.proxy.protocol.client.port = null
benchi-kafka     | 	confluent.proxy.protocol.client.version = NONE
benchi-kafka     | 	connections.max.idle.ms = 300000
benchi-kafka     | 	default.api.timeout.ms = 60000
benchi-kafka     | 	enable.metrics.push = true
benchi-kafka     | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
benchi-kafka     | 	metadata.max.age.ms = 300000
benchi-kafka     | 	metadata.recovery.strategy = none
benchi-kafka     | 	metric.reporters = []
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.recording.level = INFO
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	receive.buffer.bytes = 65536
benchi-kafka     | 	reconnect.backoff.max.ms = 1000
benchi-kafka     | 	reconnect.backoff.ms = 50
benchi-kafka     | 	request.timeout.ms = 30000
benchi-kafka     | 	retries = 2147483647
benchi-kafka     | 	retry.backoff.max.ms = 1000
benchi-kafka     | 	retry.backoff.ms = 100
benchi-kafka     | 	sasl.client.callback.handler.class = null
benchi-kafka     | 	sasl.jaas.config = null
benchi-kafka     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
benchi-kafka     | 	sasl.kerberos.min.time.before.relogin = 60000
benchi-kafka     | 	sasl.kerberos.service.name = null
benchi-kafka     | 	sasl.kerberos.ticket.renew.jitter = 0.05
benchi-kafka     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
benchi-kafka     | 	sasl.login.callback.handler.class = null
benchi-kafka     | 	sasl.login.class = null
benchi-kafka     | 	sasl.login.connect.timeout.ms = null
benchi-kafka     | 	sasl.login.read.timeout.ms = null
benchi-kafka     | 	sasl.login.refresh.buffer.seconds = 300
benchi-kafka     | 	sasl.login.refresh.min.period.seconds = 60
benchi-kafka     | 	sasl.login.refresh.window.factor = 0.8
benchi-kafka     | 	sasl.login.refresh.window.jitter = 0.05
benchi-kafka     | 	sasl.login.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.login.retry.backoff.ms = 100
benchi-kafka     | 	sasl.mechanism = GSSAPI
benchi-kafka     | 	sasl.oauthbearer.clientassertion.audience = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.expiration = 5
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.jti.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.nbf.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.issuer = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.location = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key.passphrase = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.subject = null
benchi-kafka     | 	sasl.oauthbearer.clock.skew.seconds = 30
benchi-kafka     | 	sasl.oauthbearer.expected.audience = null
benchi-kafka     | 	sasl.oauthbearer.expected.issuer = null
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.url = null
benchi-kafka     | 	sasl.oauthbearer.scope.claim.name = scope
benchi-kafka     | 	sasl.oauthbearer.sub.claim.name = sub
benchi-kafka     | 	sasl.oauthbearer.token.endpoint.url = null
benchi-kafka     | 	security.protocol = PLAINTEXT
benchi-kafka     | 	security.providers = null
benchi-kafka     | 	send.buffer.bytes = 131072
benchi-kafka     | 	socket.connection.setup.timeout.max.ms = 30000
benchi-kafka     | 	socket.connection.setup.timeout.ms = 10000
benchi-kafka     | 	ssl.cipher.suites = null
benchi-kafka     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	ssl.engine.factory.class = null
benchi-kafka     | 	ssl.key.password = null
benchi-kafka     | 	ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	ssl.keystore.certificate.chain = null
benchi-kafka     | 	ssl.keystore.key = null
benchi-kafka     | 	ssl.keystore.location = null
benchi-kafka     | 	ssl.keystore.password = null
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.protocol = TLSv1.3
benchi-kafka     | 	ssl.provider = null
benchi-kafka     | 	ssl.secure.random.implementation = null
benchi-kafka     | 	ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	ssl.truststore.certificates = null
benchi-kafka     | 	ssl.truststore.location = null
benchi-kafka     | 	ssl.truststore.password = null
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     |  (org.apache.kafka.clients.admin.AdminClientConfig)
benchi-kafka     | [2025-03-18 16:09:41,344] INFO Kafka version: 7.8.1-ce (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:09:41,344] INFO Kafka commitId: eefb51c6f2f1b888b8b03296353338afbac8e213 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:09:41,344] INFO Kafka startTimeMs: 1742314181344 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:09:41,350] INFO [AdminClient clientId=adminclient-7] The DescribeTopicPartitions API is not supported, using Metadata API to describe topics. (org.apache.kafka.clients.admin.KafkaAdminClient)
benchi-kafka     | [2025-03-18 16:09:41,352] INFO [ControllerServer id=1] CreateTopics result(s): CreatableTopic(name='_confluent-metrics', numPartitions=12, replicationFactor=3, assignments=[], configs=[CreateableTopicConfig(name='max.message.bytes', value='10485760'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='259200000'), CreateableTopicConfig(name='segment.ms', value='14400000'), CreateableTopicConfig(name='retention.bytes', value='-1')], linkName=null, mirrorTopic=null, sourceTopicId=AAAAAAAAAAAAAAAAAAAAAA, mirrorStartOffsetSpec=-9223372036854775808, mirrorStartOffsets=[], stoppedSequenceNumber=0): INVALID_REPLICATION_FACTOR (Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.) (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:09:41,354] INFO App info kafka.admin.client for adminclient-7 unregistered (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:09:41,354] ERROR Error checking or creating metrics topic (io.confluent.metrics.reporter.ConfluentMetricsReporter)
benchi-kafka     | org.apache.kafka.common.errors.InvalidReplicationFactorException: Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.
benchi-kafka     | [2025-03-18 16:09:51,399] INFO [AdminClient clientId=adminclient-5] The DescribeTopicPartitions API is not supported, using Metadata API to describe topics. (org.apache.kafka.clients.admin.KafkaAdminClient)
benchi-kafka     | [2025-03-18 16:09:51,401] INFO Notifying listeners about metadata change. (com.linkedin.kafka.cruisecontrol.common.MetadataClient)
benchi-kafka     | [2025-03-18 16:09:51,402] INFO Skipping goal violation detection due to previous new broker change - will resume at 16:39:15.798 (1742315955798) (com.linkedin.kafka.cruisecontrol.detector.GoalViolationDetector)
benchi-kafka     | [2025-03-18 16:09:51,402] INFO Skipping goal violation detection due to previous new broker change - will resume at 16:39:15.798 (1742315955798) (com.linkedin.kafka.cruisecontrol.detector.GoalViolationDetector)
benchi-kafka     | [2025-03-18 16:09:56,343] INFO AdminClientConfig values: 
benchi-kafka     | 	auto.include.jmx.reporter = true
benchi-kafka     | 	bootstrap.controllers = []
benchi-kafka     | 	bootstrap.servers = [localhost:9092]
benchi-kafka     | 	client.dns.lookup = use_all_dns_ips
benchi-kafka     | 	client.id = 
benchi-kafka     | 	confluent.lkc.id = null
benchi-kafka     | 	confluent.metrics.reporter.bootstrap.servers = kafka-0:9071
benchi-kafka     | 	confluent.proxy.protocol.client.address = null
benchi-kafka     | 	confluent.proxy.protocol.client.mode = PROXY
benchi-kafka     | 	confluent.proxy.protocol.client.port = null
benchi-kafka     | 	confluent.proxy.protocol.client.version = NONE
benchi-kafka     | 	connections.max.idle.ms = 300000
benchi-kafka     | 	default.api.timeout.ms = 60000
benchi-kafka     | 	enable.metrics.push = true
benchi-kafka     | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
benchi-kafka     | 	metadata.max.age.ms = 300000
benchi-kafka     | 	metadata.recovery.strategy = none
benchi-kafka     | 	metric.reporters = []
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.recording.level = INFO
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	receive.buffer.bytes = 65536
benchi-kafka     | 	reconnect.backoff.max.ms = 1000
benchi-kafka     | 	reconnect.backoff.ms = 50
benchi-kafka     | 	request.timeout.ms = 30000
benchi-kafka     | 	retries = 2147483647
benchi-kafka     | 	retry.backoff.max.ms = 1000
benchi-kafka     | 	retry.backoff.ms = 100
benchi-kafka     | 	sasl.client.callback.handler.class = null
benchi-kafka     | 	sasl.jaas.config = null
benchi-kafka     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
benchi-kafka     | 	sasl.kerberos.min.time.before.relogin = 60000
benchi-kafka     | 	sasl.kerberos.service.name = null
benchi-kafka     | 	sasl.kerberos.ticket.renew.jitter = 0.05
benchi-kafka     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
benchi-kafka     | 	sasl.login.callback.handler.class = null
benchi-kafka     | 	sasl.login.class = null
benchi-kafka     | 	sasl.login.connect.timeout.ms = null
benchi-kafka     | 	sasl.login.read.timeout.ms = null
benchi-kafka     | 	sasl.login.refresh.buffer.seconds = 300
benchi-kafka     | 	sasl.login.refresh.min.period.seconds = 60
benchi-kafka     | 	sasl.login.refresh.window.factor = 0.8
benchi-kafka     | 	sasl.login.refresh.window.jitter = 0.05
benchi-kafka     | 	sasl.login.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.login.retry.backoff.ms = 100
benchi-kafka     | 	sasl.mechanism = GSSAPI
benchi-kafka     | 	sasl.oauthbearer.clientassertion.audience = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.expiration = 5
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.jti.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.nbf.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.issuer = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.location = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key.passphrase = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.subject = null
benchi-kafka     | 	sasl.oauthbearer.clock.skew.seconds = 30
benchi-kafka     | 	sasl.oauthbearer.expected.audience = null
benchi-kafka     | 	sasl.oauthbearer.expected.issuer = null
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.url = null
benchi-kafka     | 	sasl.oauthbearer.scope.claim.name = scope
benchi-kafka     | 	sasl.oauthbearer.sub.claim.name = sub
benchi-kafka     | 	sasl.oauthbearer.token.endpoint.url = null
benchi-kafka     | 	security.protocol = PLAINTEXT
benchi-kafka     | 	security.providers = null
benchi-kafka     | 	send.buffer.bytes = 131072
benchi-kafka     | 	socket.connection.setup.timeout.max.ms = 30000
benchi-kafka     | 	socket.connection.setup.timeout.ms = 10000
benchi-kafka     | 	ssl.cipher.suites = null
benchi-kafka     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	ssl.engine.factory.class = null
benchi-kafka     | 	ssl.key.password = null
benchi-kafka     | 	ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	ssl.keystore.certificate.chain = null
benchi-kafka     | 	ssl.keystore.key = null
benchi-kafka     | 	ssl.keystore.location = null
benchi-kafka     | 	ssl.keystore.password = null
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.protocol = TLSv1.3
benchi-kafka     | 	ssl.provider = null
benchi-kafka     | 	ssl.secure.random.implementation = null
benchi-kafka     | 	ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	ssl.truststore.certificates = null
benchi-kafka     | 	ssl.truststore.location = null
benchi-kafka     | 	ssl.truststore.password = null
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     |  (org.apache.kafka.clients.admin.AdminClientConfig)
benchi-kafka     | [2025-03-18 16:09:56,345] INFO Kafka version: 7.8.1-ce (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:09:56,345] INFO Kafka commitId: eefb51c6f2f1b888b8b03296353338afbac8e213 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:09:56,345] INFO Kafka startTimeMs: 1742314196345 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:09:56,353] INFO [AdminClient clientId=adminclient-8] The DescribeTopicPartitions API is not supported, using Metadata API to describe topics. (org.apache.kafka.clients.admin.KafkaAdminClient)
benchi-kafka     | [2025-03-18 16:09:56,356] INFO [ControllerServer id=1] CreateTopics result(s): CreatableTopic(name='_confluent-metrics', numPartitions=12, replicationFactor=3, assignments=[], configs=[CreateableTopicConfig(name='max.message.bytes', value='10485760'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='259200000'), CreateableTopicConfig(name='segment.ms', value='14400000'), CreateableTopicConfig(name='retention.bytes', value='-1')], linkName=null, mirrorTopic=null, sourceTopicId=AAAAAAAAAAAAAAAAAAAAAA, mirrorStartOffsetSpec=-9223372036854775808, mirrorStartOffsets=[], stoppedSequenceNumber=0): INVALID_REPLICATION_FACTOR (Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.) (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:09:56,357] INFO App info kafka.admin.client for adminclient-8 unregistered (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:09:56,358] ERROR Error checking or creating metrics topic (io.confluent.metrics.reporter.ConfluentMetricsReporter)
benchi-kafka     | org.apache.kafka.common.errors.InvalidReplicationFactorException: Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.
benchi-kafka     | [2025-03-18 16:10:10,706] INFO [CelltControllerMetricsPublisher id=1] No cells found. Skipping cell metrics refresh. (org.apache.kafka.controller.metrics.CellControllerMetricsPublisher)
benchi-kafka     | [2025-03-18 16:10:11,339] INFO AdminClientConfig values: 
benchi-kafka     | 	auto.include.jmx.reporter = true
benchi-kafka     | 	bootstrap.controllers = []
benchi-kafka     | 	bootstrap.servers = [localhost:9092]
benchi-kafka     | 	client.dns.lookup = use_all_dns_ips
benchi-kafka     | 	client.id = 
benchi-kafka     | 	confluent.lkc.id = null
benchi-kafka     | 	confluent.metrics.reporter.bootstrap.servers = kafka-0:9071
benchi-kafka     | 	confluent.proxy.protocol.client.address = null
benchi-kafka     | 	confluent.proxy.protocol.client.mode = PROXY
benchi-kafka     | 	confluent.proxy.protocol.client.port = null
benchi-kafka     | 	confluent.proxy.protocol.client.version = NONE
benchi-kafka     | 	connections.max.idle.ms = 300000
benchi-kafka     | 	default.api.timeout.ms = 60000
benchi-kafka     | 	enable.metrics.push = true
benchi-kafka     | 	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
benchi-kafka     | 	metadata.max.age.ms = 300000
benchi-kafka     | 	metadata.recovery.strategy = none
benchi-kafka     | 	metric.reporters = []
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.recording.level = INFO
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	receive.buffer.bytes = 65536
benchi-kafka     | 	reconnect.backoff.max.ms = 1000
benchi-kafka     | 	reconnect.backoff.ms = 50
benchi-kafka     | 	request.timeout.ms = 30000
benchi-kafka     | 	retries = 2147483647
benchi-kafka     | 	retry.backoff.max.ms = 1000
benchi-kafka     | 	retry.backoff.ms = 100
benchi-kafka     | 	sasl.client.callback.handler.class = null
benchi-kafka     | 	sasl.jaas.config = null
benchi-kafka     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
benchi-kafka     | 	sasl.kerberos.min.time.before.relogin = 60000
benchi-kafka     | 	sasl.kerberos.service.name = null
benchi-kafka     | 	sasl.kerberos.ticket.renew.jitter = 0.05
benchi-kafka     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
benchi-kafka     | 	sasl.login.callback.handler.class = null
benchi-kafka     | 	sasl.login.class = null
benchi-kafka     | 	sasl.login.connect.timeout.ms = null
benchi-kafka     | 	sasl.login.read.timeout.ms = null
benchi-kafka     | 	sasl.login.refresh.buffer.seconds = 300
benchi-kafka     | 	sasl.login.refresh.min.period.seconds = 60
benchi-kafka     | 	sasl.login.refresh.window.factor = 0.8
benchi-kafka     | 	sasl.login.refresh.window.jitter = 0.05
benchi-kafka     | 	sasl.login.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.login.retry.backoff.ms = 100
benchi-kafka     | 	sasl.mechanism = GSSAPI
benchi-kafka     | 	sasl.oauthbearer.clientassertion.audience = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.expiration = 5
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.jti.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.include.nbf.claim = false
benchi-kafka     | 	sasl.oauthbearer.clientassertion.issuer = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.location = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.private.key.passphrase = null
benchi-kafka     | 	sasl.oauthbearer.clientassertion.subject = null
benchi-kafka     | 	sasl.oauthbearer.clock.skew.seconds = 30
benchi-kafka     | 	sasl.oauthbearer.expected.audience = null
benchi-kafka     | 	sasl.oauthbearer.expected.issuer = null
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.url = null
benchi-kafka     | 	sasl.oauthbearer.scope.claim.name = scope
benchi-kafka     | 	sasl.oauthbearer.sub.claim.name = sub
benchi-kafka     | 	sasl.oauthbearer.token.endpoint.url = null
benchi-kafka     | 	security.protocol = PLAINTEXT
benchi-kafka     | 	security.providers = null
benchi-kafka     | 	send.buffer.bytes = 131072
benchi-kafka     | 	socket.connection.setup.timeout.max.ms = 30000
benchi-kafka     | 	socket.connection.setup.timeout.ms = 10000
benchi-kafka     | 	ssl.cipher.suites = null
benchi-kafka     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	ssl.engine.factory.class = null
benchi-kafka     | 	ssl.key.password = null
benchi-kafka     | 	ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	ssl.keystore.certificate.chain = null
benchi-kafka     | 	ssl.keystore.key = null
benchi-kafka     | 	ssl.keystore.location = null
benchi-kafka     | 	ssl.keystore.password = null
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.protocol = TLSv1.3
benchi-kafka     | 	ssl.provider = null
benchi-kafka     | 	ssl.secure.random.implementation = null
benchi-kafka     | 	ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	ssl.truststore.certificates = null
benchi-kafka     | 	ssl.truststore.location = null
benchi-kafka     | 	ssl.truststore.password = null
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     |  (org.apache.kafka.clients.admin.AdminClientConfig)
benchi-kafka     | [2025-03-18 16:10:11,340] INFO Kafka version: 7.8.1-ce (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:10:11,340] INFO Kafka commitId: eefb51c6f2f1b888b8b03296353338afbac8e213 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:10:11,340] INFO Kafka startTimeMs: 1742314211340 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:10:11,344] INFO [AdminClient clientId=adminclient-9] The DescribeTopicPartitions API is not supported, using Metadata API to describe topics. (org.apache.kafka.clients.admin.KafkaAdminClient)
benchi-kafka     | [2025-03-18 16:10:11,346] INFO [ControllerServer id=1] CreateTopics result(s): CreatableTopic(name='_confluent-metrics', numPartitions=12, replicationFactor=3, assignments=[], configs=[CreateableTopicConfig(name='max.message.bytes', value='10485760'), CreateableTopicConfig(name='message.timestamp.type', value='CreateTime'), CreateableTopicConfig(name='min.insync.replicas', value='1'), CreateableTopicConfig(name='retention.ms', value='259200000'), CreateableTopicConfig(name='segment.ms', value='14400000'), CreateableTopicConfig(name='retention.bytes', value='-1')], linkName=null, mirrorTopic=null, sourceTopicId=AAAAAAAAAAAAAAAAAAAAAA, mirrorStartOffsetSpec=-9223372036854775808, mirrorStartOffsets=[], stoppedSequenceNumber=0): INVALID_REPLICATION_FACTOR (Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.) (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:10:11,347] INFO App info kafka.admin.client for adminclient-9 unregistered (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:10:11,348] ERROR Error checking or creating metrics topic (io.confluent.metrics.reporter.ConfluentMetricsReporter)
benchi-kafka     | org.apache.kafka.common.errors.InvalidReplicationFactorException: Unable to replicate the partition 3 time(s): The target replication factor of 3 cannot be reached because only 1 broker(s) are registered.
benchi-kafka     | [2025-03-18 16:10:20,127] INFO Terminating process due to signal SIGTERM (org.apache.kafka.common.utils.LoggingSignalHandler)
benchi-postgres  | 2025-03-18 16:10:20.127 UTC [1] LOG:  received fast shutdown request
benchi-postgres  | 2025-03-18 16:10:20.129 UTC [1] LOG:  aborting any active transactions
benchi-kafka     | [2025-03-18 16:10:20,129] INFO [BrokerServer id=1] Transition from STARTED to SHUTTING_DOWN (kafka.server.BrokerServer)
benchi-kafka     | [2025-03-18 16:10:20,129] INFO [BrokerServer id=1] shutting down (kafka.server.BrokerServer)
benchi-kafka     | [2025-03-18 16:10:20,130] INFO Closing License Store (io.confluent.license.LicenseStore)
benchi-kafka     | [2025-03-18 16:10:20,130] INFO Stopping KafkaBasedLog for topic _confluent-command (org.apache.kafka.connect.util.KafkaBasedLog)
benchi-postgres  | 2025-03-18 16:10:20.130 UTC [1] LOG:  background worker "logical replication launcher" (PID 69) exited with exit code 1
benchi-postgres  | 2025-03-18 16:10:20.130 UTC [64] LOG:  shutting down
benchi-kafka     | [2025-03-18 16:10:20,131] INFO [Producer clientId=_confluent-license-producer-1] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer)
benchi-postgres  | 2025-03-18 16:10:20.132 UTC [64] LOG:  checkpoint starting: shutdown immediate
benchi-kafka     | [2025-03-18 16:10:20,135] INFO Stopped NetworkTrafficServerConnector@77e1dacd{HTTP/1.1, (http/1.1, h2c)}{0.0.0.0:8090} (org.eclipse.jetty.server.AbstractConnector)
benchi-kafka     | [2025-03-18 16:10:20,135] INFO node0 Stopped scavenging (org.eclipse.jetty.server.session)
benchi-kafka     | [2025-03-18 16:10:20,136] INFO Stopped o.e.j.s.ServletContextHandler@7feb5a3e{/ws,null,STOPPED} (org.eclipse.jetty.server.handler.ContextHandler)
benchi-kafka     | [2025-03-18 16:10:20,136] INFO Stopped o.e.j.s.ServletContextHandler@10e9059e{/ws,null,STOPPED} (org.eclipse.jetty.server.handler.ContextHandler)
benchi-kafka     | [2025-03-18 16:10:20,138] INFO Stopped o.e.j.s.ServletContextHandler@3e73d551{/kafka,null,STOPPED} (org.eclipse.jetty.server.handler.ContextHandler)
benchi-kafka     | [2025-03-18 16:10:20,139] INFO Stopped o.e.j.s.ServletContextHandler@666c2f40{/v1/metadata,null,STOPPED} (org.eclipse.jetty.server.handler.ContextHandler)
benchi-kafka     | [2025-03-18 16:10:20,142] INFO Stopping TelemetryReporter collectorTask (io.confluent.telemetry.reporter.TelemetryReporter)
benchi-kafka     | [2025-03-18 16:10:20,142] INFO Closing the event logger (io.confluent.telemetry.reporter.TelemetryReporter)
benchi-kafka     | [2025-03-18 16:10:20,147] INFO Stopping TelemetryReporter remoteConfigTask (io.confluent.telemetry.reporter.TelemetryReporter)
benchi-kafka     | [2025-03-18 16:10:20,149] INFO App info kafka.producer for _confluent-license-producer-1 unregistered (org.apache.kafka.common.utils.AppInfoParser)
benchi-postgres  | 2025-03-18 16:10:20.162 UTC [64] LOG:  checkpoint complete: wrote 3701 buffers (22.6%); 0 WAL file(s) added, 0 removed, 4 recycled; write=0.012 s, sync=0.010 s, total=0.033 s; sync files=21, longest=0.009 s, average=0.001 s; distance=58930 kB, estimate=58930 kB; lsn=0/5365B90, redo lsn=0/5365B90
benchi-postgres  | 2025-03-18 16:10:20.165 UTC [1] LOG:  database system is shut down
[Kbenchi-postgres exited with code 0
benchi-kafka     | [2025-03-18 16:10:20,456] INFO App info kafka.consumer for _confluent-license-consumer-1 unregistered (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:10:20,456] INFO Stopped KafkaBasedLog for topic _confluent-command (org.apache.kafka.connect.util.KafkaBasedLog)
benchi-kafka     | [2025-03-18 16:10:20,456] INFO Closed License Store (io.confluent.license.LicenseStore)
benchi-kafka     | [2025-03-18 16:10:20,457] INFO [BrokerLifecycleManager id=1] Beginning controlled shutdown. (kafka.server.BrokerLifecycleManager)
benchi-kafka     | [2025-03-18 16:10:20,458] INFO [ControllerServer id=1] Unfenced broker 1 has requested and been granted a controlled shutdown. (org.apache.kafka.controller.BrokerHeartbeatManager)
benchi-kafka     | [2025-03-18 16:10:20,458] INFO [ControllerServer id=1] Replayed BrokerRegistrationChangeRecord modifying the registration for broker 1: BrokerRegistrationChangeRecord(brokerId=1, brokerEpoch=6, fenced=0, inControlledShutdown=1, degradedComponents=null, logDirs=[]) (org.apache.kafka.controller.ClusterControlManager)
benchi-kafka     | [2025-03-18 16:10:20,458] INFO [ControllerServer id=1] Marking broker 1 as shutting down in heartbeat manager at offset 404 (org.apache.kafka.controller.BrokerHeartbeatManager)
benchi-kafka     | [2025-03-18 16:10:20,460] INFO [ControllerServer id=1] checkLastBatchForBroker[1]: changing 1 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:10:20,488] INFO [BrokerLifecycleManager id=1] The broker is in PENDING_CONTROLLED_SHUTDOWN state, still waiting for the active controller. (kafka.server.BrokerLifecycleManager)
benchi-kafka     | [2025-03-18 16:10:20,542] INFO [ControllerServer id=1] Broker 1 is in controlled shutdown state, but can not shut down because more leaders still need to be moved. (org.apache.kafka.controller.BrokerHeartbeatManager)
benchi-kafka     | [2025-03-18 16:10:20,543] INFO [BrokerLifecycleManager id=1] The broker is in PENDING_CONTROLLED_SHUTDOWN state, still waiting for the active controller. (kafka.server.BrokerLifecycleManager)
benchi-kafka     | [2025-03-18 16:10:20,565] INFO [ControllerServer id=1] partitionChangeInControlledShutdownForBroker[1]: changing 114 partition(s) (org.apache.kafka.controller.ReplicationControlManager)
benchi-kafka     | [2025-03-18 16:10:20,565] INFO [ControllerServer id=1] maybeScheduleNextPartitionChangeShuttingDownBrokers: generated 114 partition change records for broker 1. (org.apache.kafka.controller.QuorumController)
benchi-kafka     | [2025-03-18 16:10:20,601] INFO [Broker id=1] Transitioning 114 partition(s) to local followers. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,611] INFO [Broker id=1] Follower __consumer_offsets-12 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,611] INFO [Broker id=1] Follower _confluent-link-metadata-27 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,611] INFO [Broker id=1] Follower __consumer_offsets-20 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,611] INFO [Broker id=1] Follower __consumer_offsets-23 starts at leader epoch 1 from offset 1 with partition epoch 1 and high watermark 1. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,611] INFO [Broker id=1] Follower _confluent-link-metadata-24 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,611] INFO [Broker id=1] Follower _confluent-link-metadata-34 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,611] INFO [Broker id=1] Follower _confluent-link-metadata-33 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,611] INFO [Broker id=1] Follower __consumer_offsets-22 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,611] INFO [Broker id=1] Follower __consumer_offsets-24 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,611] INFO [Broker id=1] Follower _confluent-link-metadata-25 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,612] INFO [Broker id=1] Follower _confluent-link-metadata-21 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,612] INFO [Broker id=1] Follower _confluent-link-metadata-49 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,612] INFO [Broker id=1] Follower __consumer_offsets-32 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,612] INFO [Broker id=1] Follower _confluent-link-metadata-16 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,612] INFO [Broker id=1] Follower _confluent-link-metadata-46 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,612] INFO [Broker id=1] Follower __consumer_offsets-31 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,612] INFO [Broker id=1] Follower _confluent-telemetry-metrics-11 starts at leader epoch 1 from offset 115 with partition epoch 1 and high watermark 115. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,612] INFO [Broker id=1] Follower _confluent-link-metadata-17 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,612] INFO [Broker id=1] Follower _confluent-link-metadata-0 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,612] INFO [Broker id=1] Follower __consumer_offsets-30 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,612] INFO [Broker id=1] Follower _confluent-telemetry-metrics-8 starts at leader epoch 1 from offset 138 with partition epoch 1 and high watermark 138. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,612] INFO [Broker id=1] Follower __consumer_offsets-49 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,612] INFO [Broker id=1] Follower __consumer_offsets-7 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,612] INFO [Broker id=1] Follower _confluent-link-metadata-41 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,612] INFO [Broker id=1] Follower _confluent-link-metadata-19 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,612] INFO [Broker id=1] Follower _confluent-link-metadata-32 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,613] INFO [Broker id=1] Follower __consumer_offsets-40 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,613] INFO [Broker id=1] Follower _confluent-link-metadata-8 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,613] INFO [Broker id=1] Follower _confluent-command-0 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,613] INFO [Broker id=1] Follower __consumer_offsets-39 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,613] INFO [Broker id=1] Follower _confluent-link-metadata-38 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,613] INFO [Broker id=1] Follower __consumer_offsets-6 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,613] INFO [Broker id=1] Follower __consumer_offsets-18 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,613] INFO [Broker id=1] Follower __consumer_offsets-15 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,613] INFO [Broker id=1] Follower __consumer_offsets-43 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,613] INFO [Broker id=1] Follower __consumer_offsets-48 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,613] INFO [Broker id=1] Follower __consumer_offsets-10 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,613] INFO [Broker id=1] Follower _confluent-link-metadata-15 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,613] INFO [Broker id=1] Follower __consumer_offsets-28 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,613] INFO [Broker id=1] Follower __consumer_offsets-37 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,613] INFO [Broker id=1] Follower _confluent-link-metadata-29 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,613] INFO [Broker id=1] Follower _confluent-link-metadata-48 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,613] INFO [Broker id=1] Follower __consumer_offsets-4 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,613] INFO [Broker id=1] Follower __consumer_offsets-3 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,612] INFO [Broker id=1] Follower _confluent-telemetry-metrics-1 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,613] INFO [Broker id=1] Follower _confluent-telemetry-metrics-0 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,613] INFO [Broker id=1] Follower __consumer_offsets-36 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,613] INFO [Broker id=1] Follower _confluent-telemetry-metrics-9 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,613] INFO [Broker id=1] Follower _confluent-link-metadata-1 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,613] INFO [Broker id=1] Follower _confluent-link-metadata-35 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,613] INFO [Broker id=1] Follower _confluent-link-metadata-37 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,613] INFO [Broker id=1] Follower _confluent-link-metadata-2 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,613] INFO [Broker id=1] Follower __consumer_offsets-41 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,613] INFO [Broker id=1] Follower _confluent-link-metadata-4 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,613] INFO [Broker id=1] Follower __consumer_offsets-16 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,613] INFO [Broker id=1] Follower _confluent-link-metadata-26 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,613] INFO [Broker id=1] Follower _confluent-telemetry-metrics-3 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,614] INFO [Broker id=1] Follower __consumer_offsets-29 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,613] INFO [Broker id=1] Follower _confluent-link-metadata-42 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,614] INFO [Broker id=1] Follower _confluent-link-metadata-11 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,614] INFO [Broker id=1] Follower __consumer_offsets-19 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,614] INFO [Broker id=1] Follower _confluent-link-metadata-44 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,614] INFO [Broker id=1] Follower _confluent-link-metadata-23 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,613] INFO [Broker id=1] Follower _confluent-link-metadata-3 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,614] INFO [Broker id=1] Follower __consumer_offsets-8 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,614] INFO [Broker id=1] Follower _confluent_balancer_api_state-0 starts at leader epoch 1 from offset 1 with partition epoch 1 and high watermark 1. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,614] INFO [Broker id=1] Follower __consumer_offsets-45 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,614] INFO [Broker id=1] Follower _confluent-link-metadata-5 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,614] INFO [Broker id=1] Follower __consumer_offsets-26 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,614] INFO [Broker id=1] Follower _confluent-link-metadata-31 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,614] INFO [Broker id=1] Follower __consumer_offsets-11 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,614] INFO [Broker id=1] Follower __consumer_offsets-1 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,614] INFO [Broker id=1] Follower _confluent-link-metadata-30 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,614] INFO [Broker id=1] Follower __consumer_offsets-33 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,614] INFO [Broker id=1] Follower __consumer_offsets-44 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,614] INFO [Broker id=1] Follower _confluent-telemetry-metrics-5 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,615] INFO [Broker id=1] Follower _confluent-telemetry-metrics-4 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,613] INFO [Broker id=1] Follower __consumer_offsets-5 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,613] INFO [Broker id=1] Follower _confluent-link-metadata-9 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,615] INFO [Broker id=1] Follower _confluent-link-metadata-45 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,615] INFO [Broker id=1] Follower __consumer_offsets-38 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,615] INFO [Broker id=1] Follower _confluent-link-metadata-12 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,615] INFO [Broker id=1] Follower __consumer_offsets-0 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,613] INFO [Broker id=1] Follower _confluent-link-metadata-20 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,613] INFO [Broker id=1] Follower _confluent-telemetry-metrics-7 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,615] INFO [Broker id=1] Follower _confluent-telemetry-metrics-6 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,615] INFO [Broker id=1] Follower _confluent-telemetry-metrics-10 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,615] INFO [Broker id=1] Follower _confluent-link-metadata-7 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,615] INFO [Broker id=1] Follower _confluent-link-metadata-39 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,615] INFO [Broker id=1] Follower _confluent-link-metadata-18 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,615] INFO [Broker id=1] Follower _confluent-link-metadata-13 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,615] INFO [Broker id=1] Follower __consumer_offsets-21 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,615] INFO [Broker id=1] Follower __consumer_offsets-34 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,615] INFO [Broker id=1] Follower __consumer_offsets-35 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,616] INFO [Broker id=1] Follower __consumer_offsets-27 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,614] INFO [Broker id=1] Follower _confluent-link-metadata-22 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,616] INFO [Broker id=1] Follower _confluent-link-metadata-28 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,614] INFO [Broker id=1] Follower _confluent-link-metadata-47 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,616] INFO [Broker id=1] Follower __consumer_offsets-25 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,616] INFO [Broker id=1] Follower _confluent-link-metadata-14 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,614] INFO [Broker id=1] Follower _confluent-link-metadata-36 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,616] INFO [Broker id=1] Follower __consumer_offsets-17 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,616] INFO [Broker id=1] Follower __consumer_offsets-2 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,615] INFO [Broker id=1] Follower _confluent-link-metadata-6 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,616] INFO [Broker id=1] Follower _confluent-telemetry-metrics-2 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,615] INFO [Broker id=1] Follower _confluent-link-metadata-40 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,616] INFO [Broker id=1] Follower __consumer_offsets-9 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,615] INFO [Broker id=1] Follower _confluent-link-metadata-43 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,615] INFO [Broker id=1] Follower __consumer_offsets-47 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,616] INFO [Broker id=1] Follower _confluent-link-metadata-10 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,616] INFO [Broker id=1] Follower __consumer_offsets-42 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,616] INFO [Broker id=1] Follower __consumer_offsets-14 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,616] INFO [Broker id=1] Follower __consumer_offsets-13 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,617] INFO [Broker id=1] Follower __consumer_offsets-46 starts at leader epoch 1 from offset 0 with partition epoch 1 and high watermark 0. Current leader is -1. Previous leader Some(-1) and previous leader epoch was 0. (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,618] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions HashSet(__consumer_offsets-13, _confluent-link-metadata-14, _confluent-telemetry-metrics-9, __consumer_offsets-22, _confluent-link-metadata-8, _confluent-link-metadata-24, _confluent-link-metadata-16, __consumer_offsets-30, _confluent-telemetry-metrics-5, _confluent-link-metadata-45, __consumer_offsets-8, __consumer_offsets-21, __consumer_offsets-4, _confluent-link-metadata-3, _confluent-telemetry-metrics-10, _confluent-link-metadata-32, _confluent-link-metadata-2, _confluent-link-metadata-21, _confluent-link-metadata-39, __consumer_offsets-27, __consumer_offsets-7, _confluent-link-metadata-12, __consumer_offsets-9, __consumer_offsets-46, _confluent-link-metadata-20, _confluent-link-metadata-42, __consumer_offsets-25, __consumer_offsets-35, _confluent-link-metadata-28, __consumer_offsets-41, _confluent-link-metadata-6, __consumer_offsets-33, _confluent-link-metadata-35, __consumer_offsets-49, __consumer_offsets-23, _confluent-link-metadata-15, _confluent-link-metadata-22, _confluent-telemetry-metrics-4, __consumer_offsets-47, __consumer_offsets-16, _confluent-link-metadata-46, __consumer_offsets-28, _confluent-telemetry-metrics-7, __consumer_offsets-31, __consumer_offsets-36, _confluent-link-metadata-1, _confluent-link-metadata-27, __consumer_offsets-42, _confluent-link-metadata-19, _confluent-link-metadata-31, _confluent-link-metadata-23, _confluent-telemetry-metrics-11, __consumer_offsets-18, _confluent-link-metadata-11, __consumer_offsets-3, _confluent-link-metadata-49, __consumer_offsets-37, __consumer_offsets-15, __consumer_offsets-24, _confluent-link-metadata-7, _confluent-command-0, _confluent-link-metadata-41, _confluent-link-metadata-5, _confluent-link-metadata-29, _confluent-link-metadata-13, _confluent_balancer_api_state-0, _confluent-telemetry-metrics-8, _confluent-link-metadata-47, __consumer_offsets-38, __consumer_offsets-17, _confluent-link-metadata-40, __consumer_offsets-48, _confluent-link-metadata-48, _confluent-link-metadata-37, _confluent-telemetry-metrics-6, _confluent-link-metadata-0, __consumer_offsets-19, _confluent-telemetry-metrics-3, _confluent-link-metadata-18, __consumer_offsets-11, _confluent-link-metadata-26, _confluent-link-metadata-10, __consumer_offsets-2, __consumer_offsets-43, __consumer_offsets-6, __consumer_offsets-14, _confluent-link-metadata-36, _confluent-link-metadata-44, _confluent-link-metadata-34, _confluent-telemetry-metrics-0, _confluent-link-metadata-4, _confluent-link-metadata-9, __consumer_offsets-20, __consumer_offsets-0, __consumer_offsets-44, _confluent-link-metadata-38, _confluent-telemetry-metrics-2, __consumer_offsets-39, _confluent-link-metadata-30, __consumer_offsets-12, __consumer_offsets-45, _confluent-link-metadata-33, __consumer_offsets-1, __consumer_offsets-5, __consumer_offsets-26, _confluent-link-metadata-17, __consumer_offsets-29, _confluent-telemetry-metrics-1, __consumer_offsets-34, __consumer_offsets-10, __consumer_offsets-32, __consumer_offsets-40, _confluent-link-metadata-25, _confluent-link-metadata-43) (kafka.server.ReplicaFetcherManager)
benchi-kafka     | [2025-03-18 16:10:20,618] INFO [ReplicaAlterLogDirsManager on broker 1] Removed fetcher for partitions HashSet(__consumer_offsets-13, _confluent-link-metadata-14, _confluent-telemetry-metrics-9, __consumer_offsets-22, _confluent-link-metadata-8, _confluent-link-metadata-24, _confluent-link-metadata-16, __consumer_offsets-30, _confluent-telemetry-metrics-5, _confluent-link-metadata-45, __consumer_offsets-8, __consumer_offsets-21, __consumer_offsets-4, _confluent-link-metadata-3, _confluent-telemetry-metrics-10, _confluent-link-metadata-32, _confluent-link-metadata-2, _confluent-link-metadata-21, _confluent-link-metadata-39, __consumer_offsets-27, __consumer_offsets-7, _confluent-link-metadata-12, __consumer_offsets-9, __consumer_offsets-46, _confluent-link-metadata-20, _confluent-link-metadata-42, __consumer_offsets-25, __consumer_offsets-35, _confluent-link-metadata-28, __consumer_offsets-41, _confluent-link-metadata-6, __consumer_offsets-33, _confluent-link-metadata-35, __consumer_offsets-49, __consumer_offsets-23, _confluent-link-metadata-15, _confluent-link-metadata-22, _confluent-telemetry-metrics-4, __consumer_offsets-47, __consumer_offsets-16, _confluent-link-metadata-46, __consumer_offsets-28, _confluent-telemetry-metrics-7, __consumer_offsets-31, __consumer_offsets-36, _confluent-link-metadata-1, _confluent-link-metadata-27, __consumer_offsets-42, _confluent-link-metadata-19, _confluent-link-metadata-31, _confluent-link-metadata-23, _confluent-telemetry-metrics-11, __consumer_offsets-18, _confluent-link-metadata-11, __consumer_offsets-3, _confluent-link-metadata-49, __consumer_offsets-37, __consumer_offsets-15, __consumer_offsets-24, _confluent-link-metadata-7, _confluent-command-0, _confluent-link-metadata-41, _confluent-link-metadata-5, _confluent-link-metadata-29, _confluent-link-metadata-13, _confluent_balancer_api_state-0, _confluent-telemetry-metrics-8, _confluent-link-metadata-47, __consumer_offsets-38, __consumer_offsets-17, _confluent-link-metadata-40, __consumer_offsets-48, _confluent-link-metadata-48, _confluent-link-metadata-37, _confluent-telemetry-metrics-6, _confluent-link-metadata-0, __consumer_offsets-19, _confluent-telemetry-metrics-3, _confluent-link-metadata-18, __consumer_offsets-11, _confluent-link-metadata-26, _confluent-link-metadata-10, __consumer_offsets-2, __consumer_offsets-43, __consumer_offsets-6, __consumer_offsets-14, _confluent-link-metadata-36, _confluent-link-metadata-44, _confluent-link-metadata-34, _confluent-telemetry-metrics-0, _confluent-link-metadata-4, _confluent-link-metadata-9, __consumer_offsets-20, __consumer_offsets-0, __consumer_offsets-44, _confluent-link-metadata-38, _confluent-telemetry-metrics-2, __consumer_offsets-39, _confluent-link-metadata-30, __consumer_offsets-12, __consumer_offsets-45, _confluent-link-metadata-33, __consumer_offsets-1, __consumer_offsets-5, __consumer_offsets-26, _confluent-link-metadata-17, __consumer_offsets-29, _confluent-telemetry-metrics-1, __consumer_offsets-34, __consumer_offsets-10, __consumer_offsets-32, __consumer_offsets-40, _confluent-link-metadata-25, _confluent-link-metadata-43) (kafka.server.ReplicaAlterLogDirsManager)
benchi-kafka     | [2025-03-18 16:10:20,620] INFO [Broker id=1] Stopped fetchers as part of controlled shutdown for 114 partitions (state.change.logger)
benchi-kafka     | [2025-03-18 16:10:20,620] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 13 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:10:20,621] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-13 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,621] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 46 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:10:20,621] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-46 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,621] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 9 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:10:20,621] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-9 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,622] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 42 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:10:20,622] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-42 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,622] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 21 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:10:20,622] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-21 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,622] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 17 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:10:20,622] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-17 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,622] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-42 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,622] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-46 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,622] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-21 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,622] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 30 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:10:20,622] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-30 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,622] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-13 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,622] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-9 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,622] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-17 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,623] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 26 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:10:20,623] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-26 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,623] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-30 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,623] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 5 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:10:20,623] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-5 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,623] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-26 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,623] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 38 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:10:20,623] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-38 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,623] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-5 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,623] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 1 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:10:20,623] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-1 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,623] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-38 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,623] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 34 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:10:20,623] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-34 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,624] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-1 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,624] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 16 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:10:20,624] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-16 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,624] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-34 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,624] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 45 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:10:20,624] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-45 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,624] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-16 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,624] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 12 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:10:20,624] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-12 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,624] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-45 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,624] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 41 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:10:20,624] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-41 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,624] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-12 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,625] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 24 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:10:20,625] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-24 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,625] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-41 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,625] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 20 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:10:20,625] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-20 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,625] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-24 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,625] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 49 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:10:20,625] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-49 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,625] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-20 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,625] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 0 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:10:20,625] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-0 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,625] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-49 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,625] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 29 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:10:20,625] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-29 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,625] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-0 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,625] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 25 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:10:20,625] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-25 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,626] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-29 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,626] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 8 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:10:20,626] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-8 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,626] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-25 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,626] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 37 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:10:20,626] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-37 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,626] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-8 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,626] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 4 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:10:20,626] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-4 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,626] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-37 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,626] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 33 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:10:20,626] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-33 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,626] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-4 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,626] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 15 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:10:20,626] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-15 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,627] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-33 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,627] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 48 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:10:20,627] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-48 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,627] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-15 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,627] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 11 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:10:20,627] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-11 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,627] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 44 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:10:20,627] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-44 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,627] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-48 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,627] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-11 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,627] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 23 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:10:20,627] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-23 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,627] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-44 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,627] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 19 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:10:20,627] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-19 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,627] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 32 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:10:20,627] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-32 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,627] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 28 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:10:20,627] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-28 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,627] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 7 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:10:20,627] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-7 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,627] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 40 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:10:20,627] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-40 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,627] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 3 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:10:20,627] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-3 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,627] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 36 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:10:20,627] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-36 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,627] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 47 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:10:20,627] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-47 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,627] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 14 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:10:20,627] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-14 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,627] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 43 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:10:20,627] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-43 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,627] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 10 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:10:20,628] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-10 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,628] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 22 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:10:20,628] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-22 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,628] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 18 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:10:20,628] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-18 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,628] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 31 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:10:20,628] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-31 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,628] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 27 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:10:20,628] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-27 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,628] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 39 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:10:20,628] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-39 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,628] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 6 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:10:20,628] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-6 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,628] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 35 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:10:20,628] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-35 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,628] INFO [GroupCoordinator 1]: Resigned as the group coordinator for partition 2 in epoch OptionalInt[1] (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:10:20,628] INFO [GroupMetadataManager brokerId=1] Scheduling unloading of offsets and group metadata from __consumer_offsets-2 (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,628] INFO [GroupCoordinator 1]: Unloading group metadata for ConfluentTelemetryReporterSampler--2946986101702943459 with generation 1 (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:10:20,628] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-23 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 1 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,628] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-19 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,628] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-32 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,628] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-28 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,628] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-7 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,628] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-40 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,628] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-3 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,629] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-36 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,629] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-47 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,629] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-14 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,629] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-43 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,629] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-10 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,629] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-22 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,629] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-18 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,629] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-31 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,629] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-27 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,629] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-6 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,629] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-35 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,629] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-2 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:20,629] INFO [GroupMetadataManager brokerId=1] Finished unloading __consumer_offsets-39 for coordinator epoch OptionalInt[1]. Removed 0 cached offsets and 0 cached groups. (kafka.coordinator.group.GroupMetadataManager)
benchi-kafka     | [2025-03-18 16:10:22,544] INFO [ControllerServer id=1] The request from broker 1 to shut down has been granted since the lowest active offset 9223372036854775807 is now greater than the broker's controlled shutdown offset 518. (org.apache.kafka.controller.BrokerHeartbeatManager)
benchi-kafka     | [2025-03-18 16:10:22,545] INFO [ControllerServer id=1] Replayed BrokerRegistrationChangeRecord modifying the registration for broker 1: BrokerRegistrationChangeRecord(brokerId=1, brokerEpoch=6, fenced=1, inControlledShutdown=0, degradedComponents=null, logDirs=[]) (org.apache.kafka.controller.ClusterControlManager)
benchi-kafka     | [2025-03-18 16:10:22,572] INFO SBC Event SbcMetadataUpdateEvent-284 generated 1 more events to enqueue in the following order - [SbcBrokerFailureEvent-285]. Enqueuing... (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:10:22,572] INFO Handling event SbcBrokerFailureEvent-285 (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:10:22,572] INFO Notify broker failure: [1] (io.confluent.databalancer.KafkaDataBalanceManager)
benchi-kafka     | [2025-03-18 16:10:22,572] INFO [BrokerLifecycleManager id=1] The controller has asked us to exit controlled shutdown. (kafka.server.BrokerLifecycleManager)
benchi-kafka     | [2025-03-18 16:10:22,572] INFO [BrokerLifecycleManager id=1] beginShutdown: shutting down event queue. (org.apache.kafka.queue.KafkaEventQueue)
benchi-kafka     | [2025-03-18 16:10:22,572] INFO Notify DEAD_BROKER event for brokers: [1] (io.confluent.databalancer.ConfluentDataBalanceEngine)
benchi-kafka     | [2025-03-18 16:10:22,573] INFO [BrokerLifecycleManager id=1] Transitioning from PENDING_CONTROLLED_SHUTDOWN to SHUTTING_DOWN. (kafka.server.BrokerLifecycleManager)
benchi-kafka     | [2025-03-18 16:10:22,573] INFO [broker-1-to-controller-heartbeat-channel-manager]: Shutting down (kafka.server.NodeToControllerRequestThread)
benchi-kafka     | [2025-03-18 16:10:22,573] INFO Databalancer: Working on tracking DEAD_BROKER event for brokers ([1]) (io.confluent.databalancer.ConfluentDataBalanceEngine)
benchi-kafka     | [2025-03-18 16:10:22,573] INFO [broker-1-to-controller-heartbeat-channel-manager]: Stopped (kafka.server.NodeToControllerRequestThread)
benchi-kafka     | [2025-03-18 16:10:22,573] INFO [broker-1-to-controller-heartbeat-channel-manager]: Shutdown completed (kafka.server.NodeToControllerRequestThread)
benchi-kafka     | [2025-03-18 16:10:22,573] INFO Notify broker removal: [1] (com.linkedin.kafka.cruisecontrol.detector.BrokerFailureDetector)
benchi-kafka     | [2025-03-18 16:10:22,573] INFO Scheduled check for broker failure detection triggered (com.linkedin.kafka.cruisecontrol.detector.BrokerFailureDetector)
benchi-kafka     | [2025-03-18 16:10:22,573] INFO GoalViolationDetector's view on DEAD brokers are updated from [] to [1]. (com.linkedin.kafka.cruisecontrol.detector.GoalViolationDetector)
benchi-kafka     | [2025-03-18 16:10:22,573] INFO Broker change event(s) detected: [Dead brokers: [1]] (com.linkedin.kafka.cruisecontrol.detector.BrokerFailureDetector)
benchi-kafka     | [2025-03-18 16:10:22,573] INFO [BrokerServer id=1] Controlled shutdown initiation completed successfully in 2.1 seconds (kafka.server.BrokerServer)
benchi-kafka     | [2025-03-18 16:10:22,574] INFO KafkaHttpServer transitioned from RUNNING to STOPPING.. (io.confluent.http.server.KafkaHttpServerImpl)
benchi-kafka     | [2025-03-18 16:10:22,574] INFO KafkaHttpServer transitioned from STOPPING to TERMINATED.. (io.confluent.http.server.KafkaHttpServerImpl)
benchi-kafka     | [2025-03-18 16:10:22,575] INFO Node to controller channel manager for heartbeat shutdown (kafka.server.NodeToControllerChannelManagerImpl)
benchi-kafka     | [2025-03-18 16:10:22,575] INFO Broker Load Metric closed. (kafka.metrics.BrokerLoad)
benchi-kafka     | [2025-03-18 16:10:22,575] INFO [SocketServer listenerType=BROKER, nodeId=1] Stopping socket server request processors (kafka.network.SocketServer)
benchi-kafka     | [2025-03-18 16:10:22,576] INFO [Producer clientId=confluent-metrics-reporter] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:10:22,576] INFO [AdminClient clientId=adminclient-5] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:10:22,576] INFO [Producer clientId=_confluent_balancer_api_state-producer-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:10:22,576] INFO [AdminClient clientId=adminclient-5] Cancelled in-flight DESCRIBE_BROKER_REPLICA_EXCLUSIONS request with correlation id 29 due to node 1 being disconnected (elapsed time since creation: 1ms, elapsed time since send: 1ms, throttle time: 0ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:10:22,576] INFO [AdminClient clientId=cluster-link--local-admin-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:10:22,576] INFO [Producer clientId=confluent-telemetry-reporter-local-producer] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:10:22,576] INFO [Producer clientId=_confluent_balancer_api_state-producer-1] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:10:22,576] INFO [AdminClient clientId=null-admin-1] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:10:22,576] INFO [AdminClient clientId=cluster-link--local-admin-1] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:10:22,576] INFO [Producer clientId=confluent-telemetry-reporter-local-producer] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:10:22,576] INFO [AdminClient clientId=cluster-link--local-admin-1] Cancelled in-flight METADATA request with correlation id 4 due to node 1 being disconnected (elapsed time since creation: 0ms, elapsed time since send: 0ms, throttle time: 0ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:10:22,576] INFO [Consumer clientId=_confluent_balancer_api_state-consumer-1, groupId=null] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:10:22,576] INFO [AdminClient clientId=null-admin-1] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:10:22,577] INFO [AdminClient clientId=null-admin-1] Cancelled in-flight METADATA request with correlation id 7 due to node 1 being disconnected (elapsed time since creation: 0ms, elapsed time since send: 0ms, throttle time: 0ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:10:22,576] INFO [Producer clientId=_confluent_balancer_api_state-producer-1] Cancelled in-flight METADATA request with correlation id 6 due to node 1 being disconnected (elapsed time since creation: 0ms, elapsed time since send: 0ms, throttle time: 0ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:10:22,576] INFO [Consumer clientId=_confluent_balancer_api_state-consumer-1, groupId=null] Cancelled in-flight FETCH request with correlation id 11653 due to node 1 being disconnected (elapsed time since creation: 1ms, elapsed time since send: 1ms, throttle time: 0ms, request timeout: 30000ms) (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:10:22,577] INFO [Consumer clientId=_confluent_balancer_api_state-consumer-1, groupId=null] Error sending fetch request (sessionId=298805610, epoch=11640) to node 1: (org.apache.kafka.clients.FetchSessionHandler)
benchi-kafka     | org.apache.kafka.common.errors.DisconnectException
benchi-kafka     | [2025-03-18 16:10:22,577] INFO [AdminClient clientId=adminclient-5] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:10:22,577] INFO [Producer clientId=confluent-metrics-reporter] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:10:22,577] WARN [Producer clientId=confluent-metrics-reporter] Connection to node 1 (localhost/127.0.0.1:9092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:10:22,577] INFO [Consumer clientId=_confluent_balancer_api_state-consumer-1, groupId=null] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:10:22,583] INFO [SocketServer listenerType=BROKER, nodeId=1] Stopped socket server request processors (kafka.network.SocketServer)
benchi-kafka     | [2025-03-18 16:10:22,584] INFO [data-plane Kafka Request Handler on Broker 1], shutting down (kafka.server.KafkaRequestHandlerPool)
benchi-kafka     | [2025-03-18 16:10:22,584] INFO [data-plane Kafka Request Handler on Broker 1], shut down completely (kafka.server.KafkaRequestHandlerPool)
benchi-kafka     | [2025-03-18 16:10:22,585] INFO [ExpirationReaper-1-AlterAcls]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:10:22,585] INFO [ExpirationReaper-1-AlterAcls]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:10:22,585] INFO [ExpirationReaper-1-AlterAcls]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:10:22,585] INFO [KafkaApi-1] Shutdown complete. (kafka.server.KafkaApis)
benchi-kafka     | [2025-03-18 16:10:22,587] INFO [TransactionCoordinator id=1] Shutting down. (kafka.coordinator.transaction.TransactionCoordinator)
benchi-kafka     | [2025-03-18 16:10:22,587] INFO [Transaction State Manager 1]: Shutdown complete (kafka.coordinator.transaction.TransactionStateManager)
benchi-kafka     | [2025-03-18 16:10:22,587] INFO [TxnMarkerSenderThread-1]: Shutting down (kafka.coordinator.transaction.TransactionMarkerChannelManager)
benchi-kafka     | [2025-03-18 16:10:22,587] INFO [TxnMarkerSenderThread-1]: Stopped (kafka.coordinator.transaction.TransactionMarkerChannelManager)
benchi-kafka     | [2025-03-18 16:10:22,587] INFO [TxnMarkerSenderThread-1]: Shutdown completed (kafka.coordinator.transaction.TransactionMarkerChannelManager)
benchi-kafka     | [2025-03-18 16:10:22,588] INFO [TransactionCoordinator id=1] Shutdown complete. (kafka.coordinator.transaction.TransactionCoordinator)
benchi-kafka     | [2025-03-18 16:10:22,588] INFO [GroupCoordinator 1]: Shutting down. (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:10:22,588] INFO [ExpirationReaper-1-Heartbeat]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:10:22,588] INFO [ExpirationReaper-1-Heartbeat]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:10:22,588] INFO [ExpirationReaper-1-Heartbeat]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:10:22,588] INFO [ExpirationReaper-1-Rebalance]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:10:22,588] INFO [ExpirationReaper-1-Rebalance]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:10:22,588] INFO [ExpirationReaper-1-Rebalance]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:10:22,589] INFO [GroupCoordinator 1]: Shutdown complete. (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:10:22,589] INFO [AssignmentsManager id=1]KafkaEventQueue#close: shutting down event queue. (org.apache.kafka.queue.KafkaEventQueue)
benchi-kafka     | [2025-03-18 16:10:22,589] INFO [broker-1-to-controller-directory-assignments-channel-manager]: Shutting down (kafka.server.NodeToControllerRequestThread)
benchi-kafka     | [2025-03-18 16:10:22,589] INFO [broker-1-to-controller-directory-assignments-channel-manager]: Stopped (kafka.server.NodeToControllerRequestThread)
benchi-kafka     | [2025-03-18 16:10:22,589] INFO [broker-1-to-controller-directory-assignments-channel-manager]: Shutdown completed (kafka.server.NodeToControllerRequestThread)
benchi-kafka     | [2025-03-18 16:10:22,589] INFO Node to controller channel manager for directory-assignments shutdown (kafka.server.NodeToControllerChannelManagerImpl)
benchi-kafka     | [2025-03-18 16:10:22,589] INFO [AssignmentsManager id=1]closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
benchi-kafka     | [2025-03-18 16:10:22,591] INFO [ReplicaManager broker=1] Shutting down (kafka.server.ReplicaManager)
benchi-kafka     | [2025-03-18 16:10:22,591] INFO [LogDirFailureHandler]: Shutting down (kafka.server.ReplicaManager$LogDirFailureHandler)
benchi-kafka     | [2025-03-18 16:10:22,591] INFO [LogDirFailureHandler]: Stopped (kafka.server.ReplicaManager$LogDirFailureHandler)
benchi-kafka     | [2025-03-18 16:10:22,591] INFO [LogDirFailureHandler]: Shutdown completed (kafka.server.ReplicaManager$LogDirFailureHandler)
benchi-kafka     | [2025-03-18 16:10:22,591] INFO [ReplicaFetcherManager on broker 1] shutting down (kafka.server.ReplicaFetcherManager)
benchi-kafka     | [2025-03-18 16:10:22,592] INFO [ReplicaFetcherManager on broker 1] shutdown completed (kafka.server.ReplicaFetcherManager)
benchi-kafka     | [2025-03-18 16:10:22,592] INFO [ReplicaAlterLogDirsManager on broker 1] shutting down (kafka.server.ReplicaAlterLogDirsManager)
benchi-kafka     | [2025-03-18 16:10:22,592] INFO [ReplicaAlterLogDirsManager on broker 1] shutdown completed (kafka.server.ReplicaAlterLogDirsManager)
benchi-kafka     | [2025-03-18 16:10:22,592] INFO [ExpirationReaper-1-Fetch]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:10:22,592] INFO [ExpirationReaper-1-Fetch]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:10:22,592] INFO [ExpirationReaper-1-Fetch]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:10:22,592] INFO [ExpirationReaper-1-Produce]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:10:22,592] INFO [ExpirationReaper-1-Produce]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:10:22,592] INFO [ExpirationReaper-1-Produce]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:10:22,592] INFO [ExpirationReaper-1-DeleteRecords]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:10:22,593] INFO [ExpirationReaper-1-DeleteRecords]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:10:22,593] INFO [ExpirationReaper-1-DeleteRecords]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:10:22,593] INFO [ExpirationReaper-1-ElectLeader]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:10:22,593] INFO [ExpirationReaper-1-ElectLeader]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:10:22,593] INFO [ExpirationReaper-1-ElectLeader]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:10:22,593] INFO [ExpirationReaper-1-ListOffsets]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:10:22,593] INFO [ExpirationReaper-1-ListOffsets]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:10:22,593] INFO [ExpirationReaper-1-ListOffsets]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:10:22,596] INFO [AddPartitionsToTxnSenderThread-1]: Shutting down (kafka.server.AddPartitionsToTxnManager)
benchi-kafka     | [2025-03-18 16:10:22,596] INFO [AddPartitionsToTxnSenderThread-1]: Stopped (kafka.server.AddPartitionsToTxnManager)
benchi-kafka     | [2025-03-18 16:10:22,596] INFO [AddPartitionsToTxnSenderThread-1]: Shutdown completed (kafka.server.AddPartitionsToTxnManager)
benchi-kafka     | [2025-03-18 16:10:22,597] INFO [ReplicaManager broker=1] Shut down completely (kafka.server.ReplicaManager)
benchi-kafka     | [2025-03-18 16:10:22,597] INFO [ClusterLinkManager-broker-1] Shutting down (kafka.server.link.ClusterLinkManager)
benchi-kafka     | [2025-03-18 16:10:22,599] INFO App info kafka.admin.client for cluster-link--local-admin-1 unregistered (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:10:22,599] INFO [AdminClient clientId=cluster-link--local-admin-1] Metadata update failed (org.apache.kafka.clients.admin.internals.AdminMetadataManager)
benchi-kafka     | org.apache.kafka.common.errors.TimeoutException: The AdminClient thread has exited. Call: fetchMetadata
benchi-kafka     | [2025-03-18 16:10:22,599] INFO [AdminClient clientId=cluster-link--local-admin-1] Timed out 1 remaining operation(s) during close. (org.apache.kafka.clients.admin.KafkaAdminClient)
benchi-kafka     | [2025-03-18 16:10:22,599] INFO [ExpirationReaper-1-ClusterLink]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:10:22,599] INFO [ExpirationReaper-1-ClusterLink]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:10:22,599] INFO [ExpirationReaper-1-ClusterLink]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:10:22,601] INFO [ClusterLinkManager-broker-1] Shutdown completed (kafka.server.link.ClusterLinkManager)
benchi-kafka     | [2025-03-18 16:10:22,601] INFO [broker-1-to-controller-alter-partition-channel-manager]: Shutting down (kafka.server.NodeToControllerRequestThread)
benchi-kafka     | [2025-03-18 16:10:22,601] INFO [broker-1-to-controller-alter-partition-channel-manager]: Stopped (kafka.server.NodeToControllerRequestThread)
benchi-kafka     | [2025-03-18 16:10:22,601] INFO [broker-1-to-controller-alter-partition-channel-manager]: Shutdown completed (kafka.server.NodeToControllerRequestThread)
benchi-kafka     | [2025-03-18 16:10:22,602] INFO Node to controller channel manager for alter-partition shutdown (kafka.server.NodeToControllerChannelManagerImpl)
benchi-kafka     | [2025-03-18 16:10:22,602] INFO [broker-1-to-controller-forwarding-channel-manager]: Shutting down (kafka.server.NodeToControllerRequestThread)
benchi-kafka     | [2025-03-18 16:10:22,602] INFO [broker-1-to-controller-forwarding-channel-manager]: Stopped (kafka.server.NodeToControllerRequestThread)
benchi-kafka     | [2025-03-18 16:10:22,602] INFO [broker-1-to-controller-forwarding-channel-manager]: Shutdown completed (kafka.server.NodeToControllerRequestThread)
benchi-kafka     | [2025-03-18 16:10:22,602] INFO Node to controller channel manager for forwarding shutdown (kafka.server.NodeToControllerChannelManagerImpl)
benchi-kafka     | [2025-03-18 16:10:22,602] INFO Shutting down. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,603] INFO Shutting down the log cleaner. (kafka.log.LogCleaner)
benchi-kafka     | [2025-03-18 16:10:22,603] INFO [kafka-log-cleaner-thread-0]: Shutting down (kafka.log.LogCleaner$CleanerThread)
benchi-kafka     | [2025-03-18 16:10:22,603] INFO [kafka-log-cleaner-thread-0]: Stopped (kafka.log.LogCleaner$CleanerThread)
benchi-kafka     | [2025-03-18 16:10:22,603] INFO [kafka-log-cleaner-thread-0]: Shutdown completed (kafka.log.LogCleaner$CleanerThread)
benchi-kafka     | [2025-03-18 16:10:22,607] INFO Tier partition state for 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-43 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,609] INFO Closing log for __consumer_offsets-43 took 3 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,610] INFO Tier partition state for 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-0 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,610] INFO Closing log for _confluent-link-metadata-0 took 2 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,611] INFO Tier partition state for 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-10 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,611] INFO Closing log for _confluent-link-metadata-10 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,613] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:10:22,613] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Node -1 disconnected. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:10:22,613] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Node 2147483646 disconnected. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:10:22,613] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Group coordinator broker:29092 (id: 2147483646 rack: null) is unavailable or invalid due to cause: coordinator unavailable. isDisconnected: true. Rediscovery will be attempted. (org.apache.kafka.clients.consumer.internals.ConsumerCoordinator)
benchi-kafka     | [2025-03-18 16:10:22,615] INFO Tier partition state for 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-30 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,615] INFO Closing log for __consumer_offsets-30 took 4 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,617] INFO Tier partition state for 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-32 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,617] INFO Closing log for __consumer_offsets-32 took 2 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,618] INFO Tier partition state for 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-15 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,618] INFO Closing log for _confluent-link-metadata-15 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,621] INFO Tier partition state for 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-25 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,621] INFO Closing log for __consumer_offsets-25 took 2 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,622] INFO Tier partition state for 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-7 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,622] INFO Closing log for _confluent-link-metadata-7 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,623] INFO Tier partition state for 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-41 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,623] INFO Closing log for __consumer_offsets-41 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,625] INFO Tier partition state for rACWIAgeSLaNjkitMhNiUQ:_confluent-telemetry-metrics-6 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,625] INFO Closing log for _confluent-telemetry-metrics-6 took 2 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,627] INFO Tier partition state for rACWIAgeSLaNjkitMhNiUQ:_confluent-telemetry-metrics-4 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,627] INFO Closing log for _confluent-telemetry-metrics-4 took 2 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,629] INFO Tier partition state for Tl_wzRtXT7-I7ZNugmF7hg:_confluent-command-0 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,629] INFO Closing log for _confluent-command-0 took 2 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,630] INFO Tier partition state for 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-28 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,630] INFO Closing log for _confluent-link-metadata-28 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,632] INFO Tier partition state for 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-49 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,632] INFO Closing log for __consumer_offsets-49 took 2 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,634] INFO Tier partition state for rACWIAgeSLaNjkitMhNiUQ:_confluent-telemetry-metrics-2 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,635] INFO Closing log for _confluent-telemetry-metrics-2 took 2 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,636] INFO Tier partition state for 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-47 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,636] INFO Closing log for _confluent-link-metadata-47 took 2 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,637] INFO Tier partition state for 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-40 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,637] INFO Closing log for __consumer_offsets-40 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,638] INFO Tier partition state for 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-30 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,638] INFO Closing log for _confluent-link-metadata-30 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,640] INFO Tier partition state for 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-27 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,640] INFO Closing log for _confluent-link-metadata-27 took 2 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,641] INFO Tier partition state for 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-27 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,641] INFO Closing log for __consumer_offsets-27 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,642] INFO Tier partition state for 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-5 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,643] INFO Closing log for _confluent-link-metadata-5 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,645] INFO Tier partition state for 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-24 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,645] INFO Closing log for _confluent-link-metadata-24 took 2 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,646] INFO Tier partition state for 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-12 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,646] INFO Closing log for __consumer_offsets-12 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,647] INFO Tier partition state for 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-36 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,647] INFO Closing log for __consumer_offsets-36 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,648] INFO Tier partition state for 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-45 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,649] INFO Closing log for __consumer_offsets-45 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,650] INFO Tier partition state for 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-10 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,650] INFO Closing log for __consumer_offsets-10 took 2 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,651] INFO Tier partition state for 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-36 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,651] INFO Closing log for _confluent-link-metadata-36 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,653] INFO Tier partition state for 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-11 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,654] INFO Closing log for __consumer_offsets-11 took 2 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,655] INFO Tier partition state for 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-4 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,655] INFO Closing log for _confluent-link-metadata-4 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,656] INFO Tier partition state for 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-40 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,657] INFO Closing log for _confluent-link-metadata-40 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,658] INFO Tier partition state for 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-20 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,658] INFO Closing log for __consumer_offsets-20 took 2 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,659] INFO Tier partition state for 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-18 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,659] INFO Closing log for __consumer_offsets-18 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,660] INFO Tier partition state for 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-42 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,660] INFO Closing log for _confluent-link-metadata-42 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,663] INFO Tier partition state for 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-31 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,663] INFO Closing log for _confluent-link-metadata-31 took 3 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,664] INFO Tier partition state for 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-48 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,664] INFO Closing log for __consumer_offsets-48 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,666] INFO Tier partition state for 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-44 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,666] INFO Closing log for _confluent-link-metadata-44 took 2 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,667] INFO Tier partition state for 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-43 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,667] INFO Closing log for _confluent-link-metadata-43 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,668] INFO Tier partition state for 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-49 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,668] INFO Closing log for _confluent-link-metadata-49 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,669] INFO Tier partition state for 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-21 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,669] INFO Closing log for _confluent-link-metadata-21 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,671] INFO Tier partition state for 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-41 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,671] INFO Closing log for _confluent-link-metadata-41 took 2 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,673] INFO Tier partition state for pCMvnNWjTh6QVp7erZq34g:_confluent_balancer_api_state-0 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,674] INFO [ProducerStateManager partition=_confluent_balancer_api_state-0] Wrote producer snapshot at offset 1 with 0 producer ids in 1 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
benchi-kafka     | [2025-03-18 16:10:22,675] INFO Closing log for _confluent_balancer_api_state-0 took 3 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,676] INFO Tier partition state for 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-23 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,676] INFO [ProducerStateManager partition=__consumer_offsets-23] Wrote producer snapshot at offset 1 with 0 producer ids in 0 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
benchi-kafka     | [2025-03-18 16:10:22,676] INFO Closing log for __consumer_offsets-23 took 2 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,677] INFO Tier partition state for 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-1 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,678] INFO Closing log for _confluent-link-metadata-1 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,678] INFO [Consumer clientId=_confluent_balancer_api_state-consumer-1, groupId=null] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:10:22,678] WARN [Consumer clientId=_confluent_balancer_api_state-consumer-1, groupId=null] Connection to node 1 (broker/172.20.0.2:29092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:10:22,678] INFO [Consumer clientId=_confluent_balancer_api_state-consumer-1, groupId=null] Error sending fetch request (sessionId=298805610, epoch=INITIAL) to node 1: (org.apache.kafka.clients.FetchSessionHandler)
benchi-kafka     | org.apache.kafka.common.errors.DisconnectException
benchi-kafka     | [2025-03-18 16:10:22,681] INFO Tier partition state for 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-11 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,681] INFO Closing log for _confluent-link-metadata-11 took 3 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,682] INFO Tier partition state for 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-42 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,682] INFO Closing log for __consumer_offsets-42 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,684] INFO Tier partition state for 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-28 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,684] INFO Closing log for __consumer_offsets-28 took 2 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,685] INFO Tier partition state for 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-17 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,685] INFO Closing log for _confluent-link-metadata-17 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,688] INFO Tier partition state for 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-4 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,688] INFO Closing log for __consumer_offsets-4 took 3 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,689] INFO Tier partition state for 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-18 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,690] INFO Closing log for _confluent-link-metadata-18 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,691] INFO Tier partition state for 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-1 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,691] INFO Closing log for __consumer_offsets-1 took 2 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,692] INFO Tier partition state for 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-38 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,692] INFO Closing log for __consumer_offsets-38 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,694] INFO Tier partition state for rACWIAgeSLaNjkitMhNiUQ:_confluent-telemetry-metrics-9 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,694] INFO Closing log for _confluent-telemetry-metrics-9 took 2 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,695] INFO Tier partition state for 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-14 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,695] INFO Closing log for __consumer_offsets-14 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,696] INFO [Producer clientId=_confluent_balancer_api_state-producer-1] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:10:22,696] WARN [Producer clientId=_confluent_balancer_api_state-producer-1] Connection to node 1 (broker/172.20.0.2:29092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:10:22,697] INFO Tier partition state for 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-16 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,697] INFO Closing log for _confluent-link-metadata-16 took 2 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,700] INFO Tier partition state for 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-8 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,700] INFO Closing log for _confluent-link-metadata-8 took 3 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,702] INFO Tier partition state for 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-46 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,702] INFO Closing log for _confluent-link-metadata-46 took 2 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,704] INFO Tier partition state for 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-29 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,704] INFO Closing log for __consumer_offsets-29 took 2 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,706] INFO Tier partition state for 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-6 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,706] INFO Closing log for __consumer_offsets-6 took 2 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,707] INFO Tier partition state for 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-0 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,708] INFO Closing log for __consumer_offsets-0 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,709] INFO Tier partition state for 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-35 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,709] INFO Closing log for __consumer_offsets-35 took 2 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,711] INFO Tier partition state for 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-13 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,711] INFO Closing log for __consumer_offsets-13 took 2 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,713] INFO Tier partition state for 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-26 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,713] INFO Closing log for __consumer_offsets-26 took 2 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,714] INFO Tier partition state for 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-21 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,714] INFO Closing log for __consumer_offsets-21 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,716] INFO Tier partition state for 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-19 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,716] INFO Closing log for __consumer_offsets-19 took 2 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,718] INFO Tier partition state for 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-6 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,718] INFO Closing log for _confluent-link-metadata-6 took 2 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,719] INFO Tier partition state for 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-33 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,719] INFO Closing log for __consumer_offsets-33 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,721] INFO Tier partition state for 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-25 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,721] INFO Closing log for _confluent-link-metadata-25 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,722] INFO Tier partition state for 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-2 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,722] INFO Closing log for _confluent-link-metadata-2 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,724] INFO Tier partition state for 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-37 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,724] INFO Closing log for __consumer_offsets-37 took 2 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,726] INFO Tier partition state for 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-8 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,726] INFO Closing log for __consumer_offsets-8 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,727] INFO Tier partition state for 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-24 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,727] INFO Closing log for __consumer_offsets-24 took 2 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,728] INFO Tier partition state for rACWIAgeSLaNjkitMhNiUQ:_confluent-telemetry-metrics-11 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,728] INFO [ProducerStateManager partition=_confluent-telemetry-metrics-11] Wrote producer snapshot at offset 115 with 0 producer ids in 0 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
benchi-kafka     | [2025-03-18 16:10:22,729] INFO Closing log for _confluent-telemetry-metrics-11 took 2 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,730] INFO Tier partition state for 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-48 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,731] INFO Closing log for _confluent-link-metadata-48 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,733] INFO Tier partition state for 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-45 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,733] INFO Closing log for _confluent-link-metadata-45 took 3 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,734] INFO Tier partition state for rACWIAgeSLaNjkitMhNiUQ:_confluent-telemetry-metrics-7 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,734] INFO Closing log for _confluent-telemetry-metrics-7 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,735] INFO Tier partition state for 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-29 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,735] INFO Closing log for _confluent-link-metadata-29 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,738] INFO Tier partition state for 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-3 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,738] INFO Closing log for __consumer_offsets-3 took 3 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,739] INFO Tier partition state for 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-20 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,740] INFO Closing log for _confluent-link-metadata-20 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,741] INFO Tier partition state for 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-17 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,741] INFO Closing log for __consumer_offsets-17 took 2 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,742] INFO Tier partition state for 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-39 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,743] INFO Closing log for __consumer_offsets-39 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,744] INFO Tier partition state for 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-2 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,744] INFO Closing log for __consumer_offsets-2 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,746] INFO Tier partition state for 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-37 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,746] INFO Closing log for _confluent-link-metadata-37 took 2 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,747] INFO Tier partition state for 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-44 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,748] INFO Closing log for __consumer_offsets-44 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,749] INFO Tier partition state for 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-12 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,749] INFO Closing log for _confluent-link-metadata-12 took 2 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,750] INFO Tier partition state for 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-13 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,750] INFO Closing log for _confluent-link-metadata-13 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,751] INFO Tier partition state for 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-16 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,751] INFO Closing log for __consumer_offsets-16 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,753] INFO Tier partition state for 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-14 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,753] INFO Closing log for _confluent-link-metadata-14 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,755] INFO Tier partition state for rACWIAgeSLaNjkitMhNiUQ:_confluent-telemetry-metrics-3 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,755] INFO Closing log for _confluent-telemetry-metrics-3 took 3 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,756] INFO Tier partition state for rACWIAgeSLaNjkitMhNiUQ:_confluent-telemetry-metrics-10 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,756] INFO Closing log for _confluent-telemetry-metrics-10 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,757] INFO Tier partition state for rACWIAgeSLaNjkitMhNiUQ:_confluent-telemetry-metrics-1 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,757] INFO Closing log for _confluent-telemetry-metrics-1 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,759] INFO Tier partition state for 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-22 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,759] INFO Closing log for _confluent-link-metadata-22 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,760] INFO Tier partition state for 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-47 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,760] INFO Closing log for __consumer_offsets-47 took 2 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,762] INFO Tier partition state for 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-3 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,762] INFO Closing log for _confluent-link-metadata-3 took 2 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,765] INFO Tier partition state for 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-26 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,765] INFO Closing log for _confluent-link-metadata-26 took 3 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,766] INFO Tier partition state for 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-7 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,766] INFO Closing log for __consumer_offsets-7 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,768] INFO Tier partition state for 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-35 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,768] INFO Closing log for _confluent-link-metadata-35 took 2 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,769] INFO Tier partition state for 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-38 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,769] INFO Closing log for _confluent-link-metadata-38 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,772] INFO Tier partition state for 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-39 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,772] INFO Closing log for _confluent-link-metadata-39 took 3 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,773] INFO Tier partition state for 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-34 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,773] INFO Closing log for _confluent-link-metadata-34 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,775] INFO Tier partition state for 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-22 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,775] INFO Closing log for __consumer_offsets-22 took 2 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,777] INFO Tier partition state for 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-32 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,777] INFO Closing log for _confluent-link-metadata-32 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,778] INFO Tier partition state for 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-19 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,778] INFO Closing log for _confluent-link-metadata-19 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,781] INFO Tier partition state for rACWIAgeSLaNjkitMhNiUQ:_confluent-telemetry-metrics-8 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,781] INFO [ProducerStateManager partition=_confluent-telemetry-metrics-8] Wrote producer snapshot at offset 138 with 0 producer ids in 0 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
benchi-kafka     | [2025-03-18 16:10:22,782] INFO Closing log for _confluent-telemetry-metrics-8 took 4 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,783] INFO Tier partition state for 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-46 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,784] INFO Closing log for __consumer_offsets-46 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,785] INFO Tier partition state for rACWIAgeSLaNjkitMhNiUQ:_confluent-telemetry-metrics-0 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,785] INFO Closing log for _confluent-telemetry-metrics-0 took 2 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,787] INFO Tier partition state for 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-23 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,787] INFO Closing log for _confluent-link-metadata-23 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,788] INFO Tier partition state for 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-9 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,788] INFO Closing log for _confluent-link-metadata-9 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,790] INFO Tier partition state for 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-31 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,790] INFO Closing log for __consumer_offsets-31 took 2 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,792] INFO Tier partition state for 9kJAA8KZRoWZtgk1eJTuEA:_confluent-link-metadata-33 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,792] INFO Closing log for _confluent-link-metadata-33 took 2 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,793] INFO Tier partition state for 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-5 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,793] INFO Closing log for __consumer_offsets-5 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,794] INFO Tier partition state for rACWIAgeSLaNjkitMhNiUQ:_confluent-telemetry-metrics-5 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,794] INFO Closing log for _confluent-telemetry-metrics-5 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,797] INFO Tier partition state for 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-15 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,797] INFO Closing log for __consumer_offsets-15 took 3 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,798] INFO Tier partition state for 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-34 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,799] INFO Closing log for __consumer_offsets-34 took 1 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,800] INFO Tier partition state for 3VYZksSVS72hxNY0NkQPGQ:__consumer_offsets-9 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,800] INFO Closing log for __consumer_offsets-9 took 2 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,816] INFO [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Node 1 disconnected. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:10:22,816] WARN [Consumer clientId=kafka-cruise-control, groupId=ConfluentTelemetryReporterSampler--2946986101702943459] Connection to node 1 (broker/172.20.0.2:29092) could not be established. Node may not be available. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:10:22,816] INFO Shutdown complete. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:10:22,817] INFO [BrokerHealthManager]: Shutting down (kafka.availability.BrokerHealthManager)
benchi-kafka     | [2025-03-18 16:10:22,817] INFO [BrokerHealthManager]: Stopped (kafka.availability.BrokerHealthManager)
benchi-kafka     | [2025-03-18 16:10:22,817] INFO [BrokerHealthManager]: Shutdown completed (kafka.availability.BrokerHealthManager)
benchi-kafka     | [2025-03-18 16:10:22,817] INFO [broker-1-ThrottledChannelReaper-Fetch]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:10:22,817] INFO [broker-1-ThrottledChannelReaper-Fetch]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:10:22,817] INFO [broker-1-ThrottledChannelReaper-Fetch]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:10:22,817] INFO [broker-1-ThrottledChannelReaper-Produce]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:10:22,817] INFO [broker-1-ThrottledChannelReaper-Produce]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:10:22,817] INFO [broker-1-ThrottledChannelReaper-Produce]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:10:22,817] INFO [broker-1-ThrottledChannelReaper-Request]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:10:22,818] INFO [broker-1-ThrottledChannelReaper-Request]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:10:22,818] INFO [broker-1-ThrottledChannelReaper-Request]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:10:22,818] INFO [broker-1-ThrottledChannelReaper-ClusterLinkRequest]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:10:22,818] INFO [broker-1-ThrottledChannelReaper-ClusterLinkRequest]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:10:22,818] INFO [broker-1-ThrottledChannelReaper-ClusterLinkRequest]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:10:22,818] INFO [broker-1-ThrottledChannelReaper-ControllerMutation]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:10:22,818] INFO [broker-1-ThrottledChannelReaper-ControllerMutation]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:10:22,818] INFO [broker-1-ThrottledChannelReaper-ControllerMutation]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:10:22,818] INFO [SocketServer listenerType=BROKER, nodeId=1] Shutting down socket server (kafka.network.SocketServer)
benchi-kafka     | [2025-03-18 16:10:22,825] INFO [SocketServer listenerType=BROKER, nodeId=1] Shutdown completed (kafka.network.SocketServer)
benchi-kafka     | [2025-03-18 16:10:22,825] INFO Broker and topic stats closed (kafka.server.BrokerTopicStats)
benchi-kafka     | [2025-03-18 16:10:22,826] INFO [BrokerLifecycleManager id=1] closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
benchi-kafka     | [2025-03-18 16:10:22,826] INFO [client-metrics-reaper]: Shutting down (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
benchi-kafka     | [2025-03-18 16:10:22,826] INFO [client-metrics-reaper]: Stopped (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
benchi-kafka     | [2025-03-18 16:10:22,826] INFO [client-metrics-reaper]: Shutdown completed (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
benchi-kafka     | [2025-03-18 16:10:22,828] INFO [BrokerServer id=1] shut down completed (kafka.server.BrokerServer)
benchi-kafka     | [2025-03-18 16:10:22,828] INFO [BrokerServer id=1] Transition from SHUTTING_DOWN to SHUTDOWN (kafka.server.BrokerServer)
benchi-kafka     | [2025-03-18 16:10:22,828] INFO [ControllerServer id=1] shutting down (kafka.server.ControllerServer)
benchi-kafka     | [2025-03-18 16:10:22,829] INFO [raft-expiration-reaper]: Shutting down (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:10:22,851] INFO [raft-expiration-reaper]: Stopped (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:10:22,851] INFO [raft-expiration-reaper]: Shutdown completed (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:10:22,851] INFO [kafka-1-raft-io-thread]: Shutting down (org.apache.kafka.raft.KafkaRaftClientDriver)
benchi-kafka     | [2025-03-18 16:10:22,851] INFO [RaftManager id=1] Beginning graceful shutdown (org.apache.kafka.raft.KafkaRaftClient)
benchi-kafka     | [2025-03-18 16:10:22,851] INFO [RaftManager id=1] Graceful shutdown completed (org.apache.kafka.raft.KafkaRaftClient)
benchi-kafka     | [2025-03-18 16:10:22,851] INFO [RaftManager id=1] Completed graceful shutdown of RaftClient (org.apache.kafka.raft.KafkaRaftClientDriver)
benchi-kafka     | [2025-03-18 16:10:22,851] INFO [kafka-1-raft-io-thread]: Stopped (org.apache.kafka.raft.KafkaRaftClientDriver)
benchi-kafka     | [2025-03-18 16:10:22,852] INFO [kafka-1-raft-io-thread]: Shutdown completed (org.apache.kafka.raft.KafkaRaftClientDriver)
benchi-kafka     | [2025-03-18 16:10:22,853] INFO [kafka-1-raft-outbound-request-thread]: Shutting down (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
benchi-kafka     | [2025-03-18 16:10:22,853] INFO [kafka-1-raft-outbound-request-thread]: Stopped (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
benchi-kafka     | [2025-03-18 16:10:22,853] INFO [kafka-1-raft-outbound-request-thread]: Shutdown completed (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
benchi-kafka     | [2025-03-18 16:10:22,854] INFO Tier partition state for __cluster_metadata-0 closed. (kafka.tier.state.FileTierPartitionState)
benchi-kafka     | [2025-03-18 16:10:22,854] INFO [ProducerStateManager partition=__cluster_metadata-0] Wrote producer snapshot at offset 524 with 0 producer ids in 0 ms. (org.apache.kafka.storage.internals.log.ProducerStateManager)
benchi-kafka     | [2025-03-18 16:10:22,856] INFO [ControllerRegistrationManager id=1 incarnation=7_mCSDvPTJS1-SXDmw9VyQ] beginShutdown: shutting down event queue. (org.apache.kafka.queue.KafkaEventQueue)
benchi-kafka     | [2025-03-18 16:10:22,856] INFO [ControllerRegistrationManager id=1 incarnation=7_mCSDvPTJS1-SXDmw9VyQ] shutting down. (kafka.server.ControllerRegistrationManager)
benchi-kafka     | [2025-03-18 16:10:22,856] INFO [controller-1-to-controller-registration-channel-manager]: Shutting down (kafka.server.NodeToControllerRequestThread)
benchi-kafka     | [2025-03-18 16:10:22,856] INFO [controller-1-to-controller-registration-channel-manager]: Stopped (kafka.server.NodeToControllerRequestThread)
benchi-kafka     | [2025-03-18 16:10:22,856] INFO [controller-1-to-controller-registration-channel-manager]: Shutdown completed (kafka.server.NodeToControllerRequestThread)
benchi-kafka     | [2025-03-18 16:10:22,857] INFO Node to controller channel manager for registration shutdown (kafka.server.NodeToControllerChannelManagerImpl)
benchi-kafka     | [2025-03-18 16:10:22,857] INFO [ControllerRegistrationManager id=1 incarnation=7_mCSDvPTJS1-SXDmw9VyQ] closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
benchi-kafka     | [2025-03-18 16:10:22,857] INFO [controller-1-to-controller-registration-channel-manager]: Shutdown completed (kafka.server.NodeToControllerRequestThread)
benchi-kafka     | [2025-03-18 16:10:22,857] WARN [NodeToControllerChannelManager id=1 name=registration] Attempting to close NetworkClient that has already been closed. (org.apache.kafka.clients.NetworkClient)
benchi-kafka     | [2025-03-18 16:10:22,857] INFO Node to controller channel manager for registration shutdown (kafka.server.NodeToControllerChannelManagerImpl)
benchi-kafka     | [2025-03-18 16:10:22,857] INFO [ControllerRegistrationManager id=1 incarnation=7_mCSDvPTJS1-SXDmw9VyQ] closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
benchi-kafka     | [2025-03-18 16:10:22,858] INFO [CelltControllerMetricsPublisher id=1] KafkaEventQueue#close: shutting down event queue. (org.apache.kafka.queue.KafkaEventQueue)
benchi-kafka     | [2025-03-18 16:10:22,858] INFO [CelltControllerMetricsPublisher id=1] closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
benchi-kafka     | [2025-03-18 16:10:22,859] INFO SbcEventQueueKafkaEventQueue#close: shutting down event queue. (org.apache.kafka.queue.KafkaEventQueue)
benchi-kafka     | [2025-03-18 16:10:22,859] INFO Handling event SbcShutdownEvent-286 (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:10:22,859] INFO SBC shutdown initiated. (io.confluent.databalancer.event.SbcEvent)
benchi-kafka     | [2025-03-18 16:10:22,859] INFO SbcEventQueueclosed event queue. (org.apache.kafka.queue.KafkaEventQueue)
benchi-kafka     | [2025-03-18 16:10:22,859] INFO [SocketServer listenerType=CONTROLLER, nodeId=1] Stopping socket server request processors (kafka.network.SocketServer)
benchi-kafka     | [2025-03-18 16:10:22,861] INFO [SocketServer listenerType=CONTROLLER, nodeId=1] Stopped socket server request processors (kafka.network.SocketServer)
benchi-kafka     | [2025-03-18 16:10:22,862] INFO [ControllerServer id=1] QuorumController#beginShutdown: shutting down event queue. (org.apache.kafka.queue.KafkaEventQueue)
benchi-kafka     | [2025-03-18 16:10:22,862] INFO [SocketServer listenerType=CONTROLLER, nodeId=1] Shutting down socket server (kafka.network.SocketServer)
benchi-kafka     | [2025-03-18 16:10:22,862] INFO [ControllerServer id=1] writeNoOpRecord: event unable to start processing because of RejectedExecutionException (treated as TimeoutException). Exception message: The event queue is shutting down (org.apache.kafka.controller.QuorumController)
benchi-kafka     | [2025-03-18 16:10:22,862] INFO [ControllerServer id=1] maybeBalancePartitionLeaders: event unable to start processing because of RejectedExecutionException (treated as TimeoutException). Exception message: The event queue is shutting down (org.apache.kafka.controller.QuorumController)
benchi-kafka     | [2025-03-18 16:10:22,864] INFO [SocketServer listenerType=CONTROLLER, nodeId=1] Shutdown completed (kafka.network.SocketServer)
benchi-kafka     | [2025-03-18 16:10:22,864] INFO [data-plane Kafka Request Handler on Broker 1], shutting down (kafka.server.KafkaRequestHandlerPool)
benchi-kafka     | [2025-03-18 16:10:22,864] INFO [data-plane Kafka Request Handler on Broker 1], shut down completely (kafka.server.KafkaRequestHandlerPool)
benchi-kafka     | [2025-03-18 16:10:22,864] INFO [ExpirationReaper-1-AlterAcls]: Shutting down (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:10:22,864] INFO [ExpirationReaper-1-AlterAcls]: Stopped (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:10:22,864] INFO [ExpirationReaper-1-AlterAcls]: Shutdown completed (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:10:22,865] INFO [controller-1-ThrottledChannelReaper-Fetch]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:10:22,865] INFO [controller-1-ThrottledChannelReaper-Fetch]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:10:22,865] INFO [controller-1-ThrottledChannelReaper-Fetch]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:10:22,865] INFO [controller-1-ThrottledChannelReaper-Produce]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:10:22,865] INFO [controller-1-ThrottledChannelReaper-Produce]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:10:22,865] INFO [controller-1-ThrottledChannelReaper-Produce]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:10:22,865] INFO [controller-1-ThrottledChannelReaper-Request]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:10:22,865] INFO [controller-1-ThrottledChannelReaper-Request]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:10:22,865] INFO [controller-1-ThrottledChannelReaper-Request]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:10:22,865] INFO [controller-1-ThrottledChannelReaper-ClusterLinkRequest]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:10:22,865] INFO [controller-1-ThrottledChannelReaper-ClusterLinkRequest]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:10:22,865] INFO [controller-1-ThrottledChannelReaper-ClusterLinkRequest]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:10:22,865] INFO [controller-1-ThrottledChannelReaper-ControllerMutation]: Shutting down (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:10:22,865] INFO [controller-1-ThrottledChannelReaper-ControllerMutation]: Stopped (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:10:22,865] INFO [controller-1-ThrottledChannelReaper-ControllerMutation]: Shutdown completed (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:10:22,865] INFO [ControllerServer id=1] closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
benchi-kafka     | [2025-03-18 16:10:22,867] INFO [SharedServer id=1] Stopping SharedServer (kafka.server.SharedServer)
benchi-kafka     | [2025-03-18 16:10:22,867] INFO [MetadataLoader id=1] beginShutdown: shutting down event queue. (org.apache.kafka.queue.KafkaEventQueue)
benchi-kafka     | [2025-03-18 16:10:22,867] INFO [SnapshotGenerator id=1] beginShutdown: shutting down event queue. (org.apache.kafka.queue.KafkaEventQueue)
benchi-kafka     | [2025-03-18 16:10:22,867] INFO [SnapshotGenerator id=1] closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
benchi-kafka     | [2025-03-18 16:10:22,867] INFO [MetadataLoader id=1] closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
benchi-kafka     | [2025-03-18 16:10:22,867] INFO [SnapshotGenerator id=1] closed event queue. (org.apache.kafka.queue.KafkaEventQueue)
benchi-kafka     | [2025-03-18 16:10:22,868] INFO Stopping Confluent metrics reporter (io.confluent.metrics.reporter.ConfluentMetricsReporter)
benchi-kafka     | [2025-03-18 16:10:22,868] INFO [Producer clientId=confluent-metrics-reporter] Closing the Kafka producer with timeoutMillis = 0 ms. (org.apache.kafka.clients.producer.KafkaProducer)
benchi-kafka     | [2025-03-18 16:10:22,868] INFO [Producer clientId=confluent-metrics-reporter] Proceeding to force close the producer since pending requests could not be completed within timeout 0 ms. (org.apache.kafka.clients.producer.KafkaProducer)
benchi-kafka     | [2025-03-18 16:10:22,869] INFO App info kafka.producer for confluent-metrics-reporter unregistered (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:10:22,869] INFO Stopping TelemetryReporter collectorTask (io.confluent.telemetry.reporter.TelemetryReporter)
benchi-kafka     | [2025-03-18 16:10:22,869] INFO Closing the event logger (io.confluent.telemetry.reporter.TelemetryReporter)
benchi-kafka     | [2025-03-18 16:10:22,872] INFO [Producer clientId=confluent-telemetry-reporter-local-producer] Closing the Kafka producer with timeoutMillis = 0 ms. (org.apache.kafka.clients.producer.KafkaProducer)
benchi-kafka     | [2025-03-18 16:10:22,872] INFO [Producer clientId=confluent-telemetry-reporter-local-producer] Proceeding to force close the producer since pending requests could not be completed within timeout 0 ms. (org.apache.kafka.clients.producer.KafkaProducer)
benchi-kafka     | [2025-03-18 16:10:22,873] INFO App info kafka.producer for confluent-telemetry-reporter-local-producer unregistered (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:10:22,873] INFO Stopping TelemetryReporter remoteConfigTask (io.confluent.telemetry.reporter.TelemetryReporter)
benchi-kafka     | [2025-03-18 16:10:22,876] INFO App info kafka.server for 1 unregistered (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:10:22,876] INFO App info kafka.server for 1 unregistered (org.apache.kafka.common.utils.AppInfoParser)
[Kbenchi-kafka exited with code 143

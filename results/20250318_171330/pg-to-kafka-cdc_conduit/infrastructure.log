 benchi-kafka Pulling 
 b8965cf5bd4c Pulling fs layer 
 d754a55bcb1a Pulling fs layer 
 acf463c2f208 Pulling fs layer 
 710b67c35e63 Pulling fs layer 
 e706238dead8 Pulling fs layer 
 5da4027ba785 Pulling fs layer 
 068e70c969c9 Pulling fs layer 
 838d653125b3 Pulling fs layer 
 43c4264eed91 Pulling fs layer 
 21462eb7489f Pulling fs layer 
 21462eb7489f Downloading [===============================>                   ]  11.53MB/18.31MB
 5da4027ba785 Download complete 
 b8965cf5bd4c Download complete 
 21462eb7489f Download complete 
 068e70c969c9 Download complete 
 d754a55bcb1a Downloading [====>                                              ]  5.243MB/53.04MB
 710b67c35e63 Download complete 
 838d653125b3 Downloading [==================================================>]  4.212MB/4.212MB
 838d653125b3 Download complete 
 d754a55bcb1a Downloading [=========>                                         ]  10.49MB/53.04MB
 acf463c2f208 Downloading [>                                                  ]  2.097MB/125.5MB
 e706238dead8 Downloading [========>                                          ]  2.097MB/12.99MB
 43c4264eed91 Downloading [============================>                      ]  2.097MB/3.624MB
 43c4264eed91 Download complete 
 d754a55bcb1a Downloading [==============>                                    ]  15.73MB/53.04MB
 acf463c2f208 Downloading [==>                                                ]  5.243MB/125.5MB
 e706238dead8 Downloading [================>                                  ]  4.194MB/12.99MB
 d754a55bcb1a Downloading [===================>                               ]  20.97MB/53.04MB
 acf463c2f208 Downloading [===>                                               ]  9.437MB/125.5MB
 e706238dead8 Downloading [============================>                      ]   7.34MB/12.99MB
 d754a55bcb1a Downloading [=======================>                           ]  25.17MB/53.04MB
 acf463c2f208 Downloading [=====>                                             ]  13.63MB/125.5MB
 e706238dead8 Downloading [========================================>          ]  10.49MB/12.99MB
 d754a55bcb1a Downloading [===========================>                       ]  29.36MB/53.04MB
 acf463c2f208 Downloading [=======>                                           ]  18.87MB/125.5MB
 e706238dead8 Download complete 
 d754a55bcb1a Downloading [================================>                  ]   34.6MB/53.04MB
 acf463c2f208 Downloading [==========>                                        ]  25.17MB/125.5MB
 d754a55bcb1a Downloading [=====================================>             ]  39.85MB/53.04MB
 acf463c2f208 Downloading [============>                                      ]  31.46MB/125.5MB
 d754a55bcb1a Downloading [==========================================>        ]  45.09MB/53.04MB
 acf463c2f208 Downloading [===============>                                   ]   38.8MB/125.5MB
 acf463c2f208 Downloading [==================>                                ]  46.14MB/125.5MB
 d754a55bcb1a Downloading [==============================================>    ]  49.28MB/53.04MB
 d754a55bcb1a Download complete 
 acf463c2f208 Downloading [=====================>                             ]  54.53MB/125.5MB
 acf463c2f208 Downloading [=========================>                         ]  62.91MB/125.5MB
 acf463c2f208 Downloading [=============================>                     ]   73.4MB/125.5MB
 acf463c2f208 Downloading [================================>                  ]  80.74MB/125.5MB
 acf463c2f208 Downloading [===================================>               ]  89.13MB/125.5MB
 acf463c2f208 Downloading [========================================>          ]  100.7MB/125.5MB
 acf463c2f208 Downloading [===========================================>       ]  110.1MB/125.5MB
 acf463c2f208 Downloading [===========================================>       ]  110.1MB/125.5MB
 acf463c2f208 Downloading [===========================================>       ]  110.1MB/125.5MB
 acf463c2f208 Downloading [==============================================>    ]  117.4MB/125.5MB
 acf463c2f208 Download complete 
 benchi-kafka Pulled 
 Network infra_default  Creating
 Network infra_default  Created
 Container benchi-kafka  Creating
 Container benchi-postgres  Creating
 Container benchi-kafka  Created
 Container benchi-postgres  Created
Attaching to benchi-kafka, benchi-postgres
benchi-postgres  | The files belonging to this database system will be owned by user "postgres".
benchi-postgres  | This user must also own the server process.
benchi-postgres  | 
benchi-postgres  | The database cluster will be initialized with locale "en_US.utf8".
benchi-postgres  | The default database encoding has accordingly been set to "UTF8".
benchi-postgres  | The default text search configuration will be set to "english".
benchi-postgres  | 
benchi-postgres  | Data page checksums are disabled.
benchi-postgres  | 
benchi-postgres  | fixing permissions on existing directory /var/lib/postgresql/data ... ok
benchi-postgres  | creating subdirectories ... ok
benchi-postgres  | selecting dynamic shared memory implementation ... posix
benchi-postgres  | selecting default max_connections ... 100
benchi-postgres  | selecting default shared_buffers ... 128MB
benchi-kafka     | ===> User
benchi-postgres  | selecting default time zone ... Etc/UTC
benchi-postgres  | creating configuration files ... ok
benchi-kafka     | uid=1000(appuser) gid=1000(appuser) groups=1000(appuser)
benchi-kafka     | ===> Setting default values of environment variables if not already set.
benchi-kafka     | CLUSTER_ID not set. Setting it to default value: "5L6g3nShT-eMCtK--X86sw"
benchi-kafka     | ===> Configuring ...
benchi-kafka     | Running in KRaft mode...
benchi-kafka     | KAFKA_OPTS should contain 'com.sun.management.jmxremote.rmi.port' property. It is required for accessing the JMX metrics externally.
benchi-kafka     | ===> Launching ... 
benchi-kafka     | ===> Using provided cluster id 5L6g3nShT-eMCtK--X86sw ...
benchi-postgres  | running bootstrap script ... ok
benchi-postgres  | performing post-bootstrap initialization ... ok
benchi-postgres  | syncing data to disk ... ok
benchi-postgres  | 
benchi-postgres  | 
benchi-postgres  | Success. You can now start the database server using:
benchi-postgres  | 
benchi-postgres  |     pg_ctl -D /var/lib/postgresql/data -l logfile start
benchi-postgres  | 
benchi-postgres  | initdb: warning: enabling "trust" authentication for local connections
benchi-postgres  | initdb: hint: You can change this by editing pg_hba.conf or using the option -A, or --auth-local and --auth-host, the next time you run initdb.
benchi-postgres  | waiting for server to start....2025-03-18 16:13:37.039 UTC [46] LOG:  starting PostgreSQL 16.4 (Debian 16.4-1.pgdg110+2) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 10.2.1-6) 10.2.1 20210110, 64-bit
benchi-postgres  | 2025-03-18 16:13:37.041 UTC [46] LOG:  listening on Unix socket "/var/run/postgresql/.s.PGSQL.5432"
benchi-postgres  | 2025-03-18 16:13:37.043 UTC [49] LOG:  database system was shut down at 2025-03-18 16:13:36 UTC
benchi-postgres  | 2025-03-18 16:13:37.044 UTC [46] LOG:  database system is ready to accept connections
benchi-postgres  |  done
benchi-postgres  | server started
benchi-postgres  | CREATE DATABASE
benchi-postgres  | 
benchi-postgres  | 
benchi-postgres  | /usr/local/bin/docker-entrypoint.sh: sourcing /docker-entrypoint-initdb.d/init-permissions.sh
benchi-postgres  | 
benchi-postgres  | /usr/local/bin/docker-entrypoint.sh: running /docker-entrypoint-initdb.d/init.sql
benchi-postgres  | psql:/docker-entrypoint-initdb.d/init.sql:1: NOTICE:  table "employees" does not exist, skipping
benchi-postgres  | DROP TABLE
benchi-postgres  | DROP SEQUENCE
benchi-postgres  | psql:/docker-entrypoint-initdb.d/init.sql:2: NOTICE:  sequence "employees_id_seq" does not exist, skipping
benchi-postgres  | CREATE TABLE
benchi-postgres  | CREATE SEQUENCE
benchi-postgres  | ALTER TABLE
benchi-postgres  | ALTER TABLE
benchi-postgres  | 
benchi-postgres  | 
benchi-postgres  | waiting for server to shut down...2025-03-18 16:13:37.191 UTC [46] LOG:  received fast shutdown request
benchi-postgres  | .2025-03-18 16:13:37.195 UTC [46] LOG:  aborting any active transactions
benchi-postgres  | 2025-03-18 16:13:37.196 UTC [46] LOG:  background worker "logical replication launcher" (PID 52) exited with exit code 1
benchi-postgres  | 2025-03-18 16:13:37.196 UTC [47] LOG:  shutting down
benchi-postgres  | 2025-03-18 16:13:37.196 UTC [47] LOG:  checkpoint starting: shutdown immediate
benchi-postgres  | 2025-03-18 16:13:37.223 UTC [47] LOG:  checkpoint complete: wrote 928 buffers (5.7%); 0 WAL file(s) added, 0 removed, 0 recycled; write=0.006 s, sync=0.017 s, total=0.028 s; sync files=304, longest=0.004 s, average=0.001 s; distance=4267 kB, estimate=4267 kB; lsn=0/19D91E8, redo lsn=0/19D91E8
benchi-postgres  | 2025-03-18 16:13:37.225 UTC [46] LOG:  database system is shut down
benchi-postgres  |  done
benchi-postgres  | server stopped
benchi-postgres  | 
benchi-postgres  | PostgreSQL init process complete; ready for start up.
benchi-postgres  | 
benchi-postgres  | 2025-03-18 16:13:37.301 UTC [1] LOG:  starting PostgreSQL 16.4 (Debian 16.4-1.pgdg110+2) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 10.2.1-6) 10.2.1 20210110, 64-bit
benchi-postgres  | 2025-03-18 16:13:37.301 UTC [1] LOG:  listening on IPv4 address "0.0.0.0", port 5432
benchi-postgres  | 2025-03-18 16:13:37.301 UTC [1] LOG:  listening on IPv6 address "::", port 5432
benchi-postgres  | 2025-03-18 16:13:37.303 UTC [1] LOG:  listening on Unix socket "/var/run/postgresql/.s.PGSQL.5432"
benchi-postgres  | 2025-03-18 16:13:37.308 UTC [64] LOG:  database system was shut down at 2025-03-18 16:13:37 UTC
benchi-postgres  | 2025-03-18 16:13:37.310 UTC [1] LOG:  database system is ready to accept connections
benchi-kafka     | 2025-03-18 16:13:41.300 | main | INFO | io.prometheus.jmx.JavaAgent | Starting ...
benchi-kafka     | 2025-03-18 16:13:41.767 | main | INFO | io.prometheus.jmx.JavaAgent | HTTP enabled [true]
benchi-kafka     | 2025-03-18 16:13:41.768 | main | INFO | io.prometheus.jmx.JavaAgent | HTTP host:port [0.0.0.0:7071]
benchi-kafka     | 2025-03-18 16:13:41.782 | main | INFO | io.prometheus.jmx.JavaAgent | OpenTelemetry enabled [false]
benchi-kafka     | 2025-03-18 16:13:41.859 | main | INFO | io.prometheus.jmx.JavaAgent | Running ...
benchi-kafka     | [2025-03-18 16:13:41,982] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)
benchi-kafka     | [2025-03-18 16:13:42,039] INFO Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation (org.apache.zookeeper.common.X509Util)
benchi-kafka     | [2025-03-18 16:13:42,163] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)
benchi-kafka     | [2025-03-18 16:13:42,166] INFO [ControllerServer id=1] Starting controller (kafka.server.ControllerServer)
benchi-kafka     | [2025-03-18 16:13:42,367] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
benchi-kafka     | [2025-03-18 16:13:42,390] INFO [SocketServer listenerType=CONTROLLER, nodeId=1] Created data-plane acceptor and processors for endpoint : ListenerName(CONTROLLER) (kafka.network.SocketServer)
benchi-kafka     | [2025-03-18 16:13:42,396] INFO authorizerStart completed for endpoint CONTROLLER. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
benchi-kafka     | [2025-03-18 16:13:42,397] INFO [SharedServer id=1] Starting SharedServer (kafka.server.SharedServer)
benchi-kafka     | [2025-03-18 16:13:42,462] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Loading producer state till offset 0 with message format version 2 (kafka.log.UnifiedLog$)
benchi-kafka     | [2025-03-18 16:13:42,463] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (kafka.log.UnifiedLog$)
benchi-kafka     | [2025-03-18 16:13:42,463] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kafka-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (kafka.log.UnifiedLog$)
benchi-kafka     | [2025-03-18 16:13:42,477] INFO Initialized snapshots with IDs SortedSet() from /tmp/kafka-logs/__cluster_metadata-0 (kafka.raft.KafkaMetadataLog$)
benchi-kafka     | [2025-03-18 16:13:42,482] INFO [raft-expiration-reaper]: Starting (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:13:42,485] INFO [RaftManager id=1] Reading KRaft snapshot and log as part of the initialization (org.apache.kafka.raft.KafkaRaftClient)
benchi-kafka     | [2025-03-18 16:13:42,487] INFO [RaftManager id=1] Starting voters are VoterSet(voters={1=VoterNode(voterKey=ReplicaKey(id=1, directoryId=Optional.empty), listeners=Endpoints(endpoints={ListenerName(CONTROLLER)=benchi-kafka/172.19.0.2:9093}), supportedKRaftVersion=SupportedVersionRange[min_version:0, max_version:0])}) (org.apache.kafka.raft.KafkaRaftClient)
benchi-kafka     | [2025-03-18 16:13:42,487] INFO [RaftManager id=1] Starting request manager with static voters: [benchi-kafka:9093 (id: 1 rack: null)] (org.apache.kafka.raft.KafkaRaftClient)
benchi-kafka     | [2025-03-18 16:13:42,490] INFO [RaftManager id=1] Attempting durable transition to Unattached(epoch=0, votedKey=null, voters=[1], electionTimeoutMs=1887, highWatermark=Optional.empty) from null (org.apache.kafka.raft.QuorumState)
benchi-kafka     | [2025-03-18 16:13:42,509] INFO [RaftManager id=1] Completed transition to Unattached(epoch=0, votedKey=null, voters=[1], electionTimeoutMs=1887, highWatermark=Optional.empty) from null (org.apache.kafka.raft.QuorumState)
benchi-kafka     | [2025-03-18 16:13:42,511] INFO [RaftManager id=1] Attempting durable transition to CandidateState(localId=1, localDirectoryId=sl4VA4ZnVuJw1vZ2wMMsVQ,epoch=1, retries=1, voteStates={1=org.apache.kafka.raft.CandidateState$VoterState@5f0fd5a0}, highWatermark=Optional.empty, electionTimeoutMs=1777) from Unattached(epoch=0, votedKey=null, voters=[1], electionTimeoutMs=1887, highWatermark=Optional.empty) (org.apache.kafka.raft.QuorumState)
benchi-kafka     | [2025-03-18 16:13:42,514] INFO [RaftManager id=1] Completed transition to CandidateState(localId=1, localDirectoryId=sl4VA4ZnVuJw1vZ2wMMsVQ,epoch=1, retries=1, voteStates={1=org.apache.kafka.raft.CandidateState$VoterState@5f0fd5a0}, highWatermark=Optional.empty, electionTimeoutMs=1777) from Unattached(epoch=0, votedKey=null, voters=[1], electionTimeoutMs=1887, highWatermark=Optional.empty) (org.apache.kafka.raft.QuorumState)
benchi-kafka     | [2025-03-18 16:13:42,516] INFO [RaftManager id=1] Attempting durable transition to Leader(localReplicaKey=ReplicaKey(id=1, directoryId=Optional[sl4VA4ZnVuJw1vZ2wMMsVQ]), epoch=1, epochStartOffset=0, highWatermark=Optional.empty, voterStates={1=ReplicaState(replicaKey=ReplicaKey(id=1, directoryId=Optional.empty), endOffset=Optional.empty, lastFetchTimestamp=-1, lastCaughtUpTimestamp=-1, hasAcknowledgedLeader=true)}) from CandidateState(localId=1, localDirectoryId=sl4VA4ZnVuJw1vZ2wMMsVQ,epoch=1, retries=1, voteStates={1=org.apache.kafka.raft.CandidateState$VoterState@5f0fd5a0}, highWatermark=Optional.empty, electionTimeoutMs=1777) (org.apache.kafka.raft.QuorumState)
benchi-kafka     | [2025-03-18 16:13:42,517] INFO [RaftManager id=1] Completed transition to Leader(localReplicaKey=ReplicaKey(id=1, directoryId=Optional[sl4VA4ZnVuJw1vZ2wMMsVQ]), epoch=1, epochStartOffset=0, highWatermark=Optional.empty, voterStates={1=ReplicaState(replicaKey=ReplicaKey(id=1, directoryId=Optional.empty), endOffset=Optional.empty, lastFetchTimestamp=-1, lastCaughtUpTimestamp=-1, hasAcknowledgedLeader=true)}) from CandidateState(localId=1, localDirectoryId=sl4VA4ZnVuJw1vZ2wMMsVQ,epoch=1, retries=1, voteStates={1=org.apache.kafka.raft.CandidateState$VoterState@5f0fd5a0}, highWatermark=Optional.empty, electionTimeoutMs=1777) (org.apache.kafka.raft.QuorumState)
benchi-kafka     | [2025-03-18 16:13:42,529] INFO [kafka-1-raft-outbound-request-thread]: Starting (org.apache.kafka.raft.KafkaNetworkChannel$SendThread)
benchi-kafka     | [2025-03-18 16:13:42,529] INFO [kafka-1-raft-io-thread]: Starting (org.apache.kafka.raft.KafkaRaftClientDriver)
benchi-kafka     | [2025-03-18 16:13:42,534] INFO [MetadataLoader id=1] initializeNewPublishers: the loader is still catching up because we still don't know the high water mark yet. (org.apache.kafka.image.loader.MetadataLoader)
benchi-kafka     | [2025-03-18 16:13:42,535] INFO [ControllerServer id=1] Waiting for controller quorum voters future (kafka.server.ControllerServer)
benchi-kafka     | [2025-03-18 16:13:42,536] INFO [ControllerServer id=1] Finished waiting for controller quorum voters future (kafka.server.ControllerServer)
benchi-kafka     | [2025-03-18 16:13:42,550] INFO [RaftManager id=1] High watermark set to LogOffsetMetadata(offset=1, metadata=Optional[(segmentBaseOffset=0,relativePositionInSegment=91)]) for the first time for epoch 1 based on indexOfHw 0 and voters [ReplicaState(replicaKey=ReplicaKey(id=1, directoryId=Optional.empty), endOffset=Optional[LogOffsetMetadata(offset=1, metadata=Optional[(segmentBaseOffset=0,relativePositionInSegment=91)])], lastFetchTimestamp=-1, lastCaughtUpTimestamp=-1, hasAcknowledgedLeader=true)] (org.apache.kafka.raft.LeaderState)
benchi-kafka     | [2025-03-18 16:13:42,551] INFO [controller-1-ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:13:42,552] INFO [controller-1-ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:13:42,552] INFO [controller-1-ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:13:42,553] INFO [controller-1-ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:13:42,559] INFO [ExpirationReaper-1-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:13:42,562] INFO [RaftManager id=1] Registered the listener org.apache.kafka.image.loader.MetadataLoader@1457998628 (org.apache.kafka.raft.KafkaRaftClient)
benchi-kafka     | [2025-03-18 16:13:42,562] INFO [RaftManager id=1] Registered the listener org.apache.kafka.controller.QuorumController$QuorumMetaLogListener@1725830685 (org.apache.kafka.raft.KafkaRaftClient)
benchi-kafka     | [2025-03-18 16:13:42,674] INFO [MetadataLoader id=1] initializeNewPublishers: The loader is still catching up because we have loaded up to offset -1, but the high water mark is 1 (org.apache.kafka.image.loader.MetadataLoader)
benchi-kafka     | [2025-03-18 16:13:42,721] INFO [ControllerServer id=1] Waiting for the controller metadata publishers to be installed (kafka.server.ControllerServer)
benchi-kafka     | [2025-03-18 16:13:42,723] INFO [MetadataLoader id=1] initializeNewPublishers: The loader is still catching up because we have loaded up to offset -1, but the high water mark is 1 (org.apache.kafka.image.loader.MetadataLoader)
benchi-kafka     | [2025-03-18 16:13:42,721] INFO [RaftManager id=1] Setting the next offset of org.apache.kafka.image.loader.MetadataLoader@1457998628 to 0 since there are no snapshots (org.apache.kafka.raft.KafkaRaftClient)
benchi-kafka     | [2025-03-18 16:13:42,724] INFO [ControllerServer id=1] Finished waiting for the controller metadata publishers to be installed (kafka.server.ControllerServer)
benchi-kafka     | [2025-03-18 16:13:42,728] INFO [SocketServer listenerType=CONTROLLER, nodeId=1] Enabling request processing. (kafka.network.SocketServer)
benchi-kafka     | [2025-03-18 16:13:42,729] INFO [RaftManager id=1] Setting the next offset of org.apache.kafka.controller.QuorumController$QuorumMetaLogListener@1725830685 to 0 since there are no snapshots (org.apache.kafka.raft.KafkaRaftClient)
benchi-kafka     | [2025-03-18 16:13:42,735] INFO [MetadataLoader id=1] maybePublishMetadata(LOG_DELTA): The loader is still catching up because we have not loaded a controller record as of offset 0 and high water mark is 1 (org.apache.kafka.image.loader.MetadataLoader)
benchi-kafka     | [2025-03-18 16:13:42,749] INFO Awaiting socket connections on 0.0.0.0:9093. (kafka.network.DataPlaneAcceptor)
benchi-kafka     | [2025-03-18 16:13:42,780] INFO [controller-1-to-controller-registration-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
benchi-kafka     | [2025-03-18 16:13:42,781] INFO [ControllerServer id=1] Waiting for all of the authorizer futures to be completed (kafka.server.ControllerServer)
benchi-kafka     | [2025-03-18 16:13:42,783] INFO [ControllerServer id=1] Finished waiting for all of the authorizer futures to be completed (kafka.server.ControllerServer)
benchi-kafka     | [2025-03-18 16:13:42,783] INFO [ControllerServer id=1] Waiting for all of the SocketServer Acceptors to be started (kafka.server.ControllerServer)
benchi-kafka     | [2025-03-18 16:13:42,783] INFO [ControllerServer id=1] Finished waiting for all of the SocketServer Acceptors to be started (kafka.server.ControllerServer)
benchi-kafka     | [2025-03-18 16:13:42,783] INFO [controller-1-to-controller-registration-channel-manager]: Recorded new KRaft controller, from now on will use node benchi-kafka:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
benchi-kafka     | [2025-03-18 16:13:42,781] INFO [ControllerRegistrationManager id=1 incarnation=bMzGgRktQBiUlzQK8vD4Hg] initialized channel manager. (kafka.server.ControllerRegistrationManager)
benchi-kafka     | [2025-03-18 16:13:42,786] INFO [BrokerServer id=1] Transition from SHUTDOWN to STARTING (kafka.server.BrokerServer)
benchi-kafka     | [2025-03-18 16:13:42,786] INFO [ControllerRegistrationManager id=1 incarnation=bMzGgRktQBiUlzQK8vD4Hg] maybeSendControllerRegistration: cannot register yet because the metadata.version is still 3.0-IV1, which does not support KIP-919 controller registration. (kafka.server.ControllerRegistrationManager)
benchi-kafka     | [2025-03-18 16:13:42,786] INFO [BrokerServer id=1] Starting broker (kafka.server.BrokerServer)
benchi-kafka     | [2025-03-18 16:13:42,791] INFO [MetadataLoader id=1] maybePublishMetadata(LOG_DELTA): The loader finished catching up to the current high water mark of 5 (org.apache.kafka.image.loader.MetadataLoader)
benchi-kafka     | [2025-03-18 16:13:42,795] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing SnapshotGenerator with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)
benchi-kafka     | [2025-03-18 16:13:42,795] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing KRaftMetadataCachePublisher with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)
benchi-kafka     | [2025-03-18 16:13:42,797] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing FeaturesPublisher with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)
benchi-kafka     | [2025-03-18 16:13:42,797] INFO [ControllerServer id=1] Loaded new metadata Features(metadataVersion=3.9-IV0, finalizedFeatures={metadata.version=21}, finalizedFeaturesEpoch=4). (org.apache.kafka.metadata.publisher.FeaturesPublisher)
benchi-kafka     | [2025-03-18 16:13:42,797] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing ControllerRegistrationsPublisher with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)
benchi-kafka     | [2025-03-18 16:13:42,797] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing ControllerRegistrationManager with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)
benchi-kafka     | [2025-03-18 16:13:42,797] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing DynamicConfigPublisher controller id=1 with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)
benchi-kafka     | [2025-03-18 16:13:42,812] INFO [ControllerRegistrationManager id=1 incarnation=bMzGgRktQBiUlzQK8vD4Hg] sendControllerRegistration: attempting to send ControllerRegistrationRequestData(controllerId=1, incarnationId=bMzGgRktQBiUlzQK8vD4Hg, zkMigrationReady=false, listeners=[Listener(name='CONTROLLER', host='benchi-kafka', port=9093, securityProtocol=0)], features=[Feature(name='kraft.version', minSupportedVersion=0, maxSupportedVersion=1), Feature(name='metadata.version', minSupportedVersion=1, maxSupportedVersion=21)]) (kafka.server.ControllerRegistrationManager)
benchi-kafka     | [2025-03-18 16:13:42,813] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing DynamicClientQuotaPublisher controller id=1 with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)
benchi-kafka     | [2025-03-18 16:13:42,817] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing ScramPublisher controller id=1 with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)
benchi-kafka     | [2025-03-18 16:13:42,818] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing DelegationTokenPublisher controller id=1 with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)
benchi-kafka     | [2025-03-18 16:13:42,822] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing ControllerMetadataMetricsPublisher with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)
benchi-kafka     | [2025-03-18 16:13:42,822] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing AclPublisher controller id=1 with a snapshot at offset 4 (org.apache.kafka.image.loader.MetadataLoader)
benchi-kafka     | [2025-03-18 16:13:42,857] INFO [broker-1-ThrottledChannelReaper-Fetch]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:13:42,859] INFO [broker-1-ThrottledChannelReaper-Produce]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:13:42,860] INFO [broker-1-ThrottledChannelReaper-Request]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:13:42,860] INFO [broker-1-ThrottledChannelReaper-ControllerMutation]: Starting (kafka.server.ClientQuotaManager$ThrottledChannelReaper)
benchi-kafka     | [2025-03-18 16:13:42,880] INFO [BrokerServer id=1] Waiting for controller quorum voters future (kafka.server.BrokerServer)
benchi-kafka     | [2025-03-18 16:13:42,881] INFO [BrokerServer id=1] Finished waiting for controller quorum voters future (kafka.server.BrokerServer)
benchi-kafka     | [2025-03-18 16:13:42,886] INFO [broker-1-to-controller-forwarding-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
benchi-kafka     | [2025-03-18 16:13:42,887] INFO [broker-1-to-controller-forwarding-channel-manager]: Recorded new KRaft controller, from now on will use node benchi-kafka:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
benchi-kafka     | [2025-03-18 16:13:42,889] INFO [client-metrics-reaper]: Starting (org.apache.kafka.server.util.timer.SystemTimerReaper$Reaper)
benchi-kafka     | [2025-03-18 16:13:42,943] INFO [ControllerRegistrationManager id=1 incarnation=bMzGgRktQBiUlzQK8vD4Hg] Our registration has been persisted to the metadata log. (kafka.server.ControllerRegistrationManager)
benchi-kafka     | [2025-03-18 16:13:42,951] INFO [ControllerRegistrationManager id=1 incarnation=bMzGgRktQBiUlzQK8vD4Hg] RegistrationResponseHandler: controller acknowledged ControllerRegistrationRequest. (kafka.server.ControllerRegistrationManager)
benchi-kafka     | [2025-03-18 16:13:42,961] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)
benchi-kafka     | [2025-03-18 16:13:42,971] INFO [SocketServer listenerType=BROKER, nodeId=1] Created data-plane acceptor and processors for endpoint : ListenerName(PLAINTEXT) (kafka.network.SocketServer)
benchi-kafka     | [2025-03-18 16:13:42,975] INFO [broker-1-to-controller-alter-partition-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
benchi-kafka     | [2025-03-18 16:13:42,975] INFO [broker-1-to-controller-alter-partition-channel-manager]: Recorded new KRaft controller, from now on will use node benchi-kafka:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
benchi-kafka     | [2025-03-18 16:13:42,980] INFO [broker-1-to-controller-directory-assignments-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
benchi-kafka     | [2025-03-18 16:13:42,981] INFO [broker-1-to-controller-directory-assignments-channel-manager]: Recorded new KRaft controller, from now on will use node benchi-kafka:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
benchi-kafka     | [2025-03-18 16:13:42,985] INFO [ExpirationReaper-1-Produce]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:13:42,986] INFO [ExpirationReaper-1-Fetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:13:42,987] INFO [ExpirationReaper-1-DeleteRecords]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:13:42,988] INFO [ExpirationReaper-1-ElectLeader]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:13:42,988] INFO [ExpirationReaper-1-RemoteFetch]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:13:42,994] INFO [ExpirationReaper-1-Heartbeat]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:13:42,994] INFO [ExpirationReaper-1-Rebalance]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:13:43,002] INFO Unable to read the broker epoch in /tmp/kafka-logs. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:13:43,003] INFO [broker-1-to-controller-heartbeat-channel-manager]: Starting (kafka.server.NodeToControllerRequestThread)
benchi-kafka     | [2025-03-18 16:13:43,004] INFO [broker-1-to-controller-heartbeat-channel-manager]: Recorded new KRaft controller, from now on will use node benchi-kafka:9093 (id: 1 rack: null) (kafka.server.NodeToControllerRequestThread)
benchi-kafka     | [2025-03-18 16:13:43,005] INFO [BrokerLifecycleManager id=1] Incarnation u4jo5XqXQzS5tDpcTALL4A of broker 1 in cluster 5L6g3nShT-eMCtK--X86sw is now STARTING. (kafka.server.BrokerLifecycleManager)
benchi-kafka     | [2025-03-18 16:13:43,007] INFO [ExpirationReaper-1-AlterAcls]: Starting (kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper)
benchi-kafka     | [2025-03-18 16:13:43,024] INFO [BrokerServer id=1] Waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
benchi-kafka     | [2025-03-18 16:13:43,024] INFO [BrokerServer id=1] Finished waiting for the broker metadata publishers to be installed (kafka.server.BrokerServer)
benchi-kafka     | [2025-03-18 16:13:43,024] INFO [BrokerServer id=1] Waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
benchi-kafka     | [2025-03-18 16:13:43,025] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing MetadataVersionPublisher(id=1) with a snapshot at offset 5 (org.apache.kafka.image.loader.MetadataLoader)
benchi-kafka     | [2025-03-18 16:13:43,025] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing BrokerMetadataPublisher with a snapshot at offset 5 (org.apache.kafka.image.loader.MetadataLoader)
benchi-kafka     | [2025-03-18 16:13:43,026] INFO [BrokerMetadataPublisher id=1] Publishing initial metadata at offset OffsetAndEpoch(offset=5, epoch=1) with metadata.version 3.9-IV0. (kafka.server.metadata.BrokerMetadataPublisher)
benchi-kafka     | [2025-03-18 16:13:43,027] INFO Loading logs from log dirs ArrayBuffer(/tmp/kafka-logs) (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:13:43,033] INFO No logs found to be loaded in /tmp/kafka-logs (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:13:43,043] INFO Loaded 0 logs in 15ms (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:13:43,043] INFO Starting log cleanup with a period of 300000 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:13:43,044] INFO Starting log flusher with a default period of 9223372036854775807 ms. (kafka.log.LogManager)
benchi-kafka     | [2025-03-18 16:13:43,054] INFO [BrokerLifecycleManager id=1] Successfully registered broker 1 with broker epoch 6 (kafka.server.BrokerLifecycleManager)
benchi-kafka     | [2025-03-18 16:13:43,077] INFO [kafka-log-cleaner-thread-0]: Starting (kafka.log.LogCleaner$CleanerThread)
benchi-kafka     | [2025-03-18 16:13:43,082] INFO [LogDirFailureHandler]: Starting (kafka.server.ReplicaManager$LogDirFailureHandler)
benchi-kafka     | [2025-03-18 16:13:43,082] INFO [AddPartitionsToTxnSenderThread-1]: Starting (kafka.server.AddPartitionsToTxnManager)
benchi-kafka     | [2025-03-18 16:13:43,083] INFO [GroupCoordinator 1]: Starting up. (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:13:43,087] INFO [GroupCoordinator 1]: Startup complete. (kafka.coordinator.group.GroupCoordinator)
benchi-kafka     | [2025-03-18 16:13:43,087] INFO [TransactionCoordinator id=1] Starting up. (kafka.coordinator.transaction.TransactionCoordinator)
benchi-kafka     | [2025-03-18 16:13:43,088] INFO [TxnMarkerSenderThread-1]: Starting (kafka.coordinator.transaction.TransactionMarkerChannelManager)
benchi-kafka     | [2025-03-18 16:13:43,088] INFO [TransactionCoordinator id=1] Startup complete. (kafka.coordinator.transaction.TransactionCoordinator)
benchi-kafka     | [2025-03-18 16:13:43,091] INFO [MetadataLoader id=1] InitializeNewPublishers: initializing BrokerRegistrationTracker(id=1) with a snapshot at offset 5 (org.apache.kafka.image.loader.MetadataLoader)
benchi-kafka     | [2025-03-18 16:13:43,100] INFO [BrokerLifecycleManager id=1] The broker has caught up. Transitioning from STARTING to RECOVERY. (kafka.server.BrokerLifecycleManager)
benchi-kafka     | [2025-03-18 16:13:43,100] INFO [BrokerServer id=1] Finished waiting for the controller to acknowledge that we are caught up (kafka.server.BrokerServer)
benchi-kafka     | [2025-03-18 16:13:43,100] INFO [BrokerServer id=1] Waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
benchi-kafka     | [2025-03-18 16:13:43,101] INFO [BrokerServer id=1] Finished waiting for the initial broker metadata update to be published (kafka.server.BrokerServer)
benchi-kafka     | [2025-03-18 16:13:43,103] INFO KafkaConfig values: 
benchi-kafka     | 	advertised.listeners = PLAINTEXT://benchi-kafka:9092,CONTROLLER://benchi-kafka:9093
benchi-kafka     | 	alter.config.policy.class.name = null
benchi-kafka     | 	alter.log.dirs.replication.quota.window.num = 11
benchi-kafka     | 	alter.log.dirs.replication.quota.window.size.seconds = 1
benchi-kafka     | 	authorizer.class.name = 
benchi-kafka     | 	auto.create.topics.enable = true
benchi-kafka     | 	auto.include.jmx.reporter = true
benchi-kafka     | 	auto.leader.rebalance.enable = true
benchi-kafka     | 	background.threads = 10
benchi-kafka     | 	broker.heartbeat.interval.ms = 2000
benchi-kafka     | 	broker.id = 1
benchi-kafka     | 	broker.id.generation.enable = true
benchi-kafka     | 	broker.rack = null
benchi-kafka     | 	broker.session.timeout.ms = 9000
benchi-kafka     | 	client.quota.callback.class = null
benchi-kafka     | 	compression.gzip.level = -1
benchi-kafka     | 	compression.lz4.level = 9
benchi-kafka     | 	compression.type = producer
benchi-kafka     | 	compression.zstd.level = 3
benchi-kafka     | 	connection.failed.authentication.delay.ms = 100
benchi-kafka     | 	connections.max.idle.ms = 600000
benchi-kafka     | 	connections.max.reauth.ms = 0
benchi-kafka     | 	control.plane.listener.name = null
benchi-kafka     | 	controlled.shutdown.enable = true
benchi-kafka     | 	controlled.shutdown.max.retries = 3
benchi-kafka     | 	controlled.shutdown.retry.backoff.ms = 5000
benchi-kafka     | 	controller.listener.names = CONTROLLER
benchi-kafka     | 	controller.quorum.append.linger.ms = 25
benchi-kafka     | 	controller.quorum.bootstrap.servers = []
benchi-kafka     | 	controller.quorum.election.backoff.max.ms = 1000
benchi-kafka     | 	controller.quorum.election.timeout.ms = 1000
benchi-kafka     | 	controller.quorum.fetch.timeout.ms = 2000
benchi-kafka     | 	controller.quorum.request.timeout.ms = 2000
benchi-kafka     | 	controller.quorum.retry.backoff.ms = 20
benchi-kafka     | 	controller.quorum.voters = [1@benchi-kafka:9093]
benchi-kafka     | 	controller.quota.window.num = 11
benchi-kafka     | 	controller.quota.window.size.seconds = 1
benchi-kafka     | 	controller.socket.timeout.ms = 30000
benchi-kafka     | 	create.topic.policy.class.name = null
benchi-kafka     | 	default.replication.factor = 1
benchi-kafka     | 	delegation.token.expiry.check.interval.ms = 3600000
benchi-kafka     | 	delegation.token.expiry.time.ms = 86400000
benchi-kafka     | 	delegation.token.master.key = null
benchi-kafka     | 	delegation.token.max.lifetime.ms = 604800000
benchi-kafka     | 	delegation.token.secret.key = null
benchi-kafka     | 	delete.records.purgatory.purge.interval.requests = 1
benchi-kafka     | 	delete.topic.enable = true
benchi-kafka     | 	early.start.listeners = null
benchi-kafka     | 	eligible.leader.replicas.enable = false
benchi-kafka     | 	fetch.max.bytes = 57671680
benchi-kafka     | 	fetch.purgatory.purge.interval.requests = 1000
benchi-kafka     | 	group.consumer.assignors = [org.apache.kafka.coordinator.group.assignor.UniformAssignor, org.apache.kafka.coordinator.group.assignor.RangeAssignor]
benchi-kafka     | 	group.consumer.heartbeat.interval.ms = 5000
benchi-kafka     | 	group.consumer.max.heartbeat.interval.ms = 15000
benchi-kafka     | 	group.consumer.max.session.timeout.ms = 60000
benchi-kafka     | 	group.consumer.max.size = 2147483647
benchi-kafka     | 	group.consumer.migration.policy = disabled
benchi-kafka     | 	group.consumer.min.heartbeat.interval.ms = 5000
benchi-kafka     | 	group.consumer.min.session.timeout.ms = 45000
benchi-kafka     | 	group.consumer.session.timeout.ms = 45000
benchi-kafka     | 	group.coordinator.append.linger.ms = 10
benchi-kafka     | 	group.coordinator.new.enable = false
benchi-kafka     | 	group.coordinator.rebalance.protocols = [classic]
benchi-kafka     | 	group.coordinator.threads = 1
benchi-kafka     | 	group.initial.rebalance.delay.ms = 3000
benchi-kafka     | 	group.max.session.timeout.ms = 1800000
benchi-kafka     | 	group.max.size = 2147483647
benchi-kafka     | 	group.min.session.timeout.ms = 6000
benchi-kafka     | 	group.share.delivery.count.limit = 5
benchi-kafka     | 	group.share.enable = false
benchi-kafka     | 	group.share.heartbeat.interval.ms = 5000
benchi-kafka     | 	group.share.max.groups = 10
benchi-kafka     | 	group.share.max.heartbeat.interval.ms = 15000
benchi-kafka     | 	group.share.max.record.lock.duration.ms = 60000
benchi-kafka     | 	group.share.max.session.timeout.ms = 60000
benchi-kafka     | 	group.share.max.size = 200
benchi-kafka     | 	group.share.min.heartbeat.interval.ms = 5000
benchi-kafka     | 	group.share.min.record.lock.duration.ms = 15000
benchi-kafka     | 	group.share.min.session.timeout.ms = 45000
benchi-kafka     | 	group.share.partition.max.record.locks = 200
benchi-kafka     | 	group.share.record.lock.duration.ms = 30000
benchi-kafka     | 	group.share.session.timeout.ms = 45000
benchi-kafka     | 	initial.broker.registration.timeout.ms = 60000
benchi-kafka     | 	inter.broker.listener.name = PLAINTEXT
benchi-kafka     | 	inter.broker.protocol.version = 3.9-IV0
benchi-kafka     | 	kafka.metrics.polling.interval.secs = 10
benchi-kafka     | 	kafka.metrics.reporters = []
benchi-kafka     | 	leader.imbalance.check.interval.seconds = 300
benchi-kafka     | 	leader.imbalance.per.broker.percentage = 10
benchi-kafka     | 	listener.security.protocol.map = CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
benchi-kafka     | 	listeners = PLAINTEXT://:9092,CONTROLLER://:9093
benchi-kafka     | 	log.cleaner.backoff.ms = 15000
benchi-kafka     | 	log.cleaner.dedupe.buffer.size = 134217728
benchi-kafka     | 	log.cleaner.delete.retention.ms = 86400000
benchi-kafka     | 	log.cleaner.enable = true
benchi-kafka     | 	log.cleaner.io.buffer.load.factor = 0.9
benchi-kafka     | 	log.cleaner.io.buffer.size = 524288
benchi-kafka     | 	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
benchi-kafka     | 	log.cleaner.max.compaction.lag.ms = 9223372036854775807
benchi-kafka     | 	log.cleaner.min.cleanable.ratio = 0.5
benchi-kafka     | 	log.cleaner.min.compaction.lag.ms = 0
benchi-kafka     | 	log.cleaner.threads = 1
benchi-kafka     | 	log.cleanup.policy = [delete]
benchi-kafka     | 	log.dir = /tmp/kafka-logs
benchi-kafka     | 	log.dir.failure.timeout.ms = 30000
benchi-kafka     | 	log.dirs = null
benchi-kafka     | 	log.flush.interval.messages = 9223372036854775807
benchi-kafka     | 	log.flush.interval.ms = null
benchi-kafka     | 	log.flush.offset.checkpoint.interval.ms = 60000
benchi-kafka     | 	log.flush.scheduler.interval.ms = 9223372036854775807
benchi-kafka     | 	log.flush.start.offset.checkpoint.interval.ms = 60000
benchi-kafka     | 	log.index.interval.bytes = 4096
benchi-kafka     | 	log.index.size.max.bytes = 10485760
benchi-kafka     | 	log.initial.task.delay.ms = 30000
benchi-kafka     | 	log.local.retention.bytes = -2
benchi-kafka     | 	log.local.retention.ms = -2
benchi-kafka     | 	log.message.downconversion.enable = true
benchi-kafka     | 	log.message.format.version = 3.0-IV1
benchi-kafka     | 	log.message.timestamp.after.max.ms = 9223372036854775807
benchi-kafka     | 	log.message.timestamp.before.max.ms = 9223372036854775807
benchi-kafka     | 	log.message.timestamp.difference.max.ms = 9223372036854775807
benchi-kafka     | 	log.message.timestamp.type = CreateTime
benchi-kafka     | 	log.preallocate = false
benchi-kafka     | 	log.retention.bytes = -1
benchi-kafka     | 	log.retention.check.interval.ms = 300000
benchi-kafka     | 	log.retention.hours = 168
benchi-kafka     | 	log.retention.minutes = null
benchi-kafka     | 	log.retention.ms = null
benchi-kafka     | 	log.roll.hours = 168
benchi-kafka     | 	log.roll.jitter.hours = 0
benchi-kafka     | 	log.roll.jitter.ms = null
benchi-kafka     | 	log.roll.ms = null
benchi-kafka     | 	log.segment.bytes = 1073741824
benchi-kafka     | 	log.segment.delete.delay.ms = 60000
benchi-kafka     | 	max.connection.creation.rate = 2147483647
benchi-kafka     | 	max.connections = 2147483647
benchi-kafka     | 	max.connections.per.ip = 2147483647
benchi-kafka     | 	max.connections.per.ip.overrides = 
benchi-kafka     | 	max.incremental.fetch.session.cache.slots = 1000
benchi-kafka     | 	max.request.partition.size.limit = 2000
benchi-kafka     | 	message.max.bytes = 1048588
benchi-kafka     | 	metadata.log.dir = null
benchi-kafka     | 	metadata.log.max.record.bytes.between.snapshots = 20971520
benchi-kafka     | 	metadata.log.max.snapshot.interval.ms = 3600000
benchi-kafka     | 	metadata.log.segment.bytes = 1073741824
benchi-kafka     | 	metadata.log.segment.min.bytes = 8388608
benchi-kafka     | 	metadata.log.segment.ms = 604800000
benchi-kafka     | 	metadata.max.idle.interval.ms = 500
benchi-kafka     | 	metadata.max.retention.bytes = 104857600
benchi-kafka     | 	metadata.max.retention.ms = 604800000
benchi-kafka     | 	metric.reporters = []
benchi-kafka     | 	metrics.num.samples = 2
benchi-kafka     | 	metrics.recording.level = INFO
benchi-kafka     | 	metrics.sample.window.ms = 30000
benchi-kafka     | 	min.insync.replicas = 1
benchi-kafka     | 	node.id = 1
benchi-kafka     | 	num.io.threads = 8
benchi-kafka     | 	num.network.threads = 3
benchi-kafka     | 	num.partitions = 1
benchi-kafka     | 	num.recovery.threads.per.data.dir = 1
benchi-kafka     | 	num.replica.alter.log.dirs.threads = null
benchi-kafka     | 	num.replica.fetchers = 1
benchi-kafka     | 	offset.metadata.max.bytes = 4096
benchi-kafka     | 	offsets.commit.required.acks = -1
benchi-kafka     | 	offsets.commit.timeout.ms = 5000
benchi-kafka     | 	offsets.load.buffer.size = 5242880
benchi-kafka     | 	offsets.retention.check.interval.ms = 600000
benchi-kafka     | 	offsets.retention.minutes = 10080
benchi-kafka     | 	offsets.topic.compression.codec = 0
benchi-kafka     | 	offsets.topic.num.partitions = 50
benchi-kafka     | 	offsets.topic.replication.factor = 1
benchi-kafka     | 	offsets.topic.segment.bytes = 104857600
benchi-kafka     | 	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
benchi-kafka     | 	password.encoder.iterations = 4096
benchi-kafka     | 	password.encoder.key.length = 128
benchi-kafka     | 	password.encoder.keyfactory.algorithm = null
benchi-kafka     | 	password.encoder.old.secret = null
benchi-kafka     | 	password.encoder.secret = null
benchi-kafka     | 	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
benchi-kafka     | 	process.roles = [broker, controller]
benchi-kafka     | 	producer.id.expiration.check.interval.ms = 600000
benchi-kafka     | 	producer.id.expiration.ms = 86400000
benchi-kafka     | 	producer.purgatory.purge.interval.requests = 1000
benchi-kafka     | 	queued.max.request.bytes = -1
benchi-kafka     | 	queued.max.requests = 500
benchi-kafka     | 	quota.window.num = 11
benchi-kafka     | 	quota.window.size.seconds = 1
benchi-kafka     | 	remote.fetch.max.wait.ms = 500
benchi-kafka     | 	remote.log.index.file.cache.total.size.bytes = 1073741824
benchi-kafka     | 	remote.log.manager.copier.thread.pool.size = -1
benchi-kafka     | 	remote.log.manager.copy.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.copy.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.copy.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.expiration.thread.pool.size = -1
benchi-kafka     | 	remote.log.manager.fetch.max.bytes.per.second = 9223372036854775807
benchi-kafka     | 	remote.log.manager.fetch.quota.window.num = 11
benchi-kafka     | 	remote.log.manager.fetch.quota.window.size.seconds = 1
benchi-kafka     | 	remote.log.manager.task.interval.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.max.ms = 30000
benchi-kafka     | 	remote.log.manager.task.retry.backoff.ms = 500
benchi-kafka     | 	remote.log.manager.task.retry.jitter = 0.2
benchi-kafka     | 	remote.log.manager.thread.pool.size = 10
benchi-kafka     | 	remote.log.metadata.custom.metadata.max.bytes = 128
benchi-kafka     | 	remote.log.metadata.manager.class.name = org.apache.kafka.server.log.remote.metadata.storage.TopicBasedRemoteLogMetadataManager
benchi-kafka     | 	remote.log.metadata.manager.class.path = null
benchi-kafka     | 	remote.log.metadata.manager.impl.prefix = rlmm.config.
benchi-kafka     | 	remote.log.metadata.manager.listener.name = null
benchi-kafka     | 	remote.log.reader.max.pending.tasks = 100
benchi-kafka     | 	remote.log.reader.threads = 10
benchi-kafka     | 	remote.log.storage.manager.class.name = null
benchi-kafka     | 	remote.log.storage.manager.class.path = null
benchi-kafka     | 	remote.log.storage.manager.impl.prefix = rsm.config.
benchi-kafka     | 	remote.log.storage.system.enable = false
benchi-kafka     | 	replica.fetch.backoff.ms = 1000
benchi-kafka     | 	replica.fetch.max.bytes = 1048576
benchi-kafka     | 	replica.fetch.min.bytes = 1
benchi-kafka     | 	replica.fetch.response.max.bytes = 10485760
benchi-kafka     | 	replica.fetch.wait.max.ms = 500
benchi-kafka     | 	replica.high.watermark.checkpoint.interval.ms = 5000
benchi-kafka     | 	replica.lag.time.max.ms = 30000
benchi-kafka     | 	replica.selector.class = null
benchi-kafka     | 	replica.socket.receive.buffer.bytes = 65536
benchi-kafka     | 	replica.socket.timeout.ms = 30000
benchi-kafka     | 	replication.quota.window.num = 11
benchi-kafka     | 	replication.quota.window.size.seconds = 1
benchi-kafka     | 	request.timeout.ms = 30000
benchi-kafka     | 	reserved.broker.max.id = 1000
benchi-kafka     | 	sasl.client.callback.handler.class = null
benchi-kafka     | 	sasl.enabled.mechanisms = [GSSAPI]
benchi-kafka     | 	sasl.jaas.config = null
benchi-kafka     | 	sasl.kerberos.kinit.cmd = /usr/bin/kinit
benchi-kafka     | 	sasl.kerberos.min.time.before.relogin = 60000
benchi-kafka     | 	sasl.kerberos.principal.to.local.rules = [DEFAULT]
benchi-kafka     | 	sasl.kerberos.service.name = null
benchi-kafka     | 	sasl.kerberos.ticket.renew.jitter = 0.05
benchi-kafka     | 	sasl.kerberos.ticket.renew.window.factor = 0.8
benchi-kafka     | 	sasl.login.callback.handler.class = null
benchi-kafka     | 	sasl.login.class = null
benchi-kafka     | 	sasl.login.connect.timeout.ms = null
benchi-kafka     | 	sasl.login.read.timeout.ms = null
benchi-kafka     | 	sasl.login.refresh.buffer.seconds = 300
benchi-kafka     | 	sasl.login.refresh.min.period.seconds = 60
benchi-kafka     | 	sasl.login.refresh.window.factor = 0.8
benchi-kafka     | 	sasl.login.refresh.window.jitter = 0.05
benchi-kafka     | 	sasl.login.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.login.retry.backoff.ms = 100
benchi-kafka     | 	sasl.mechanism.controller.protocol = GSSAPI
benchi-kafka     | 	sasl.mechanism.inter.broker.protocol = GSSAPI
benchi-kafka     | 	sasl.oauthbearer.clock.skew.seconds = 30
benchi-kafka     | 	sasl.oauthbearer.expected.audience = null
benchi-kafka     | 	sasl.oauthbearer.expected.issuer = null
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
benchi-kafka     | 	sasl.oauthbearer.jwks.endpoint.url = null
benchi-kafka     | 	sasl.oauthbearer.scope.claim.name = scope
benchi-kafka     | 	sasl.oauthbearer.sub.claim.name = sub
benchi-kafka     | 	sasl.oauthbearer.token.endpoint.url = null
benchi-kafka     | 	sasl.server.callback.handler.class = null
benchi-kafka     | 	sasl.server.max.receive.size = 524288
benchi-kafka     | 	security.inter.broker.protocol = PLAINTEXT
benchi-kafka     | 	security.providers = null
benchi-kafka     | 	server.max.startup.time.ms = 9223372036854775807
benchi-kafka     | 	socket.connection.setup.timeout.max.ms = 30000
benchi-kafka     | 	socket.connection.setup.timeout.ms = 10000
benchi-kafka     | 	socket.listen.backlog.size = 50
benchi-kafka     | 	socket.receive.buffer.bytes = 102400
benchi-kafka     | 	socket.request.max.bytes = 104857600
benchi-kafka     | 	socket.send.buffer.bytes = 102400
benchi-kafka     | 	ssl.allow.dn.changes = false
benchi-kafka     | 	ssl.allow.san.changes = false
benchi-kafka     | 	ssl.cipher.suites = []
benchi-kafka     | 	ssl.client.auth = none
benchi-kafka     | 	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
benchi-kafka     | 	ssl.endpoint.identification.algorithm = https
benchi-kafka     | 	ssl.engine.factory.class = null
benchi-kafka     | 	ssl.key.password = null
benchi-kafka     | 	ssl.keymanager.algorithm = SunX509
benchi-kafka     | 	ssl.keystore.certificate.chain = null
benchi-kafka     | 	ssl.keystore.key = null
benchi-kafka     | 	ssl.keystore.location = null
benchi-kafka     | 	ssl.keystore.password = null
benchi-kafka     | 	ssl.keystore.type = JKS
benchi-kafka     | 	ssl.principal.mapping.rules = DEFAULT
benchi-kafka     | 	ssl.protocol = TLSv1.3
benchi-kafka     | 	ssl.provider = null
benchi-kafka     | 	ssl.secure.random.implementation = null
benchi-kafka     | 	ssl.trustmanager.algorithm = PKIX
benchi-kafka     | 	ssl.truststore.certificates = null
benchi-kafka     | 	ssl.truststore.location = null
benchi-kafka     | 	ssl.truststore.password = null
benchi-kafka     | 	ssl.truststore.type = JKS
benchi-kafka     | 	telemetry.max.bytes = 1048576
benchi-kafka     | 	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
benchi-kafka     | 	transaction.max.timeout.ms = 900000
benchi-kafka     | 	transaction.partition.verification.enable = true
benchi-kafka     | 	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
benchi-kafka     | 	transaction.state.log.load.buffer.size = 5242880
benchi-kafka     | 	transaction.state.log.min.isr = 2
benchi-kafka     | 	transaction.state.log.num.partitions = 50
benchi-kafka     | 	transaction.state.log.replication.factor = 3
benchi-kafka     | 	transaction.state.log.segment.bytes = 104857600
benchi-kafka     | 	transactional.id.expiration.ms = 604800000
benchi-kafka     | 	unclean.leader.election.enable = false
benchi-kafka     | 	unclean.leader.election.interval.ms = 300000
benchi-kafka     | 	unstable.api.versions.enable = false
benchi-kafka     | 	unstable.feature.versions.enable = false
benchi-kafka     | 	zookeeper.clientCnxnSocket = null
benchi-kafka     | 	zookeeper.connect = null
benchi-kafka     | 	zookeeper.connection.timeout.ms = null
benchi-kafka     | 	zookeeper.max.in.flight.requests = 10
benchi-kafka     | 	zookeeper.metadata.migration.enable = false
benchi-kafka     | 	zookeeper.metadata.migration.min.batch.size = 200
benchi-kafka     | 	zookeeper.session.timeout.ms = 18000
benchi-kafka     | 	zookeeper.set.acl = false
benchi-kafka     | 	zookeeper.ssl.cipher.suites = null
benchi-kafka     | 	zookeeper.ssl.client.enable = false
benchi-kafka     | 	zookeeper.ssl.crl.enable = false
benchi-kafka     | 	zookeeper.ssl.enabled.protocols = null
benchi-kafka     | 	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
benchi-kafka     | 	zookeeper.ssl.keystore.location = null
benchi-kafka     | 	zookeeper.ssl.keystore.password = null
benchi-kafka     | 	zookeeper.ssl.keystore.type = null
benchi-kafka     | 	zookeeper.ssl.ocsp.enable = false
benchi-kafka     | 	zookeeper.ssl.protocol = TLSv1.2
benchi-kafka     | 	zookeeper.ssl.truststore.location = null
benchi-kafka     | 	zookeeper.ssl.truststore.password = null
benchi-kafka     | 	zookeeper.ssl.truststore.type = null
benchi-kafka     |  (kafka.server.KafkaConfig)
benchi-kafka     | [2025-03-18 16:13:43,108] INFO [BrokerServer id=1] Waiting for the broker to be unfenced (kafka.server.BrokerServer)
benchi-kafka     | [2025-03-18 16:13:43,145] INFO [BrokerLifecycleManager id=1] The broker has been unfenced. Transitioning from RECOVERY to RUNNING. (kafka.server.BrokerLifecycleManager)
benchi-kafka     | [2025-03-18 16:13:43,145] INFO [BrokerServer id=1] Finished waiting for the broker to be unfenced (kafka.server.BrokerServer)
benchi-kafka     | [2025-03-18 16:13:43,145] INFO authorizerStart completed for endpoint PLAINTEXT. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)
benchi-kafka     | [2025-03-18 16:13:43,146] INFO [SocketServer listenerType=BROKER, nodeId=1] Enabling request processing. (kafka.network.SocketServer)
benchi-kafka     | [2025-03-18 16:13:43,146] INFO Awaiting socket connections on 0.0.0.0:9092. (kafka.network.DataPlaneAcceptor)
benchi-kafka     | [2025-03-18 16:13:43,147] INFO [BrokerServer id=1] Waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
benchi-kafka     | [2025-03-18 16:13:43,147] INFO [BrokerServer id=1] Finished waiting for all of the authorizer futures to be completed (kafka.server.BrokerServer)
benchi-kafka     | [2025-03-18 16:13:43,147] INFO [BrokerServer id=1] Waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
benchi-kafka     | [2025-03-18 16:13:43,147] INFO [BrokerServer id=1] Finished waiting for all of the SocketServer Acceptors to be started (kafka.server.BrokerServer)
benchi-kafka     | [2025-03-18 16:13:43,147] INFO [BrokerServer id=1] Transition from STARTING to STARTED (kafka.server.BrokerServer)
benchi-kafka     | [2025-03-18 16:13:43,148] INFO Kafka version: 3.9.0 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:13:43,148] INFO Kafka commitId: a60e31147e6b01ee (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:13:43,148] INFO Kafka startTimeMs: 1742314423147 (org.apache.kafka.common.utils.AppInfoParser)
benchi-kafka     | [2025-03-18 16:13:43,153] INFO [KafkaRaftServer nodeId=1] Kafka Server started (kafka.server.KafkaRaftServer)
Gracefully stopping... (press Ctrl+C again to force)
 Container benchi-kafka  Stopping
 Container benchi-postgres  Stopping
 Container benchi-postgres  Stopped
 Container benchi-kafka  Stopped
canceled
